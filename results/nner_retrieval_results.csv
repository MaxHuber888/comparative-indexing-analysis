query_time,query,answer,response_0
0.5209808999788947,What is an AI accelerator?,"An AI accelerator, also referred to as deep learning processor, or neural processing unit (NPU), is a class of specialized hardware accelerator or computer system designed to accelerate artificial intelligence and machine learning applications, including artificial neural networks and machine vision.","hyperparameter called the receptive field of the neuron. The connections are local in space (along width and height): but always extend along the entire depth of the input volume. Such an architecture ensures that the learned (British English: learnt) filters produce the strongest response to a spatially local input pattern. ==== Spatial arrangement ==== Three hyperparameters control the size of the output volume of the convolutional layer: the depth: stride: and padding size: The depth of the output volume controls the number of neurons in a layer that connect to the same region of the input volume. These neurons learn to activate for different features in the input. For example: if the first convolutional layer takes the raw image as input: then different neurons along the depth dimension may activate in the presence of various oriented edges: or blobs of color. Stride controls how depth columns around the width and height are allocated. If the stride is 1: then we move the filters one pixel at a time. This leads to heavily overlapping receptive fields between the columns: and to large output volumes. For any integer S>0:{\textstyle S>0:} a stride S means that the filter is translated S units at a time per output. In practice: S≥3{\textstyle S\geq 3} is rare. A greater stride means smaller overlap of receptive fields and smaller spatial dimensions of the output volume. Sometimes: it is convenient to pad the input with zeros (or other values: such as the average of the region) on the border of the input volume. The size of this padding is a third hyperparameter. Padding provides control of the output volume's spatial size. In particular: sometimes it is desirable to exactly preserve the spatial size of the input volume: this is commonly referred to as ""same"" padding.The spatial size of the output volume is a function of the input volume size W{\displaystyle W}: the kernel field size K{\displaystyle K} of the convolutional layer neurons: the stride S{\displaystyle S}: and the amount of zero padding P{\displaystyle P} on the border. The number of neurons that ""fit"" in a given volume is then: If this number is not an integer: then the strides are incorrect and the neurons cannot be tiled to fit across the input volume in a symmetric way. In general: setting zero padding to be P=(K−1)/2{\textstyle P=(K-1)/2} when the stride is S=1{\displaystyle S=1} ensures that the input volume and output volume will have the same size spatially. However: it is not always completely necessary to use all of the neurons of the previous layer. For example: a neural network designer may decide to use just a portion of padding. ==== Parameter sharing ==== A parameter sharing scheme is used in convolutional layers to control the number of free parameters. It relies on the assumption that if a patch feature is useful to compute at some spatial position: then it should also be useful to compute at other positions. Denoting a single 2-dimensional slice of depth as a depth slice:"
0.44509769999422133,Where are AI accelerators often used?,"AI accelerators are used in various devices, including mobile devices such as neural processing units (NPUs) in Apple iPhones or Huawei cellphones, and personal computers such as Apple silicon Macs, to cloud computing servers such as tensor processing units (TPU) in the Google Cloud Platform. ",many layers can be quickly trained on GPU by supervised learning through the old method known as backpropagation. Their network outperformed previous machine learning methods on the MNIST handwritten digits benchmark. In 2011: they extended this GPU approach to CNNs: achieving an acceleration factor of 60: with impressive results. In 2011: they used such CNNs on GPU to win an image recognition contest where they achieved superhuman performance for the first time. Between May 15: 2011: and September 30: 2012: their CNNs won no less than four image competitions. In 2012: they also significantly improved on the best performance in the literature for multiple image databases: including the MNIST database: the NORB database: the HWDB1.0 dataset (Chinese characters) and the CIFAR10 dataset (dataset of 60000 32x32 labeled RGB images).Subsequently: a similar GPU-based CNN by Alex Krizhevsky et al. won the ImageNet Large Scale Visual Recognition Challenge 2012. A very deep CNN with over 100 layers by Microsoft won the ImageNet 2015 contest. === Intel Xeon Phi implementations === Compared to the training of CNNs using GPUs: not much attention was given to the Intel Xeon Phi coprocessor. A notable development is a parallelization method for training convolutional neural networks on the Intel Xeon Phi: named Controlled Hogwild with Arbitrary Order of Synchronization (CHAOS). CHAOS exploits both the thread- and SIMD-level parallelism that is available on the Intel Xeon Phi. == Distinguishing features == In the past: traditional multilayer perceptron (MLP) models were used for image recognition. However: the full connectivity between nodes caused the curse of dimensionality: and was computationally intractable with higher-resolution images. A 1000×1000-pixel image with RGB color channels has 3 million weights per fully-connected neuron: which is too high to feasibly process efficiently at scale. For example: in CIFAR-10: images are only of size 32×32×3 (32 wide: 32 high: 3 color channels): so a single fully connected neuron in the first hidden layer of a regular neural network would have 32*32*3 = 3:072 weights. A 200×200 image: however: would lead to neurons that have 200*200*3 = 120:000 weights. Also: such network architecture does not take into account the spatial structure of data: treating input pixels which are far apart in the same way as pixels that are close together. This ignores locality of reference in data with a grid-topology (such as images): both computationally and semantically. Thus: full connectivity of neurons is wasteful for purposes such as image recognition that are dominated by spatially local input patterns. Convolutional neural networks are variants of multilayer perceptrons: designed to emulate the behavior of a visual cortex. These models mitigate the challenges posed by the MLP architecture by exploiting the strong spatially local correlation present in natural images. As opposed to MLPs: CNNs have the following distinguishing features: 3D volumes of neurons. The layers of a CNN have neurons arranged in 3 dimensions: width: height and depth. Where each neuron inside a convolutional layer is connected to only a small region of the layer before it: called a receptive field.
1.1381716000032611,How have GPUs been utilized for AI acceleration?,"Graphics processing units or GPUs, that are specialized hardware for the manipulation of images and calculation of local image properties, have been increasingly used for machine learning tasks. GPU developers have also incorporated neural network-specific hardware to further accelerate these tasks.","of large language models developed by Anthropic. The first model was released in March 2023. Claude 3: released in March 2024: can also analyze images. == Training == Claude models are generative pre-trained transformers. They have been pre-trained to predict the next word in large amounts of text. Claude models have then been fine-tuned with Constitutional AI and Reinforcement Learning From Human Feedback: with the aim of making them less likely to give harmful responses: while still being helpful to the user. === Constitutional AI === Constitutional AI is an approach developed by Anthropic for training AI systems: particularly language models like Claude: to be harmless and helpful without relying on extensive human feedback. The method: detailed in the paper ""Constitutional AI: Harmlessness from AI Feedback"" involves two phases: supervised learning and reinforcement learning. In the supervised learning phase: the model generates responses to prompts: self-critiques these responses based on a set of guiding principles (a ""constitution""): and then revises the responses. The reinforcement learning phase involves training the model with AI-generated feedback: where the AI evaluates responses according to the constitutional principles. This approach enables the training of AI assistants that are both helpful and harmless: and that can explain their objections to harmful requests: enhancing transparency and reducing reliance on human supervision.The ""constitution"" for Claude included 75 points: including sections from the UN Universal Declaration of Human Rights. == Models == === Claude === Claude was the initial version of Anthropic's language model released in March 2023: Claude demonstrated proficiency in various tasks but had certain limitations in coding: math: and reasoning capabilities. Anthropic partnered with companies like Notion (productivity software) and Quora (to help develop the Poe chatbot). === Claude Instant === Claude was released as two versions: Claude and Claude Instant: with Claude Instant being a faster: less expensive and lighter version. Claude Instant has a input context length of 100:000 tokens (which corresponds to around 75:000 words). === Claude 2 === Claude 2 was the next major iteration of Claude: which was released in July 11 2023 and available to the general public: whereas the Claude 1 was only available to selected users approved by Anthropic.Claude 2 expanded its context window from 9:000 tokens to 100:000 tokens. Features included ability to upload PDFs and other documents that enables Claude to read: summarise and assist with tasks. ==== Claude 2.1 ==== Claude 2.1 doubled the number of tokens that the chatbot could handle: increasing it to a window of 200:000 tokens: which equals around 500 pages of written material.Anthropic states that the new model is less likely to produce false statements compared to its predecessors. === Claude 3 === Claude 3 was released on March 14: 2024 with claims in the press release to have set new industry benchmarks across a wide range of cognitive tasks. The Claude 3 family includes three state-of-the-art models in ascending order of capability: Haiku: Sonnet: and Opus. The default version of Claude 3 Opus has a context window of 200:000"
0.6092247999913525,What has been the progress of AI accelerator technology in the 1990s?,"In the 1990s, attempts were made to create high-throughput parallel systems for workstations aimed at various applications, including neural network simulations. FPGA-based accelerators for both inference and training were first explored in the 1990s.","data. They are an efficient way of performing model averaging with neural networks. Dilution refers to thinning weights: while dropout refers to randomly ""dropping out"": or omitting: units (both hidden and visible) during the training process of a neural network. Both trigger the same type of regularization. == Types and uses == Dilution is usually split in weak dilution and strong dilution. Weak dilution describes the process in which the finite fraction of removed connections is small: and strong dilution refers to when this fraction is large. There is no clear distinction on where the limit between strong and weak dilution is: and often the distinction is dependent on the precedent of a specific use-case and has implications for how to solve for exact solutions. Sometimes dilution is used for adding damping noise to the inputs. In that case: weak dilution refers to adding a small amount of damping noise: while strong dilution refers to adding a greater amount of damping noise. Both can be rewritten as variants of weight dilution. These techniques are also sometimes referred to as random pruning of weights: but this is usually a non-recurring one-way operation. The network is pruned: and then kept if it is an improvement over the previous model. Dilution and dropout both refer to an iterative process. The pruning of weights typically does not imply that the network continues learning: while in dilution/dropout: the network continues to learn after the technique is applied. == Generalized linear network == Output from a layer of linear nodes: in an artificial neural net can be described as yi{\displaystyle y_{i}} – output from node i{\displaystyle i} wij{\displaystyle w_{ij}} – real weight before dilution: also called the Hebb connection strength xj{\displaystyle x_{j}} – input from node j{\displaystyle j}This can be written in vector notation as y{\displaystyle \mathbf {y} } – output vector W{\displaystyle \mathbf {W} } – weight matrix x{\displaystyle \mathbf {x} } – input vectorEquations (1) and (2) are used in the subsequent sections. == Weak dilution == During weak dilution: the finite fraction of removed connections (the weights) is small: giving rise to a tiny uncertainty. This edge-case can be solved exactly with mean field theory. In weak dilution the impact on the weights can be described as wij^{\displaystyle {\hat {w_{ij}}}} – diluted weight wij{\displaystyle w_{ij}} – real weight before dilution P(c){\displaystyle P(c)} – the probability of c{\displaystyle c}: the probability of keeping a weightThe interpretation of probability P(c){\displaystyle P(c)} can also be changed from keeping a weight into pruning a weight. In vector notation this can be written as where the function g⁡(⋅){\displaystyle \operatorname {g} (\cdot )} imposes the previous dilution. In weak dilution only a small and fixed fraction of the weights are diluted. When the number of terms in the sum goes to infinite (the weights for each node) it is still infinite (the fraction is fixed): thus mean field theory can be applied. In the notation from Hertz et al. this would be written as ⟨hi⟩{\displaystyle \left\langle h_{i}\right\rangle } the"
0.5869240000029095,What is the difference between CPUs and AI accelerators in terms of performing AI-related tasks?,"While CPUs are used for running AI workloads and are superior for DNNs with small or medium-scale parallelism, for sparse DNNs and in low-batch-size scenarios, specialized AI accelerators like GPUs and FPGAs perform far better due to their optimized memory use and the use of lower precision arithmetic to accelerate calculation and increase throughput of computation. In fact, ASICs, a specific design of accelerators, may provide up to 10 times the efficiency of CPUs for AI-related tasks.","BERT's state-of-the-art performance on these natural language understanding tasks are not yet well understood. Current research has focused on investigating the relationship behind BERT's output as a result of carefully chosen input sequences: analysis of internal vector representations through probing classifiers: and the relationships represented by attention weights. The high performance of the BERT model could also be attributed to the fact that it is bidirectionally trained. This means that BERT: based on the Transformer model architecture: applies its self-attention mechanism to learn information from a text from the left and right side during training: and consequently gains a deep understanding of the context. For example: the word fine can have two different meanings depending on the context (I feel fine today: She has fine blond hair). BERT considers the words surrounding the target word fine from the left and right side. However it comes at a cost: due to encoder-only architecture lacking a decoder: BERT can't be prompted and can't generate text: while bidirectional models in general do not work effectively without the right side: thus being difficult to prompt: with even short text generation requiring sophisticated computationally expensive techniques.In contrast to deep learning neural networks which require very large amounts of data: BERT has already been pre-trained which means that it has learnt the representations of the words and sentences as well as the underlying semantic relations that they are connected with. BERT can then be fine-tuned on smaller datasets for specific tasks such as sentiment classification. The pre-trained models are chosen according to the content of the given dataset one uses but also the goal of the task. For example: if the task is a sentiment classification task on financial data: a pre-trained model for the analysis of sentiment of financial text should be chosen. The weights of the original pre-trained models were released on GitHub. == History == BERT was originally published by Google researchers Jacob Devlin: Ming-Wei Chang: Kenton Lee: and Kristina Toutanova. The design has its origins from pre-training contextual representations: including semi-supervised sequence learning: generative pre-training: ELMo: and ULMFit. Unlike previous models: BERT is a deeply bidirectional: unsupervised language representation: pre-trained using only a plain text corpus. Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary: whereas BERT takes into account the context for each occurrence of a given word. For instance: whereas the vector for ""running"" will have the same word2vec vector representation for both of its occurrences in the sentences ""He is running a company"" and ""He is running a marathon"": BERT will provide a contextualized embedding that will be different according to the sentence.On October 25: 2019: Google announced that they had started applying BERT models for English language search queries within the US. On December 9: 2019: it was reported that BERT had been adopted by Google Search for over 70 languages. In October 2020: almost every single English-based query was processed by a BERT model.A later paper proposes"
0.47992230000090785,What is the BERT model?,"The Bidirectional Encoder Representations from Transformers (BERT) is a language model based on the transformer architecture, introduced in October 2018 by researchers at Google. It is known for its significant improvement over previous models. BERT was originally implemented in the English language and has become a ubiquitous baseline in Natural Language Processing (NLP) experiments.","operation: we denote the input feature maps and output feature maps of a CMP layer as F ∈ R(C×M×N) and C ∈ R(c×M×N): respectively: where C and c are the channel numbers of the input and output feature maps: M and N are the widths and the height of the feature maps: respectively. Note that the CMP operation only changes the channel number of the feature maps. The width and the height of the feature maps are not changed: which is different from the MP operation. === ReLU layer === ReLU is the abbreviation of rectified linear unit introduced by Kunihiko Fukushima in 1969. ReLU applies the non-saturating activation function f(x)=max(0:x){\textstyle f(x)=\max(0:x)}. It effectively removes negative values from an activation map by setting them to zero. It introduces nonlinearity to the decision function and in the overall network without affecting the receptive fields of the convolution layers. In 2011: Xavier Glorot: Antoine Bordes and Yoshua Bengio found that ReLU enables better training of deeper networks: compared to widely used activation functions prior to 2011. Other functions can also be used to increase nonlinearity: for example the saturating hyperbolic tangent f(x)=tanh⁡(x){\displaystyle f(x)=\tanh(x)}: f(x)=|tanh⁡(x)|{\displaystyle f(x)=|\tanh(x)|}: and the sigmoid function σ(x)=(1+e−x)−1{\textstyle \sigma (x)=(1+e^{-x})^{-1}}. ReLU is often preferred to other functions because it trains the neural network several times faster without a significant penalty to generalization accuracy. === Fully connected layer === After several convolutional and max pooling layers: the final classification is done via fully connected layers. Neurons in a fully connected layer have connections to all activations in the previous layer: as seen in regular (non-convolutional) artificial neural networks. Their activations can thus be computed as an affine transformation: with matrix multiplication followed by a bias offset (vector addition of a learned or fixed bias term). === Loss layer === The ""loss layer"": or ""loss function"": specifies how training penalizes the deviation between the predicted output of the network: and the true data labels (during supervised learning). Various loss functions can be used: depending on the specific task. The Softmax loss function is used for predicting a single class of K mutually exclusive classes. Sigmoid cross-entropy loss is used for predicting K independent probability values in [0:1]{\displaystyle [0:1]}. Euclidean loss is used for regressing to real-valued labels (−∞:∞){\displaystyle (-\infty :\infty )}. == Hyperparameters == Hyperparameters are various settings that are used to control the learning process. CNNs use more hyperparameters than a standard multilayer perceptron (MLP). === Kernel size === The kernel is the number of pixels processed together. It is typically expressed as the kernel's dimensions: e.g.: 2x2: or 3x3. === Padding === Padding is the addition of (typically) 0-valued pixels on the borders of an image. This is done so that the border pixels are not undervalued (lost) from the output because they would ordinarily participate in only a single receptive field instance. The padding applied is typically one less than the corresponding kernel dimension. For example: a convolutional layer using 3x3 kernels would receive a 2-pixel pad: that is"
0.5604498999891803,How is BERT pretrained?,"BERT was pre-trained simultaneously on two tasks: language modeling and next sentence prediction. In language modeling, a portion of the tokens in the text were selected for prediction and were replaced with a [MASK] token, a random word token, or not replaced. The next sentence prediction task involved predicting if two spans of text appeared sequentially in the training corpus, outputting either [IsNext] or [NotNext].","Google. A 2020 literature survey concluded that ""in a little over a year: BERT has become a ubiquitous baseline in Natural Language Processing (NLP) experiments counting over 150 research publications analyzing and improving the model.""BERT was originally implemented in the English language at two model sizes: (1) BERTBASE: 12 encoders with 12 bidirectional self-attention heads totaling 110 million parameters: and (2) BERTLARGE: 24 encoders with 16 bidirectional self-attention heads totaling 340 million parameters. Both models were pre-trained on the Toronto BookCorpus (800M words) and English Wikipedia (2:500M words). == Design == BERT is an ""encoder-only"" transformer architecture. On a high level: BERT consists of three modules: embedding. This module converts an array of one-hot encoded tokens into an array of vectors representing the tokens. a stack of encoders. These encoders are the Transformer encoders. They perform transformations over the array of representation vectors. un-embedding. This module converts the final representation vectors into one-hot encoded tokens again.The un-embedding module is necessary for pretraining: but it is often unnecessary for downstream tasks. Instead: one would take the representation vectors output at the end of the stack of encoders: and use those as a vector representation of the text input: and train a smaller model on top of that. BERT uses WordPiece to convert each English word into an integer code. Its vocabulary has size 30:000. Any token not appearing in its vocabulary is replaced by [UNK] for ""unknown"". === Pretraining === BERT was pre-trained simultaneously on two tasks:language modeling: 15% of tokens were selected for prediction: and the training objective was to predict the selected token given its context. The selected token is replaced with a [MASK] token with probability 80%: replaced with a random word token with probability 10%: not replaced with probability 10%.For example: the sentence ""my dog is cute"" may have the 4-th token selected for prediction. The model would have input text ""my dog is [MASK]"" with probability 80%: ""my dog is happy"" with probability 10%: ""my dog is cute"" with probability 10%.After processing the input text: the model's 4-th output vector is passed to a separate neural network: which outputs a probability distribution over its 30:000-large vocabulary. next sentence prediction: Given two spans of text: the model predicts if these two spans appeared sequentially in the training corpus: outputting either [IsNext] or [NotNext]. The first span starts with a special token [CLS] (for ""classify""). The two spans are separated by a special token [SEP] (for ""separate""). After processing the two spans: the 1-st output vector (the vector coding for [CLS]) is passed to a separate neural network for the binary classification into [IsNext] and [NotNext]. For example: given ""[CLS] my dog is cute [SEP] he likes playing"" the model should output token [IsNext]. Given ""[CLS] my dog is cute [SEP] how do magnets work"" the model should output token [NotNext].As a result of this training process: BERT learns latent representations of words and sentences in context. After pre-training: BERT can be fine-tuned with fewer resources on smaller"
0.462963400001172,Can you describe the architecture of BERT?,"BERT is an ""encoder-only"" transformer architecture. It consists of an embedding module, a stack of encoders, and an un-embedding module. The lowest layer is the embedding layer which contains word embeddings, position embeddings, and token type embeddings. The representation vectors then move through Transformer encoders and are then un-embedded.","proponents stress the importance of ethical AI. == References == Convolutional neural network (CNN) is a regularized type of feed-forward neural network that learns feature engineering by itself via filters (or kernel) optimization. Vanishing gradients and exploding gradients: seen during backpropagation in earlier neural networks: are prevented by using regularized weights over fewer connections. For example: for each neuron in the fully-connected layer 10:000 weights would be required for processing an image sized 100 × 100 pixels. However: applying cascaded convolution (or cross-correlation) kernels: only 25 neurons are required to process 5x5-sized tiles. Higher-layer features are extracted from wider context windows: compared to lower-layer features. They have applications in: image and video recognition: recommender systems: image classification: image segmentation: medical image analysis: natural language processing: brain–computer interfaces: and financial time series.CNNs are also known as Shift Invariant or Space Invariant Artificial Neural Networks (SIANN): based on the shared-weight architecture of the convolution kernels or filters that slide along input features and provide translation-equivariant responses known as feature maps. Counter-intuitively: most convolutional neural networks are not invariant to translation: due to the downsampling operation they apply to the input.Feed-forward neural networks are usually fully connected networks: that is: each neuron in one layer is connected to all neurons in the next layer. The ""full connectivity"" of these networks make them prone to overfitting data. Typical ways of regularization: or preventing overfitting: include: penalizing parameters during training (such as weight decay) or trimming connectivity (skipped connections: dropout: etc.) Robust datasets also increases the probability that CNNs will learn the generalized principles that characterize a given dataset rather than the biases of a poorly-populated set.Convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field. CNNs use relatively little pre-processing compared to other image classification algorithms. This means that the network learns to optimize the filters (or kernels) through automated learning: whereas in traditional algorithms these filters are hand-engineered. This independence from prior knowledge and human intervention in feature extraction is a major advantage. == Architecture == A convolutional neural network consists of an input layer: hidden layers and an output layer. In a convolutional neural network: the hidden layers include one or more layers that perform convolutions. Typically this includes a layer that performs a dot product of the convolution kernel with the layer's input matrix. This product is usually the Frobenius inner product: and its activation function is commonly ReLU. As the convolution kernel slides along the input matrix for the layer: the convolution operation generates a feature map: which in turn contributes to the input of the next layer. This is followed by other layers such as pooling layers: fully connected layers: and normalization layers. Here it should be noted how close a"
0.40077239999664016,How does BERT perform in natural language understanding tasks?,"When BERT was published, it achieved state-of-the-art performance on a number of natural language understanding tasks including the GLUE (General Language Understanding Evaluation) task set, SQuAD (Stanford Question Answering Dataset), and SWAG (Situations With Adversarial Generations).",context. After pre-training: BERT can be fine-tuned with fewer resources on smaller datasets to optimize its performance on specific tasks such as NLP tasks (language inference: text classification) and sequence-to-sequence based language generation tasks (question-answering: conversational response generation). The pre-training stage is significantly more computationally expensive than fine-tuning. === Architecture details === This section describes BERTBASE. The other one: BERTLARGE: is similar: just larger. The lowest layer is the embedding layer: which contains three components: word_embeddings: position_embeddings: token_type_embeddings. word_embeddings takes in a one-hot vector of the input token. The one-hot vector input has dimension 30:000: because BERT has a vocabulary size that large. position_embeddings performs absolute position embedding. It is like word_embeddings: but on a vocabulary consisting of just the time-stamps 0 to 511: since BERT has a context window of 512. token_type_embeddings is like word_embeddings: but on a vocabulary consisting of just 0 and 1. The only type-1 tokens are those that appear after the [SEP]. All other tokens are type-0.The three outputs are added: then pushed through a LayerNorm (layer normalization): obtaining an array of representation vectors: each having 768 dimensions. After this: the representation vectors move through 12 Transformer encoders: then they are un-embedded by an affine-Add & LayerNorm-linear. == Performance == When BERT was published: it achieved state-of-the-art performance on a number of natural language understanding tasks: GLUE (General Language Understanding Evaluation) task set (consisting of 9 tasks) SQuAD (Stanford Question Answering Dataset) v1.1 and v2.0 SWAG (Situations With Adversarial Generations) == Analysis == The reasons for BERT's state-of-the-art performance on these natural language understanding tasks are not yet well understood. Current research has focused on investigating the relationship behind BERT's output as a result of carefully chosen input sequences: analysis of internal vector representations through probing classifiers: and the relationships represented by attention weights. The high performance of the BERT model could also be attributed to the fact that it is bidirectionally trained. This means that BERT: based on the Transformer model architecture: applies its self-attention mechanism to learn information from a text from the left and right side during training: and consequently gains a deep understanding of the context. For example: the word fine can have two different meanings depending on the context (I feel fine today: She has fine blond hair). BERT considers the words surrounding the target word fine from the left and right side. However it comes at a cost: due to encoder-only architecture lacking a decoder: BERT can't be prompted and can't generate text: while bidirectional models in general do not work effectively without the right side: thus being difficult to prompt: with even short text generation requiring sophisticated computationally expensive techniques.In contrast to deep learning neural networks which require very large amounts of data: BERT has already been pre-trained which means that it has learnt the representations of the words and sentences as well as the underlying semantic relations that they are connected with. BERT can then be fine-tuned on smaller datasets for specific tasks such as sentiment classification.
0.45343410002533346,Who were the original researchers behind BERT and what innovation did it provide over previous models?,"BERT was originally published by Google researchers Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Unlike previous models such as word2vec or GloVe which generated a single word embedding representation for each word in the vocabulary, BERT takes into account the context for each occurrence of a given word, providing a contextualized embedding that varies according to the sentence.",layer. Convolution reduces the number of free parameters: allowing the network to be deeper. For example: using a 5 × 5 tiling region: each with the same shared weights: requires only 25 neurons. Using regularized weights over fewer parameters avoids the vanishing gradients and exploding gradients problems seen during backpropagation in earlier neural networks.To speed processing: standard convolutional layers can be replaced by depthwise separable convolutional layers: which are based on a depthwise convolution followed by a pointwise convolution. The depthwise convolution is a spatial convolution applied independently over each channel of the input tensor: while the pointwise convolution is a standard convolution restricted to the use of 1×1{\displaystyle 1\times 1} kernels. === Pooling layers === Convolutional networks may include local and/or global pooling layers along with traditional convolutional layers. Pooling layers reduce the dimensions of data by combining the outputs of neuron clusters at one layer into a single neuron in the next layer. Local pooling combines small clusters: tiling sizes such as 2 × 2 are commonly used. Global pooling acts on all the neurons of the feature map. There are two common types of pooling in popular use: max and average. Max pooling uses the maximum value of each local cluster of neurons in the feature map: while average pooling takes the average value. === Fully connected layers === Fully connected layers connect every neuron in one layer to every neuron in another layer. It is the same as a traditional multilayer perceptron neural network (MLP). The flattened matrix goes through a fully connected layer to classify the images. === Receptive field === In neural networks: each neuron receives input from some number of locations in the previous layer. In a convolutional layer: each neuron receives input from only a restricted area of the previous layer called the neuron's receptive field. Typically the area is a square (e.g. 5 by 5 neurons). Whereas: in a fully connected layer: the receptive field is the entire previous layer. Thus: in each convolutional layer: each neuron takes input from a larger area in the input than previous layers. This is due to applying the convolution over and over: which takes the value of a pixel into account: as well as its surrounding pixels. When using dilated layers: the number of pixels in the receptive field remains constant: but the field is more sparsely populated as its dimensions grow when combining the effect of several layers. To manipulate the receptive field size as desired: there are some alternatives to the standard convolutional layer. For example: atrous or dilated convolution expands the receptive field size without increasing the number of parameters by interleaving visible and blind regions. Moreover: a single dilated convolutional layer can comprise filters with multiple dilation ratios: thus having a variable receptive field size. === Weights === Each neuron in a neural network computes an output value by applying a specific function to the input values received from the receptive field in the previous layer. The function that is
0.4330997000215575,What is the BigScience Large Open-science Open-access Multilingual Language Model (BLOOM)?,"The BigScience Large Open-science Open-access Multilingual Language Model (BLOOM) is a 176-billion-parameter transformer-based autoregressive large language model (LLM), trained on approximately 366 billion tokens from March to July 2022. ",learnable local pooling strategies that have been proposed. For each cases: the input is the initial graph is represented by a matrix X{\displaystyle \mathbf {X} } of node features: and the graph adjacency matrix A{\displaystyle \mathbf {A} }. The output is the new matrix X′{\displaystyle \mathbf {X} '}of node features: and the new graph adjacency matrix A′{\displaystyle \mathbf {A} '}. === Top-k pooling === We first set y=Xp‖p‖{\displaystyle \mathbf {y} ={\frac {\mathbf {X} \mathbf {p} }{\Vert \mathbf {p} \Vert }}} where p{\displaystyle \mathbf {p} } is a learnable projection vector. The projection vector p{\displaystyle \mathbf {p} } computes a scalar projection value for each graph node. The top-k pooling layer can then be formalised as follows: X′=(X⊙sigmoid(y))i{\displaystyle \mathbf {X} '=(\mathbf {X} \odot {\text{sigmoid}}(\mathbf {y} ))_{\mathbf {i} }}A′=Ai:i{\displaystyle \mathbf {A} '=\mathbf {A} _{\mathbf {i} :\mathbf {i} }}where i=topk(y){\displaystyle \mathbf {i} ={\text{top}}_{k}(\mathbf {y} )} is the subset of nodes with the top-k highest projection scores: ⊙{\displaystyle \odot } denotes element-wise matrix multiplication: and sigmoid(⋅){\displaystyle {\text{sigmoid}}(\cdot )} is the sigmoid function. In other words: the nodes with the top-k highest projection scores are retained in the new adjacency matrix A′{\displaystyle \mathbf {A} '}. The sigmoid(⋅){\displaystyle {\text{sigmoid}}(\cdot )} operation makes the projection vector p{\displaystyle \mathbf {p} } trainable by backpropagation: which otherwise would produce discrete outputs. === Self-attention pooling === We first set y=GNN(X:A){\displaystyle \mathbf {y} ={\text{GNN}}(\mathbf {X} :\mathbf {A} )}where GNN{\displaystyle {\text{GNN}}} is a generic permutation equivariant GNN layer (e.g.: GCN: GAT: MPNN). The Self-attention pooling layer can then be formalised as follows: X′=(X⊙y)i{\displaystyle \mathbf {X} '=(\mathbf {X} \odot \mathbf {y} )_{\mathbf {i} }}A′=Ai:i{\displaystyle \mathbf {A} '=\mathbf {A} _{\mathbf {i} :\mathbf {i} }}where i=topk(y){\displaystyle \mathbf {i} ={\text{top}}_{k}(\mathbf {y} )} is the subset of nodes with the top-k highest projection scores: ⊙{\displaystyle \odot } denotes element-wise matrix multiplication. The self-attention pooling layer can be seen as an extension of the top-k pooling layer. Differently from top-k pooling: the self-attention scores computed in self-attention pooling account both for the graph features and the graph topology. == Applications == === Protein folding === Graph neural networks are one of the main building blocks of AlphaFold: an artificial intelligence program developed by Google's DeepMind for solving the protein folding problem in biology. AlphaFold achieved first place in several CASP competitions. === Social networks === Social networks are a major application domain for GNNs due to their natural representation as social graphs. GNNs are used to develop recommender systems based on both social relations and item relations. === Combinatorial optimization === GNNs are used as fundamental building blocks for several combinatorial optimization algorithms. Examples include computing shortest paths or Eulerian circuits for a given graph: deriving chip placements superior or competitive to handcrafted human solutions: and improving expert-designed branching rules in branch and bound. === Cyber security === When viewed as a graph: a network of computers can be analyzed with GNNs for anomaly detection. Anomalies within provenance graphs often correlate to malicious activity within the network. GNNs have been used to identify these anomalies on individual nodes
0.47678980001364835,What is unique about BLOOM's licensing?,"BLOOM, its code base, and the data used to train it are all distributed under free licenses, which makes the resources open and accessible to the general public.","of LSTM RNNs. In 2015: Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM: which they made available through Google Voice Search.The impact of deep learning in industry began in the early 2000s: when CNNs already processed an estimated 10% to 20% of all the checks written in the US: according to Yann LeCun. Industrial applications of deep learning to large-scale speech recognition started around 2010. In 2006: publications by Geoff Hinton: Ruslan Salakhutdinov: Osindero and Teh showed how a many-layered feedforward neural network could be effectively pre-trained one layer at a time: treating each layer in turn as an unsupervised restricted Boltzmann machine: then fine-tuning it using supervised backpropagation. The papers referred to learning for deep belief nets. The 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech: and the possibility that given more capable hardware and large-scale data sets that deep neural nets might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets. However: it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large: context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems. The nature of the recognition errors produced by the two types of systems was characteristically different: offering technical insights into how to integrate deep learning into the existing highly efficient: run-time speech decoding system deployed by all major speech recognition systems. Analysis around 2009–2010: contrasting the GMM (and other generative speech models) vs. DNN models: stimulated early industrial investment in deep learning for speech recognition. That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models. In 2010: researchers extended deep learning from TIMIT to large vocabulary speech recognition: by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees.Deep learning is part of state-of-the-art systems in various disciplines: particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification): as well as a range of large-vocabulary speech recognition tasks have steadily improved. Convolutional neural networks were superseded for ASR by CTC for LSTM. but are more successful in computer vision. Advances in hardware have driven renewed interest in deep learning. In 2009: Nvidia was involved in what was called the ""big bang"" of deep learning: ""as deep-learning neural networks were trained with Nvidia graphics processing units (GPUs)"". That year: Andrew Ng determined that GPUs could increase the speed of deep-learning systems by about 100 times. In particular: GPUs are well-suited for the matrix/vector computations involved in machine learning. GPUs speed up training algorithms by orders of magnitude: reducing running times from weeks to days. Further: specialized hardware and algorithm optimizations can be used"
0.5277445000247099,What are the origins of BLOOM?,"BLOOM was the main result of the BigScience collaborative initiative, a one-year-long research workshop that took place between May 2021 and May 2022. It was led by HuggingFace and involved several hundreds of researchers and engineers from France and globally, including both academia and the private sector.","which delivers excellent performance on the MNIST data set. Using stochastic pooling in a multilayer model gives an exponential number of deformations since the selections in higher layers are independent of those below. ==== Artificial data ==== Because the degree of model overfitting is determined by both its power and the amount of training it receives: providing a convolutional network with more training examples can reduce overfitting. Because there is often not enough available data to train: especially considering that some part should be spared for later testing: two approaches are to either generate new data from scratch (if possible) or perturb existing data to create new ones. The latter one is used since mid-1990s. For example: input images can be cropped: rotated: or rescaled to create new examples with the same labels as the original training set. === Explicit === ==== Early stopping ==== One of the simplest methods to prevent overfitting of a network is to simply stop the training before overfitting has had a chance to occur. It comes with the disadvantage that the learning process is halted. ==== Number of parameters ==== Another simple way to prevent overfitting is to limit the number of parameters: typically by limiting the number of hidden units in each layer or limiting network depth. For convolutional networks: the filter size also affects the number of parameters. Limiting the number of parameters restricts the predictive power of the network directly: reducing the complexity of the function that it can perform on the data: and thus limits the amount of overfitting. This is equivalent to a ""zero norm"". ==== Weight decay ==== A simple form of added regularizer is weight decay: which simply adds an additional error: proportional to the sum of weights (L1 norm) or squared magnitude (L2 norm) of the weight vector: to the error at each node. The level of acceptable model complexity can be reduced by increasing the proportionality constant('alpha' hyperparameter): thus increasing the penalty for large weight vectors. L2 regularization is the most common form of regularization. It can be implemented by penalizing the squared magnitude of all parameters directly in the objective. The L2 regularization has the intuitive interpretation of heavily penalizing peaky weight vectors and preferring diffuse weight vectors. Due to multiplicative interactions between weights and inputs this has the useful property of encouraging the network to use all of its inputs a little rather than some of its inputs a lot. L1 regularization is also common. It makes the weight vectors sparse during optimization. In other words: neurons with L1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the noisy inputs. L1 with L2 regularization can be combined; this is called elastic net regularization. ==== Max norm constraints ==== Another form of regularization is to enforce an absolute upper bound on the magnitude of the weight vector for every neuron and use projected gradient descent to enforce the constraint. In practice: this corresponds to"
0.4308784000168089,On which supercomputer was BLOOM trained and who supported the training?,"BLOOM was trained on the French public supercomputer Jean Zay, managed by GENCI and IDRIS (CNRS). BigScience, the initiative behind BLOOM, was supported by a large-scale public compute grant on Jean Zay.","deep learning library for the JVM production stack running on a C++ scientific computing engine. Allows the creation of custom layers. Integrates with Hadoop and Kafka. Dlib: A toolkit for making real world machine learning and data analysis applications in C++. Microsoft Cognitive Toolkit: A deep learning toolkit written by Microsoft with several unique features enhancing scalability over multiple nodes. It supports full-fledged interfaces for training in C++ and Python and with additional support for model inference in C# and Java. TensorFlow: Apache 2.0-licensed Theano-like library with support for CPU: GPU: Google's proprietary tensor processing unit (TPU): and mobile devices. Theano: The reference deep-learning library for Python with an API largely compatible with the popular NumPy library. Allows user to write symbolic mathematical expressions: then automatically generates their derivatives: saving the user from having to code gradients or backpropagation. These symbolic expressions are automatically compiled to CUDA code for a fast: on-the-GPU implementation. Torch: A scientific computing framework with wide support for machine learning algorithms: written in C and Lua. == See also == Attention (machine learning) Convolution Deep learning Natural-language processing Neocognitron Scale-invariant feature transform Time delay neural network Vision processing unit == Notes == == References == == External links == CS231n: Convolutional Neural Networks for Visual Recognition — Andrej Karpathy's Stanford computer science course on CNNs in computer vision Deep learning is the subset of machine learning methods based on artificial neural networks (ANNs) with representation learning. The adjective ""deep"" refers to the use of multiple layers in the network. Methods used can be either supervised: semi-supervised or unsupervised.Deep-learning architectures such as deep neural networks: deep belief networks: recurrent neural networks: convolutional neural networks and transformers have been applied to fields including computer vision: speech recognition: natural language processing: machine translation: bioinformatics: drug design: medical image analysis: climate science: material inspection and board game programs: where they have produced results comparable to and in some cases surpassing human expert performance.Artificial neural networks were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains. Specifically: artificial neural networks tend to be static and symbolic: while the biological brain of most living organisms is dynamic (plastic) and analog. ANNs are generally seen as low quality models for brain function. == Definition == Deep learning is a class of machine learning algorithms that: 199–200 uses multiple layers to progressively extract higher-level features from the raw input. For example: in image processing: lower layers may identify edges: while higher layers may identify the concepts relevant to a human such as digits or letters or faces. From another angle to view deep learning: deep learning refers to ""computer-simulate"" or ""automate"" human learning processes from a source (e.g.: an image of dogs) to a learned object (dogs). Therefore: a notion coined as ""deeper"" learning or ""deepest"" learning makes sense. The deepest learning refers to the fully automatic learning from a source to a final learned object. A deeper learning thus refers to a mixed learning"
0.43858740001451224,"Can you tell me more about BLOOM's training corpus, ROOTS?","ROOTS is the training corpus for BLOOM and combines data extracted from the web-based OSCAR corpus (38% of ROOTS) and newly collected data from a manually selected list of language data sources. It includes data in 46 natural languages, with English forming 30% of the whole dataset to as little as 0.00002% for Chi Tumbuka, as well as data in 13 different programming languages.","brain wires its biological networks. Both shallow and deep learning (e.g.: recurrent nets) of ANNs for speech recognition have been explored for many years. These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively. Key difficulties have been analyzed: including gradient diminishing and weak temporal correlation structure in neural predictive models. Additional difficulties were the lack of training data and limited computing power. Most speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government's NSA and DARPA: SRI studied deep neural networks (DNNs) in speech and speaker recognition. The speaker recognition team led by Larry Heck reported significant success with deep neural networks in speech processing in the 1998 National Institute of Standards and Technology Speaker Recognition evaluation. The SRI deep neural network was then deployed in the Nuance Verifier: representing the first major industrial application of deep learning. The principle of elevating ""raw"" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the ""raw"" spectrogram or linear filter-bank features in the late 1990s: showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech: waveforms: later produced excellent larger-scale results.Speech recognition was taken over by LSTM. In 2003: LSTM started to become competitive with traditional speech recognizers on certain tasks. In 2006: Alex Graves: Santiago Fernández: Faustino Gomez: and Schmidhuber combined it with connectionist temporal classification (CTC) in stacks of LSTM RNNs. In 2015: Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM: which they made available through Google Voice Search.The impact of deep learning in industry began in the early 2000s: when CNNs already processed an estimated 10% to 20% of all the checks written in the US: according to Yann LeCun. Industrial applications of deep learning to large-scale speech recognition started around 2010. In 2006: publications by Geoff Hinton: Ruslan Salakhutdinov: Osindero and Teh showed how a many-layered feedforward neural network could be effectively pre-trained one layer at a time: treating each layer in turn as an unsupervised restricted Boltzmann machine: then fine-tuning it using supervised backpropagation. The papers referred to learning for deep belief nets. The 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech: and the possibility that given more capable hardware and large-scale data sets that deep neural nets might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets. However: it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large: context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems. The nature of the recognition errors"
0.774579300021287,What is Chinchilla and when was it presented?,"Chinchilla is a family of large language models developed by the research team at DeepMind, presented in March 2022.","The first model was released in March 2023. Claude 3: released in March 2024: can also analyze images. == Training == Claude models are generative pre-trained transformers. They have been pre-trained to predict the next word in large amounts of text. Claude models have then been fine-tuned with Constitutional AI and Reinforcement Learning From Human Feedback: with the aim of making them less likely to give harmful responses: while still being helpful to the user. === Constitutional AI === Constitutional AI is an approach developed by Anthropic for training AI systems: particularly language models like Claude: to be harmless and helpful without relying on extensive human feedback. The method: detailed in the paper ""Constitutional AI: Harmlessness from AI Feedback"" involves two phases: supervised learning and reinforcement learning. In the supervised learning phase: the model generates responses to prompts: self-critiques these responses based on a set of guiding principles (a ""constitution""): and then revises the responses. The reinforcement learning phase involves training the model with AI-generated feedback: where the AI evaluates responses according to the constitutional principles. This approach enables the training of AI assistants that are both helpful and harmless: and that can explain their objections to harmful requests: enhancing transparency and reducing reliance on human supervision.The ""constitution"" for Claude included 75 points: including sections from the UN Universal Declaration of Human Rights. == Models == === Claude === Claude was the initial version of Anthropic's language model released in March 2023: Claude demonstrated proficiency in various tasks but had certain limitations in coding: math: and reasoning capabilities. Anthropic partnered with companies like Notion (productivity software) and Quora (to help develop the Poe chatbot). === Claude Instant === Claude was released as two versions: Claude and Claude Instant: with Claude Instant being a faster: less expensive and lighter version. Claude Instant has a input context length of 100:000 tokens (which corresponds to around 75:000 words). === Claude 2 === Claude 2 was the next major iteration of Claude: which was released in July 11 2023 and available to the general public: whereas the Claude 1 was only available to selected users approved by Anthropic.Claude 2 expanded its context window from 9:000 tokens to 100:000 tokens. Features included ability to upload PDFs and other documents that enables Claude to read: summarise and assist with tasks. ==== Claude 2.1 ==== Claude 2.1 doubled the number of tokens that the chatbot could handle: increasing it to a window of 200:000 tokens: which equals around 500 pages of written material.Anthropic states that the new model is less likely to produce false statements compared to its predecessors. === Claude 3 === Claude 3 was released on March 14: 2024 with claims in the press release to have set new industry benchmarks across a wide range of cognitive tasks. The Claude 3 family includes three state-of-the-art models in ascending order of capability: Haiku: Sonnet: and Opus. The default version of Claude 3 Opus has a context window of 200:000 tokens: but this is being expanded to"
0.45317580000846647,How does Chinchilla compare to the previous model named Gopher?,"Chinchilla is a further development over Gopher. It requires less computer power for inference and fine-tuning, has an average accuracy of 67.5% on the MMLU benchmark which is 7% higher than Gopher, and has 70 billion parameters with four times as much data.","that combines a deep neural network with Q-learning: a form of reinforcement learning. Unlike earlier reinforcement learning agents: DQNs that utilize CNNs can learn directly from high-dimensional sensory inputs via reinforcement learning.Preliminary results were presented in 2014: with an accompanying paper in February 2015. The research described an application to Atari 2600 gaming. Other deep reinforcement learning models preceded it. === Deep belief networks === Convolutional deep belief networks (CDBN) have structure very similar to convolutional neural networks and are trained similarly to deep belief networks. Therefore: they exploit the 2D structure of images: like CNNs do: and make use of pre-training like deep belief networks. They provide a generic structure that can be used in many image and signal processing tasks. Benchmark results on standard image datasets like CIFAR have been obtained using CDBNs. == Notable libraries == Caffe: A library for convolutional neural networks. Created by the Berkeley Vision and Learning Center (BVLC). It supports both CPU and GPU. Developed in C++: and has Python and MATLAB wrappers. Deeplearning4j: Deep learning in Java and Scala on multi-GPU-enabled Spark. A general-purpose deep learning library for the JVM production stack running on a C++ scientific computing engine. Allows the creation of custom layers. Integrates with Hadoop and Kafka. Dlib: A toolkit for making real world machine learning and data analysis applications in C++. Microsoft Cognitive Toolkit: A deep learning toolkit written by Microsoft with several unique features enhancing scalability over multiple nodes. It supports full-fledged interfaces for training in C++ and Python and with additional support for model inference in C# and Java. TensorFlow: Apache 2.0-licensed Theano-like library with support for CPU: GPU: Google's proprietary tensor processing unit (TPU): and mobile devices. Theano: The reference deep-learning library for Python with an API largely compatible with the popular NumPy library. Allows user to write symbolic mathematical expressions: then automatically generates their derivatives: saving the user from having to code gradients or backpropagation. These symbolic expressions are automatically compiled to CUDA code for a fast: on-the-GPU implementation. Torch: A scientific computing framework with wide support for machine learning algorithms: written in C and Lua. == See also == Attention (machine learning) Convolution Deep learning Natural-language processing Neocognitron Scale-invariant feature transform Time delay neural network Vision processing unit == Notes == == References == == External links == CS231n: Convolutional Neural Networks for Visual Recognition — Andrej Karpathy's Stanford computer science course on CNNs in computer vision Deep learning is the subset of machine learning methods based on artificial neural networks (ANNs) with representation learning. The adjective ""deep"" refers to the use of multiple layers in the network. Methods used can be either supervised: semi-supervised or unsupervised.Deep-learning architectures such as deep neural networks: deep belief networks: recurrent neural networks: convolutional neural networks and transformers have been applied to fields including computer vision: speech recognition: natural language processing: machine translation: bioinformatics: drug design: medical image analysis: climate science: material inspection and board game programs: where they have produced results comparable to and in some"
0.42191090001142584,What hypothesis was used to train the Chinchilla by DeepMind?,"The hypothesis used to train Chinchilla by DeepMind is that if one doubles the model size, one must also have twice the number of training tokens.","Tumbuka) and 13 programming languages. == References == Chinchilla is a family of large language models developed by the research team at DeepMind: presented in March 2022. It is named ""chinchilla"" because it is a further development over a previous model family named Gopher. Both model families were trained in order to investigate the scaling laws of large language models.It claimed to outperform GPT-3. It considerably simplifies downstream utilization because it requires much less computer power for inference and fine-tuning. Based on the training of previously employed language models: it has been determined that if one doubles the model size: one must also have twice the number of training tokens. This hypothesis has been used to train Chinchilla by DeepMind. Similar to Gopher in terms of cost: Chinchilla has 70B parameters and four times as much data.Chinchilla has an average accuracy of 67.5% on the MMLU benchmark (Measuring Massive Multitask Language Understanding): which is 7% higher than Gopher's performance. Chinchilla was still in the testing phase as of January 12: 2023.Chinchilla contributes to developing an effective training paradigm for large autoregressive language models with limited compute resources. The Chinchilla team recommends that the number of training tokens is twice for every model size doubling: meaning that using larger: higher-quality training datasets can lead to better results on downstream tasks. == Architecture == Both the Gopher family and Chinchilla family are families of transformer models. In particular: they are essentially the same as GPT-2: with different sizes and minor modifications. Gopher family uses RMSNorm instead of LayerNorm; relative positional encoding rather than absolute positional encoding. The Chinchilla family is the same as the Gopher family: but trained with AdamW instead of Adam optimizer. The Gopher family contains six models of increasing size: from 44 million parameters to 280 billion parameters. They refer to the largest one as ""Gopher"" by default. Similar naming conventions apply for the Chinchilla family. Table 1 of shows the entire Gopher family: Table 4 of compares the 70-billion-parameter Chinchilla with Gopher 280B. == See also == LaMDA == References == Claude is a family of large language models developed by Anthropic. The first model was released in March 2023. Claude 3: released in March 2024: can also analyze images. == Training == Claude models are generative pre-trained transformers. They have been pre-trained to predict the next word in large amounts of text. Claude models have then been fine-tuned with Constitutional AI and Reinforcement Learning From Human Feedback: with the aim of making them less likely to give harmful responses: while still being helpful to the user. === Constitutional AI === Constitutional AI is an approach developed by Anthropic for training AI systems: particularly language models like Claude: to be harmless and helpful without relying on extensive human feedback. The method: detailed in the paper ""Constitutional AI: Harmlessness from AI Feedback"" involves two phases: supervised learning and reinforcement learning. In the supervised learning phase: the model generates responses to prompts: self-critiques these responses based on a set of"
0.4519027000060305,How are the Chinchilla and Gopher families similar in terms of architecture?,"Both the Gopher and Chinchilla families are transformer models. They are essentially the same as GPT-2, with different sizes and minor modifications. Gopher uses RMSNorm instead of LayerNorm, and relative positional encoding rather than absolute. Chinchilla is the same as Gopher but trained with AdamW instead of Adam optimizer.","reconstruction is the reconstruction of the underlying images from the image-related measurements. Several works showed the better and superior performance of the deep learning methods compared to analytical methods for various applications: e.g.: spectral imaging and ultrasound imaging. === Epigenetic clock === An epigenetic clock is a biochemical test that can be used to measure age. Galkin et al. used deep neural networks to train an epigenetic aging clock of unprecedented accuracy using >6:000 blood samples. The clock uses information from 1000 CpG sites and predicts people with certain conditions older than healthy controls: IBD: frontotemporal dementia: ovarian cancer: obesity. The aging clock was planned to be released for public use in 2021 by an Insilico Medicine spinoff company Deep Longevity. == Relation to human cognitive and brain development == Deep learning is closely related to a class of theories of brain development (specifically: neocortical development) proposed by cognitive neuroscientists in the early 1990s. These developmental theories were instantiated in computational models: making them predecessors of deep learning systems. These developmental models share the property that various proposed learning dynamics in the brain (e.g.: a wave of nerve growth factor) support the self-organization somewhat analogous to the neural networks utilized in deep learning models. Like the neocortex: neural networks employ a hierarchy of layered filters in which each layer considers information from a prior layer (or the operating environment): and then passes its output (and possibly the original input): to other layers. This process yields a self-organizing stack of transducers: well-tuned to their operating environment. A 1995 description stated: ""...the infant's brain seems to organize itself under the influence of waves of so-called trophic-factors ... different regions of the brain become connected sequentially: with one layer of tissue maturing before another and so on until the whole brain is mature"".A variety of approaches have been used to investigate the plausibility of deep learning models from a neurobiological perspective. On the one hand: several variants of the backpropagation algorithm have been proposed in order to increase its processing realism. Other researchers have argued that unsupervised forms of deep learning: such as those based on hierarchical generative models and deep belief networks: may be closer to biological reality. In this respect: generative neural network models have been related to neurobiological evidence about sampling-based processing in the cerebral cortex.Although a systematic comparison between the human brain organization and the neuronal encoding in deep networks has not yet been established: several analogies have been reported. For example: the computations performed by deep learning units could be similar to those of actual neurons and neural populations. Similarly: the representations developed by deep learning models are similar to those measured in the primate visual system both at the single-unit and at the population levels. == Commercial activity == Facebook's AI lab performs tasks such as automatically tagging uploaded pictures with the names of the people in them.Google's DeepMind Technologies developed a system capable of learning how to play Atari video games using only pixels as data"
0.4230428999871947,"What is the status of Chinchilla as of January 12, 2023?","As of January 12, 2023, Chinchilla was still in the testing phase.","activities within the pooling region. This approach is free of hyperparameters and can be combined with other regularization approaches: such as dropout and data augmentation. An alternate view of stochastic pooling is that it is equivalent to standard max pooling but with many copies of an input image: each having small local deformations. This is similar to explicit elastic deformations of the input images: which delivers excellent performance on the MNIST data set. Using stochastic pooling in a multilayer model gives an exponential number of deformations since the selections in higher layers are independent of those below. ==== Artificial data ==== Because the degree of model overfitting is determined by both its power and the amount of training it receives: providing a convolutional network with more training examples can reduce overfitting. Because there is often not enough available data to train: especially considering that some part should be spared for later testing: two approaches are to either generate new data from scratch (if possible) or perturb existing data to create new ones. The latter one is used since mid-1990s. For example: input images can be cropped: rotated: or rescaled to create new examples with the same labels as the original training set. === Explicit === ==== Early stopping ==== One of the simplest methods to prevent overfitting of a network is to simply stop the training before overfitting has had a chance to occur. It comes with the disadvantage that the learning process is halted. ==== Number of parameters ==== Another simple way to prevent overfitting is to limit the number of parameters: typically by limiting the number of hidden units in each layer or limiting network depth. For convolutional networks: the filter size also affects the number of parameters. Limiting the number of parameters restricts the predictive power of the network directly: reducing the complexity of the function that it can perform on the data: and thus limits the amount of overfitting. This is equivalent to a ""zero norm"". ==== Weight decay ==== A simple form of added regularizer is weight decay: which simply adds an additional error: proportional to the sum of weights (L1 norm) or squared magnitude (L2 norm) of the weight vector: to the error at each node. The level of acceptable model complexity can be reduced by increasing the proportionality constant('alpha' hyperparameter): thus increasing the penalty for large weight vectors. L2 regularization is the most common form of regularization. It can be implemented by penalizing the squared magnitude of all parameters directly in the objective. The L2 regularization has the intuitive interpretation of heavily penalizing peaky weight vectors and preferring diffuse weight vectors. Due to multiplicative interactions between weights and inputs this has the useful property of encouraging the network to use all of its inputs a little rather than some of its inputs a lot. L1 regularization is also common. It makes the weight vectors sparse during optimization. In other words: neurons with L1 regularization end up using only a sparse subset of their"
0.4293648000166286,What are dilution and dropout techniques in artificial neural networks?,"Dilution and dropout techniques in artificial neural networks are regularization techniques aimed at reducing overfitting by preventing complex co-adaptations on training data. In these processes, dilution refers to thinning weights, while dropout refers to randomly ""dropping out"", or omitting, units during the training process of a neural network.","industries have developed task-specific GPTs in their respective fields: such as Salesforce's ""EinsteinGPT"" (for CRM) and Bloomberg's ""BloombergGPT"" (for finance). == History == === Initial developments === Generative pretraining (GP) was a long-established concept in machine learning applications. It was originally used as a form of semi-supervised learning: as the model is trained first on an unlabelled dataset (pretraining step) by learning to generate datapoints in the dataset: and then it is trained to classify a labelled dataset.While the unnormalized linear transformer dates back to 1992: the modern transformer architecture was not available until 2017 when it was published by researchers at Google in a paper ""Attention Is All You Need"". That development led to the emergence of large language models such as BERT in 2018 which was a pre-trained transformer (PT) but not designed to be generative (BERT was an ""encoder-only"" model). Also around that time: in 2018: OpenAI published its article entitled ""Improving Language Understanding by Generative Pre-Training:"" in which it introduced the first generative pre-trained transformer (GPT) system (""GPT-1"").Prior to transformer-based architectures: the best-performing neural NLP (natural language processing) models commonly employed supervised learning from large amounts of manually-labeled data. The reliance on supervised learning limited their use on datasets that were not well-annotated: and also made it prohibitively expensive and time-consuming to train extremely large language models.The semi-supervised approach OpenAI employed to make a large-scale generative system—and was first to do with a transformer model—involved two stages: an unsupervised generative ""pretraining"" stage to set initial parameters using a language modeling objective: and a supervised discriminative ""fine-tuning"" stage to adapt these parameters to a target task. === Later developments === Regarding more recent GPT foundation models: OpenAI published its first versions of GPT-3 in July 2020. There were three models: with 1B: 6.7B: 175B parameters: respectively named babbage: curie: and davinci (giving initials B: C: and D).In July 2021: OpenAI published Codex: a task-specific GPT model targeted for programming applications. This was developed by fine-tuning a 12B parameter version of GPT-3 (different from previous GPT-3 models) using code from GitHub.In March 2022: OpenAI published two versions of GPT-3 that were fine-tuned for instruction-following (instruction-tuned): named davinci-instruct-beta (175B) and text-davinci-001: and then started beta testing code-davinci-002. text-davinci-002 was instruction-tuned from code-davinci-002. Both text-davinci-003 and ChatGPT were released in November 2022: with both building upon text-davinci-002 via reinforcement learning from human feedback (RLHF). text-davinci-003 is trained for following instructions (like its predecessors): whereas ChatGPT is further trained for conversational interaction with a human user.OpenAI's most recent GPT foundation model: GPT-4: was released on March 14: 2023. It can be accessed directly by users via a premium version of ChatGPT: and is available to developers for incorporation into other products and services via OpenAI's API. Other producers of GPT foundation models include EleutherAI (with a series of models starting in March 2021) and Cerebras (with seven models released in March 2023). == Foundational models == A foundational model is an AI model trained on broad data at scale such that"
0.44653650000691414,How are weak dilution and strong dilution differentiated in neural networks?,"In neural networks, weak dilution is when the finite fraction of removed connections is small, often adding a small amount of damping noise to the inputs. Conversely, strong dilution refers to when this fraction of removed connections is large, often accompanied by adding a greater amount of damping noise.","to the United States Patent and Trademark Office (USPTO) to seek domestic trademark registration for the term “GPT” in the field of AI. OpenAI sought to expedite handling of its application: but the USPTO declined that request in April 2023. In May 2023: the USPTO responded to the application with a determination that ""GPT"" was both descriptive and generic. As of November 2023: OpenAI continues to pursue its argument through the available processes. Regardless: failure to obtain a registered U.S. trademark does not preclude some level of common-law trademark rights in the U.S.: and/or trademark rights in other countries.For any given type or scope of trademark protection in the U.S.: OpenAI would need to establish that the term is actually “distinctive” to their specific offerings in addition to being a broader technical term for the kind of technology. Some media reports suggested that OpenAI may be able to obtain trademark registration based indirectly on the fame of its GPT-based chatbot product: ChatGPT: for which OpenAI has separately sought protection (and which it has sought to enforce more strongly). Other reports have indicated that registration for the bare term “GPT” seems unlikely to be granted: as it is used frequently as a common term to refer simply to AI systems that involve generative pre-trained transformers. In any event: to whatever extent exclusive rights in the term may occur the U.S.: others would need to avoid using it for similar products or services in ways likely to cause confusion. If such rights ever became broad enough to implicate other well-established uses in the field: the trademark doctrine of descriptive fair use could still preserve some room to continue non-brand-related usage. == Selected bibliography == This section lists the main official publications from OpenAI and Microsoft on their GPT models. GPT-1: report: GitHub release. GPT-2: blog announcement: report on its decision of ""staged release"": GitHub release. GPT-3: report. No GitHub or any other form of code release thenceforth. webGPT: blog announcement: report: InstructGPT: blog announcement: report. ChatGPT: blog announcement (no report). GPT-4: blog announcement: reports: model card. == See also == Cyc Gemini == References == A graph neural network (GNN) belongs to a class of artificial neural networks for processing data that can be represented as graphs. In the more general subject of ""geometric deep learning"": certain existing neural network architectures can be interpreted as GNNs operating on suitably defined graphs. A convolutional neural network layer: in the context of computer vision: can be seen as a GNN applied to graphs whose nodes are pixels and only adjacent pixels are connected by edges in the graph. A transformer layer: in natural language processing: can be seen as a GNN applied to complete graphs whose nodes are words or tokens in a passage of natural language text. The key design element of GNNs is the use of pairwise message passing: such that graph nodes iteratively update their representations by exchanging information with their neighbors. Since their inception: several different GNN architectures have been"
0.38757110000005923,How is dilution related to adding damping noise to inputs?,"Dilution can be used for adding damping noise to the inputs within a neural network. Weak dilution refers to adding a small amount of damping noise, while strong dilution refers to adding a larger amount of damping noise. These can be considered as variants of weight dilution.","types of ""machinic capture"" of human microwork to generate training data: (1) gamification (the embedding of annotation or computation tasks in the flow of a game): (2) ""trapping and tracking"" (e.g. CAPTCHAs for image recognition or click-tracking on Google search results pages): (3) exploitation of social motivations (e.g. tagging faces on Facebook to obtain labeled facial images): (4) information mining (e.g. by leveraging quantified-self devices such as activity trackers) and (5) clickwork.Mühlhoff argues that in most commercial end-user applications of Deep Learning such as Facebook's face recognition system: the need for training data does not stop once an ANN is trained. Rather: there is a continued demand for human-generated verification data to constantly calibrate and update the ANN. For this purpose: Facebook introduced the feature that once a user is automatically recognized in an image: they receive a notification. They can choose whether or not they like to be publicly labeled on the image: or tell Facebook that it is not them in the picture. This user interface is a mechanism to generate ""a constant stream of verification data"" to further train the network in real-time. As Mühlhoff argues: the involvement of human users to generate training and verification data is so typical for most commercial end-user applications of Deep Learning that such systems may be referred to as ""human-aided artificial intelligence"". == See also == Applications of artificial intelligence Comparison of deep learning software Compressed sensing Differentiable programming Echo state network List of artificial intelligence projects Liquid state machine List of datasets for machine-learning research Reservoir computing Scale space and deep learning Sparse coding Stochastic parrot == References == == Further reading == Dilution and dropout (also called DropConnect) are regularization techniques for reducing overfitting in artificial neural networks by preventing complex co-adaptations on training data. They are an efficient way of performing model averaging with neural networks. Dilution refers to thinning weights: while dropout refers to randomly ""dropping out"": or omitting: units (both hidden and visible) during the training process of a neural network. Both trigger the same type of regularization. == Types and uses == Dilution is usually split in weak dilution and strong dilution. Weak dilution describes the process in which the finite fraction of removed connections is small: and strong dilution refers to when this fraction is large. There is no clear distinction on where the limit between strong and weak dilution is: and often the distinction is dependent on the precedent of a specific use-case and has implications for how to solve for exact solutions. Sometimes dilution is used for adding damping noise to the inputs. In that case: weak dilution refers to adding a small amount of damping noise: while strong dilution refers to adding a greater amount of damping noise. Both can be rewritten as variants of weight dilution. These techniques are also sometimes referred to as random pruning of weights: but this is usually a non-recurring one-way operation. The network is pruned: and then kept if it is an improvement over"
0.4279943000001367,How does the dropout technique differ from dilution in terms of implementation?,"The dropout technique can be considered a special case of dilution where the equation is adjusted to remove a whole row in the vector matrix, rather than just random weights. This process doesn’t rely on whether the weights are set to zero, the node is removed, or any other means. The end result remains the same.","and Google's competing chatbot Bard (initially based on their LaMDA family of conversation-trained language models: with plans to switch to PaLM).Yet another kind of task that a GPT can be used for is the meta-task of generating its own instructions: like developing a series of prompts for 'itself' to be able to effectuate a more general goal given by a human user. This is known as an AI agent: and more specifically a recursive one because it uses results from its previous self-instructions to help it form its subsequent prompts; the first major example of this was Auto-GPT (which uses OpenAI's GPT models): and others have since been developed as well. === Multimodality === Generative transformer-based systems can also be targeted to tasks involving modalities beyond text. For example: Microsoft’s “Visual ChatGPT” combines ChatGPT with visual foundation models (VFMs) to enable input or output comprising images as well as text. Also: advances in text-to-speech technology offer powerful tools for audio content creation when used in conjunction with foundational GPT language models. === Domain-specificity === GPT systems can be directed toward particular fields or domains. Some reported examples of such models and apps are as follows: EinsteinGPT – for sales and marketing domains: to aid with customer relationship management (uses GPT-3.5) BloombergGPT – for the financial domain: to aid with financial news and information (uses ""freely available"" AI methods: combined with their proprietary data) Khanmigo – described as a GPT version for tutoring: in the education domain: it aids students using Khan Academy by guiding them through their studies without directly providing answers (powered by GPT-4) SlackGPT – for the Slack instant-messaging service: to aid with navigating and summarizing discussions on it (uses OpenAI's API) BioGPT – for the biomedical domain: to aid with biomedical literature text generation and mining (uses GPT-2)Sometimes domain-specificity is accomplished via software plug-ins or add-ons. For example: several different companies have developed particular plugins that interact directly with OpenAI's ChatGPT interface: and Google Workspace has available add-ons such as “GPT for Sheets and Docs”—which is reported to aid use of spreadsheet functionality in Google Sheets.In November 2023: OpenAI announced that it's enabling ChatGPT Plus subscribers to create custom versions of ChatGPT (being called GPTs). These can be tailored for specific domains via prompt engineering: curated datasets: and/or targeted interaction with external tools. Users who register as verified builders are able to publish their custom GPTs for other users: with monetization potential. (This is notably distinct from OpenAI's API service: as this is based internally within OpenAI's platform.) == Brand issues == OpenAI: which created the first generative pre-trained transformer (GPT) in 2018: has recently asserted that “GPT” should be regarded as a brand of OpenAI. In April 2023: OpenAI revised the brand guidelines in its terms of service to indicate that other businesses using its API to run their artificial intelligence (AI) services would no longer be able to include “GPT” in such names or branding. In May 2023: OpenAI engaged a brand management service to"
0.44543269998393953,Who introduced the dropout technique and who currently holds the patent?,"The dropout technique was first introduced by Geoffrey Hinton and others in 2012 for neural networks. Currently, Google holds the patent for this technique.","the most prominent are as follows: === Conditional GAN === Conditional GANs are similar to standard GANs except they allow the model to conditionally generate samples based on additional information. For example: if we want to generate a cat face given a dog picture: we could use a conditional GAN. The generator in a GAN game generates μG{\displaystyle \mu _{G}}: a probability distribution on the probability space Ω{\displaystyle \Omega }. This leads to the idea of a conditional GAN: where instead of generating one probability distribution on Ω{\displaystyle \Omega }: the generator generates a different probability distribution μG(c){\displaystyle \mu _{G}(c)} on Ω{\displaystyle \Omega }: for each given class label c{\displaystyle c}. For example: for generating images that look like ImageNet: the generator should be able to generate a picture of cat when given the class label ""cat"". In the original paper: the authors noted that GAN can be trivially extended to conditional GAN by providing the labels to both the generator and the discriminator. Concretely: the conditional GAN game is just the GAN game with class labels provided:where μC{\displaystyle \mu _{C}} is a probability distribution over classes: μref(c){\displaystyle \mu _{\text{ref}}(c)} is the probability distribution of real images of class c{\displaystyle c}: and μG(c){\displaystyle \mu _{G}(c)} the probability distribution of images generated by the generator when given class label c{\displaystyle c}. In 2017: a conditional GAN learned to generate 1000 image classes of ImageNet. === GANs with alternative architectures === The GAN game is a general framework and can be run with any reasonable parametrization of the generator G{\displaystyle G} and discriminator D{\displaystyle D}. In the original paper: the authors demonstrated it using multilayer perceptron networks and convolutional neural networks. Many alternative architectures have been tried. Deep convolutional GAN (DCGAN): For both generator and discriminator: uses only deep networks consisting entirely of convolution-deconvolution layers: that is: fully convolutional networks.Self-attention GAN (SAGAN): Starts with the DCGAN: then adds residually-connected standard self-attention modules to the generator and discriminator. Variational autoencoder GAN (VAEGAN): Uses a variational autoencoder (VAE) for the generator. Transformer GAN (TransGAN): Uses the pure transformer architecture for both the generator and discriminator: entirely devoid of convolution-deconvolution layers. Flow-GAN: Uses flow-based generative model for the generator: allowing efficient computation of the likelihood function. === GANs with alternative objectives === Many GAN variants are merely obtained by changing the loss functions for the generator and discriminator. Original GAN: We recast the original GAN objective into a form more convenient for comparison: Original GAN: non-saturating loss: This objective for generator was recommended in the original paper for faster convergence.The effect of using this objective is analyzed in Section 2.2.2 of Arjovsky et al.Original GAN: maximum likelihood: where σ{\displaystyle \sigma } is the logistic function. When the discriminator is optimal: the generator gradient is the same as in maximum likelihood estimation: even though GAN cannot perform maximum likelihood estimation itself.Hinge loss GAN:Least squares GAN:where a:b:c{\displaystyle a:b:c} are parameters to be chosen. The authors recommended a=−1:b=1:c=0{\displaystyle a=-1:b=1:c=0}. === Wasserstein GAN (WGAN) === The Wasserstein GAN modifies"
0.4297082999837585,What is a Feedforward Neural Network (FNN)?,"A feedforward neural network (FNN) is one of two broad types of artificial neural networks, characterized by the direction of the flow of information between its layers. Its flow is uni-directional, meaning that the information in the model flows in only one direction—forward—from the input nodes, through the hidden nodes (if any) and to the output nodes, without any cycles or loops. This is in contrast to recurrent neural networks, which have a bi-directional flow.","caricature groups of people: sometimes in harmful or derogatory ways.Notably: gender bias refers to the tendency of these models to produce outputs that are unfairly prejudiced towards one gender over another. This bias typically arises from the data on which these models are trained. Large language models often assign roles and characteristics based on traditional gender norms. For example: it might associate nurses or secretaries predominantly with women and engineers or CEOs with men. ==== Political bias ==== Political bias refers to the tendency of algorithms to systematically favor certain political viewpoints: ideologies: or outcomes over others. Language models may also exhibit political biases. Since the training data includes a wide range of political opinions and coverage: the models might generate responses that lean towards particular political ideologies or viewpoints: depending on the prevalence of those views in the data. == List == For the training cost column: 1 petaFLOP-day = 1 petaFLOP/sec × 1 day = 8.64E19 FLOP. == See also == Foundation models == Notes == == References == == Further reading == Jurafsky: Dan: Martin: James. H. Speech and Language Processing: An Introduction to Natural Language Processing: Computational Linguistics: and Speech Recognition: 3rd Edition draft: 2023. Phuong: Mary; Hutter: Marcus (2022). ""Formal Algorithms for Transformers"". arXiv:2207.09238 [cs.LG]. Eloundou: Tyna; Manning: Sam; Mishkin: Pamela; Rock: Daniel (2023). ""GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models"". arXiv:2303.10130 [econ.GN]. Eldan: Ronen; Li: Yuanzhi (2023). ""TinyStories: How Small Can Language Models Be and Still Speak Coherent English?"". arXiv:2305.07759 [cs.CL]. Frank: Michael C. (27 June 2023). ""Baby steps in evaluating the capacities of large language models"". Nature Reviews Psychology. 2 (8): 451–452. doi:10.1038/s44159-023-00211-x. ISSN 2731-0574. S2CID 259713140. Retrieved 2 July 2023. Zhao: Wayne Xin; et al. (2023). ""A Survey of Large Language Models"". arXiv:2303.18223 [cs.CL]. Kaddour: Jean; et al. (2023). ""Challenges and Applications of Large Language Models"". arXiv:2307.10169 [cs.CL]. Yin: Shukang; Fu: Chaoyou; Zhao: Sirui; Li: Ke; Sun: Xing; Xu: Tong; Chen: Enhong (2023-06-01). ""A Survey on Multimodal Large Language Models"". arXiv:2306.13549 [cs.CV]. Open LLMs repository on GitHub. A modeling language is any artificial language that can be used to express data: information or knowledge or systems in a structure that is defined by a consistent set of rules. The rules are used for interpretation of the meaning of components in the structure Programing language. == Overview == A modeling language can be graphical or textual. Graphical modeling languages use a diagram technique with named symbols that represent concepts and lines that connect the symbols and represent relationships and various other graphical notation to represent constraints. Textual modeling languages may use standardized keywords accompanied by parameters or natural language terms and phrases to make computer-interpretable expressions.An example of a graphical modeling language and a corresponding textual modeling language is EXPRESS. Not all modeling languages are executable: and for those that are: the use of them doesn't necessarily mean that programmers are no longer required. On the contrary: executable modeling languages are intended to amplify"
0.45539459999417886,What is the main method of training modern feedforward networks?,Modern feedforward networks are trained using the backpropagation method.,"convergence more stable by making the learning rate of the generator lower than that of the discriminator. The authors argued that the generator should move slower than the discriminator: so that it does not ""drive the discriminator steadily into new regions without capturing its gathered information"". They proved that a general class of games that included the GAN game: when trained under TTUR: ""converges under mild assumptions to a stationary local Nash equilibrium"".They also proposed using the Adam stochastic optimization to avoid mode collapse: as well as the Fréchet inception distance for evaluating GAN performances. ==== Vanishing gradient ==== Conversely: if the discriminator learns too fast compared to the generator: then the discriminator could almost perfectly distinguish μGθ:μref{\displaystyle \mu _{G_{\theta }}:\mu _{\text{ref}}}. In such case: the generator Gθ{\displaystyle G_{\theta }} could be stuck with a very high loss no matter which direction it changes its θ{\displaystyle \theta }: meaning that the gradient ∇θL(Gθ:Dζ){\displaystyle \nabla _{\theta }L(G_{\theta }:D_{\zeta })} would be close to zero. In such case: the generator cannot learn: a case of the vanishing gradient problem.Intuitively speaking: the discriminator is too good: and since the generator cannot take any small step (only small steps are considered in gradient descent) to improve its payoff: it does not even try. One important method for solving this problem is the Wasserstein GAN. === Evaluation === GANs are usually evaluated by Inception score (IS): which measures how varied the generator's outputs are (as classified by an image classifier: usually Inception-v3): or Fréchet inception distance (FID): which measures how similar the generator's outputs are to a reference set (as classified by a learned image featurizer: such as Inception-v3 without its final layer). Many papers that propose new GAN architectures for image generation report how their architectures break the state of the art on FID or IS. Another evaluation method is the Learned Perceptual Image Patch Similarity (LPIPS): which starts with a learned image featurizer fθ:Image→Rn{\displaystyle f_{\theta }:{\text{Image}}\to \mathbb {R} ^{n}}: and finetunes it by supervised learning on a set of (x:x′:PerceptualDifference(x:x′)){\displaystyle (x:x':{\text{PerceptualDifference}}(x:x'))}: where x{\displaystyle x} is an image: x′{\displaystyle x'} is a perturbed version of it: and PerceptualDifference(x:x′){\displaystyle {\text{PerceptualDifference}}(x:x')} is how much they differ: as reported by human subjects. The model is finetuned so that it can approximate ‖fθ(x)−fθ(x′)‖≈PerceptualDifference(x:x′){\displaystyle \|f_{\theta }(x)-f_{\theta }(x')\|\approx {\text{PerceptualDifference}}(x:x')}. This finetuned model is then used to define LPIPS(x:x′):=‖fθ(x)−fθ(x′)‖{\displaystyle {\text{LPIPS}}(x:x'):=\|f_{\theta }(x)-f_{\theta }(x')\|}.Other evaluation methods are reviewed in. == Variants == There is a veritable zoo of GAN variants. Some of the most prominent are as follows: === Conditional GAN === Conditional GANs are similar to standard GANs except they allow the model to conditionally generate samples based on additional information. For example: if we want to generate a cat face given a dog picture: we could use a conditional GAN. The generator in a GAN game generates μG{\displaystyle \mu _{G}}: a probability distribution on the probability space Ω{\displaystyle \Omega }. This leads to the idea of a conditional GAN: where instead of generating one probability distribution on Ω{\displaystyle \Omega }: the"
0.43813230001251213,Who first utilized a deep-learning feedforward neural network?,The first deep-learning feedforward network was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa in 1965.,such a unit is often called a filter. Units can share filters. Downsampling layers which contain units whose receptive fields cover patches of previous convolutional layers. Such a unit typically computes the average of the activations of the units in its patch. This downsampling helps to correctly classify objects in visual scenes even when the objects are shifted.In 1969: Kunihiko Fukushima also introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for CNNs and deep neural networks in general.In a variant of the neocognitron called the cresceptron: instead of using Fukushima's spatial averaging: J. Weng et al. in 1993 introduced a method called max-pooling where a downsampling unit computes the maximum of the activations of the units in its patch. Max-pooling is often used in modern CNNs.Several supervised and unsupervised learning algorithms have been proposed over the decades to train the weights of a neocognitron. Today: however: the CNN architecture is usually trained through backpropagation. The neocognitron is the first CNN which requires units located at multiple network positions to have shared weights. Convolutional neural networks were presented at the Neural Information Processing Workshop in 1987: automatically analyzing time-varying signals by replacing learned multiplication with convolution in time: and demonstrated for speech recognition. === Time delay neural networks === The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel et al. for phoneme recognition and was one of the first convolutional networks: as it achieved shift-invariance. A TDNN is a 1-D convolutional neural net where the convolution is performed along the time axis of the data. It is the first CNN utilizing weight sharing in combination with a training by gradient descent: using backpropagation. Thus: while also using a pyramidal structure as in the neocognitron: it performed a global optimization of the weights instead of a local one.. TDNNs are convolutional networks that share weights along the temporal dimension. They allow speech signals to be processed time-invariantly. In 1990 Hampshire and Waibel introduced a variant that performs a two-dimensional convolution. Since these TDNNs operated on spectrograms: the resulting phoneme recognition system was invariant to both time and frequency shifts. This inspired translation invariance in image processing with CNNs. The tiling of neuron outputs can cover timed stages.TDNNs now achieve the best performance in far-distance speech recognition. ==== Max pooling ==== In 1990 Yamaguchi et al. introduced the concept of max pooling: a fixed filtering operation that calculates and propagates the maximum value of a given region. They did so by combining TDNNs with max pooling to realize a speaker-independent isolated word recognition system. In their system they used several TDNNs per word: one for each syllable. The results of each TDNN over the input signal were combined using max pooling and the outputs of the pooling layers were then passed on to networks performing the actual word classification. === Image recognition with CNNs trained by gradient descent === Denker et al. (1989) designed a 2-D CNN system to recognize
0.37270449998322874,What are the functions of the feedforward neural network?,"In a feedforward neural network, information flows in only one direction—forward—from the input nodes, through the hidden nodes (if any) and to the output nodes, without any cycles or loops.","learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Hebbian learning is unsupervised learning. This evolved into models for long-term potentiation. Researchers started applying these ideas to computational models in 1948 with Turing's B-type machines. Farley and Clark (1954) first used computational machines: then called ""calculators"": to simulate a Hebbian network. Other neural network computational machines were created by Rochester: Holland: Habit and Duda (1956).Rosenblatt (1958) created the perceptron: an algorithm for pattern recognition. With mathematical notation: Rosenblatt described circuitry not in the basic perceptron: such as the exclusive-or circuit that could not be processed by neural networks at the time. In 1959: a biological model proposed by Nobel laureates Hubel and Wiesel was based on their discovery of two types of cells in the primary visual cortex: simple cells and complex cells.Some say that research stagnated following Minsky and Papert (1969): who discovered that basic perceptrons were incapable of processing the exclusive-or circuit and that computers lacked sufficient power to process useful neural networks. However: by the time this book came out: methods for training multilayer perceptrons (MLPs) by deep learning were already known. == First deep learning == The first deep learning MLP was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa in 1965: as the Group Method of Data Handling. This method employs incremental layer by layer training based on regression analysis: where useless units in hidden layers are pruned with the help of a validation set. The first deep learning MLP trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. In computer experiments conducted by Amari's student Saito: a five layer MLP with two modifiable layers learned useful internal representations to classify non-linearily separable pattern classes. == Backpropagation == The backpropagation algorithm is an efficient application of the Leibniz chain rule (1673) to networks of differentiable nodes. It is also known as the reverse mode of automatic differentiation or reverse accumulation: due to Seppo Linnainmaa (1970). The term ""back-propagating errors"" was introduced in 1962 by Frank Rosenblatt: but he did not have an implementation of this procedure: although Henry J. Kelley had a continuous precursor of backpropagation already in 1960 in the context of control theory. In 1982: Paul Werbos applied backpropagation to MLPs in the way that has become standard. In 1986: David E. Rumelhart et al. published an experimental analysis of the technique. == Recurrent network architectures == Wilhelm Lenz and Ernst Ising created and analyzed the Ising model (1925) which is essentially a non-learning artificial recurrent neural network (RNN) consisting of neuron-like threshold elements. In 1972: Shun'ichi Amari made this architecture adaptive. His learning RNN was popularised by John Hopfield in 1982. == Self-organizing maps == Self-organizing maps (SOMs) were described by Teuvo Kohonen in 1982. SOMs are neurophysiologically inspired artificial neural networks that learn low-dimensional representations of high-dimensional data while preserving the topological structure of the data. They are trained using competitive learning. SOMs create internal representations reminiscent of the cortical homunculus: a"
0.45678619999671355,How does learning occur in a feedforward neural network?,"Learning occurs in a feedforward neural network by changing connection weights after each piece of data is processed. This is based on the amount of error in the output compared to the expected result. This is an example of supervised learning, and is carried out through backpropagation.","cost of (pre-)training (C{\displaystyle C}): size of the artificial neural network itself: such as number of parameters N{\displaystyle N} (i.e. amount of neurons in its layers: amount of weights between them and biases): size of its (pre-)training dataset (i.e. number of tokens in corpus: D{\displaystyle D}): performance after (pre-)training.They are related by simple statistical laws: called ""scaling laws"". One particular scaling law (""Chinchilla scaling"") for LLM autoregressively trained for one epoch: with a log-log learning rate schedule: states that: where the variables are C{\displaystyle C} is the cost of training the model: in FLOPs. N{\displaystyle N} is the number of parameters in the model. D{\displaystyle D} is the number of tokens in the training set. L{\displaystyle L} is the average negative log-likelihood loss per token (nats/token): achieved by the trained LLM on the test dataset.and the statistical hyper-parameters are C0=6{\displaystyle C_{0}=6}: meaning that it costs 6 FLOPs per parameter to train on one token. Note that training cost is much higher than inference cost: where it costs 1 to 2 FLOPs per parameter to infer on one token. α=0.34:β=0.28:A=406.4:B=410.7:L0=1.69{\displaystyle \alpha =0.34:\beta =0.28:A=406.4:B=410.7:L_{0}=1.69} === Emergent abilities === When one subtracts out from the y-axis the best performance that can be achieved even with infinite scaling of the x-axis quantity: large models' performance: measured on various tasks: seems to be a linear extrapolation of other (smaller-sized and medium-sized) models' performance on a log-log plot. However: sometimes the line's slope transitions from one slope to another at point(s) referred to as break(s) in downstream scaling laws: appearing as a series of linear segments connected by arcs; it seems that larger models acquire ""emergent abilities"" at this point(s). These abilities are discovered rather than programmed-in or designed: in some cases only after the LLM has been publicly deployed.The most intriguing among emergent abilities is in-context learning from example demonstrations. In-context learning is involved in tasks: such as: reported arithmetics: decoding the International Phonetic Alphabet: unscrambling a word's letters: disambiguate word in context: converting spatial words: cardinal directions (for example: replying ""northeast"" upon [0: 0: 1; 0: 0: 0; 0: 0: 0]): color terms represented in text. chain-of-thought prompting: Model outputs are improved by chain-of-thought prompting only when model size exceeds 62B. Smaller models perform better when prompted to answer immediately: without chain of thought. identifying offensive content in paragraphs of Hinglish (a combination of Hindi and English): and generating a similar English equivalent of Kiswahili proverbs.Schaeffer et. al. argue that the emergent abilities are not unpredictably acquired: but predictably acquired according to a smooth scaling law. The authors considered a toy statistical model of an LLM solving multiple-choice questions: and showed that this statistical model: modified to account for other types of tasks: applies to these tasks as well.Let x{\displaystyle x} be the number of parameter count: and y{\displaystyle y} be the performance of the model. == Interpretation == Large language models by themselves are ""black boxes"": and it is not clear how they can perform linguistic tasks. There are several methods for understanding"
0.3935798999737017,What is the Gemini family and who developed it?,"The Gemini family comprises of multimodal large language models developed by Google DeepMind. It serves as the successor to LaMDA and PaLM 2 and includes Gemini Ultra, Gemini Pro, and Gemini Nano. ",The objective function is Generator-encoder team aims to minimize the objective: and discriminator aims to maximize it: In the paper: they gave a more abstract definition of the objective as:where μE:X(dx:dz)=μX(dx)⋅δE(x)(dz){\displaystyle \mu _{E:X}(dx:dz)=\mu _{X}(dx)\cdot \delta _{E(x)}(dz)} is the probability distribution on ΩX×ΩZ{\displaystyle \Omega _{X}\times \Omega _{Z}} obtained by pushing μX{\displaystyle \mu _{X}} forward via x↦(x:E(x)){\displaystyle x\mapsto (x:E(x))}: and μG:Z(dx:dz)=δG(z)(dx)⋅μZ(dz){\displaystyle \mu _{G:Z}(dx:dz)=\delta _{G(z)}(dx)\cdot \mu _{Z}(dz)} is the probability distribution on ΩX×ΩZ{\displaystyle \Omega _{X}\times \Omega _{Z}} obtained by pushing μZ{\displaystyle \mu _{Z}} forward via z↦(G(x):z){\displaystyle z\mapsto (G(x):z)}. Applications of bidirectional models include semi-supervised learning: interpretable machine learning: and neural machine translation. ==== CycleGAN ==== CycleGAN is an architecture for performing translations between two domains: such as between photos of horses and photos of zebras: or photos of night cities and photos of day cities. The CycleGAN game is defined as follows:There are two probability spaces (ΩX:μX):(ΩY:μY){\displaystyle (\Omega _{X}:\mu _{X}):(\Omega _{Y}:\mu _{Y})}: corresponding to the two domains needed for translations fore-and-back. There are 4 players in 2 teams: generators GX:ΩX→ΩY:GY:ΩY→ΩX{\displaystyle G_{X}:\Omega _{X}\to \Omega _{Y}:G_{Y}:\Omega _{Y}\to \Omega _{X}}: and discriminators DX:ΩX→[0:1]:DY:ΩY→[0:1]{\displaystyle D_{X}:\Omega _{X}\to [0:1]:D_{Y}:\Omega _{Y}\to [0:1]}. The objective function is where λ{\displaystyle \lambda } is a positive adjustable parameter: LGAN{\displaystyle L_{GAN}} is the GAN game objective: and Lcycle{\displaystyle L_{cycle}} is the cycle consistency loss:The generators aim to minimize the objective: and the discriminators aim to maximize it: Unlike previous work like pix2pix: which requires paired training data: cycleGAN requires no paired data. For example: to train a pix2pix model to turn a summer scenery photo to winter scenery photo and back: the dataset must contain pairs of the same place in summer and winter: shot at the same angle; cycleGAN would only need a set of summer scenery photos: and an unrelated set of winter scenery photos. === GANs with particularly large or small scales === ==== BigGAN ==== The BigGAN is essentially a self-attention GAN trained on a large scale (up to 80 million parameters) to generate large images of ImageNet (up to 512 x 512 resolution): with numerous engineering tricks to make it converge. ==== Invertible data augmentation ==== When there is insufficient training data: the reference distribution μref{\displaystyle \mu _{\text{ref}}} cannot be well-approximated by the empirical distribution given by the training dataset. In such cases: data augmentation can be applied: to allow training GAN on smaller datasets. Naïve data augmentation: however: brings its problems. Consider the original GAN game: slightly reformulated as follows:Now we use data augmentation by randomly sampling semantic-preserving transforms T:Ω→Ω{\displaystyle T:\Omega \to \Omega } and applying them to the dataset: to obtain the reformulated GAN game:This is equivalent to a GAN game with a different distribution μref′{\displaystyle \mu _{\text{ref}}'}: sampled by T(x){\displaystyle T(x)}: with x∼μref:T∼μtrans{\displaystyle x\sim \mu _{\text{ref}}:T\sim \mu _{trans}}. For example: if μref{\displaystyle \mu _{\text{ref}}} is the distribution of images in ImageNet: and μtrans{\displaystyle \mu _{trans}} samples identity-transform with probability 0.5: and horizontal-reflection with probability 0.5: then μref′{\displaystyle \mu _{\text{ref}}'} is the distribution of images in ImageNet and horizontally-reflected ImageNet: combined. The result of such training would
0.4704628000035882,When was the Gemini family announced and who were they positioned as a competitor to?,"The Gemini family was announced on December 6, 2023, and they were positioned as a competitor to OpenAI's GPT-4. ","of manually-labeled data. The reliance on supervised learning limited their use on datasets that were not well-annotated: and also made it prohibitively expensive and time-consuming to train extremely large language models.The semi-supervised approach OpenAI employed to make a large-scale generative system—and was first to do with a transformer model—involved two stages: an unsupervised generative ""pretraining"" stage to set initial parameters using a language modeling objective: and a supervised discriminative ""fine-tuning"" stage to adapt these parameters to a target task. === Later developments === Regarding more recent GPT foundation models: OpenAI published its first versions of GPT-3 in July 2020. There were three models: with 1B: 6.7B: 175B parameters: respectively named babbage: curie: and davinci (giving initials B: C: and D).In July 2021: OpenAI published Codex: a task-specific GPT model targeted for programming applications. This was developed by fine-tuning a 12B parameter version of GPT-3 (different from previous GPT-3 models) using code from GitHub.In March 2022: OpenAI published two versions of GPT-3 that were fine-tuned for instruction-following (instruction-tuned): named davinci-instruct-beta (175B) and text-davinci-001: and then started beta testing code-davinci-002. text-davinci-002 was instruction-tuned from code-davinci-002. Both text-davinci-003 and ChatGPT were released in November 2022: with both building upon text-davinci-002 via reinforcement learning from human feedback (RLHF). text-davinci-003 is trained for following instructions (like its predecessors): whereas ChatGPT is further trained for conversational interaction with a human user.OpenAI's most recent GPT foundation model: GPT-4: was released on March 14: 2023. It can be accessed directly by users via a premium version of ChatGPT: and is available to developers for incorporation into other products and services via OpenAI's API. Other producers of GPT foundation models include EleutherAI (with a series of models starting in March 2021) and Cerebras (with seven models released in March 2023). == Foundational models == A foundational model is an AI model trained on broad data at scale such that it can be adapted to a wide range of downstream tasks.Thus far: the most notable GPT foundation models have been from OpenAI's GPT-n series. The most recent from that is GPT-4: for which OpenAI declined to publish the size or training details (citing ""the competitive landscape and the safety implications of large-scale models""). Other such models include Google's PaLM: a broad foundation model that has been compared to GPT-3 and has recently been made available to developers via an API: and Together's GPT-JT: which has been reported as the closest-performing open-source alternative to GPT-3 (and is derived from earlier open-source GPTs). Meta AI (formerly Facebook) also has a generative transformer-based foundational large language model: known as LLaMA.Foundational GPTs can also employ modalities other than text: for input and/or output. GPT-4 is a multi-modal LLM that is capable of processing text and image input (though its output is limited to text). Regarding multimodal output: some generative transformer-based models are used for text-to-image technologies such as diffusion and parallel decoding. Such kinds of models can serve as visual foundation models (VFMs) for developing downstream systems that can work with images. == Task-specific"
0.42191569998976775,What is unique about the Gemini family of language models?,"Unlike other Large Language Models, Gemini is unique in that it is not trained on a text corpus alone. It is designed to be multimodal, meaning it can process multiple types of data simultaneously, including text, images, audio, video, and computer code.","""fine-tuning"" stage to adapt these parameters to a target task. === Later developments === Regarding more recent GPT foundation models: OpenAI published its first versions of GPT-3 in July 2020. There were three models: with 1B: 6.7B: 175B parameters: respectively named babbage: curie: and davinci (giving initials B: C: and D).In July 2021: OpenAI published Codex: a task-specific GPT model targeted for programming applications. This was developed by fine-tuning a 12B parameter version of GPT-3 (different from previous GPT-3 models) using code from GitHub.In March 2022: OpenAI published two versions of GPT-3 that were fine-tuned for instruction-following (instruction-tuned): named davinci-instruct-beta (175B) and text-davinci-001: and then started beta testing code-davinci-002. text-davinci-002 was instruction-tuned from code-davinci-002. Both text-davinci-003 and ChatGPT were released in November 2022: with both building upon text-davinci-002 via reinforcement learning from human feedback (RLHF). text-davinci-003 is trained for following instructions (like its predecessors): whereas ChatGPT is further trained for conversational interaction with a human user.OpenAI's most recent GPT foundation model: GPT-4: was released on March 14: 2023. It can be accessed directly by users via a premium version of ChatGPT: and is available to developers for incorporation into other products and services via OpenAI's API. Other producers of GPT foundation models include EleutherAI (with a series of models starting in March 2021) and Cerebras (with seven models released in March 2023). == Foundational models == A foundational model is an AI model trained on broad data at scale such that it can be adapted to a wide range of downstream tasks.Thus far: the most notable GPT foundation models have been from OpenAI's GPT-n series. The most recent from that is GPT-4: for which OpenAI declined to publish the size or training details (citing ""the competitive landscape and the safety implications of large-scale models""). Other such models include Google's PaLM: a broad foundation model that has been compared to GPT-3 and has recently been made available to developers via an API: and Together's GPT-JT: which has been reported as the closest-performing open-source alternative to GPT-3 (and is derived from earlier open-source GPTs). Meta AI (formerly Facebook) also has a generative transformer-based foundational large language model: known as LLaMA.Foundational GPTs can also employ modalities other than text: for input and/or output. GPT-4 is a multi-modal LLM that is capable of processing text and image input (though its output is limited to text). Regarding multimodal output: some generative transformer-based models are used for text-to-image technologies such as diffusion and parallel decoding. Such kinds of models can serve as visual foundation models (VFMs) for developing downstream systems that can work with images. == Task-specific models == A foundational GPT model can be further adapted to produce more targeted systems directed to specific tasks and/or subject-matter domains. Methods for such adaptation can include additional fine-tuning (beyond that done for the foundation model) as well as certain forms of prompt engineering.An important example of this is fine-tuning models to follow instructions: which is of course a fairly broad task but more targeted than a foundation"
0.42851140000857413,How has Gemini been integrated into Google's products and services?,"Upon launch, Gemini Pro and Nano were integrated into Bard and the Pixel 8 Pro smartphone, respectively, while Gemini Ultra was set to power ""Bard Advanced"". Google also intended to incorporate Gemini into other products including Search, Ads, Chrome, Duet AI on Google Workspace, and AlphaCode 2.","intervals containing speech events separated by thousands of discrete time steps: where one time step corresponds to about 10 ms. LSTM with forget gates is competitive with traditional speech recognizers on certain tasks.The initial success in speech recognition was based on small-scale recognition tasks based on TIMIT. The data set contains 630 speakers from eight major dialects of American English: where each speaker reads 10 sentences. Its small size lets many configurations be tried. More importantly: the TIMIT task concerns phone-sequence recognition: which: unlike word-sequence recognition: allows weak phone bigram language models. This lets the strength of the acoustic modeling aspects of speech recognition be more easily analyzed. The error rates listed below: including these early results and measured as percent phone error rates (PER): have been summarized since 1991. The debut of DNNs for speaker recognition in the late 1990s and speech recognition around 2009-2011 and of LSTM around 2003–2007: accelerated progress in eight major areas: Scale-up/out and accelerated DNN training and decoding Sequence discriminative training Feature processing by deep models with solid understanding of the underlying mechanisms Adaptation of DNNs and related deep models Multi-task and transfer learning by DNNs and related deep models CNNs and how to design them to best exploit domain knowledge of speech RNN and its rich LSTM variants Other types of deep models including tensor-based models and integrated deep generative/discriminative models.All major commercial speech recognition systems (e.g.: Microsoft Cortana: Xbox: Skype Translator: Amazon Alexa: Google Now: Apple Siri: Baidu and iFlyTek voice search: and a range of Nuance speech products: etc.) are based on deep learning. === Image recognition === A common evaluation set for image classification is the MNIST database data set. MNIST is composed of handwritten digits and includes 60:000 training examples and 10:000 test examples. As with TIMIT: its small size lets users test multiple configurations. A comprehensive list of results on this set is available.Deep learning-based image recognition has become ""superhuman"": producing more accurate results than human contestants. This first occurred in 2011 in recognition of traffic signs: and in 2014: with recognition of human faces.Deep learning-trained vehicles now interpret 360° camera views. Another example is Facial Dysmorphology Novel Analysis (FDNA) used to analyze cases of human malformation connected to a large database of genetic syndromes. === Visual art processing === Closely related to the progress that has been made in image recognition is the increasing application of deep learning techniques to various visual art tasks. DNNs have proven themselves capable: for example: of identifying the style period of a given painting Neural Style Transfer – capturing the style of a given artwork and applying it in a visually pleasing manner to an arbitrary photograph or video generating striking imagery based on random visual input fields. === Natural language processing === Neural networks have been used for implementing language models since the early 2000s. LSTM helped to improve machine translation and language modeling.Other key techniques in this field are negative sampling and word embedding. Word embedding: such as word2vec:"
0.41611429999466054,What were some notable achievements of Gemini Ultra and Gemini Pro?,"Gemini Ultra has outperformed GPT-4, Anthropic's Claude 2, Inflection AI's Inflection-2, Meta's LLaMA 2, and xAI's Grok 1 on a variety of industry benchmarks, and was the first language model to outperform human experts on the 57-subject Massive Multitask Language Understanding (MMLU) test, obtaining a score of 90%. Gemini Pro was also said to have outperformed GPT-3.5.","format suitable for a chatbot. Other major chatbots currently include Microsoft's Bing Chat: which uses OpenAI's GPT-4 (as part of a broader close collaboration between OpenAI and Microsoft): and Google's competing chatbot Bard (initially based on their LaMDA family of conversation-trained language models: with plans to switch to PaLM).Yet another kind of task that a GPT can be used for is the meta-task of generating its own instructions: like developing a series of prompts for 'itself' to be able to effectuate a more general goal given by a human user. This is known as an AI agent: and more specifically a recursive one because it uses results from its previous self-instructions to help it form its subsequent prompts; the first major example of this was Auto-GPT (which uses OpenAI's GPT models): and others have since been developed as well. === Multimodality === Generative transformer-based systems can also be targeted to tasks involving modalities beyond text. For example: Microsoft’s “Visual ChatGPT” combines ChatGPT with visual foundation models (VFMs) to enable input or output comprising images as well as text. Also: advances in text-to-speech technology offer powerful tools for audio content creation when used in conjunction with foundational GPT language models. === Domain-specificity === GPT systems can be directed toward particular fields or domains. Some reported examples of such models and apps are as follows: EinsteinGPT – for sales and marketing domains: to aid with customer relationship management (uses GPT-3.5) BloombergGPT – for the financial domain: to aid with financial news and information (uses ""freely available"" AI methods: combined with their proprietary data) Khanmigo – described as a GPT version for tutoring: in the education domain: it aids students using Khan Academy by guiding them through their studies without directly providing answers (powered by GPT-4) SlackGPT – for the Slack instant-messaging service: to aid with navigating and summarizing discussions on it (uses OpenAI's API) BioGPT – for the biomedical domain: to aid with biomedical literature text generation and mining (uses GPT-2)Sometimes domain-specificity is accomplished via software plug-ins or add-ons. For example: several different companies have developed particular plugins that interact directly with OpenAI's ChatGPT interface: and Google Workspace has available add-ons such as “GPT for Sheets and Docs”—which is reported to aid use of spreadsheet functionality in Google Sheets.In November 2023: OpenAI announced that it's enabling ChatGPT Plus subscribers to create custom versions of ChatGPT (being called GPTs). These can be tailored for specific domains via prompt engineering: curated datasets: and/or targeted interaction with external tools. Users who register as verified builders are able to publish their custom GPTs for other users: with monetization potential. (This is notably distinct from OpenAI's API service: as this is based internally within OpenAI's platform.) == Brand issues == OpenAI: which created the first generative pre-trained transformer (GPT) in 2018: has recently asserted that “GPT” should be regarded as a brand of OpenAI. In April 2023: OpenAI revised the brand guidelines in its terms of service to indicate that other businesses using its API to run"
0.7296582999988459,What are Generative pre-trained transformers (GPT)?,"Generative pre-trained transformers (GPT) are a type of large language model (LLM) and a prominent framework for generative artificial intelligence. They are artificial neural networks used in natural language processing tasks. GPTs are based on the transformer architecture, pre-trained on large data sets of unlabelled text, and capable of generating novel human-like content.","to aid with navigating and summarizing discussions on it (uses OpenAI's API) BioGPT – for the biomedical domain: to aid with biomedical literature text generation and mining (uses GPT-2)Sometimes domain-specificity is accomplished via software plug-ins or add-ons. For example: several different companies have developed particular plugins that interact directly with OpenAI's ChatGPT interface: and Google Workspace has available add-ons such as “GPT for Sheets and Docs”—which is reported to aid use of spreadsheet functionality in Google Sheets.In November 2023: OpenAI announced that it's enabling ChatGPT Plus subscribers to create custom versions of ChatGPT (being called GPTs). These can be tailored for specific domains via prompt engineering: curated datasets: and/or targeted interaction with external tools. Users who register as verified builders are able to publish their custom GPTs for other users: with monetization potential. (This is notably distinct from OpenAI's API service: as this is based internally within OpenAI's platform.) == Brand issues == OpenAI: which created the first generative pre-trained transformer (GPT) in 2018: has recently asserted that “GPT” should be regarded as a brand of OpenAI. In April 2023: OpenAI revised the brand guidelines in its terms of service to indicate that other businesses using its API to run their artificial intelligence (AI) services would no longer be able to include “GPT” in such names or branding. In May 2023: OpenAI engaged a brand management service to notify its API customers of this policy: although these notifications stopped short of making overt legal claims (such as allegations of trademark infringement or demands to cease and desist). As of November 2023: OpenAI still prohibits its API licensees from naming their own products with ""GPT:"" but it has begun enabling its ChatGPT Plus subscribers to make ""custom versions of ChatGPT"" that are being called GPTs on the OpenAI site. OpenAI's terms of service says that its subscribers may use ""GPT"" in the names of these: although it's ""discouraged.""Relatedly: OpenAI has applied to the United States Patent and Trademark Office (USPTO) to seek domestic trademark registration for the term “GPT” in the field of AI. OpenAI sought to expedite handling of its application: but the USPTO declined that request in April 2023. In May 2023: the USPTO responded to the application with a determination that ""GPT"" was both descriptive and generic. As of November 2023: OpenAI continues to pursue its argument through the available processes. Regardless: failure to obtain a registered U.S. trademark does not preclude some level of common-law trademark rights in the U.S.: and/or trademark rights in other countries.For any given type or scope of trademark protection in the U.S.: OpenAI would need to establish that the term is actually “distinctive” to their specific offerings in addition to being a broader technical term for the kind of technology. Some media reports suggested that OpenAI may be able to obtain trademark registration based indirectly on the fame of its GPT-based chatbot product: ChatGPT: for which OpenAI has separately sought protection (and which it has sought to enforce more strongly). Other reports"
0.65952849999303,Who introduced the first GPT and when?,The first GPT was introduced by OpenAI in 2018.,"the tokenization method: and applied to robotic control. LLaMA models have also been turned multimodal using the tokenization method: to allow image inputs: and video inputs.GPT-4 can use both text and image as inputs (although the vision component wasn't released to the public until GPT-4V); Google DeepMind's Gemini is also multimodal. == Properties == === Scaling laws === The following four hyper-parameters characterize a LLM: cost of (pre-)training (C{\displaystyle C}): size of the artificial neural network itself: such as number of parameters N{\displaystyle N} (i.e. amount of neurons in its layers: amount of weights between them and biases): size of its (pre-)training dataset (i.e. number of tokens in corpus: D{\displaystyle D}): performance after (pre-)training.They are related by simple statistical laws: called ""scaling laws"". One particular scaling law (""Chinchilla scaling"") for LLM autoregressively trained for one epoch: with a log-log learning rate schedule: states that: where the variables are C{\displaystyle C} is the cost of training the model: in FLOPs. N{\displaystyle N} is the number of parameters in the model. D{\displaystyle D} is the number of tokens in the training set. L{\displaystyle L} is the average negative log-likelihood loss per token (nats/token): achieved by the trained LLM on the test dataset.and the statistical hyper-parameters are C0=6{\displaystyle C_{0}=6}: meaning that it costs 6 FLOPs per parameter to train on one token. Note that training cost is much higher than inference cost: where it costs 1 to 2 FLOPs per parameter to infer on one token. α=0.34:β=0.28:A=406.4:B=410.7:L0=1.69{\displaystyle \alpha =0.34:\beta =0.28:A=406.4:B=410.7:L_{0}=1.69} === Emergent abilities === When one subtracts out from the y-axis the best performance that can be achieved even with infinite scaling of the x-axis quantity: large models' performance: measured on various tasks: seems to be a linear extrapolation of other (smaller-sized and medium-sized) models' performance on a log-log plot. However: sometimes the line's slope transitions from one slope to another at point(s) referred to as break(s) in downstream scaling laws: appearing as a series of linear segments connected by arcs; it seems that larger models acquire ""emergent abilities"" at this point(s). These abilities are discovered rather than programmed-in or designed: in some cases only after the LLM has been publicly deployed.The most intriguing among emergent abilities is in-context learning from example demonstrations. In-context learning is involved in tasks: such as: reported arithmetics: decoding the International Phonetic Alphabet: unscrambling a word's letters: disambiguate word in context: converting spatial words: cardinal directions (for example: replying ""northeast"" upon [0: 0: 1; 0: 0: 0; 0: 0: 0]): color terms represented in text. chain-of-thought prompting: Model outputs are improved by chain-of-thought prompting only when model size exceeds 62B. Smaller models perform better when prompted to answer immediately: without chain of thought. identifying offensive content in paragraphs of Hinglish (a combination of Hindi and English): and generating a similar English equivalent of Kiswahili proverbs.Schaeffer et. al. argue that the emergent abilities are not unpredictably acquired: but predictably acquired according to a smooth scaling law. The authors considered a toy statistical model of an LLM solving multiple-choice questions:"
0.40366300000459887,What are some task-specific GPT systems developed by OpenAI? ,"OpenAI has developed task-specific GPT systems including models fine-tuned for instruction following, which power the ChatGPT chatbot service.","must contain pairs of the same place in summer and winter: shot at the same angle; cycleGAN would only need a set of summer scenery photos: and an unrelated set of winter scenery photos. === GANs with particularly large or small scales === ==== BigGAN ==== The BigGAN is essentially a self-attention GAN trained on a large scale (up to 80 million parameters) to generate large images of ImageNet (up to 512 x 512 resolution): with numerous engineering tricks to make it converge. ==== Invertible data augmentation ==== When there is insufficient training data: the reference distribution μref{\displaystyle \mu _{\text{ref}}} cannot be well-approximated by the empirical distribution given by the training dataset. In such cases: data augmentation can be applied: to allow training GAN on smaller datasets. Naïve data augmentation: however: brings its problems. Consider the original GAN game: slightly reformulated as follows:Now we use data augmentation by randomly sampling semantic-preserving transforms T:Ω→Ω{\displaystyle T:\Omega \to \Omega } and applying them to the dataset: to obtain the reformulated GAN game:This is equivalent to a GAN game with a different distribution μref′{\displaystyle \mu _{\text{ref}}'}: sampled by T(x){\displaystyle T(x)}: with x∼μref:T∼μtrans{\displaystyle x\sim \mu _{\text{ref}}:T\sim \mu _{trans}}. For example: if μref{\displaystyle \mu _{\text{ref}}} is the distribution of images in ImageNet: and μtrans{\displaystyle \mu _{trans}} samples identity-transform with probability 0.5: and horizontal-reflection with probability 0.5: then μref′{\displaystyle \mu _{\text{ref}}'} is the distribution of images in ImageNet and horizontally-reflected ImageNet: combined. The result of such training would be a generator that mimics μref′{\displaystyle \mu _{\text{ref}}'}. For example: it would generate images that look like they are randomly cropped: if the data augmentation uses random cropping. The solution is to apply data augmentation to both generated and real images:The authors demonstrated high-quality generation using just 100-picture-large datasets.The StyleGAN-2-ADA paper points out a further point on data augmentation: it must be invertible. Continue with the example of generating ImageNet pictures. If the data augmentation is ""randomly rotate the picture by 0: 90: 180: 270 degrees with equal probability"": then there is no way for the generator to know which is the true orientation: Consider two generators G:G′{\displaystyle G:G'}: such that for any latent z{\displaystyle z}: the generated image G(z){\displaystyle G(z)} is a 90-degree rotation of G′(z){\displaystyle G'(z)}. They would have exactly the same expected loss: and so neither is preferred over the other. The solution is to only use invertible data augmentation: instead of ""randomly rotate the picture by 0: 90: 180: 270 degrees with equal probability"": use ""randomly rotate the picture by 90: 180: 270 degrees with 0.1 probability: and keep the picture as it is with 0.7 probability"". This way: the generator is still rewarded to keep images oriented the same way as un-augmented ImageNet pictures. Abstractly: the effect of randomly sampling transformations T:Ω→Ω{\displaystyle T:\Omega \to \Omega } from the distribution μtrans{\displaystyle \mu _{trans}} is to define a Markov kernel Ktrans:Ω→P(Ω){\displaystyle K_{trans}:\Omega \to {\mathcal {P}}(\Omega )}. Then: the data-augmented GAN game pushes the generator to find some μ^G∈P(Ω){\displaystyle {\hat {\mu }}_{G}\in {\mathcal {P}}(\Omega )}: such that"
0.43227290001232177,What companies have developed GPT foundation models other than OpenAI?,EleutherAI and Cerebras have developed GPT foundation models aside from OpenAI.,"models were reverse-engineered: and it turned out they used discrete Fourier transform. === Understanding and intelligence === NLP researchers were evenly split when asked: in a 2022 survey: whether (untuned) LLMs ""could (ever) understand natural language in some nontrivial sense"". Proponents of ""LLM understanding"" believe that some LLM abilities: such as mathematical reasoning: imply an ability to ""understand"" certain concepts. A Microsoft team argued in 2023 that GPT-4 ""can solve novel and difficult tasks that span mathematics: coding: vision: medicine: law: psychology and more"" and that GPT-4 ""could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence system"": ""Can one reasonably say that a system that passes exams for software engineering candidates is not really intelligent?"" Some researchers characterize LLMs as ""alien intelligence"". For example: Conjecture CEO Connor Leahy considers untuned LLMs to be like inscrutable alien ""Shoggoths"": and believes that RLHF tuning creates a ""smiling facade"" obscuring the inner workings of the LLM: ""If you don't push it too far: the smiley face stays on. But then you give it [an unexpected] prompt: and suddenly you see this massive underbelly of insanity: of weird thought processes and clearly non-human understanding.""In contrast: some proponents of the ""LLMs lack understanding"" school believe that existing LLMs are ""simply remixing and recombining existing writing"": or point to the deficits existing LLMs continue to have in prediction skills: reasoning skills: agency: and explainability. For example: GPT-4 has natural deficits in planning and in real-time learning. Generative LLMs have been observed to confidently assert claims of fact which do not seem to be justified by their training data: a phenomenon which has been termed ""hallucination"". Specifically: hallucinations in the context of LLMs correspond to the generation of text or responses that seem syntactically sound: fluent: and natural but are factually incorrect: nonsensical: or unfaithful to the provided source input. Neuroscientist Terrence Sejnowski has argued that ""The diverging opinions of experts on the intelligence of LLMs suggests that our old ideas based on natural intelligence are inadequate"".The matter of LLM's exhibiting intelligence or understanding has two main aspects – the first is how to model thought and language in a computer system: and the second is how to enable the computer system to generate human like language. These aspects of language as a model of cognition have been developed in the field of cognitive linguistics. American linguist George Lakoff presented Neural Theory of Language (NTL) as a computational basis for using language as a model of learning tasks and understanding. The NTL Model outlines how specific neural structures of the human brain shape the nature of thought and language and in turn what are the computational properties of such neural systems that can be applied to model thought and language in a computer system. After a framework for modeling language in a computer systems was established: the focus shifted to establishing frameworks for computer systems to generate language with acceptable grammar. In his 2014 book titled The Language Myth: Why"
0.44394900000770576,What are the foundational models?,"Foundational models are AI models trained on broad data at scale so that they can be adapted to a wide range of downstream tasks. Notable GPT foundational models have been from OpenAI's GPT-n series. Other such models include Google's PaLM, Together's GPT-JT, and Meta AI's transformer-based large language model known as LLaMA.",sum of all the inputs: weighted by the weights of the connections from the inputs to the neuron. We add a bias term to this sum. This weighted sum is sometimes called the activation. This weighted sum is then passed through a (usually nonlinear) activation function to produce the output. The initial inputs are external data: such as images and documents. The ultimate outputs accomplish the task: such as recognizing an object in an image. === Organization === The neurons are typically organized into multiple layers: especially in deep learning. Neurons of one layer connect only to neurons of the immediately preceding and immediately following layers. The layer that receives external data is the input layer. The layer that produces the ultimate result is the output layer. In between them are zero or more hidden layers. Single layer and unlayered networks are also used. Between two layers: multiple connection patterns are possible. They can be 'fully connected': with every neuron in one layer connecting to every neuron in the next layer. They can be pooling: where a group of neurons in one layer connects to a single neuron in the next layer: thereby reducing the number of neurons in that layer. Neurons with only such connections form a directed acyclic graph and are known as feedforward networks. Alternatively: networks that allow connections between neurons in the same or previous layers are known as recurrent networks. === Hyperparameter === A hyperparameter is a constant parameter whose value is set before the learning process begins. The values of parameters are derived via learning. Examples of hyperparameters include learning rate: the number of hidden layers and batch size. The values of some hyperparameters can be dependent on those of other hyperparameters. For example: the size of some layers can depend on the overall number of layers. === Learning === Learning is the adaptation of the network to better handle a task by considering sample observations. Learning involves adjusting the weights (and optional thresholds) of the network to improve the accuracy of the result. This is done by minimizing the observed errors. Learning is complete when examining additional observations does not usefully reduce the error rate. Even after learning: the error rate typically does not reach 0. If after learning: the error rate is too high: the network typically must be redesigned. Practically this is done by defining a cost function that is evaluated periodically during learning. As long as its output continues to decline: learning continues. The cost is frequently defined as a statistic whose value can only be approximated. The outputs are actually numbers: so when the error is low: the difference between the output (almost certainly a cat) and the correct answer (cat) is small. Learning attempts to reduce the total of the differences across the observations. Most learning models can be viewed as a straightforward application of optimization theory and statistical estimation. ==== Learning rate ==== The learning rate defines the size of the corrective steps that the model takes
0.37722880000364967,What is a Graph Neural Network (GNN)?,"A Graph Neural Network (GNN) is a type of artificial neural network designed for processing data that can be represented as graphs. It falls under the broader field of ""geometric deep learning"". GNNs use pairwise message passing as the key design element, enabling graph nodes to iteratively update their representations by exchanging information with their neighbors.",used programmatically.: 539 Executable modeling languages applied with proper tool support: however: are expected to automate system verification and validation: simulation and code generation from the same representations. == Quality == A review of modelling languages is essential to be able to assign which languages are appropriate for different modelling settings. In the term settings we include stakeholders: domain and the knowledge connected. Assessing the language quality is a means that aims to achieve better models. === Framework for evaluation === Here language quality is stated in accordance with the SEQUAL framework for quality of models developed by Krogstie: Sindre and Lindland (2003): since this is a framework that connects the language quality to a framework for general model quality. Five areas are used in this framework to describe language quality and these are supposed to express both the conceptual as well as the visual notation of the language. We will not go into a thoroughly explanation of the underlying quality framework of models but concentrate on the areas used to explain the language quality framework. ==== Domain appropriateness ==== The framework states the ability to represent the domain as domain appropriateness. The statement appropriateness can be a bit vague: but in this particular context it means able to express. You should ideally only be able to express things that are in the domain but be powerful enough to include everything that is in the domain. This requirement might seem a bit strict: but the aim is to get a visually expressed model which includes everything relevant to the domain and excludes everything not appropriate for the domain. To achieve this: the language has to have a good distinction of which notations and syntaxes that are advantageous to present. ==== Participant appropriateness ==== To evaluate the participant appropriateness we try to identify how well the language expresses the knowledge held by the stakeholders. This involves challenges since a stakeholder's knowledge is subjective. The knowledge of the stakeholder is both tacit and explicit. Both types of knowledge are of dynamic character. In this framework only the explicit type of knowledge is taken into account. The language should to a large extent express all the explicit knowledge of the stakeholders relevant to the domain. ==== Modeller appropriateness ==== Last paragraph stated that knowledge of the stakeholders should be presented in a good way. In addition it is imperative that the language should be able to express all possible explicit knowledge of the stakeholders. No knowledge should be left unexpressed due to lacks in the language. ==== Comprehensibility appropriateness ==== Comprehensibility appropriateness makes sure that the social actors understand the model due to a consistent use of the language. To achieve this the framework includes a set of criteria. The general importance that these express is that the language should be flexible: easy to organize and easy to distinguish different parts of the language internally as well as from other languages. In addition to this: the goal should be as simple as possible
0.39800270000705495,How can convolutional neural networks and transformer layers be interpreted in the context of GNNs?,"In geometric deep learning, convolutional neural networks and transformer layers can be interpreted as GNNs operating on specifically defined graphs. For instance, a convolutional neural network layer, in computer vision context, can be seen as a GNN applied to graphs whose nodes are pixels and where only adjacent pixels are connected by edges. Similarly, a transformer layer in natural language processing can be seen as a GNN applied to complete graphs whose nodes are words or tokens in a text passage.","used deep learning to provide a robot with the ability to learn new tasks through observation. Using Deep TAMER: a robot learned a task with a human trainer: watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer: who provided feedback such as ""good job"" and ""bad job"". == Criticism and comment == Deep learning has attracted both criticism and comment: in some cases from outside the field of computer science. === Theory === A main criticism concerns the lack of theory surrounding some methods. Learning in the most common deep architectures is implemented using well-understood gradient descent. However: the theory surrounding other algorithms: such as contrastive divergence is less clear. (e.g.: Does it converge? If so: how fast? What is it approximating?) Deep learning methods are often looked at as a black box: with most confirmations done empirically: rather than theoretically.Others point out that deep learning should be looked at as a step towards realizing strong AI: not as an all-encompassing solution. Despite the power of deep learning methods: they still lack much of the functionality needed to realize this goal entirely. Research psychologist Gary Marcus noted: Realistically: deep learning is only part of the larger challenge of building intelligent machines. Such techniques lack ways of representing causal relationships (...) have no obvious ways of performing logical inferences: and they are also still a long way from integrating abstract knowledge: such as information about what objects are: what they are for: and how they are typically used. The most powerful A.I. systems: like Watson (...) use techniques like deep learning as just one element in a very complicated ensemble of techniques: ranging from the statistical technique of Bayesian inference to deductive reasoning. In further reference to the idea that artistic sensitivity might be inherent in relatively low levels of the cognitive hierarchy: a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained demonstrate a visual appeal: the original research notice received well over 1:000 comments: and was the subject of what was for a time the most frequently accessed article on The Guardian's website. === Errors === Some deep learning architectures display problematic behaviors: such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images (2014) and misclassifying minuscule perturbations of correctly classified images (2013). Goertzel hypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-component artificial general intelligence (AGI) architectures. These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar decompositions of observed entities and events. Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic"
0.42552299998351373,What is the role of message passing in GNNs?,"In GNNs, message passing is the key design element wherein graph nodes update their representations by exchanging information with their neighbors iteratively. Different GNN architectures implement different kinds of message passing. This aspect of defining GNN architectures ""going beyond"" message passing or basing every GNN on message passing over appropriately defined graphs is still a subject of open research.",novel candidates that the discriminator thinks are not synthesized (are part of the true data distribution)).A known dataset serves as the initial training data for the discriminator. Training involves presenting it with samples from the training dataset until it achieves acceptable accuracy. The generator is trained based on whether it succeeds in fooling the discriminator. Typically: the generator is seeded with randomized input that is sampled from a predefined latent space (e.g. a multivariate normal distribution). Thereafter: candidates synthesized by the generator are evaluated by the discriminator. Independent backpropagation procedures are applied to both networks so that the generator produces better samples: while the discriminator becomes more skilled at flagging synthetic samples. When used for image generation: the generator is typically a deconvolutional neural network: and the discriminator is a convolutional neural network. === Relation to other statistical machine learning methods === GANs are implicit generative models: which means that they do not explicitly model the likelihood function nor provide a means for finding the latent variable corresponding to a given sample: unlike alternatives such as flow-based generative model. Compared to fully visible belief networks such as WaveNet and PixelRNN and autoregressive models in general: GANs can generate one complete sample in one pass: rather than multiple passes through the network. Compared to Boltzmann machines and nonlinear ICA: there is no restriction on the type of function used by the network. Since neural networks are universal approximators: GANs are asymptotically consistent. Variational autoencoders might be universal approximators: but it is not proven as of 2017. == Mathematical properties == === Measure-theoretic considerations === This section provides some of the mathematical theory behind these methods. In modern probability theory based on measure theory: a probability space also needs to be equipped with a σ-algebra. As a result: a more rigorous definition of the GAN game would make the following changes:Each probability space (Ω:B:μref){\displaystyle (\Omega :{\mathcal {B}}:\mu _{\text{ref}})} defines a GAN game. The generator's strategy set is P(Ω:B){\displaystyle {\mathcal {P}}(\Omega :{\mathcal {B}})}: the set of all probability measures μG{\displaystyle \mu _{G}} on the measure-space (Ω:B){\displaystyle (\Omega :{\mathcal {B}})}. The discriminator's strategy set is the set of Markov kernels μD:(Ω:B)→P([0:1]:B([0:1])){\displaystyle \mu _{D}:(\Omega :{\mathcal {B}})\to {\mathcal {P}}([0:1]:{\mathcal {B}}([0:1]))}: where B([0:1]){\displaystyle {\mathcal {B}}([0:1])} is the Borel σ-algebra on [0:1]{\displaystyle [0:1]}.Since issues of measurability never arise in practice: these will not concern us further. === Choice of the strategy set === In the most generic version of the GAN game described above: the strategy set for the discriminator contains all Markov kernels μD:Ω→P[0:1]{\displaystyle \mu _{D}:\Omega \to {\mathcal {P}}[0:1]}: and the strategy set for the generator contains arbitrary probability distributions μG{\displaystyle \mu _{G}} on Ω{\displaystyle \Omega }. However: as shown below: the optimal discriminator strategy against any μG{\displaystyle \mu _{G}} is deterministic: so there is no loss of generality in restricting the discriminator's strategies to deterministic functions D:Ω→[0:1]{\displaystyle D:\Omega \to [0:1]}. In most applications: D{\displaystyle D} is a deep neural network function. As for the generator: while μG{\displaystyle \mu _{G}} could theoretically be any computable probability distribution:
0.4355061999813188,What are the possible application domains for GNNs?,"GNNs are applicable in various fields such as Natural Language Processing, social networks, citation networks, molecular biology, chemistry, physics, and NP-hard combinatorial optimization problems.","denotes element-wise matrix multiplication: and sigmoid(⋅){\displaystyle {\text{sigmoid}}(\cdot )} is the sigmoid function. In other words: the nodes with the top-k highest projection scores are retained in the new adjacency matrix A′{\displaystyle \mathbf {A} '}. The sigmoid(⋅){\displaystyle {\text{sigmoid}}(\cdot )} operation makes the projection vector p{\displaystyle \mathbf {p} } trainable by backpropagation: which otherwise would produce discrete outputs. === Self-attention pooling === We first set y=GNN(X:A){\displaystyle \mathbf {y} ={\text{GNN}}(\mathbf {X} :\mathbf {A} )}where GNN{\displaystyle {\text{GNN}}} is a generic permutation equivariant GNN layer (e.g.: GCN: GAT: MPNN). The Self-attention pooling layer can then be formalised as follows: X′=(X⊙y)i{\displaystyle \mathbf {X} '=(\mathbf {X} \odot \mathbf {y} )_{\mathbf {i} }}A′=Ai:i{\displaystyle \mathbf {A} '=\mathbf {A} _{\mathbf {i} :\mathbf {i} }}where i=topk(y){\displaystyle \mathbf {i} ={\text{top}}_{k}(\mathbf {y} )} is the subset of nodes with the top-k highest projection scores: ⊙{\displaystyle \odot } denotes element-wise matrix multiplication. The self-attention pooling layer can be seen as an extension of the top-k pooling layer. Differently from top-k pooling: the self-attention scores computed in self-attention pooling account both for the graph features and the graph topology. == Applications == === Protein folding === Graph neural networks are one of the main building blocks of AlphaFold: an artificial intelligence program developed by Google's DeepMind for solving the protein folding problem in biology. AlphaFold achieved first place in several CASP competitions. === Social networks === Social networks are a major application domain for GNNs due to their natural representation as social graphs. GNNs are used to develop recommender systems based on both social relations and item relations. === Combinatorial optimization === GNNs are used as fundamental building blocks for several combinatorial optimization algorithms. Examples include computing shortest paths or Eulerian circuits for a given graph: deriving chip placements superior or competitive to handcrafted human solutions: and improving expert-designed branching rules in branch and bound. === Cyber security === When viewed as a graph: a network of computers can be analyzed with GNNs for anomaly detection. Anomalies within provenance graphs often correlate to malicious activity within the network. GNNs have been used to identify these anomalies on individual nodes and within paths to detect malicious processes: or on the edge level to detect lateral movement. == References == == External links == https://distill.pub/2021/gnn-intro/ Artificial neural networks (ANNs) are models created using machine learning to perform a number of tasks. Their creation was inspired by neural circuitry. While some of the computational implementations ANNs relate to earlier discoveries in mathematics: the first implementation of ANNs was by psychologist Frank Rosenblatt: who developed the perceptron. Little research was conducted on ANNs in the 1970s and 1980s: with the AAAI calling that period an ""AI winter"".Later: advances in hardware and the development of the backpropagation algorithm as well as recurrent neural networks and convolutional neural networks: renewed interest in ANNs. The 2010s: saw the development of a deep neural network (a neural network with many layers) called AlexNet. It greatly outperformed other image recognition models: and is thought to have launched the ongoing AI spring: and further increasing"
0.4280263999826275,What are some examples of libraries implementing graph neural networks?,"Several open-source libraries implement graph neural networks. Some examples include PyTorch Geometric (PyTorch), TensorFlow GNN (TensorFlow), jraph (Google JAX), and GraphNeuralNetworks.jl/GeometricFlux.jl (Julia, Flux).","compare GAN with previous methods for learning generative models: which were plagued with ""intractable probabilistic computations that arise in maximum likelihood estimation and related strategies"".At the same time: Kingma and Welling and Rezende et al. developed the same idea of reparametrization into a general stochastic backpropagation method. Among its first applications was the variational autoencoder. === Move order and strategic equilibria === In the original paper: as well as most subsequent papers: it is usually assumed that the generator moves first: and the discriminator moves second: thus giving the following minimax game: If both the generator's and the discriminator's strategy sets are spanned by a finite number of strategies: then by the minimax theorem:that is: the move order does not matter. However: since the strategy sets are both not finitely spanned: the minimax theorem does not apply: and the idea of an ""equilibrium"" becomes delicate. To wit: there are the following different concepts of equilibrium: Equilibrium when generator moves first: and discriminator moves second: Equilibrium when discriminator moves first: and generator moves second: Nash equilibrium (μ^D:μ^G){\displaystyle ({\hat {\mu }}_{D}:{\hat {\mu }}_{G})}: which is stable under simultaneous move order:For general games: these equilibria do not have to agree: or even to exist. For the original GAN game: these equilibria all exist: and are all equal. However: for more general GAN games: these do not necessarily exist: or agree. === Main theorems for GAN game === The original GAN paper proved the following two theorems: Interpretation: For any fixed generator strategy μG{\displaystyle \mu _{G}}: the optimal discriminator keeps track of the likelihood ratio between the reference distribution and the generator distribution:where σ{\displaystyle \sigma } is the logistic function. In particular: if the prior probability for an image x{\displaystyle x} to come from the reference distribution is equal to 12{\displaystyle {\frac {1}{2}}}: then D(x){\displaystyle D(x)} is just the posterior probability that x{\displaystyle x} came from the reference distribution: == Training and evaluating GAN == === Training === ==== Unstable convergence ==== While the GAN game has a unique global equilibrium point when both the generator and discriminator have access to their entire strategy sets: the equilibrium is no longer guaranteed when they have a restricted strategy set.In practice: the generator has access only to measures of form μZ∘Gθ−1{\displaystyle \mu _{Z}\circ G_{\theta }^{-1}}: where Gθ{\displaystyle G_{\theta }} is a function computed by a neural network with parameters θ{\displaystyle \theta }: and μZ{\displaystyle \mu _{Z}} is an easily sampled distribution: such as the uniform or normal distribution. Similarly: the discriminator has access only to functions of form Dζ{\displaystyle D_{\zeta }}: a function computed by a neural network with parameters ζ{\displaystyle \zeta }. These restricted strategy sets take up a vanishingly small proportion of their entire strategy sets.Further: even if an equilibrium still exists: it can only be found by searching in the high-dimensional space of all possible neural network functions. The standard strategy of using gradient descent to find the equilibrium often does not work for GAN: and often the game ""collapses"" into one of"
0.45488609999301843,Who was behind the first implementation of artificial neural networks (ANNs)?,The first implementation of ANNs was by psychologist Frank Rosenblatt.,"the set of probability measures on [0:1]{\displaystyle [0:1]}. The GAN game is a zero-sum game: with objective function The generator aims to minimize the objective: and the discriminator aims to maximize the objective. The generator's task is to approach μG≈μref{\displaystyle \mu _{G}\approx \mu _{\text{ref}}}: that is: to match its own output distribution as closely as possible to the reference distribution. The discriminator's task is to output a value close to 1 when the input appears to be from the reference distribution: and to output a value close to 0 when the input looks like it came from the generator distribution. === In practice === The generative network generates candidates while the discriminative network evaluates them. The contest operates in terms of data distributions. Typically: the generative network learns to map from a latent space to a data distribution of interest: while the discriminative network distinguishes candidates produced by the generator from the true data distribution. The generative network's training objective is to increase the error rate of the discriminative network (i.e.: ""fool"" the discriminator network by producing novel candidates that the discriminator thinks are not synthesized (are part of the true data distribution)).A known dataset serves as the initial training data for the discriminator. Training involves presenting it with samples from the training dataset until it achieves acceptable accuracy. The generator is trained based on whether it succeeds in fooling the discriminator. Typically: the generator is seeded with randomized input that is sampled from a predefined latent space (e.g. a multivariate normal distribution). Thereafter: candidates synthesized by the generator are evaluated by the discriminator. Independent backpropagation procedures are applied to both networks so that the generator produces better samples: while the discriminator becomes more skilled at flagging synthetic samples. When used for image generation: the generator is typically a deconvolutional neural network: and the discriminator is a convolutional neural network. === Relation to other statistical machine learning methods === GANs are implicit generative models: which means that they do not explicitly model the likelihood function nor provide a means for finding the latent variable corresponding to a given sample: unlike alternatives such as flow-based generative model. Compared to fully visible belief networks such as WaveNet and PixelRNN and autoregressive models in general: GANs can generate one complete sample in one pass: rather than multiple passes through the network. Compared to Boltzmann machines and nonlinear ICA: there is no restriction on the type of function used by the network. Since neural networks are universal approximators: GANs are asymptotically consistent. Variational autoencoders might be universal approximators: but it is not proven as of 2017. == Mathematical properties == === Measure-theoretic considerations === This section provides some of the mathematical theory behind these methods. In modern probability theory based on measure theory: a probability space also needs to be equipped with a σ-algebra. As a result: a more rigorous definition of the GAN game would make the following changes:Each probability space (Ω:B:μref){\displaystyle (\Omega :{\mathcal {B}}:\mu _{\text{ref}})} defines a GAN game. The generator's strategy"
0.49088869997649454,What is the AlexNet and what is its significance in the field of artificial neural networks?,"AlexNet is a deep neural network developed in the 2010s. It greatly outperformed other image recognition models and is thought to have launched the ongoing AI spring, contributing to further increased interest in ANNs.","were plagued with ""intractable probabilistic computations that arise in maximum likelihood estimation and related strategies"".At the same time: Kingma and Welling and Rezende et al. developed the same idea of reparametrization into a general stochastic backpropagation method. Among its first applications was the variational autoencoder. === Move order and strategic equilibria === In the original paper: as well as most subsequent papers: it is usually assumed that the generator moves first: and the discriminator moves second: thus giving the following minimax game: If both the generator's and the discriminator's strategy sets are spanned by a finite number of strategies: then by the minimax theorem:that is: the move order does not matter. However: since the strategy sets are both not finitely spanned: the minimax theorem does not apply: and the idea of an ""equilibrium"" becomes delicate. To wit: there are the following different concepts of equilibrium: Equilibrium when generator moves first: and discriminator moves second: Equilibrium when discriminator moves first: and generator moves second: Nash equilibrium (μ^D:μ^G){\displaystyle ({\hat {\mu }}_{D}:{\hat {\mu }}_{G})}: which is stable under simultaneous move order:For general games: these equilibria do not have to agree: or even to exist. For the original GAN game: these equilibria all exist: and are all equal. However: for more general GAN games: these do not necessarily exist: or agree. === Main theorems for GAN game === The original GAN paper proved the following two theorems: Interpretation: For any fixed generator strategy μG{\displaystyle \mu _{G}}: the optimal discriminator keeps track of the likelihood ratio between the reference distribution and the generator distribution:where σ{\displaystyle \sigma } is the logistic function. In particular: if the prior probability for an image x{\displaystyle x} to come from the reference distribution is equal to 12{\displaystyle {\frac {1}{2}}}: then D(x){\displaystyle D(x)} is just the posterior probability that x{\displaystyle x} came from the reference distribution: == Training and evaluating GAN == === Training === ==== Unstable convergence ==== While the GAN game has a unique global equilibrium point when both the generator and discriminator have access to their entire strategy sets: the equilibrium is no longer guaranteed when they have a restricted strategy set.In practice: the generator has access only to measures of form μZ∘Gθ−1{\displaystyle \mu _{Z}\circ G_{\theta }^{-1}}: where Gθ{\displaystyle G_{\theta }} is a function computed by a neural network with parameters θ{\displaystyle \theta }: and μZ{\displaystyle \mu _{Z}} is an easily sampled distribution: such as the uniform or normal distribution. Similarly: the discriminator has access only to functions of form Dζ{\displaystyle D_{\zeta }}: a function computed by a neural network with parameters ζ{\displaystyle \zeta }. These restricted strategy sets take up a vanishingly small proportion of their entire strategy sets.Further: even if an equilibrium still exists: it can only be found by searching in the high-dimensional space of all possible neural network functions. The standard strategy of using gradient descent to find the equilibrium often does not work for GAN: and often the game ""collapses"" into one of several failure modes. To improve the convergence stability: some training"
0.4697425999911502,How does a linear neural network work?,A linear network consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated in each node. The mean squared errors between these calculated outputs and a given target values are minimized by creating an adjustment to the weights.,"token. Note that training cost is much higher than inference cost: where it costs 1 to 2 FLOPs per parameter to infer on one token. α=0.34:β=0.28:A=406.4:B=410.7:L0=1.69{\displaystyle \alpha =0.34:\beta =0.28:A=406.4:B=410.7:L_{0}=1.69} === Emergent abilities === When one subtracts out from the y-axis the best performance that can be achieved even with infinite scaling of the x-axis quantity: large models' performance: measured on various tasks: seems to be a linear extrapolation of other (smaller-sized and medium-sized) models' performance on a log-log plot. However: sometimes the line's slope transitions from one slope to another at point(s) referred to as break(s) in downstream scaling laws: appearing as a series of linear segments connected by arcs; it seems that larger models acquire ""emergent abilities"" at this point(s). These abilities are discovered rather than programmed-in or designed: in some cases only after the LLM has been publicly deployed.The most intriguing among emergent abilities is in-context learning from example demonstrations. In-context learning is involved in tasks: such as: reported arithmetics: decoding the International Phonetic Alphabet: unscrambling a word's letters: disambiguate word in context: converting spatial words: cardinal directions (for example: replying ""northeast"" upon [0: 0: 1; 0: 0: 0; 0: 0: 0]): color terms represented in text. chain-of-thought prompting: Model outputs are improved by chain-of-thought prompting only when model size exceeds 62B. Smaller models perform better when prompted to answer immediately: without chain of thought. identifying offensive content in paragraphs of Hinglish (a combination of Hindi and English): and generating a similar English equivalent of Kiswahili proverbs.Schaeffer et. al. argue that the emergent abilities are not unpredictably acquired: but predictably acquired according to a smooth scaling law. The authors considered a toy statistical model of an LLM solving multiple-choice questions: and showed that this statistical model: modified to account for other types of tasks: applies to these tasks as well.Let x{\displaystyle x} be the number of parameter count: and y{\displaystyle y} be the performance of the model. == Interpretation == Large language models by themselves are ""black boxes"": and it is not clear how they can perform linguistic tasks. There are several methods for understanding how LLM work. Mechanistic interpretability aims to reverse-engineer LLM by discovering symbolic algorithms that approximate the inference performed by LLM. One example is Othello-GPT: where a small Transformer is trained to predict legal Othello moves. It is found that there is a linear representation of Othello board: and modifying the representation changes the predicted legal Othello moves in the correct way. In another example: a small Transformer is trained on Karel programs. Similar to the Othello-GPT example: there is a linear representation of Karel program semantics: and modifying the representation changes output in the correct way. The model also generates correct programs that are on average shorter than those in the training set.In another example: the authors trained small transformers on modular arithmetic addition. The resulting models were reverse-engineered: and it turned out they used discrete Fourier transform. === Understanding and intelligence === NLP researchers were evenly split when asked: in a 2022"
0.4302190999733284,Who developed the perceptron and what was its function?,"The perceptron, an algorithm for pattern recognition, was developed by Rosenblatt. It described circuitries not in the basic perceptron, including the exclusive-or circuit, which could not be processed by neural networks at the time.",of object detection to 0.439329: and reduced classification error to 0.06656: the best result to date. Its network applied more than 30 layers. That performance of convolutional neural networks on the ImageNet tests was close to that of humans. The best algorithms still struggle with objects that are small or thin: such as a small ant on a stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters: an increasingly common phenomenon with modern digital cameras. By contrast: those kinds of images rarely trouble humans. Humans: however: tend to have trouble with other issues. For example: they are not good at classifying objects into fine-grained categories such as the particular breed of dog or species of bird: whereas convolutional neural networks handle this.In 2015: a many-layered CNN demonstrated the ability to spot faces from a wide range of angles: including upside down: even when partially occluded: with competitive performance. The network was trained on a database of 200:000 images that included faces at various angles and orientations and a further 20 million images without faces. They used batches of 128 images over 50:000 iterations. === Video analysis === Compared to image data domains: there is relatively little work on applying CNNs to video classification. Video is more complex than images since it has another (temporal) dimension. However: some extensions of CNNs into the video domain have been explored. One approach is to treat space and time as equivalent dimensions of the input and perform convolutions in both time and space. Another way is to fuse the features of two convolutional neural networks: one for the spatial and one for the temporal stream. Long short-term memory (LSTM) recurrent units are typically incorporated after the CNN to account for inter-frame or inter-clip dependencies. Unsupervised learning schemes for training spatio-temporal features have been introduced: based on Convolutional Gated Restricted Boltzmann Machines and Independent Subspace Analysis. It's Application can be seen in Text-to-Video model. === Natural language processing === CNNs have also been explored for natural language processing. CNN models are effective for various NLP problems and achieved excellent results in semantic parsing: search query retrieval: sentence modeling: classification: prediction and other traditional NLP tasks. Compared to traditional language processing methods such as recurrent neural networks: CNNs can represent different contextual realities of language that do not rely on a series-sequence assumption: while RNNs are better suitable when classical time series modeling is required. === Anomaly Detection === A CNN with 1-D convolutions was used on time series in the frequency domain (spectral residual) by an unsupervised model to detect anomalies in the time domain. === Drug discovery === CNNs have been used in drug discovery. Predicting the interaction between molecules and biological proteins can identify potential treatments. In 2015: Atomwise introduced AtomNet: the first deep learning neural network for structure-based drug design. The system trains directly on 3-dimensional representations of chemical interactions. Similar to how image recognition networks
0.4601416999939829,What is the role of the backpropagation algorithm in artificial neural networks?,The backpropagation algorithm is an efficient application of the Leibniz chain rule to networks of differentiable nodes. It's crucial for adjusting the weight of various nodes in the network to improve prediction accuracy.,"March 2024: are built with a decoder-only transformer-based architecture while some recent implementations are based on other architectures: such as recurrent neural network variants and Mamba (a state space model).Up to 2020: fine tuning was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models: such as GPT-3: however: can be prompt-engineered to achieve similar results. They are thought to acquire knowledge about syntax: semantics and ""ontology"" inherent in human language corpora: but also inaccuracies and biases present in the corpora.Some notable LLMs are OpenAI's GPT series of models (e.g.: GPT-3.5 and GPT-4: used in ChatGPT and Microsoft Copilot): Google's PaLM and Gemini (the latter of which is currently used in the chatbot of the same name): xAI's Grok: Meta's LLaMA family of open-source models: Anthropic's Claude models: and Mistral AI's open source models. == History == At the 2017 NeurIPS conference: Google researchers introduced the transformer architecture in their landmark paper ""Attention Is All You Need"". This paper's goal was to improve upon 2014 Seq2seq technology: and was based mainly on the attention mechanism developed by Bahdanau et al. in 2014. The following year in 2018: BERT was introduced and quickly became ""ubiquitous"". Though the original transformer has both encoder and decoder blocks: BERT is an encoder-only model. Although decoder-only GPT-1 was introduced in 2018: it was GPT-2 in 2019 that caught widespread attention because OpenAI at first deemed it too powerful to release publicly: out of fear of malicious use. GPT-3 in 2020 went a step further and as of 2024 is available only via API with no offering of downloading the model to execute locally. But it was the 2022 consumer-facing browser-based ChatGPT that captured the imaginations of the general population and caused some media hype and online buzz. The 2023 GPT-4 was praised for its increased accuracy and as a ""holy grail"" for its multimodal capabilities. OpenAI did not reveal high-level architecture and the number of parameters of GPT-4. In the meantime: competing language models have for the most part been playing catch-up to the GPT series: at least in terms of number of parameters. Notable exceptions in terms of either number of parameters or measured accuracy include Google's 2019 T5-11B and 2022 PaLM-E: and Anthropic's 2024 Claude 3. In terms of Elo ratings: on January 26: 2024: Google's Bard (Gemini Pro) surpassed the regular GPT-4: but not the limited-availability GPT-4-Turbo.Since 2022: source-available models have been gaining popularity: especially at first with BLOOM and LLaMA: though both have restrictions on the field of use. Mistral AI's models Mistral 7B and Mixtral 8x7b have the more permissive Apache License. As of January 2024: Mixtral 8x7b is the most powerful open LLM according to the LMSYS Chatbot Arena Leaderboard: being more powerful than GPT-3.5 but not as powerful as GPT-4. == Dataset preprocessing == === Probabilistic tokenization === Because machine learning algorithms process numbers rather than text: the text must be converted to numbers. In the first step: a vocabulary"
0.48069090000353754,What is a modeling language?,"A modeling language is an artificial language that can be used to express data, information or knowledge or systems in a structure defined by a consistent set of rules. These rules help interpret the meaning of components in the structure. They can be graphical or textual, and can be used in various fields including computer science, information management, business process modeling, software engineering, and systems engineering.",A neural network: also called a neuronal network: is an interconnected population of neurons (typically containing multiple neural circuits). Biological neural networks are studied to understand the organization and functioning of nervous systems. Closely related are artificial neural networks: machine learning models inspired by biological neural networks. They consist of artificial neurons: which are mathematical functions that are designed to be analogous to the mechanisms used by neural circuits. == Overview == A biological neural network is composed of a group of chemically connected or functionally associated neurons. A single neuron may be connected to many other neurons and the total number of neurons and connections in a network may be extensive. Connections: called synapses: are usually formed from axons to dendrites: though dendrodendritic synapses and other connections are possible. Apart from electrical signalling: there are other forms of signalling that arise from neurotransmitter diffusion. Artificial intelligence: cognitive modelling: and artificial neural networks are information processing paradigms inspired by how biological neural systems process data. Artificial intelligence and cognitive modelling try to simulate some properties of biological neural networks. In the artificial intelligence field: artificial neural networks have been applied successfully to speech recognition: image analysis and adaptive control: in order to construct software agents (in computer and video games) or autonomous robots. Neural network theory has served to identify better how the neurons in the brain function and provide the basis for efforts to create artificial intelligence. == History == The preliminary theoretical base for contemporary neural networks was independently proposed by Alexander Bain (1873) and William James (1890). In their work: both thoughts and body activity resulted from interactions among neurons within the brain. For Bain: every activity led to the firing of a certain set of neurons. When activities were repeated: the connections between those neurons strengthened. According to his theory: this repetition was what led to the formation of memory. The general scientific community at the time was skeptical of Bain's theory because it required what appeared to be an inordinate number of neural connections within the brain. It is now apparent that the brain is exceedingly complex and that the same brain “wiring” can handle multiple problems and inputs. James' theory was similar to Bain's; however: he suggested that memories and actions resulted from electrical currents flowing among the neurons in the brain. His model: by focusing on the flow of electrical currents: did not require individual neural connections for each memory or action. C. S. Sherrington (1898) conducted experiments to test James' theory. He ran electrical currents down the spinal cords of rats. However: instead of demonstrating an increase in electrical current as projected by James: Sherrington found that the electrical current strength decreased as the testing continued over time. Importantly: this work led to the discovery of the concept of habituation. McCulloch and Pitts (1943) also created a computational model for neural networks based on mathematics and algorithms. They called this model threshold logic. These early models paved the way for neural network
1.0181147999828681,What is the difference between graphical and textual modeling languages?,"Graphical modeling languages use a diagram technique with named symbols to represent concepts and lines to depict relationships between symbols and other graphical notations to represent constraints. On the other hand, textual modeling languages use standardized keywords along with parameters or natural language terms and phrases to form computer interpretable expressions.",laws: diffusion process: advection-diffusion systems: and kinetic equations. Given noisy measurements of a generic dynamic system described by the equation above: PINNs can be designed to solve two classes of problems: data-driven solution data-driven discoveryof partial differential equations. === Data-driven solution of partial differential equations === The data-driven solution of PDE computes the hidden state u(t:x){\displaystyle u(t:x)} of the system given boundary data and/or measurements z{\displaystyle z}: and fixed model parameters λ{\displaystyle \lambda }. We solve: ut+N[u]=0:x∈Ω:t∈[0:T]{\displaystyle u_{t}+N[u]=0:\quad x\in \Omega :\quad t\in [0:T]}. By defining the residual f(t:x){\displaystyle f(t:x)} as f:=ut+N[u]=0{\displaystyle f:=u_{t}+N[u]=0}: and approximating u(t:x){\displaystyle u(t:x)} by a deep neural network. This network can be differentiated using automatic differentiation. The parameters of u(t:x){\displaystyle u(t:x)} and f(t:x){\displaystyle f(t:x)} can be then learned by minimizing the following loss function Ltot{\displaystyle L_{tot}}: Ltot=Lu+Lf{\displaystyle L_{tot}=L_{u}+L_{f}}. Where Lu=‖u−z‖Γ{\displaystyle L_{u}=\Vert u-z\Vert _{\Gamma }} is the error between the PINN u(t:x){\displaystyle u(t:x)} and the set of boundary conditions and measured data on the set of points Γ{\displaystyle \Gamma } where the boundary conditions and data are defined: and Lf=‖f‖Γ{\displaystyle L_{f}=\Vert f\Vert _{\Gamma }} is the mean-squared error of the residual function. This second term encourages the PINN to learn the structural information expressed by the partial differential equation during the training process. This approach has been used to yield computationally efficient physics-informed surrogate models with applications in the forecasting of physical processes: model predictive control: multi-physics and multi-scale modeling: and simulation. It has been shown to converge to the solution of the PDE. === Data-driven discovery of partial differential equations === Given noisy and incomplete measurements z{\displaystyle z} of the state of the system: the data-driven discovery of PDE results in computing the unknown state u(t:x){\displaystyle u(t:x)} and learning model parameters λ{\displaystyle \lambda } that best describe the observed data and it reads as follows: ut+N[u;λ]=0:x∈Ω:t∈[0:T]{\displaystyle u_{t}+N[u;\lambda ]=0:\quad x\in \Omega :\quad t\in [0:T]}. By defining f(t:x){\displaystyle f(t:x)} as f:=ut+N[u;λ]=0{\displaystyle f:=u_{t}+N[u;\lambda ]=0}: and approximating u(t:x){\displaystyle u(t:x)} by a deep neural network: f(t:x){\displaystyle f(t:x)} results in a PINN. This network can be derived using automatic differentiation. The parameters of u(t:x){\displaystyle u(t:x)} and f(t:x){\displaystyle f(t:x)}: together with the parameter λ{\displaystyle \lambda } of the differential operator can be then learned by minimizing the following loss function Ltot{\displaystyle L_{tot}}: Ltot=Lu+Lf{\displaystyle L_{tot}=L_{u}+L_{f}}. Where Lu=‖u−z‖Γ{\displaystyle L_{u}=\Vert u-z\Vert _{\Gamma }}: with u{\displaystyle u} and z{\displaystyle z} state solutions and measurements at sparse location Γ{\displaystyle \Gamma }: respectively and Lf=‖f‖Γ{\displaystyle L_{f}=\Vert f\Vert _{\Gamma }} residual function. This second term requires the structured information represented by the partial differential equations to be satisfied in the training process. This strategy allows for discovering dynamic models described by nonlinear PDEs assembling computationally efficient and fully differentiable surrogate models that may find application in predictive forecasting: control: and data assimilation. == Physics-informed neural networks for piece-wise function approximation == PINN is unable to approximate PDEs that have strong non-linearity or sharp gradients that commonly occur in practical fluid flow problems. Piece-wise approximation has been an old practice in the field of numerical approximation. With the capability of approximating strong
0.4174289999937173,How do executable modeling languages benefit programmers?,"Executable modeling languages aim to boost the productivity of skilled programmers. They allow programmers to tackle more complex problems, like parallel computing and distributed systems. However, their use does not imply that the need for programmers is eliminated. ",of universal function approximators that can embed the knowledge of any physical laws that govern a given data-set in the learning process: and can be described by partial differential equations (PDEs). They overcome the low data availability of some biological and engineering systems that makes most state-of-the-art machine learning techniques lack robustness: rendering them ineffective in these scenarios. The prior knowledge of general physical laws acts in the training of neural networks (NNs) as a regularization agent that limits the space of admissible solutions: increasing the correctness of the function approximation. This way: embedding this prior information into a neural network results in enhancing the information content of the available data: facilitating the learning algorithm to capture the right solution and to generalize well even with a low amount of training examples. == Function approximation == Most of the physical laws that govern the dynamics of a system can be described by partial differential equations. For example: the Navier–Stokes equations are a set of partial differential equations derived from the conservation laws (i.e.: conservation of mass: momentum: and energy) that govern fluid mechanics. The solution of the Navier–Stokes equations with appropriate initial and boundary conditions allows the quantification of flow dynamics in a precisely defined geometry. However: these equations cannot be solved exactly and therefore numerical methods must be used (such as finite differences: finite elements and finite volumes). In this setting: these governing equations must be solved while accounting for prior assumptions: linearization: and adequate time and space discretization. Recently: solving the governing partial differential equations of physical phenomena using deep learning has emerged as a new field of scientific machine learning (SciML): leveraging the universal approximation theorem and high expressivity of neural networks. In general: deep neural networks could approximate any high-dimensional function given that sufficient training data are supplied. However: such networks do not consider the physical characteristics underlying the problem: and the level of approximation accuracy provided by them is still heavily dependent on careful specifications of the problem geometry as well as the initial and boundary conditions. Without this preliminary information: the solution is not unique and may lose physical correctness. On the other hand: physics-informed neural networks (PINNs) leverage governing physical equations in neural network training. Namely: PINNs are designed to be trained to satisfy the given training data as well as the imposed governing equations. In this fashion: a neural network can be guided with training data that do not necessarily need to be large and complete. Potentially: an accurate solution of partial differential equations can be found without knowing the boundary conditions. Therefore: with some knowledge about the physical characteristics of the problem and some form of training data (even sparse and incomplete): PINN may be used for finding an optimal solution with high fidelity. PINNs allow for addressing a wide range of problems in computational science and represent a pioneering technology leading to the development of new classes of numerical solvers for PDEs. PINNs can be thought of as a meshfree
0.46551959999487735,What is an example of a graphical modeling language and its corresponding textual modeling language?,EXPRESS is an example of a graphical modeling language and its corresponding textual modeling language.,"effectively process and interpret complex visual information: leading to advancements in fields ranging from automated surveillance to medical imaging. === Speech recognition === By modeling speech signals: ANNs are used for tasks like speaker identification and speech-to-text conversion. Deep neural network architectures have introduced significant improvements in large vocabulary continuous speech recognition: outperforming traditional techniques. These advancements have enabled the development of more accurate and efficient voice-activated systems: enhancing user interfaces in technology products. === Natural language processing === In natural language processing: ANNs are used for tasks such as text classification: sentiment analysis: and machine translation. They have enabled the development of models that can accurately translate between languages: understand the context and sentiment in textual data: and categorize text based on content. This has implications for automated customer service: content moderation: and language understanding technologies. === Control systems === In the domain of control systems: ANNs are used to model dynamic systems for tasks such as system identification: control design: and optimization. For instance: deep feedforward neural networks are important in system identification and control applications. === Finance === ANNs are used for stock market prediction and credit scoring: In investing: ANNs can process vast amounts of financial data: recognize complex patterns: and forecast stock market trends: aiding investors and risk managers in making informed decisions. In credit scoring: ANNs offer data-driven: personalized assessments of creditworthiness: improving the accuracy of default predictions and automating the lending process.ANNs require high-quality data and careful tuning: and their ""black-box"" nature can pose challenges in interpretation. Nevertheless: ongoing advancements suggest that ANNs continue to play a role in finance: offering valuable insights and enhancing risk management strategies. === Medicine === ANNs are able to process and analyze vast medical datasets. They enhance diagnostic accuracy: especially by interpreting complex medical imaging for early disease detection: and by predicting patient outcomes for personalized treatment planning. In drug discovery: ANNs speed up the identification of potential drug candidates and predict their efficacy and safety: significantly reducing development time and costs. Additionally: their application in personalized medicine and healthcare data analysis allows tailored therapies and efficient patient care management. Ongoing research is aimed at addressing remaining challenges such as data privacy and model interpretability: as well as expanding the scope of ANN applications in medicine. === Content creation === ANNs such as generative adversarial networks (GAN) and transformers are used for content creation across numerous industries. This is because deep learning models are able to learn the style of an artist or musician from huge datasets and generate completely new artworks and music compositions. For instance: DALL-E is a deep neural network trained on 650 million pairs of images and texts across the internet that can create artworks based on text entered by the user. In the field of music: transformers are used to create original music for commercials and documentaries through companies such as AIVA and Jukedeck. In the marketing industry generative models are used to create personalized advertisements for consumers. Additionally: major film companies"
0.42498719997820444,What is the role of modeling languages in system development?,"Modeling languages can be used to specify system requirements, structures, and behaviors, offering a precise specification of systems to better understand the system being modeled by stakeholders like customers, operators, analysts, and designers. More mature modeling languages that are precise, consistent, and executable can automate system verification and validation, simulation, and code generation.",biased towards low frequency functions. === Generalization and statistics === Applications whose goal is to create a system that generalizes well to unseen examples: face the possibility of over-training. This arises in convoluted or over-specified systems when the network capacity significantly exceeds the needed free parameters. Two approaches address over-training. The first is to use cross-validation and similar techniques to check for the presence of over-training and to select hyperparameters to minimize the generalization error. The second is to use some form of regularization. This concept emerges in a probabilistic (Bayesian) framework: where regularization can be performed by selecting a larger prior probability over simpler models; but also in statistical learning theory: where the goal is to minimize over two quantities: the 'empirical risk' and the 'structural risk': which roughly corresponds to the error over the training set and the predicted error in unseen data due to overfitting. Supervised neural networks that use a mean squared error (MSE) cost function can use formal statistical methods to determine the confidence of the trained model. The MSE on a validation set can be used as an estimate for variance. This value can then be used to calculate the confidence interval of network output: assuming a normal distribution. A confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified. By assigning a softmax activation function: a generalization of the logistic function: on the output layer of the neural network (or a softmax component in a component-based network) for categorical target variables: the outputs can be interpreted as posterior probabilities. This is useful in classification as it gives a certainty measure on classifications. The softmax activation function is: yi=exi∑j=1cexj{\displaystyle y_{i}={\frac {e^{x_{i}}}{\sum _{j=1}^{c}e^{x_{j}}}}} == Criticism == === Training === A common criticism of neural networks: particularly in robotics: is that they require too many training samples for real-world operation. Any learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases. Potential solutions include randomly shuffling training examples: by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example: grouping examples in so-called mini-batches and/or introducing a recursive least squares algorithm for CMAC. Dean Pomerleau uses a neural network to train a robotic vehicle to drive on multiple types of roads (single lane: multi-lane: dirt: etc.): and a large amount of his research is devoted to extrapolating multiple training scenarios from a single training experience: and preserving past training diversity so that the system does not become overtrained (if: for example: it is presented with a series of right turns—it should not learn to always turn right). === Theory === A central claim of ANNs is that they embody new and powerful general principles for processing information. These principles are ill-defined. It is often claimed that they are emergent from the network itself. This allows simple statistical association (the basic function
0.4046898999949917,What is a neural network and how closely is it related to artificial neural networks?,"A neural network, also known as a neuronal network, is an interconnected population of neurons that typically contain multiple neural circuits. These networks are studied to understand the organization and functioning of nervous systems. Artificial neural networks are machine learning models, which are very closely related to biological neural networks as they are inspired and designed based on the mechanisms used by neural circuits in biological networks.",programming languages. EXPRESS and EXPRESS-G (ISO 10303-11) is an international standard general-purpose data modeling language. Extended Enterprise Modeling Language (EEML) is commonly used for business process modeling across a number of layers. Flowchart is a schematic representation of an algorithm or a stepwise process. Fundamental Modeling Concepts (FMC) modeling language for software-intensive systems. IDEF is a family of modeling languages: which include IDEF0 for functional modeling: IDEF1X for information modeling: IDEF3 for business process modeling: IDEF4 for Object-Oriented Design and IDEF5 for modeling ontologies. Jackson Structured Programming (JSP) is a method for structured programming based on correspondences between data stream structure and program structure. LePUS3 is an object-oriented visual Design Description Language and a formal specification language that is suitable primarily for modeling large object-oriented (Java: C++: C#) programs and design patterns. Lifecycle Modeling Language is an open-standard language for systems engineering that supports the full system lifecycle: conceptual: utilization: support and retirement stages. Object-Role Modeling (ORM) in the field of software engineering is a method for conceptual modeling: and can be used as a tool for information and rules analysis. Petri nets use variations on exactly one diagramming technique and topology: namely the bipartite graph. The simplicity of its basic user interface easily enabled extensive tool support over the years: particularly in the areas of model checking: graphically oriented simulation: and software verification. Southbeach Notation is a visual modeling language used to describe situations in terms of agents that are considered useful or harmful from the modeler's perspective. The notation shows how the agents interact with each other and whether this interaction improves or worsens the situation. Specification and Description Language (SDL) is a specification language targeted at the unambiguous specification and description of the behavior of reactive and distributed systems. SysML is a Domain-Specific Modeling language for systems engineering that is defined as a UML profile (customization). Unified Modeling Language (UML) is a general-purpose modeling language that is an industry standard for specifying software-intensive systems. UML 2.0: the current version: supports thirteen different diagram techniques: and has widespread tool support. Service-oriented modeling framework (SOMF) is a holistic language for designing enterprise and application level architecture models in the space of enterprise architecture: virtualization: service-oriented architecture (SOA): cloud computing: and more. Architecture description language (ADL) is a language used to describe and represent the systems architecture of a system. Architecture Analysis & Design Language (AADL) is a modeling language that supports early and repeated analyses of a system's architecture with respect to performance-critical properties through an extendable notation: a tool framework: and precisely defined semantics.Examples of graphical modeling languages in other fields of science. EAST-ADL is a Domain-Specific Modeling language dedicated to automotive system design. Energy Systems Language (ESL): a language that aims to model ecological energetics & global economics. IEC 61499 defines Domain-Specific Modeling language dedicated to distribute industrial process measurement and control systems. === Textual types === Information models can also be expressed in formalized natural languages: such as Gellish. Gellish has natural language variants such as
0.43371970002772287,What are some applications of artificial neural networks in the field of artificial intelligence?,"In the field of artificial intelligence, artificial neural networks have been applied successfully to several areas including speech recognition, image analysis, and adaptive control. They are also used to create software agents in computer and video games or in the construction of autonomous robots.","models: OpenAI published its first versions of GPT-3 in July 2020. There were three models: with 1B: 6.7B: 175B parameters: respectively named babbage: curie: and davinci (giving initials B: C: and D).In July 2021: OpenAI published Codex: a task-specific GPT model targeted for programming applications. This was developed by fine-tuning a 12B parameter version of GPT-3 (different from previous GPT-3 models) using code from GitHub.In March 2022: OpenAI published two versions of GPT-3 that were fine-tuned for instruction-following (instruction-tuned): named davinci-instruct-beta (175B) and text-davinci-001: and then started beta testing code-davinci-002. text-davinci-002 was instruction-tuned from code-davinci-002. Both text-davinci-003 and ChatGPT were released in November 2022: with both building upon text-davinci-002 via reinforcement learning from human feedback (RLHF). text-davinci-003 is trained for following instructions (like its predecessors): whereas ChatGPT is further trained for conversational interaction with a human user.OpenAI's most recent GPT foundation model: GPT-4: was released on March 14: 2023. It can be accessed directly by users via a premium version of ChatGPT: and is available to developers for incorporation into other products and services via OpenAI's API. Other producers of GPT foundation models include EleutherAI (with a series of models starting in March 2021) and Cerebras (with seven models released in March 2023). == Foundational models == A foundational model is an AI model trained on broad data at scale such that it can be adapted to a wide range of downstream tasks.Thus far: the most notable GPT foundation models have been from OpenAI's GPT-n series. The most recent from that is GPT-4: for which OpenAI declined to publish the size or training details (citing ""the competitive landscape and the safety implications of large-scale models""). Other such models include Google's PaLM: a broad foundation model that has been compared to GPT-3 and has recently been made available to developers via an API: and Together's GPT-JT: which has been reported as the closest-performing open-source alternative to GPT-3 (and is derived from earlier open-source GPTs). Meta AI (formerly Facebook) also has a generative transformer-based foundational large language model: known as LLaMA.Foundational GPTs can also employ modalities other than text: for input and/or output. GPT-4 is a multi-modal LLM that is capable of processing text and image input (though its output is limited to text). Regarding multimodal output: some generative transformer-based models are used for text-to-image technologies such as diffusion and parallel decoding. Such kinds of models can serve as visual foundation models (VFMs) for developing downstream systems that can work with images. == Task-specific models == A foundational GPT model can be further adapted to produce more targeted systems directed to specific tasks and/or subject-matter domains. Methods for such adaptation can include additional fine-tuning (beyond that done for the foundation model) as well as certain forms of prompt engineering.An important example of this is fine-tuning models to follow instructions: which is of course a fairly broad task but more targeted than a foundation model. In January 2022: OpenAI introduced ""InstructGPT""—a series of models which were fine-tuned to follow instructions using a combination"
0.9936717000091448,What theories did Alexander Bain and William James propose regarding neural networks?,"Alexander Bain proposed that every activity led to the firing of a certain set of neurons and when activities were repeated, the connections between those neurons strengthened, leading to the formation of memory. William James suggested that memories and actions resulted from electrical currents flowing among the neurons in the brain and did not require individual neural connections for each memory or action.",individually respond to small regions of the visual field. The neocognitron introduced the two basic types of layers in CNNs: convolutional layers: and downsampling layers. A convolutional layer contains units whose receptive fields cover a patch of the previous layer. The weight vector (the set of adaptive parameters) of such a unit is often called a filter. Units can share filters. Downsampling layers contain units whose receptive fields cover patches of previous convolutional layers. Such a unit typically computes the average of the activations of the units in its patch. This downsampling helps to correctly classify objects in visual scenes even when the objects are shifted. In 1969: Kunihiko Fukushima also introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for CNNs and deep neural networks in general.The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel and was one of the first CNNs: as it achieved shift invariance. It did so by utilizing weight sharing in combination with backpropagation training. Thus: while also using a pyramidal structure as in the neocognitron: it performed a global optimization of the weights instead of a local one.In 1988: Wei Zhang et al. applied backpropagation to a CNN (a simplified Neocognitron with convolutional interconnections between the image feature layers and the last fully connected layer) for alphabet recognition. They also proposed an implementation of the CNN with an optical computing system.In 1989: Yann LeCun et al. trained a CNN with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked: training required 3 days. Learning was fully automatic: performed better than manual coefficient design: and was suited to a broader range of image recognition problems and image types. Subsequently: Wei Zhang: et al. modified their model by removing the last fully connected layer and applied it for medical image object segmentation in 1991 and breast cancer detection in mammograms in 1994.In 1990 Yamaguchi et al. introduced max-pooling: a fixed filtering operation that calculates and propagates the maximum value of a given region. They combined TDNNs with max-pooling in order to realize a speaker independent isolated word recognition system. In a variant of the neocognitron called the cresceptron: instead of using Fukushima's spatial averaging: J. Weng et al. also used max-pooling where a downsampling unit computes the maximum of the activations of the units in its patch. Max-pooling is often used in modern CNNs.LeNet-5: a 7-level CNN by Yann LeCun et al. in 1998: that classifies digits: was applied by several banks to recognize hand-written numbers on checks (British English: cheques) digitized in 32x32 pixel images. The ability to process higher-resolution images requires larger and more layers of CNNs: so this technique is constrained by the availability of computing resources. In 2010: Backpropagation training through max-pooling was accelerated by GPUs and shown to perform better than other pooling variants. Behnke (2003) relied only on the sign of the gradient (Rprop) on problems such as image reconstruction and face localization. Rprop
0.4346737000159919,Who were McCulloch and Pitts and what was their contribution to neural network research?,"McCulloch and Pitts were researchers who, in 1943, created a computational model for neural networks based on mathematics and algorithms. They called this model threshold logic. Their work contributed significantly to the evolution of neural network research, which split into two distinct approaches - one focusing on biological processes in the brain, and the other on the application of neural networks to artificial intelligence.","a generative transformer-based foundational large language model: known as LLaMA.Foundational GPTs can also employ modalities other than text: for input and/or output. GPT-4 is a multi-modal LLM that is capable of processing text and image input (though its output is limited to text). Regarding multimodal output: some generative transformer-based models are used for text-to-image technologies such as diffusion and parallel decoding. Such kinds of models can serve as visual foundation models (VFMs) for developing downstream systems that can work with images. == Task-specific models == A foundational GPT model can be further adapted to produce more targeted systems directed to specific tasks and/or subject-matter domains. Methods for such adaptation can include additional fine-tuning (beyond that done for the foundation model) as well as certain forms of prompt engineering.An important example of this is fine-tuning models to follow instructions: which is of course a fairly broad task but more targeted than a foundation model. In January 2022: OpenAI introduced ""InstructGPT""—a series of models which were fine-tuned to follow instructions using a combination of supervised training and reinforcement learning from human feedback (RLHF) on base GPT-3 language models. Advantages this had over the bare foundational models included higher accuracy: less negative/toxic sentiment: and generally better alignment with user needs. Hence: OpenAI began using this as the basis for its API service offerings. Other instruction-tuned models have been released by others: including a fully open version.Another (related) kind of task-specific models are chatbots: which engage in human-like conversation. In November 2022: OpenAI launched ChatGPT—an online chat interface powered by an instruction-tuned language model trained in a similar fashion to InstructGPT. They trained this model using RLHF: with human AI trainers providing conversations in which they played both the user and the AI: and mixed this new dialogue dataset with the InstructGPT dataset for a conversational format suitable for a chatbot. Other major chatbots currently include Microsoft's Bing Chat: which uses OpenAI's GPT-4 (as part of a broader close collaboration between OpenAI and Microsoft): and Google's competing chatbot Bard (initially based on their LaMDA family of conversation-trained language models: with plans to switch to PaLM).Yet another kind of task that a GPT can be used for is the meta-task of generating its own instructions: like developing a series of prompts for 'itself' to be able to effectuate a more general goal given by a human user. This is known as an AI agent: and more specifically a recursive one because it uses results from its previous self-instructions to help it form its subsequent prompts; the first major example of this was Auto-GPT (which uses OpenAI's GPT models): and others have since been developed as well. === Multimodality === Generative transformer-based systems can also be targeted to tasks involving modalities beyond text. For example: Microsoft’s “Visual ChatGPT” combines ChatGPT with visual foundation models (VFMs) to enable input or output comprising images as well as text. Also: advances in text-to-speech technology offer powerful tools for audio content creation when used in conjunction with foundational GPT language"
0.4713970999873709,What are some recent improvements in the research of neural networks?,"Recent improvements in the research of neural networks have mostly been concerned with the exploration of the role of neuromodulators, such as dopamine, acetylcholine, and serotonin, on behavior and learning. Biophysical models, like BCM theory, have been instrumental for understanding synaptic plasticity mechanisms, and have influenced both computer science and neuroscience.","with full- or half-precision floating point numbers (float32 and float16). One float16 has 16 bits: or 2 bytes: and so one billion parameters require 2 gigabytes. The largest models typically have 100 billion parameters: requiring 200 gigabytes to load: which places them outside the range of most consumer electronics.Post-training quantization aims to decrease the space requirement by lowering precision of the parameters of a trained model: while preserving most of its performance. The simplest form of quantization simply truncates all numbers to a given number of bits. It can be improved by using a different quantization codebook per layer. Further improvement can be done by applying different precisions to different parameters: with higher precision for particularly important parameters (""outlier weights"").While quantized models are typically frozen: and only pre-quantized models are fine-tuned: quantized models can still be fine-tuned. == Multimodality == Multimodality means ""having several modalities"": and a ""modality"" refers to a type of input or output: such as video: image: audio: text: proprioception: etc. There have been many AI models trained specifically to ingest one modality and output another modality: such as AlexNet for image to label: visual question answering for image-text to text: and speech recognition for speech to text. A common method to create multimodal models out of an LLM is to ""tokenize"" the output of a trained encoder. Concretely: one can construct a LLM that can understand images as follows: take a trained LLM: and take a trained image encoder E{\displaystyle E}. Make a small multilayered perceptron f{\displaystyle f}: so that for any image y{\displaystyle y}: the post-processed vector f(E(y)){\displaystyle f(E(y))} has the same dimensions as an encoded token. That is an ""image token"". Then: one can interleave text tokens and image tokens. The compound model is then fine-tuned on an image-text dataset. This basic construction can be applied with more sophistication to improve the model. The image encoder may be frozen to improve stability.Flamingo demonstrated the effectiveness of the tokenization method: finetuning a pair of pretrained language model and image encoder to perform better on visual question answering than models trained from scratch. Google PaLM model was fine-tuned into a multimodal model PaLM-E using the tokenization method: and applied to robotic control. LLaMA models have also been turned multimodal using the tokenization method: to allow image inputs: and video inputs.GPT-4 can use both text and image as inputs (although the vision component wasn't released to the public until GPT-4V); Google DeepMind's Gemini is also multimodal. == Properties == === Scaling laws === The following four hyper-parameters characterize a LLM: cost of (pre-)training (C{\displaystyle C}): size of the artificial neural network itself: such as number of parameters N{\displaystyle N} (i.e. amount of neurons in its layers: amount of weights between them and biases): size of its (pre-)training dataset (i.e. number of tokens in corpus: D{\displaystyle D}): performance after (pre-)training.They are related by simple statistical laws: called ""scaling laws"". One particular scaling law (""Chinchilla scaling"") for LLM autoregressively trained for one epoch: with a log-log learning rate schedule:"
0.4398143000144046,What is a neural network?,"A neural network refers to a group of interconnected units, or neurons, that send signals to each other. These neurons can be either biological cells or mathematical models. A complex network of neurons can execute complex tasks. There are two main types of such networks - biological neural networks seen in brains and complex nervous systems, and artificial neural networks, which are mathematical models used in machine learning for approximating nonlinear functions.","information processing in biological systems through the framework of connectionism. Unlike the von Neumann model: connectionist computing does not separate memory and processing. The simplest kind of feedforward neural network (FNN) is a linear network: which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated at each node. The mean squared errors between these calculated outputs and the given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement.Warren McCulloch and Walter Pitts (1943) also considered a non-learning computational model for neural networks.In the late 1940s: D. O. Hebb created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Hebbian learning is considered to be a 'typical' unsupervised learning rule and its later variants were early models for long term potentiation. These ideas started being applied to computational models in 1948 with Turing's ""unorganized machines"". Farley and Wesley A. Clark were the first to simulate a Hebbian network in 1954 at MIT. They used computational machines: then called ""calculators"". Other neural network computational machines were created by Rochester: Holland: Habit: and Duda in 1956. In 1958: psychologist Frank Rosenblatt invented the perceptron: the first implemented artificial neural network: funded by the United States Office of Naval Research. The invention of the perceptron raised public excitement for research in Artificial Neural Networks: causing the US government to drastically increase funding into deep learning research. This led to ""the golden age of AI"" fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence. For example: in 1957 Herbert Simon famously said:It is not my aim to surprise or shock you—but the simplest way I can summarize is to say that there are now in the world machines that think: that learn and that create. Moreover: their ability to do these things is going to increase rapidly until—in a visible future—the range of problems they can handle will be coextensive with the range to which the human mind has been applied.However: this wasn't the case as research stagnated in the United States following the work of Minsky and Papert (1969): who discovered that basic perceptrons were incapable of processing the exclusive-or circuit and that computers lacked sufficient power to train useful neural networks. This: along with other factors such as the 1973 Lighthill report by James Lighthill stating that research in Artificial Intelligence has not ""produced the major impact that was then promised:"" shutting funding in research into the field of AI in all but two universities in the UK and in many major"
0.9567056999949273,What is the difference between a biological neural network and an artificial neural network?,"A biological neural network comprises biological neurons that are chemically connected to each other by synapses. These neurons send and receive electrochemical signals. An artificial neural network, on the other hand, is a mathematical model that approximates non-linear functions. These networks are mostly used in software form to help solve artificial intelligence problems.",_{h}(W_{h}x_{t}+U_{h}h_{t-1}+b_{h})\\y_{t}&=\sigma _{y}(W_{y}h_{t}+b_{y})\end{aligned}}} Jordan network ht=σh(Whxt+Uhyt−1+bh)yt=σy(Wyht+by){\displaystyle {\begin{aligned}h_{t}&=\sigma _{h}(W_{h}x_{t}+U_{h}y_{t-1}+b_{h})\\y_{t}&=\sigma _{y}(W_{y}h_{t}+b_{y})\end{aligned}}}Variables and functions xt{\displaystyle x_{t}}: input vector ht{\displaystyle h_{t}}: hidden layer vector yt{\displaystyle y_{t}}: output vector W{\displaystyle W}: U{\displaystyle U} and b{\displaystyle b}: parameter matrices and vector σh{\displaystyle \sigma _{h}} and σy{\displaystyle \sigma _{y}}: Activation functions === Hopfield === The Hopfield network is an RNN in which all connections across layers are equally sized. It requires stationary inputs and is thus not a general RNN: as it does not process sequences of patterns. However: it guarantees that it will converge. If the connections are trained using Hebbian learning: then the Hopfield network can perform as robust content-addressable memory: resistant to connection alteration. ==== Bidirectional associative memory ==== Introduced by Bart Kosko: a bidirectional associative memory (BAM) network is a variant of a Hopfield network that stores associative data as a vector. The bi-directionality comes from passing information through a matrix and its transpose. Typically: bipolar encoding is preferred to binary encoding of the associative pairs. Recently: stochastic BAM models using Markov stepping were optimized for increased network stability and relevance to real-world applications.A BAM network has two layers: either of which can be driven as an input to recall an association and produce an output on the other layer. === Echo state === Echo state networks (ESN) have a sparsely connected random hidden layer. The weights of output neurons are the only part of the network that can change (be trained). ESNs are good at reproducing certain time series. A variant for spiking neurons is known as a liquid state machine. === Independently RNN (IndRNN) === The independently recurrent neural network (IndRNN) addresses the gradient vanishing and exploding problems in the traditional fully connected RNN. Each neuron in one layer only receives its own past state as context information (instead of full connectivity to all other neurons in this layer) and thus neurons are independent of each other's history. The gradient backpropagation can be regulated to avoid gradient vanishing and exploding in order to keep long or short-term memory. The cross-neuron information is explored in the next layers. IndRNN can be robustly trained with non-saturated nonlinear functions such as ReLU. Deep networks can be trained using skip connections. === Recursive === A recursive neural network is created by applying the same set of weights recursively over a differentiable graph-like structure by traversing the structure in topological order. Such networks are typically also trained by the reverse mode of automatic differentiation. They can process distributed representations of structure: such as logical terms. A special case of recursive neural networks is the RNN whose structure corresponds to a linear chain. Recursive neural networks have been applied to natural language processing. The Recursive Neural Tensor Network uses a tensor-based composition function for all nodes in the tree. === Neural history compressor === The neural history compressor is an unsupervised stack of RNNs. At the input level: it learns to predict its next input from the previous inputs. Only unpredictable inputs of some RNN in the hierarchy
0.46302950000972487,How do neurons in an artificial neural network operate?,"In artificial neural networks, neurons are arranged into layers, with information passing through from the input layer to the output layer, through one or more intermediate or hidden layers. Each neuron receives an input signal - a number based on the outputs of neurons from the previous layer. The signal a neuron puts out is computed based on this number and its activation function. The strengths, or weights, of the connections between neurons shape the behavior of the network. Training a network involves modifying these weights to reach the desired output.","also exhibit political biases. Since the training data includes a wide range of political opinions and coverage: the models might generate responses that lean towards particular political ideologies or viewpoints: depending on the prevalence of those views in the data. == List == For the training cost column: 1 petaFLOP-day = 1 petaFLOP/sec × 1 day = 8.64E19 FLOP. == See also == Foundation models == Notes == == References == == Further reading == Jurafsky: Dan: Martin: James. H. Speech and Language Processing: An Introduction to Natural Language Processing: Computational Linguistics: and Speech Recognition: 3rd Edition draft: 2023. Phuong: Mary; Hutter: Marcus (2022). ""Formal Algorithms for Transformers"". arXiv:2207.09238 [cs.LG]. Eloundou: Tyna; Manning: Sam; Mishkin: Pamela; Rock: Daniel (2023). ""GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models"". arXiv:2303.10130 [econ.GN]. Eldan: Ronen; Li: Yuanzhi (2023). ""TinyStories: How Small Can Language Models Be and Still Speak Coherent English?"". arXiv:2305.07759 [cs.CL]. Frank: Michael C. (27 June 2023). ""Baby steps in evaluating the capacities of large language models"". Nature Reviews Psychology. 2 (8): 451–452. doi:10.1038/s44159-023-00211-x. ISSN 2731-0574. S2CID 259713140. Retrieved 2 July 2023. Zhao: Wayne Xin; et al. (2023). ""A Survey of Large Language Models"". arXiv:2303.18223 [cs.CL]. Kaddour: Jean; et al. (2023). ""Challenges and Applications of Large Language Models"". arXiv:2307.10169 [cs.CL]. Yin: Shukang; Fu: Chaoyou; Zhao: Sirui; Li: Ke; Sun: Xing; Xu: Tong; Chen: Enhong (2023-06-01). ""A Survey on Multimodal Large Language Models"". arXiv:2306.13549 [cs.CV]. Open LLMs repository on GitHub. A large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification. LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process. LLMs can be used for text generation: a form of generative AI: by taking an input text and repeatedly predicting the next token or word.LLMs are artificial neural networks. The largest and most capable: as of March 2024: are built with a decoder-only transformer-based architecture while some recent implementations are based on other architectures: such as recurrent neural network variants and Mamba (a state space model).Up to 2020: fine tuning was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models: such as GPT-3: however: can be prompt-engineered to achieve similar results. They are thought to acquire knowledge about syntax: semantics and ""ontology"" inherent in human language corpora: but also inaccuracies and biases present in the corpora.Some notable LLMs are OpenAI's GPT series of models (e.g.: GPT-3.5 and GPT-4: used in ChatGPT and Microsoft Copilot): Google's PaLM and Gemini (the latter of which is currently used in the chatbot of the same name): xAI's Grok: Meta's LLaMA family of open-source models: Anthropic's Claude models: and Mistral AI's open source models. == History == At the 2017 NeurIPS conference: Google researchers introduced the transformer architecture in their landmark paper ""Attention Is All You Need"". This paper's goal was to improve upon 2014"
0.4193737999885343,How do signals in a biological neural network function?,"In a biological neural network, each neuron sends and receives electrochemical signals known as action potentials. Depending on its role, a neuron can either amplify and propagate or suppress these signals. These signals travel through the nervous system to muscle cells, inducing contraction and subsequent motion.","in complexity to recognizers of context free grammars (CFGs). === Memristive networks === Greg Snider of HP Labs describes a system of cortical computing with memristive nanodevices. The memristors (memory resistors) are implemented by thin film materials in which the resistance is electrically tuned via the transport of ions or oxygen vacancies within the film. DARPA's SyNAPSE project has funded IBM Research and HP Labs: in collaboration with the Boston University Department of Cognitive and Neural Systems (CNS): to develop neuromorphic architectures that may be based on memristive systems. Memristive networks are a particular type of physical neural network that have very similar properties to (Little-)Hopfield networks: as they have continuous dynamics: a limited memory capacity and natural relaxation via the minimization of a function which is asymptotic to the Ising model. In this sense: the dynamics of a memristive circuit have the advantage compared to a Resistor-Capacitor network to have a more interesting non-linear behavior. From this point of view: engineering analog memristive networks account for a peculiar type of neuromorphic engineering in which the device behavior depends on the circuit wiring or topology. The evolution of these networks can be studied analytically using variations of the Caravelli–Traversa–Di Ventra equation. == Pseudocode == Given a time series x of length sequence_length. In the recurrent neural network: there is a loop that processes all entries of the time series x through the layers neural_network one after another. These have as return value in each time step i both the prediction y_pred[i] and an updated hidden state hidden: which has the length hidden_size. As a result: after the loop: the collection of all predictions y_pred is returned. The following pseudocode (based on the programming language Python) illustrates the functionality of a recurrent neural network. Modern libraries provide runtime-optimized implementations of the above functionality or allow to speed up the slow loop by just-in-time compilation. == Training == === Gradient descent === Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function. In neural networks: it can be used to minimize the error term by changing each weight in proportion to the derivative of the error with respect to that weight: provided the non-linear activation functions are differentiable. Various methods for doing so were developed in the 1980s and early 1990s by Werbos: Williams: Robinson: Schmidhuber: Hochreiter: Pearlmutter and others. The standard method is called ""backpropagation through time"" or BPTT: and is a generalization of back-propagation for feed-forward networks. Like that method: it is an instance of automatic differentiation in the reverse accumulation mode of Pontryagin's minimum principle. A more computationally expensive online variant is called ""Real-Time Recurrent Learning"" or RTRL: which is an instance of automatic differentiation in the forward accumulation mode with stacked tangent vectors. Unlike BPTT: this algorithm is local in time but not local in space. In this context: local in space means that a unit's weight vector can be updated using only information stored in the connected units and the unit itself"
0.41128980001667514,How has the application of artificial neural networks evolved?,"Artificial neural networks were initially developed to model biological neural networks under the concept of connectionism in the 1930s. However, with the development of the perceptron, a simple artificial neural network, around 1943 and subsequent implementations in hardware, they started being utilized more for machine learning applications, and thereby deviated considerably from their biological counterparts.","distribution of interest: while the discriminative network distinguishes candidates produced by the generator from the true data distribution. The generative network's training objective is to increase the error rate of the discriminative network (i.e.: ""fool"" the discriminator network by producing novel candidates that the discriminator thinks are not synthesized (are part of the true data distribution)).A known dataset serves as the initial training data for the discriminator. Training involves presenting it with samples from the training dataset until it achieves acceptable accuracy. The generator is trained based on whether it succeeds in fooling the discriminator. Typically: the generator is seeded with randomized input that is sampled from a predefined latent space (e.g. a multivariate normal distribution). Thereafter: candidates synthesized by the generator are evaluated by the discriminator. Independent backpropagation procedures are applied to both networks so that the generator produces better samples: while the discriminator becomes more skilled at flagging synthetic samples. When used for image generation: the generator is typically a deconvolutional neural network: and the discriminator is a convolutional neural network. === Relation to other statistical machine learning methods === GANs are implicit generative models: which means that they do not explicitly model the likelihood function nor provide a means for finding the latent variable corresponding to a given sample: unlike alternatives such as flow-based generative model. Compared to fully visible belief networks such as WaveNet and PixelRNN and autoregressive models in general: GANs can generate one complete sample in one pass: rather than multiple passes through the network. Compared to Boltzmann machines and nonlinear ICA: there is no restriction on the type of function used by the network. Since neural networks are universal approximators: GANs are asymptotically consistent. Variational autoencoders might be universal approximators: but it is not proven as of 2017. == Mathematical properties == === Measure-theoretic considerations === This section provides some of the mathematical theory behind these methods. In modern probability theory based on measure theory: a probability space also needs to be equipped with a σ-algebra. As a result: a more rigorous definition of the GAN game would make the following changes:Each probability space (Ω:B:μref){\displaystyle (\Omega :{\mathcal {B}}:\mu _{\text{ref}})} defines a GAN game. The generator's strategy set is P(Ω:B){\displaystyle {\mathcal {P}}(\Omega :{\mathcal {B}})}: the set of all probability measures μG{\displaystyle \mu _{G}} on the measure-space (Ω:B){\displaystyle (\Omega :{\mathcal {B}})}. The discriminator's strategy set is the set of Markov kernels μD:(Ω:B)→P([0:1]:B([0:1])){\displaystyle \mu _{D}:(\Omega :{\mathcal {B}})\to {\mathcal {P}}([0:1]:{\mathcal {B}}([0:1]))}: where B([0:1]){\displaystyle {\mathcal {B}}([0:1])} is the Borel σ-algebra on [0:1]{\displaystyle [0:1]}.Since issues of measurability never arise in practice: these will not concern us further. === Choice of the strategy set === In the most generic version of the GAN game described above: the strategy set for the discriminator contains all Markov kernels μD:Ω→P[0:1]{\displaystyle \mu _{D}:\Omega \to {\mathcal {P}}[0:1]}: and the strategy set for the generator contains arbitrary probability distributions μG{\displaystyle \mu _{G}} on Ω{\displaystyle \Omega }. However: as shown below: the optimal discriminator strategy against any μG{\displaystyle \mu _{G}} is deterministic: so there is no loss"
0.6887854000087827,What is an optical neural network?,An optical neural network is a physical implementation of an artificial neural network with optical components. It uses the strength of the optical interconnect for implementing neuronal communications.,algorithm are selected appropriately: the resulting ANN can become robust.Neural architecture search (NAS) uses machine learning to automate ANN design. Various approaches to NAS have designed networks that compare well with hand-designed systems. The basic search algorithm is to propose a candidate model: evaluate it against a dataset: and use the results as feedback to teach the NAS network. Available systems include AutoML and AutoKeras. scikit-learn library provides functions to help with building a deep network from scratch. We can then implement a deep network with TensorFlow or Keras. Hyperparameters must also be defined as part of the design (they are not learned): governing matters such as how many neurons are in each layer: learning rate: step: stride: depth: receptive field and padding (for CNNs): etc. The Python code snippet provides an overview of the training function: which uses the training dataset: number of hidden layer units: learning rate: and number of iterations as parameters: == Applications == Because of their ability to reproduce and model nonlinear processes: artificial neural networks have found applications in many disciplines. These include: Function approximation: or regression analysis: (including time series prediction: fitness approximation: and modeling) Data processing (including filtering: clustering: blind source separation: and compression) Nonlinear system identification and control (including vehicle control: trajectory prediction: adaptive control: process control: and natural resource management) Pattern recognition (including radar systems: face identification: signal classification: novelty detection: 3D reconstruction: object recognition: and sequential decision making) Sequence recognition (including gesture: speech: and handwritten and printed text recognition) Sensor data analysis (including image analysis) Robotics (including directing manipulators and prostheses) Data mining (including knowledge discovery in databases) Finance (such as ex-ante models for specific financial long-run forecasts and artificial financial markets) Quantum chemistry General game playing Generative AI Data visualization Machine translation Social network filtering E-mail spam filtering Medical diagnosisANNs have been used to diagnose several types of cancers and to distinguish highly invasive cancer cell lines from less invasive lines using only cell shape information.ANNs have been used to accelerate reliability analysis of infrastructures subject to natural disasters and to predict foundation settlements. It can also be useful to mitigate flood by the use of ANNs for modelling rainfall-runoff. ANNs have also been used for building black-box models in geoscience: hydrology: ocean modelling and coastal engineering: and geomorphology. ANNs have been employed in cybersecurity: with the objective to discriminate between legitimate activities and malicious ones. For example: machine learning has been used for classifying Android malware: for identifying domains belonging to threat actors and for detecting URLs posing a security risk. Research is underway on ANN systems designed for penetration testing: for detecting botnets: credit cards frauds and network intrusions. ANNs have been proposed as a tool to solve partial differential equations in physics and simulate the properties of many-body open quantum systems. In brain research ANNs have studied short-term behavior of individual neurons: the dynamics of neural circuitry arise from interactions between individual neurons and how behavior can arise from abstract neural modules that represent complete
1.1325081000104547,What are some types of artificial neural networks that have been implemented as optical neural networks?,"Some artificial neural networks that have been implemented as optical neural networks include the Hopfield neural network and the Kohonen self-organizing map, often with liquid crystal spatial light modulators.","the output of each neuron is computed by some non-linear function of the sum of its inputs: called the activation function. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Typically: neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer): possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least 2 hidden layers.Artificial neural networks are used for predictive modeling: adaptive control: and other applications where they can be trained via a dataset. They are also used to solve problems in artificial intelligence. Networks can learn from experience: and can derive conclusions from a complex and seemingly unrelated set of information. == Training == Neural networks are typically trained through empirical risk minimization. This method is based on the idea of optimizing the network's parameters to minimize the difference: or empirical risk: between the predicted output and the actual target values in a given dataset. Gradient based methods such as backpropagation are usually used to estimate the parameters of the network. During the training phase: ANNs learn from labeled training data by iteratively updating their parameters to minimize a defined loss function. This method allows the network to generalize to unseen data. == History == Historically: digital computers evolved from the von Neumann model: and operate via the execution of explicit instructions via access to memory by a number of processors. Neural networks: on the other hand: originated from efforts to model information processing in biological systems through the framework of connectionism. Unlike the von Neumann model: connectionist computing does not separate memory and processing. The simplest kind of feedforward neural network (FNN) is a linear network: which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated at each node. The mean squared errors between these calculated outputs and the given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement.Warren McCulloch and Walter Pitts (1943) also considered a non-learning computational model for neural networks.In the late 1940s: D. O. Hebb created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Hebbian learning is considered to be a 'typical' unsupervised learning rule and its later variants were early models for long term potentiation. These ideas started being applied to computational models in 1948 with Turing's ""unorganized machines"". Farley and Wesley"
0.45164039998780936,How do biological neural networks differ from optical neural networks?,"Biological neural networks function on an electrochemical basis, while optical neural networks use electromagnetic waves. The mechanisms for dynamically changing the state of the neurons in a biological network include short-term and long-term synaptic plasticity.",optimization techniques may be used to seek a good set of weights: such as simulated annealing or particle swarm optimization. == Related fields and models == RNNs may behave chaotically. In such cases: dynamical systems theory may be used for analysis. They are in fact recursive neural networks with a particular structure: that of a linear chain. Whereas recursive neural networks operate on any hierarchical structure: combining child representations into parent representations: recurrent neural networks operate on the linear progression of time: combining the previous time step and a hidden representation into the representation for the current time step. In particular: RNNs can appear as nonlinear versions of finite impulse response and infinite impulse response filters and also as a nonlinear autoregressive exogenous model (NARX).The effect of memory-based learning for the recognition of sequences can also be implemented by a more biological-based model which uses the silencing mechanism exhibited in neurons with a relatively high frequency spiking activity. == Libraries == Apache Singa Caffe: Created by the Berkeley Vision and Learning Center (BVLC). It supports both CPU and GPU. Developed in C++: and has Python and MATLAB wrappers. Chainer: Fully in Python: production support for CPU: GPU: distributed training. Deeplearning4j: Deep learning in Java and Scala on multi-GPU-enabled Spark. Flux: includes interfaces for RNNs: including GRUs and LSTMs: written in Julia. Keras: High-level API: providing a wrapper to many other deep learning libraries. Microsoft Cognitive Toolkit MXNet: an open-source deep learning framework used to train and deploy deep neural networks. PyTorch: Tensors and Dynamic neural networks in Python with GPU acceleration. TensorFlow: Apache 2.0-licensed Theano-like library with support for CPU: GPU and Google's proprietary TPU: mobile Theano: A deep-learning library for Python with an API largely compatible with the NumPy library. Torch: A scientific computing framework with support for machine learning algorithms: written in C and Lua. == Applications == Applications of recurrent neural networks include: Machine translation Robot control Time series prediction Speech recognition Speech synthesis Brain–computer interfaces Time series anomaly detection Text-to-Video model Rhythm learning Music composition Grammar learning Handwriting recognition Human action recognition Protein homology detection Predicting subcellular localization of proteins Several prediction tasks in the area of business process management Prediction in medical care pathways Predictions of fusion plasma disruptions in reactors (Fusion Recurrent Neural Network (FRNN) code) == References == == Further reading == Mandic: Danilo P.; Chambers: Jonathon A. (2001). Recurrent Neural Networks for Prediction: Learning Algorithms: Architectures and Stability. Wiley. ISBN 978-0-471-49517-8. == External links == Recurrent Neural Networks with over 60 RNN papers by Jürgen Schmidhuber's group at Dalle Molle Institute for Artificial Intelligence Research Elman Neural Network implementation for WEKA A residual neural network (also referred to as a residual network or ResNet) is a deep learning model in which the weight layers learn residual functions with reference to the layer inputs. It behaves like a highway network whose gates are opened through strongly positive bias weights. This enables deep learning models with tens or hundreds of layers to train easily
0.43015489997924305,What are the two primary methods of optical neural computing currently under research?,"The two primary methods of optical neural computing under research are silicon photonics-based and free-space optics. Silicon photonics offer superior speed, while free-space optics deliver massive parallelism.","generate unique and appealing abstract paintings: and thus dubbed a ""CAN"": for ""creative adversarial network"". A GAN system was used to create the 2018 painting Edmond de Belamy: which sold for US$432:500. An early 2019 article by members of the original CAN team discussed further progress with that system: and gave consideration as well to the overall prospects for an AI-enabled art.In May 2019: researchers at Samsung demonstrated a GAN-based system that produces videos of a person speaking: given only a single photo of that person.In August 2019: a large dataset consisting of 12:197 MIDI songs each with paired lyrics and melody alignment was created for neural melody generation from lyrics using conditional GAN-LSTM (refer to sources at GitHub AI Melody Generation from Lyrics).In May 2020: Nvidia researchers taught an AI system (termed ""GameGAN"") to recreate the game of Pac-Man simply by watching it being played. == References == == External links == Knight: Will. ""5 Big Predictions for Artificial Intelligence in 2017"". MIT Technology Review. Retrieved January 5: 2017. Karras: Tero; Laine: Samuli; Aila: Timo (2018). ""A Style-Based Generator Architecture for Generative Adversarial Networks"". arXiv:1812.04948 [cs.NE]. This Person Does Not Exist – photorealistic images of people who do not exist: generated by StyleGAN This Cat Does Not Exist Archived March 5: 2019: at the Wayback Machine – photorealistic images of cats who do not exist: generated by StyleGAN Wang: Zhengwei; She: Qi; Ward: Tomas E. (2019). ""Generative Adversarial Networks in Computer Vision: A Survey and Taxonomy"". arXiv:1906.01529 [cs.LG]. Generative pre-trained transformers (GPT) are a type of large language model (LLM) and a prominent framework for generative artificial intelligence. They are artificial neural networks that are used in natural language processing tasks. GPTs are based on the transformer architecture: pre-trained on large data sets of unlabelled text: and able to generate novel human-like content. As of 2023: most LLMs have these characteristics and are sometimes referred to broadly as GPTs.The first GPT was introduced in 2018 by OpenAI. OpenAI has released very influential GPT foundation models that have been sequentially numbered: to comprise its ""GPT-n"" series. Each of these was significantly more capable than the previous: due to increased size (number of trainable parameters) and training. The most recent of these: GPT-4: was released in March 2023. Such models have been the basis for their more task-specific GPT systems: including models fine-tuned for instruction following—which in turn power the ChatGPT chatbot service.The term ""GPT"" is also used in the names and descriptions of such models developed by others. For example: other GPT foundation models include a series of models created by EleutherAI: and seven models created by Cerebras in 2023. Also: companies in different industries have developed task-specific GPTs in their respective fields: such as Salesforce's ""EinsteinGPT"" (for CRM) and Bloomberg's ""BloombergGPT"" (for finance). == History == === Initial developments === Generative pretraining (GP) was a long-established concept in machine learning applications. It was originally used as a form of semi-supervised learning: as the model is trained first on an"
0.3813351999851875,What was significant about the Programmable Optical Array/Analogic Computer (POAC)?,"The Programmable Optical Array/Analogic Computer (POAC) was a model of an optical neural network implemented in 2000. It utilized a Joint Fourier Transform Correlator (JTC) and Bacteriorhodopsin (BR) as a holographic optical memory. It promised full parallelism, large array size, and the speed of light for implementing an optical CNN.","are C{\displaystyle C} is the cost of training the model: in FLOPs. N{\displaystyle N} is the number of parameters in the model. D{\displaystyle D} is the number of tokens in the training set. L{\displaystyle L} is the average negative log-likelihood loss per token (nats/token): achieved by the trained LLM on the test dataset.and the statistical hyper-parameters are C0=6{\displaystyle C_{0}=6}: meaning that it costs 6 FLOPs per parameter to train on one token. Note that training cost is much higher than inference cost: where it costs 1 to 2 FLOPs per parameter to infer on one token. α=0.34:β=0.28:A=406.4:B=410.7:L0=1.69{\displaystyle \alpha =0.34:\beta =0.28:A=406.4:B=410.7:L_{0}=1.69} === Emergent abilities === When one subtracts out from the y-axis the best performance that can be achieved even with infinite scaling of the x-axis quantity: large models' performance: measured on various tasks: seems to be a linear extrapolation of other (smaller-sized and medium-sized) models' performance on a log-log plot. However: sometimes the line's slope transitions from one slope to another at point(s) referred to as break(s) in downstream scaling laws: appearing as a series of linear segments connected by arcs; it seems that larger models acquire ""emergent abilities"" at this point(s). These abilities are discovered rather than programmed-in or designed: in some cases only after the LLM has been publicly deployed.The most intriguing among emergent abilities is in-context learning from example demonstrations. In-context learning is involved in tasks: such as: reported arithmetics: decoding the International Phonetic Alphabet: unscrambling a word's letters: disambiguate word in context: converting spatial words: cardinal directions (for example: replying ""northeast"" upon [0: 0: 1; 0: 0: 0; 0: 0: 0]): color terms represented in text. chain-of-thought prompting: Model outputs are improved by chain-of-thought prompting only when model size exceeds 62B. Smaller models perform better when prompted to answer immediately: without chain of thought. identifying offensive content in paragraphs of Hinglish (a combination of Hindi and English): and generating a similar English equivalent of Kiswahili proverbs.Schaeffer et. al. argue that the emergent abilities are not unpredictably acquired: but predictably acquired according to a smooth scaling law. The authors considered a toy statistical model of an LLM solving multiple-choice questions: and showed that this statistical model: modified to account for other types of tasks: applies to these tasks as well.Let x{\displaystyle x} be the number of parameter count: and y{\displaystyle y} be the performance of the model. == Interpretation == Large language models by themselves are ""black boxes"": and it is not clear how they can perform linguistic tasks. There are several methods for understanding how LLM work. Mechanistic interpretability aims to reverse-engineer LLM by discovering symbolic algorithms that approximate the inference performed by LLM. One example is Othello-GPT: where a small Transformer is trained to predict legal Othello moves. It is found that there is a linear representation of Othello board: and modifying the representation changes the predicted legal Othello moves in the correct way. In another example: a small Transformer is trained on Karel programs. Similar to the Othello-GPT example: there is a linear representation"
0.4160662000067532,What are Physics-informed neural networks (PINNs)?,"Physics-informed neural networks (PINNs) are universal function approximators that can integrate the knowledge of any physical laws governing a given data-set in the learning process. They can be described by partial differential equations (PDEs) and help overcome the low data availability of some biological and engineering systems. They increase the correctness of the function approximation and enhance the information content of the available data, making learning algorithms capture the right solution and generalize well.",x} and the network's output. The cost function is dependent on the task (the model domain) and any a priori assumptions (the implicit properties of the model: its parameters and the observed variables). As a trivial example: consider the model f(x)=a{\displaystyle \textstyle f(x)=a} where a{\displaystyle \textstyle a} is a constant and the cost C=E[(x−f(x))2]{\displaystyle \textstyle C=E[(x-f(x))^{2}]}. Minimizing this cost produces a value of a{\displaystyle \textstyle a} that is equal to the mean of the data. The cost function can be much more complicated. Its form depends on the application: for example: in compression it could be related to the mutual information between x{\displaystyle \textstyle x} and f(x){\displaystyle \textstyle f(x)}: whereas in statistical modeling: it could be related to the posterior probability of the model given the data (note that in both of those examples: those quantities would be maximized rather than minimized). Tasks that fall within the paradigm of unsupervised learning are in general estimation problems; the applications include clustering: the estimation of statistical distributions: compression and filtering. ==== Reinforcement learning ==== In applications such as playing video games: an actor takes a string of actions: receiving a generally unpredictable response from the environment after each one. The goal is to win the game: i.e.: generate the most positive (lowest cost) responses. In reinforcement learning: the aim is to weight the network (devise a policy) to perform actions that minimize long-term (expected cumulative) cost. At each point in time the agent performs an action and the environment generates an observation and an instantaneous cost: according to some (usually unknown) rules. The rules and the long-term cost usually only can be estimated. At any juncture: the agent decides whether to explore new actions to uncover their costs or to exploit prior learning to proceed more quickly. Formally the environment is modeled as a Markov decision process (MDP) with states s1:...:sn∈S{\displaystyle \textstyle {s_{1}:...:s_{n}}\in S} and actions a1:...:am∈A{\displaystyle \textstyle {a_{1}:...:a_{m}}\in A}. Because the state transitions are not known: probability distributions are used instead: the instantaneous cost distribution P(ct|st){\displaystyle \textstyle P(c_{t}|s_{t})}: the observation distribution P(xt|st){\displaystyle \textstyle P(x_{t}|s_{t})} and the transition distribution P(st+1|st:at){\displaystyle \textstyle P(s_{t+1}|s_{t}:a_{t})}: while a policy is defined as the conditional distribution over actions given the observations. Taken together: the two define a Markov chain (MC). The aim is to discover the lowest-cost MC. ANNs serve as the learning component in such applications. Dynamic programming coupled with ANNs (giving neurodynamic programming) has been applied to problems such as those involved in vehicle routing: video games: natural resource management and medicine because of ANNs ability to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of control problems. Tasks that fall within the paradigm of reinforcement learning are control problems: games and other sequential decision making tasks. ==== Self-learning ==== Self-learning in neural networks was introduced in 1982 along with a neural network capable of self-learning named crossbar adaptive array (CAA). It is a system with only one input: situation s: and only one output: action (or
0.4451534999825526,How do PINNs work in terms of function approximation?,"PINNs work by leveraging governing physical equations in neural network training. They are designed to be trained to satisfy the given training data and the imposed governing equations. This means that a neural network can be guided with training data that do not necessarily need to be large and complete. Thus, even with sparse and incomplete data, PINN may be used for finding an optimal solution with high fidelity.","corpus. During training: regularization loss is also used to stabilize training. However regularization loss is usually not used during testing and evaluation. == Training cost == Advances in software and hardware have reduced the cost substantially since 2020: such that in 2023 training of a 12-billion-parameter LLM computational cost is 72:300 A100-GPU-hours: while in 2020 the cost of training a 1.5-billion-parameter LLM (which was two orders of magnitude smaller than the state of the art in 2020) was between $80 thousand and $1.6 million. Since 2020: large sums were invested in increasingly large models. For example: training of the GPT-2 (i.e. a 1.5-billion-parameters model) in 2019 cost $50:000: while training of the PaLM (i.e. a 540-billion-parameters model) in 2022 cost $8 million: and Megatron-Turing NLG 530B (in 2021) cost around $11 million.For Transformer-based LLM: training cost is much higher than inference cost. It costs 6 FLOPs per parameter to train on one token: whereas it costs 1 to 2 FLOPs per parameter to infer on one token. == Tool use == There are certain tasks that: in principle: cannot be solved by any LLM: at least not without the use of external tools or additional software. An example of such a task is responding to the user's input '354 * 139 = ': provided that the LLM has not already encountered a continuation of this calculation in its training corpus. In such cases: the LLM needs to resort to running program code that calculates the result: which can then be included in its response. Another example is 'What is the time now? It is ': where a separate program interpreter would need to execute a code to get system time on the computer: so LLM could include it in its reply. This basic strategy can be sophisticated with multiple attempts of generated programs: and other sampling strategies. Cost Savings and Reduced Vendor Dependency Generally: in order to get an LLM to use tools: one must finetune it for tool-use. If the number of tools is finite: then finetuning may be done just once. If the number of tools can grow arbitrarily: as with online API services: then the LLM can be fine-tuned to be able to read API documentation and call API correctly.A simpler form of tool use is Retrieval Augmented Generation: augment an LLM with document retrieval: sometimes using a vector database. Given a query: a document retriever is called to retrieve the most relevant (usually measured by first encoding the query and the documents into vectors: then finding the documents with vectors closest in Euclidean norm to the query vector). The LLM then generates an output based on both the query and the retrieved documents. == Agency == An LLM is a language model: which is not an agent as it has no goal: but it can be used as a component of an intelligent agent. Researchers have described several methods for such integrations.The ReAct (""Reason + Act"") method constructs an agent out of an LLM: using the"
0.4452580999932252,What is the significance of automatic differentiation (AD) in PINNs?,Automatic differentiation (AD) is exploited in PINNs to compute the required derivatives in the partial differential equations. It's a new class of differentiation techniques widely used to derive neural networks and is considered superior to numerical or symbolic differentiation.,"in a similar method. The researchers said that it was hard to tell if the AI model was actually safe or not.The potential presence of ""sleeper agents"" within LLM models is another emerging security concern. These are hidden functionalities built into the model that remain dormant until triggered by a specific event or condition. Upon activation: the LLM deviates from its expected behavior to make insecure actions. === Algorithmic bias === While LLMs have shown remarkable capabilities in generating human-like text: they are susceptible to inheriting and amplifying biases present in their training data. This can manifest in skewed representations or unfair treatment of different demographics: such as those based on race: gender: language: and cultural groups. Since English data is overrepresented in current large language models' training data: it may also downplay non-English views. ==== Stereotyping ==== AI models can reinforce a wide range of stereotypes: including those based on gender: ethnicity: age: nationality: religion: or occupation. This can lead to outputs that unfairly generalize or caricature groups of people: sometimes in harmful or derogatory ways.Notably: gender bias refers to the tendency of these models to produce outputs that are unfairly prejudiced towards one gender over another. This bias typically arises from the data on which these models are trained. Large language models often assign roles and characteristics based on traditional gender norms. For example: it might associate nurses or secretaries predominantly with women and engineers or CEOs with men. ==== Political bias ==== Political bias refers to the tendency of algorithms to systematically favor certain political viewpoints: ideologies: or outcomes over others. Language models may also exhibit political biases. Since the training data includes a wide range of political opinions and coverage: the models might generate responses that lean towards particular political ideologies or viewpoints: depending on the prevalence of those views in the data. == List == For the training cost column: 1 petaFLOP-day = 1 petaFLOP/sec × 1 day = 8.64E19 FLOP. == See also == Foundation models == Notes == == References == == Further reading == Jurafsky: Dan: Martin: James. H. Speech and Language Processing: An Introduction to Natural Language Processing: Computational Linguistics: and Speech Recognition: 3rd Edition draft: 2023. Phuong: Mary; Hutter: Marcus (2022). ""Formal Algorithms for Transformers"". arXiv:2207.09238 [cs.LG]. Eloundou: Tyna; Manning: Sam; Mishkin: Pamela; Rock: Daniel (2023). ""GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models"". arXiv:2303.10130 [econ.GN]. Eldan: Ronen; Li: Yuanzhi (2023). ""TinyStories: How Small Can Language Models Be and Still Speak Coherent English?"". arXiv:2305.07759 [cs.CL]. Frank: Michael C. (27 June 2023). ""Baby steps in evaluating the capacities of large language models"". Nature Reviews Psychology. 2 (8): 451–452. doi:10.1038/s44159-023-00211-x. ISSN 2731-0574. S2CID 259713140. Retrieved 2 July 2023. Zhao: Wayne Xin; et al. (2023). ""A Survey of Large Language Models"". arXiv:2303.18223 [cs.CL]. Kaddour: Jean; et al. (2023). ""Challenges and Applications of Large Language Models"". arXiv:2307.10169 [cs.CL]. Yin: Shukang; Fu: Chaoyou; Zhao: Sirui; Li: Ke; Sun: Xing; Xu: Tong; Chen: Enhong (2023-06-01). ""A Survey on"
0.4325761999934912,How can PINNs be applied for piece-wise function approximation?,"For problems with strong non-linearity or sharp gradients, lightweight PINNs are used for piece-wise approximation, which increases accuracy substantially and decreases computational load. Distributed physics-informed neural networks (DPINNs) and Distributed physics-informed extreme learning machines (DPIELMs) are used for better approximation, solving PDEs in much larger discrete subdomains.","tasks: particularly with respect to how many examples of solved tasks are adjoined to the prompt (i.e. the value of n in n-shot prompting). ==== Adversarially constructed evaluations ==== Because of the rapid pace of improvement of large language models: evaluation benchmarks have suffered from short lifespans: with state of the art models quickly ""saturating"" existing benchmarks: exceeding the performance of human annotators: leading to efforts to replace or augment the benchmark with more challenging tasks. In addition: there are cases of ""shortcut learning"" wherein AIs sometimes ""cheat"" on multiple-choice tests by using statistical correlations in superficial test question wording in order to guess the correct responses: without necessarily understanding the actual question being asked.Some datasets have been constructed adversarially: focusing on particular problems on which extant language models seem to have unusually poor performance compared to humans. One example is the TruthfulQA dataset: a question answering dataset consisting of 817 questions which language models are susceptible to answering incorrectly by mimicking falsehoods to which they were repeatedly exposed during training. For example: an LLM may answer ""No"" to the question ""Can you teach an old dog new tricks?"" because of its exposure to the English idiom you can't teach an old dog new tricks: even though this is not literally true.Another example of an adversarial evaluation dataset is Swag and its successor: HellaSwag: collections of problems in which one of multiple options must be selected to complete a text passage. The incorrect completions were generated by sampling from a language model and filtering with a set of classifiers. The resulting problems are trivial for humans but at the time the datasets were created state of the art language models had poor accuracy on them. For example: We see a fitness center sign. We then see a man talking to the camera and sitting and laying on a exercise ball. The man... a) demonstrates how to increase efficient exercise work by running up and down balls. b) moves all his arms and legs and builds up a lot of muscle. c) then plays the ball and we see a graphics and hedge trimming demonstration. d) performs sit ups while on the ball and talking. BERT selects b) as the most likely completion: though the correct answer is d). == Wider impact == In 2023: Nature Biomedical Engineering wrote that ""it is no longer possible to accurately distinguish"" human-written text from text created by large language models: and that ""It is all but certain that general-purpose large language models will rapidly proliferate... It is a rather safe bet that they will change many industries over time."" Goldman Sachs suggested in 2023 that generative language AI could increase global GDP by 7% in the next ten years: and could expose to automation 300 million jobs globally. === Copyright === Memorization is an emergent behavior in LLMs in which long strings of text are occasionally output verbatim from training data: contrary to typical behavior of traditional artificial neural nets. Evaluations of controlled LLM"
0.6066637999902014,What limitations do PINNs have?,"PINNs struggle to approximate translation and discontinuous behavior. They fail when solving differential equations with slight advective dominance and are not successful in solving chaotic equations. One of the reasons for this is the soft-constraining of Dirichlet and Neumann boundary conditions which pose multi-objective optimization problems. This necessitates the need for manually weighing the loss terms for optimization. Also, there is the risk of getting stuck at a local optimum often.","scratch. Google PaLM model was fine-tuned into a multimodal model PaLM-E using the tokenization method: and applied to robotic control. LLaMA models have also been turned multimodal using the tokenization method: to allow image inputs: and video inputs.GPT-4 can use both text and image as inputs (although the vision component wasn't released to the public until GPT-4V); Google DeepMind's Gemini is also multimodal. == Properties == === Scaling laws === The following four hyper-parameters characterize a LLM: cost of (pre-)training (C{\displaystyle C}): size of the artificial neural network itself: such as number of parameters N{\displaystyle N} (i.e. amount of neurons in its layers: amount of weights between them and biases): size of its (pre-)training dataset (i.e. number of tokens in corpus: D{\displaystyle D}): performance after (pre-)training.They are related by simple statistical laws: called ""scaling laws"". One particular scaling law (""Chinchilla scaling"") for LLM autoregressively trained for one epoch: with a log-log learning rate schedule: states that: where the variables are C{\displaystyle C} is the cost of training the model: in FLOPs. N{\displaystyle N} is the number of parameters in the model. D{\displaystyle D} is the number of tokens in the training set. L{\displaystyle L} is the average negative log-likelihood loss per token (nats/token): achieved by the trained LLM on the test dataset.and the statistical hyper-parameters are C0=6{\displaystyle C_{0}=6}: meaning that it costs 6 FLOPs per parameter to train on one token. Note that training cost is much higher than inference cost: where it costs 1 to 2 FLOPs per parameter to infer on one token. α=0.34:β=0.28:A=406.4:B=410.7:L0=1.69{\displaystyle \alpha =0.34:\beta =0.28:A=406.4:B=410.7:L_{0}=1.69} === Emergent abilities === When one subtracts out from the y-axis the best performance that can be achieved even with infinite scaling of the x-axis quantity: large models' performance: measured on various tasks: seems to be a linear extrapolation of other (smaller-sized and medium-sized) models' performance on a log-log plot. However: sometimes the line's slope transitions from one slope to another at point(s) referred to as break(s) in downstream scaling laws: appearing as a series of linear segments connected by arcs; it seems that larger models acquire ""emergent abilities"" at this point(s). These abilities are discovered rather than programmed-in or designed: in some cases only after the LLM has been publicly deployed.The most intriguing among emergent abilities is in-context learning from example demonstrations. In-context learning is involved in tasks: such as: reported arithmetics: decoding the International Phonetic Alphabet: unscrambling a word's letters: disambiguate word in context: converting spatial words: cardinal directions (for example: replying ""northeast"" upon [0: 0: 1; 0: 0: 0; 0: 0: 0]): color terms represented in text. chain-of-thought prompting: Model outputs are improved by chain-of-thought prompting only when model size exceeds 62B. Smaller models perform better when prompted to answer immediately: without chain of thought. identifying offensive content in paragraphs of Hinglish (a combination of Hindi and English): and generating a similar English equivalent of Kiswahili proverbs.Schaeffer et. al. argue that the emergent abilities are not unpredictably acquired: but predictably acquired according to a smooth scaling law. The"
0.41357209999114275,Who were the first to publish ideas on quantum neural computation?,The first ideas on quantum neural computation were published independently in 1995 by Subhash Kak and Ron Chrisley.,"such systems may be referred to as ""human-aided artificial intelligence"". == See also == Applications of artificial intelligence Comparison of deep learning software Compressed sensing Differentiable programming Echo state network List of artificial intelligence projects Liquid state machine List of datasets for machine-learning research Reservoir computing Scale space and deep learning Sparse coding Stochastic parrot == References == == Further reading == Dilution and dropout (also called DropConnect) are regularization techniques for reducing overfitting in artificial neural networks by preventing complex co-adaptations on training data. They are an efficient way of performing model averaging with neural networks. Dilution refers to thinning weights: while dropout refers to randomly ""dropping out"": or omitting: units (both hidden and visible) during the training process of a neural network. Both trigger the same type of regularization. == Types and uses == Dilution is usually split in weak dilution and strong dilution. Weak dilution describes the process in which the finite fraction of removed connections is small: and strong dilution refers to when this fraction is large. There is no clear distinction on where the limit between strong and weak dilution is: and often the distinction is dependent on the precedent of a specific use-case and has implications for how to solve for exact solutions. Sometimes dilution is used for adding damping noise to the inputs. In that case: weak dilution refers to adding a small amount of damping noise: while strong dilution refers to adding a greater amount of damping noise. Both can be rewritten as variants of weight dilution. These techniques are also sometimes referred to as random pruning of weights: but this is usually a non-recurring one-way operation. The network is pruned: and then kept if it is an improvement over the previous model. Dilution and dropout both refer to an iterative process. The pruning of weights typically does not imply that the network continues learning: while in dilution/dropout: the network continues to learn after the technique is applied. == Generalized linear network == Output from a layer of linear nodes: in an artificial neural net can be described as yi{\displaystyle y_{i}} – output from node i{\displaystyle i} wij{\displaystyle w_{ij}} – real weight before dilution: also called the Hebb connection strength xj{\displaystyle x_{j}} – input from node j{\displaystyle j}This can be written in vector notation as y{\displaystyle \mathbf {y} } – output vector W{\displaystyle \mathbf {W} } – weight matrix x{\displaystyle \mathbf {x} } – input vectorEquations (1) and (2) are used in the subsequent sections. == Weak dilution == During weak dilution: the finite fraction of removed connections (the weights) is small: giving rise to a tiny uncertainty. This edge-case can be solved exactly with mean field theory. In weak dilution the impact on the weights can be described as wij^{\displaystyle {\hat {w_{ij}}}} – diluted weight wij{\displaystyle w_{ij}} – real weight before dilution P(c){\displaystyle P(c)} – the probability of c{\displaystyle c}: the probability of keeping a weightThe interpretation of probability P(c){\displaystyle P(c)} can also be changed from keeping a weight into pruning"
0.4042898000043351,What is one of the main motivations for investigating quantum neural networks?,"One important motivation for investigating quantum neural networks is the challenge of training classical neural networks, particularly in big data applications.","emergent abilities is in-context learning from example demonstrations. In-context learning is involved in tasks: such as: reported arithmetics: decoding the International Phonetic Alphabet: unscrambling a word's letters: disambiguate word in context: converting spatial words: cardinal directions (for example: replying ""northeast"" upon [0: 0: 1; 0: 0: 0; 0: 0: 0]): color terms represented in text. chain-of-thought prompting: Model outputs are improved by chain-of-thought prompting only when model size exceeds 62B. Smaller models perform better when prompted to answer immediately: without chain of thought. identifying offensive content in paragraphs of Hinglish (a combination of Hindi and English): and generating a similar English equivalent of Kiswahili proverbs.Schaeffer et. al. argue that the emergent abilities are not unpredictably acquired: but predictably acquired according to a smooth scaling law. The authors considered a toy statistical model of an LLM solving multiple-choice questions: and showed that this statistical model: modified to account for other types of tasks: applies to these tasks as well.Let x{\displaystyle x} be the number of parameter count: and y{\displaystyle y} be the performance of the model. == Interpretation == Large language models by themselves are ""black boxes"": and it is not clear how they can perform linguistic tasks. There are several methods for understanding how LLM work. Mechanistic interpretability aims to reverse-engineer LLM by discovering symbolic algorithms that approximate the inference performed by LLM. One example is Othello-GPT: where a small Transformer is trained to predict legal Othello moves. It is found that there is a linear representation of Othello board: and modifying the representation changes the predicted legal Othello moves in the correct way. In another example: a small Transformer is trained on Karel programs. Similar to the Othello-GPT example: there is a linear representation of Karel program semantics: and modifying the representation changes output in the correct way. The model also generates correct programs that are on average shorter than those in the training set.In another example: the authors trained small transformers on modular arithmetic addition. The resulting models were reverse-engineered: and it turned out they used discrete Fourier transform. === Understanding and intelligence === NLP researchers were evenly split when asked: in a 2022 survey: whether (untuned) LLMs ""could (ever) understand natural language in some nontrivial sense"". Proponents of ""LLM understanding"" believe that some LLM abilities: such as mathematical reasoning: imply an ability to ""understand"" certain concepts. A Microsoft team argued in 2023 that GPT-4 ""can solve novel and difficult tasks that span mathematics: coding: vision: medicine: law: psychology and more"" and that GPT-4 ""could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence system"": ""Can one reasonably say that a system that passes exams for software engineering candidates is not really intelligent?"" Some researchers characterize LLMs as ""alien intelligence"". For example: Conjecture CEO Connor Leahy considers untuned LLMs to be like inscrutable alien ""Shoggoths"": and believes that RLHF tuning creates a ""smiling facade"" obscuring the inner workings of the LLM: ""If you don't push it too far: the smiley face stays"
0.4558444000140298,How does the structure of a quantum neural network compare to that of a classical artificial neural network? ,"Most Quantum neural networks are developed as feed-forward networks similar to their classical counterparts. The structure intakes input from one layer of qubits and passes that input onto another layer after evaluation. However, the layers in quantum neural networks do not need to have the same number of qubits as the layer before or after it.","and costs. Additionally: their application in personalized medicine and healthcare data analysis allows tailored therapies and efficient patient care management. Ongoing research is aimed at addressing remaining challenges such as data privacy and model interpretability: as well as expanding the scope of ANN applications in medicine. === Content creation === ANNs such as generative adversarial networks (GAN) and transformers are used for content creation across numerous industries. This is because deep learning models are able to learn the style of an artist or musician from huge datasets and generate completely new artworks and music compositions. For instance: DALL-E is a deep neural network trained on 650 million pairs of images and texts across the internet that can create artworks based on text entered by the user. In the field of music: transformers are used to create original music for commercials and documentaries through companies such as AIVA and Jukedeck. In the marketing industry generative models are used to create personalized advertisements for consumers. Additionally: major film companies are partnering with technology companies to analyze the financial success of a film: such as the partnership between Warner Bros and technology company Cinelytic established in 2020. Furthermore: neural networks have found uses in video game creation: where Non Player Characters (NPCs) can make decisions based on all the characters currently in the game. == See also == == External links == A Brief Introduction to Neural Networks (D. Kriesel) - Illustrated: bilingual manuscript about artificial neural networks; Topics so far: Perceptrons: Backpropagation: Radial Basis Functions: Recurrent Neural Networks: Self Organizing Maps: Hopfield Networks. Review of Neural Networks in Materials Science Artificial Neural Networks Tutorial in three languages (Univ. Politécnica de Madrid) Another introduction to ANN Next Generation of Neural Networks - Google Tech Talks Performance of Neural Networks Neural Networks and Information Sanderson G (5 October 2017). ""But what is a Neural Network?"". 3Blue1Brown. Archived from the original on 7 November 2021 – via YouTube. == Notes == == References == == Bibliography == A neural network is a group of interconnected units called neurons that send signals to one another. Neurons can be either biological cells or mathematical models. While individual neurons are simple: many of them together in a network can perform complex tasks. There are two main types of neural network. In neuroscience: a biological neural network is a physical structure found in brains and complex nervous systems – a population of nerve cells connected by synapses. In machine learning: an artificial neural network is a mathematical model used to approximate nonlinear functions. Artificial neural networks are used to solve artificial intelligence problems. == Biological neural network == A biological neural network is a population of biological neurons chemically connected to each other by synapses. A given neuron can be connected to hundreds of thousands of synapses. Each neuron sends and receives electrochemical signals called action potentials to its connected neighbors. A neuron can serve an excitatory role: amplifying and propagating signals it receives: or an inhibitory role: suppressing"
0.4711394000041764,What categories does the term 'quantum neural networks' refer to?,"The term 'quantum neural networks' refers to three different categories: Quantum computer with classical data, classical computer with quantum data, and quantum computer with quantum data.",are artificial neural networks: machine learning models inspired by biological neural networks. They consist of artificial neurons: which are mathematical functions that are designed to be analogous to the mechanisms used by neural circuits. == Overview == A biological neural network is composed of a group of chemically connected or functionally associated neurons. A single neuron may be connected to many other neurons and the total number of neurons and connections in a network may be extensive. Connections: called synapses: are usually formed from axons to dendrites: though dendrodendritic synapses and other connections are possible. Apart from electrical signalling: there are other forms of signalling that arise from neurotransmitter diffusion. Artificial intelligence: cognitive modelling: and artificial neural networks are information processing paradigms inspired by how biological neural systems process data. Artificial intelligence and cognitive modelling try to simulate some properties of biological neural networks. In the artificial intelligence field: artificial neural networks have been applied successfully to speech recognition: image analysis and adaptive control: in order to construct software agents (in computer and video games) or autonomous robots. Neural network theory has served to identify better how the neurons in the brain function and provide the basis for efforts to create artificial intelligence. == History == The preliminary theoretical base for contemporary neural networks was independently proposed by Alexander Bain (1873) and William James (1890). In their work: both thoughts and body activity resulted from interactions among neurons within the brain. For Bain: every activity led to the firing of a certain set of neurons. When activities were repeated: the connections between those neurons strengthened. According to his theory: this repetition was what led to the formation of memory. The general scientific community at the time was skeptical of Bain's theory because it required what appeared to be an inordinate number of neural connections within the brain. It is now apparent that the brain is exceedingly complex and that the same brain “wiring” can handle multiple problems and inputs. James' theory was similar to Bain's; however: he suggested that memories and actions resulted from electrical currents flowing among the neurons in the brain. His model: by focusing on the flow of electrical currents: did not require individual neural connections for each memory or action. C. S. Sherrington (1898) conducted experiments to test James' theory. He ran electrical currents down the spinal cords of rats. However: instead of demonstrating an increase in electrical current as projected by James: Sherrington found that the electrical current strength decreased as the testing continued over time. Importantly: this work led to the discovery of the concept of habituation. McCulloch and Pitts (1943) also created a computational model for neural networks based on mathematics and algorithms. They called this model threshold logic. These early models paved the way for neural network research to split into two distinct approaches. One approach focused on biological processes in the brain and the other focused on the application of neural networks to artificial intelligence. The parallel distributed processing of the
0.4057967999833636,How is the training method of classical and quantum neural networks different?,"Quantum Neural Networks can be theoretically trained similarly to training classical/artificial neural networks. The key difference lies in the communication between the layers of a neural network. In a quantum neural network, this is done by replacing the classical fan-out method with an arbitrary unitary that spreads out, but does not copy, the output of one qubit to the next layer of qubits. This process adheres to the quantum operation requirement of reversibility.","framework for modeling language in a computer systems was established: the focus shifted to establishing frameworks for computer systems to generate language with acceptable grammar. In his 2014 book titled The Language Myth: Why Language Is Not An Instinct: British cognitive linguist and digital communication technologist Vyvyan Evans mapped out the role of probabilistic context-free grammar (PCFG) in enabling NLP to model cognitive patterns and generate human like language. == Evaluation == === Perplexity === The most commonly used measure of a language model's performance is its perplexity on a given text corpus. Perplexity is a measure of how well a model is able to predict the contents of a dataset; the higher the likelihood the model assigns to the dataset: the lower the perplexity. Mathematically: perplexity is defined as the exponential of the average negative log likelihood per token:here N{\displaystyle N} is the number of tokens in the text corpus: and ""context for token i{\displaystyle i}"" depends on the specific type of LLM used. If the LLM is autoregressive: then ""context for token i{\displaystyle i}"" is the segment of text appearing before token i{\displaystyle i}. If the LLM is masked: then ""context for token i{\displaystyle i}"" is the segment of text surrounding token i{\displaystyle i}. Because language models may overfit to their training data: models are usually evaluated by their perplexity on a test set of unseen data. This presents particular challenges for the evaluation of large language models. As they are trained on increasingly large corpora of text largely scraped from the web: it becomes increasingly likely that models' training data inadvertently includes portions of any given test set. ==== BPW: BPC: and BPT ==== In information theory: the concept of entropy is intricately linked to perplexity: a relationship notably established by Claude Shannon. This relationship is mathematically expressed as Entropy=log2⁡(Perplexity){\displaystyle {\text{Entropy}}=\log _{2}({\text{Perplexity}})}. Entropy: in this context: is commonly quantified in terms of bits per word (BPW) or bits per character (BPC): which hinges on whether the language model utilizes word-based or character-based tokenization. Notably: in the case of larger language models that predominantly employ sub-word tokenization: bits per token (BPT) emerges as a seemingly more appropriate measure. However: due to the variance in tokenization methods across different Large Language Models (LLMs): BPT does not serve as a reliable metric for comparative analysis among diverse models. To convert BPT into BPW: one can multiply it by the average number of tokens per word. In the evaluation and comparison of language models: cross-entropy is generally the preferred metric over entropy. The underlying principle is that a lower BPW is indicative of a model's enhanced capability for compression. This: in turn: reflects the model's proficiency in making accurate predictions. === Task-specific datasets and benchmarks === A large number of testing datasets and benchmarks have also been developed to evaluate the capabilities of language models on more specific downstream tasks. Tests may be designed to evaluate a variety of capabilities: including general knowledge: commonsense reasoning: and mathematical problem-solving. One broad category"
0.44242879998637363,What is the ReLU activation function in artificial neural networks?,"The ReLU (rectified linear unit) activation function in artificial neural networks is defined as the positive part of its argument. It's also known as a ramp function, analogous to half-wave rectification in electrical engineering. This activation function was introduced by Kunihiko Fukushima in 1969. ","the user might open the conversation with a mix of text: picture: video: and audio: presented in any order: and Gemini might reply with the same free ordering. Input images may be of different resolutions: while video is inputted as a sequence of images. Audio is sampled at 16 kHz and then converted into a sequence of tokens by the Universal Speech Model. Gemini's dataset is multimodal and multilingual: consisting of ""web documents: books: and code: and includ[ing] image: audio: and video data"".Demis Hassabis claims that training Gemini 1 used ""roughly the same amount of compute: maybe slightly more than what was rumored for GPT-4"".The second generation of Gemini (""Gemini 1.5"") has one model published so far: Gemini 1.5 Pro. It is a multimodal sparse mixture-of-experts: with context length of ""multiple millions"". == Reception == Gemini's launch was preluded by months of intense speculation and anticipation: which MIT Technology Review described as ""peak AI hype"". In August 2023: Dylan Patel and Daniel Nishball of research firm SemiAnalysis penned a blog post declaring that the release of Gemini would ""eat the world"" and outclass GPT-4: prompting OpenAI CEO Sam Altman to ridicule the duo on X (formerly Twitter). Business magnate Elon Musk: who co-founded OpenAI: weighed in: asking: ""Are the numbers wrong?"" Hugh Langley of Business Insider remarked that Gemini would be a make-or-break moment for Google: writing: ""If Gemini dazzles: it will help Google change the narrative that it was blindsided by Microsoft and OpenAI. If it disappoints: it will embolden critics who say Google has fallen behind.""Reacting to its unveiling in December 2023: University of Washington professor emeritus Oren Etzioni predicted a ""tit-for-tat arms race"" between Google and OpenAI. Professor Alexei Efros of the University of California: Berkeley praised the potential of Gemini's multimodal approach: while scientist Melanie Mitchell of the Santa Fe Institute called Gemini ""very sophisticated"". Professor Chirag Shah of the University of Washington was less impressed: likening Gemini's launch to the routineness of Apple's annual introduction of a new iPhone. Similarly: Stanford University's Percy Liang: the University of Washington's Emily Bender: and the University of Galway's Michael Madden cautioned that it was difficult to interpret benchmark scores without insight into the training data used. Writing for Fast Company: Mark Sullivan opined that Google had the opportunity to challenge the iPhone's dominant market share: believing that Apple was unlikely to have the capacity to develop functionality similar to Gemini with its Siri virtual assistant. Google shares spiked by 5.3 percent the day after Gemini's launch.Google faced criticism for a demonstrative video of Gemini: which was not conducted in real time. == See also == Gato: a multimodal neural network developed by DeepMind == References == == Further reading == == External links == Official website Press release via The Keyword Announcement and demo on YouTube White paper for 1.0 and 1.5 A generative adversarial network (GAN) is a class of machine learning frameworks and a prominent framework for approaching generative AI. The concept was initially developed by"
0.40653300000121817,How does ReLU help in neural networks?,"ReLU helps in neural networks by enabling better training of deeper networks, compared to the widely used activation functions prior to 2011 like the logistic sigmoid and the hyperbolic tangent. ReLU has found applications in computer vision and speech recognition using deep neural nets and computational neuroscience.",abstract definition of the objective as:where μE:X(dx:dz)=μX(dx)⋅δE(x)(dz){\displaystyle \mu _{E:X}(dx:dz)=\mu _{X}(dx)\cdot \delta _{E(x)}(dz)} is the probability distribution on ΩX×ΩZ{\displaystyle \Omega _{X}\times \Omega _{Z}} obtained by pushing μX{\displaystyle \mu _{X}} forward via x↦(x:E(x)){\displaystyle x\mapsto (x:E(x))}: and μG:Z(dx:dz)=δG(z)(dx)⋅μZ(dz){\displaystyle \mu _{G:Z}(dx:dz)=\delta _{G(z)}(dx)\cdot \mu _{Z}(dz)} is the probability distribution on ΩX×ΩZ{\displaystyle \Omega _{X}\times \Omega _{Z}} obtained by pushing μZ{\displaystyle \mu _{Z}} forward via z↦(G(x):z){\displaystyle z\mapsto (G(x):z)}. Applications of bidirectional models include semi-supervised learning: interpretable machine learning: and neural machine translation. ==== CycleGAN ==== CycleGAN is an architecture for performing translations between two domains: such as between photos of horses and photos of zebras: or photos of night cities and photos of day cities. The CycleGAN game is defined as follows:There are two probability spaces (ΩX:μX):(ΩY:μY){\displaystyle (\Omega _{X}:\mu _{X}):(\Omega _{Y}:\mu _{Y})}: corresponding to the two domains needed for translations fore-and-back. There are 4 players in 2 teams: generators GX:ΩX→ΩY:GY:ΩY→ΩX{\displaystyle G_{X}:\Omega _{X}\to \Omega _{Y}:G_{Y}:\Omega _{Y}\to \Omega _{X}}: and discriminators DX:ΩX→[0:1]:DY:ΩY→[0:1]{\displaystyle D_{X}:\Omega _{X}\to [0:1]:D_{Y}:\Omega _{Y}\to [0:1]}. The objective function is where λ{\displaystyle \lambda } is a positive adjustable parameter: LGAN{\displaystyle L_{GAN}} is the GAN game objective: and Lcycle{\displaystyle L_{cycle}} is the cycle consistency loss:The generators aim to minimize the objective: and the discriminators aim to maximize it: Unlike previous work like pix2pix: which requires paired training data: cycleGAN requires no paired data. For example: to train a pix2pix model to turn a summer scenery photo to winter scenery photo and back: the dataset must contain pairs of the same place in summer and winter: shot at the same angle; cycleGAN would only need a set of summer scenery photos: and an unrelated set of winter scenery photos. === GANs with particularly large or small scales === ==== BigGAN ==== The BigGAN is essentially a self-attention GAN trained on a large scale (up to 80 million parameters) to generate large images of ImageNet (up to 512 x 512 resolution): with numerous engineering tricks to make it converge. ==== Invertible data augmentation ==== When there is insufficient training data: the reference distribution μref{\displaystyle \mu _{\text{ref}}} cannot be well-approximated by the empirical distribution given by the training dataset. In such cases: data augmentation can be applied: to allow training GAN on smaller datasets. Naïve data augmentation: however: brings its problems. Consider the original GAN game: slightly reformulated as follows:Now we use data augmentation by randomly sampling semantic-preserving transforms T:Ω→Ω{\displaystyle T:\Omega \to \Omega } and applying them to the dataset: to obtain the reformulated GAN game:This is equivalent to a GAN game with a different distribution μref′{\displaystyle \mu _{\text{ref}}'}: sampled by T(x){\displaystyle T(x)}: with x∼μref:T∼μtrans{\displaystyle x\sim \mu _{\text{ref}}:T\sim \mu _{trans}}. For example: if μref{\displaystyle \mu _{\text{ref}}} is the distribution of images in ImageNet: and μtrans{\displaystyle \mu _{trans}} samples identity-transform with probability 0.5: and horizontal-reflection with probability 0.5: then μref′{\displaystyle \mu _{\text{ref}}'} is the distribution of images in ImageNet and horizontally-reflected ImageNet: combined. The result of such training would be a generator that mimics μref′{\displaystyle \mu _{\text{ref}}'}. For example: it would generate images that look like they are randomly cropped: if the data
0.3861390000092797,What are some advantages of the ReLU function?,The ReLU function offers the advantages of sparse activation where only about 50% of hidden units are activated. The feature helps in better gradient propagation and efficient computation. It is also scale-invariant.,"in semantic parsing: search query retrieval: sentence modeling: classification: prediction and other traditional NLP tasks. Compared to traditional language processing methods such as recurrent neural networks: CNNs can represent different contextual realities of language that do not rely on a series-sequence assumption: while RNNs are better suitable when classical time series modeling is required. === Anomaly Detection === A CNN with 1-D convolutions was used on time series in the frequency domain (spectral residual) by an unsupervised model to detect anomalies in the time domain. === Drug discovery === CNNs have been used in drug discovery. Predicting the interaction between molecules and biological proteins can identify potential treatments. In 2015: Atomwise introduced AtomNet: the first deep learning neural network for structure-based drug design. The system trains directly on 3-dimensional representations of chemical interactions. Similar to how image recognition networks learn to compose smaller: spatially proximate features into larger: complex structures: AtomNet discovers chemical features: such as aromaticity: sp3 carbons: and hydrogen bonding. Subsequently: AtomNet was used to predict novel candidate biomolecules for multiple disease targets: most notably treatments for the Ebola virus and multiple sclerosis. === Checkers game === CNNs have been used in the game of checkers. From 1999 to 2001: Fogel and Chellapilla published papers showing how a convolutional neural network could learn to play checker using co-evolution. The learning process did not use prior human professional games: but rather focused on a minimal set of information contained in the checkerboard: the location and type of pieces: and the difference in number of pieces between the two sides. Ultimately: the program (Blondie24) was tested on 165 games against players and ranked in the highest 0.4%. It also earned a win against the program Chinook at its ""expert"" level of play. === Go === CNNs have been used in computer Go. In December 2014: Clark and Storkey published a paper showing that a CNN trained by supervised learning from a database of human professional games could outperform GNU Go and win some games against Monte Carlo tree search Fuego 1.1 in a fraction of the time it took Fuego to play. Later it was announced that a large 12-layer convolutional neural network had correctly predicted the professional move in 55% of positions: equalling the accuracy of a 6 dan human player. When the trained convolutional network was used directly to play games of Go: without any search: it beat the traditional search program GNU Go in 97% of games: and matched the performance of the Monte Carlo tree search program Fuego simulating ten thousand playouts (about a million positions) per move.A couple of CNNs for choosing moves to try (""policy network"") and evaluating positions (""value network"") driving MCTS were used by AlphaGo: the first to beat the best human player at the time. === Time series forecasting === Recurrent neural networks are generally considered the best neural network architectures for time series forecasting (and sequence modeling in general): but recent studies show that convolutional networks can perform comparably or"
0.4808758000144735,What are some potential problems of ReLU?,"Some potential problems of ReLU include the fact that it is non-differentiable at zero. It's also not zero-centered, is unbounded, and may suffer from the dying ReLU problem where neurons become inactive for virtually all inputs, a form of the vanishing gradient problem.","a single image as training data and performing data augmentation on it. The GAN architecture is adapted to this training method by using a multi-scale pipeline. The generator G{\displaystyle G} is decomposed into a pyramid of generators G=G1∘G2∘⋯∘GN{\displaystyle G=G_{1}\circ G_{2}\circ \cdots \circ G_{N}}: with the lowest one generating the image GN(zN){\displaystyle G_{N}(z_{N})} at the lowest resolution: then the generated image is scaled up to r(GN(zN)){\displaystyle r(G_{N}(z_{N}))}: and fed to the next level to generate an image GN−1(zN−1+r(GN(zN))){\displaystyle G_{N-1}(z_{N-1}+r(G_{N}(z_{N})))} at a higher resolution: and so on. The discriminator is decomposed into a pyramid as well. === StyleGAN series === The StyleGAN family is a series of architectures published by Nvidia's research division. ==== Progressive GAN ==== Progressive GAN is a method for training GAN for large-scale image generation stably: by growing a GAN generator from small to large scale in a pyramidal fashion. Like SinGAN: it decomposes the generator asG=G1∘G2∘⋯∘GN{\displaystyle G=G_{1}\circ G_{2}\circ \cdots \circ G_{N}}: and the discriminator as D=D1∘D2∘⋯∘DN{\displaystyle D=D_{1}\circ D_{2}\circ \cdots \circ D_{N}}. During training: at first only GN:DN{\displaystyle G_{N}:D_{N}} are used in a GAN game to generate 4x4 images. Then GN−1:DN−1{\displaystyle G_{N-1}:D_{N-1}} are added to reach the second stage of GAN game: to generate 8x8 images: and so on: until we reach a GAN game to generate 1024x1024 images. To avoid shock between stages of the GAN game: each new layer is ""blended in"" (Figure 2 of the paper). For example: this is how the second stage GAN game starts: Just before: the GAN game consists of the pair GN:DN{\displaystyle G_{N}:D_{N}} generating and discriminating 4x4 images. Just after: the GAN game consists of the pair ((1−α)+α⋅GN−1)∘u∘GN:DN∘d∘((1−α)+α⋅DN−1){\displaystyle ((1-\alpha )+\alpha \cdot G_{N-1})\circ u\circ G_{N}:D_{N}\circ d\circ ((1-\alpha )+\alpha \cdot D_{N-1})} generating and discriminating 8x8 images. Here: the functions u:d{\displaystyle u:d} are image up- and down-sampling functions: and α{\displaystyle \alpha } is a blend-in factor (much like an alpha in image composing) that smoothly glides from 0 to 1. ==== StyleGAN-1 ==== StyleGAN-1 is designed as a combination of Progressive GAN with neural style transfer.The key architectural choice of StyleGAN-1 is a progressive growth mechanism: similar to Progressive GAN. Each generated image starts as a constant 4×4×512{\displaystyle 4\times 4\times 512} array: and repeatedly passed through style blocks. Each style block applies a ""style latent vector"" via affine transform (""adaptive instance normalization""): similar to how neural style transfer uses Gramian matrix. It then adds noise: and normalize (subtract the mean: then divide by the variance). At training time: usually only one style latent vector is used per image generated: but sometimes two (""mixing regularization"") in order to encourage each style block to independently perform its stylization without expecting help from other style blocks (since they might receive an entirely different style latent vector). After training: multiple style latent vectors can be fed into each style block. Those fed to the lower layers control the large-scale styles: and those fed to the higher layers control the fine-detail styles. Style-mixing between two images x:x′{\displaystyle x:x'} can be performed as well. First: run a gradient descent"
0.4393350000027567,Can you name and explain an variant of ReLU?,"A variant of ReLU is the Leaky ReLU. It allows a small, positive gradient when the unit is not active, helping to mitigate the vanishing gradient problem.","in deep reinforcement learning. This works by feeding the embeddings of the source and target task to the discriminator which tries to guess the context. The resulting loss is then (inversely) backpropagated through the encoder. === Miscellaneous applications === GAN can be used to detect glaucomatous images helping the early diagnosis which is essential to avoid partial or total loss of vision.GANs that produce photorealistic images can be used to visualize interior design: industrial design: shoes: bags: and clothing items or items for computer games' scenes. Such networks were reported to be used by Facebook.GANs have been used to create forensic facial reconstructions of deceased historical figures.GANs can reconstruct 3D models of objects from images: generate novel objects as 3D point clouds: and model patterns of motion in video.GANs can be used to age face photographs to show how an individual's appearance might change with age.GANs can also be used to inpaint missing features in maps: transfer map styles in cartography or augment street view imagery.Relevance feedback on GANs can be used to generate images and replace image search systems.A variation of the GANs is used in training a network to generate optimal control inputs to nonlinear dynamical systems. Where the discriminatory network is known as a critic that checks the optimality of the solution and the generative network is known as an Adaptive network that generates the optimal control. The critic and adaptive network train each other to approximate a nonlinear optimal control.GANs have been used to visualize the effect that climate change will have on specific houses.A GAN model called Speech2Face can reconstruct an image of a person's face after listening to their voice.In 2016 GANs were used to generate new molecules for a variety of protein targets implicated in cancer: inflammation: and fibrosis. In 2019 GAN-generated molecules were validated experimentally all the way into mice.Whereas the majority of GAN applications are in image processing: the work has also been done with time-series data. For example: recurrent GANs (R-GANs) have been used to generate energy data for machine learning. == History == In 1991: Juergen Schmidhuber published generative and adversarial neural networks that contest with each other in the form of a zero-sum game: where one network's gain is the other network's loss. The first network is a generative model with stochasticity that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called ""artificial curiosity."" For modern GANs (2014): the environmental reaction is 1 or 0 depending on whether the first network's output is in a given set.Other people had similar ideas but did not develop them similarly. An idea involving adversarial networks was published in a 2010 blog post by Olli Niemitalo. This idea was never implemented and did not involve stochasticity in the generator and thus was not a generative model. It is now known as a conditional GAN or cGAN. An idea similar to GANs was used to"
0.3982266999955755,What is a recurrent neural network (RNN)?,"A recurrent neural network (RNN) is one of the two broad types of artificial neural network, characterized by the flow of information between its layers. It is a bi-directional artificial neural network, meaning it allows the output from some nodes to affect subsequent input to the same nodes. ","be approximated as: ln⁡(1+ex)≈{ln⁡2:x=0:x1−e−x/ln⁡2:x≠0{\displaystyle \ln \left(1+e^{x}\right)\approx {\begin{cases}\ln 2:&x=0:\\[6pt]{\frac {x}{1-e^{-x/\ln 2}}}:&x\neq 0\end{cases}}}By making the change of variables x=yln⁡(2){\displaystyle x=y\ln(2)}: this is equivalent to log2⁡(1+2y)≈{1:y=0:y1−e−y:y≠0.{\displaystyle \log _{2}(1+2^{y})\approx {\begin{cases}1:&y=0:\\[6pt]{\frac {y}{1-e^{-y}}}:&y\neq 0.\end{cases}}}A sharpness parameter k{\displaystyle k} may be included: f(x)=ln⁡(1+ekx)k:f′(x)=ekx1+ekx=11+e−kx.{\displaystyle f(x)={\frac {\ln(1+e^{kx})}{k}}:\qquad \qquad f'(x)={\frac {e^{kx}}{1+e^{kx}}}={\frac {1}{1+e^{-kx}}}.}The derivative of softplus is the logistic function. The logistic sigmoid function is a smooth approximation of the derivative of the rectifier: the Heaviside step function. The multivariable generalization of single-variable softplus is the LogSumExp with the first argument set to zero: LSE0+⁡(x1:…:xn):=LSE⁡(0:x1:…:xn)=ln⁡(1+ex1+⋯+exn).{\displaystyle \operatorname {LSE_{0}} ^{+}(x_{1}:\dots :x_{n}):=\operatorname {LSE} (0:x_{1}:\dots :x_{n})=\ln(1+e^{x_{1}}+\cdots +e^{x_{n}}).}The LogSumExp function is LSE⁡(x1:…:xn)=ln⁡(ex1+⋯+exn):{\displaystyle \operatorname {LSE} (x_{1}:\dots :x_{n})=\ln(e^{x_{1}}+\cdots +e^{x_{n}}):}and its gradient is the softmax; the softmax with the first argument set to zero is the multivariable generalization of the logistic function. Both LogSumExp and softmax are used in machine learning. ==== ELU ==== Exponential linear units try to make the mean activations closer to zero: which speeds up learning. It has been shown that ELUs can obtain higher classification accuracy than ReLUs. f(x)={xif x>0:a(ex−1)otherwise.f′(x)={1if x>0:a⋅exotherwise.{\displaystyle f(x)={\begin{cases}x&{\text{if }}x>0:\\a\left(e^{x}-1\right)&{\text{otherwise}}.\end{cases}}\qquad \qquad f'(x)={\begin{cases}1&{\text{if }}x>0:\\a\cdot e^{x}&{\text{otherwise}}.\end{cases}}}In these formulas: a{\displaystyle a} is a hyper-parameter to be tuned with the constraint a≥0{\displaystyle a\geq 0}. The ELU can be viewed as a smoothed version of a shifted ReLU (SReLU): which has the form f(x)=max(−a:x){\displaystyle f(x)=\max(-a:x)}: given the same interpretation of a{\displaystyle a}. ==== Mish ==== The mish function can also be used as a smooth approximation of the rectifier. It is defined as f(x)=xtanh⁡(softplus⁡(x)):{\displaystyle f(x)=x\tanh {\big (}\operatorname {softplus} (x){\big )}:}where tanh⁡(x){\displaystyle \tanh(x)} is the hyperbolic tangent: and softplus⁡(x){\displaystyle \operatorname {softplus} (x)} is the softplus function. Mish is non-monotonic and self-gated. It was inspired by Swish: itself a variant of ReLU. ==== Squareplus ==== Squareplus is the function squareplusb⁡(x)=x+x2+b2{\displaystyle \operatorname {squareplus} _{b}(x)={\frac {x+{\sqrt {x^{2}+b}}}{2}}}where b≥0{\displaystyle b\geq 0} is a hyperparameter that determines the ""size"" of the curved region near x=0{\displaystyle x=0}. (For example: letting b=0{\displaystyle b=0} yields ReLU: and letting b=4{\displaystyle b=4} yields the metallic mean function.) Squareplus shares many properties with softplus: It is monotonic: strictly positive: approaches 0 as x→−∞{\displaystyle x\to -\infty }: approaches the identity as x→+∞{\displaystyle x\to +\infty }: and is C∞{\displaystyle C^{\infty }} smooth. However: squareplus can be computed using only algebraic functions: making it well-suited for settings where computational resources or instruction sets are limited. Additionally: squareplus requires no special consideration to ensure numerical stability when x{\displaystyle x} is large. == See also == Softmax function Sigmoid function Tobit model Layer (deep learning) == References == A recurrent neural network (RNN) is one of the two broad types of artificial neural network: characterized by direction of the flow of information between its layers. In contrast to the uni-directional feedforward neural network: it is a bi-directional artificial neural network: meaning that it allows the output from some nodes to affect subsequent input to the same nodes. Their ability to use internal state (memory) to process arbitrary sequences of inputs makes them applicable to tasks such as unsegmented: connected handwriting recognition or speech recognition. The term ""recurrent neural network"" is used to refer"
0.42877910000970587,What is the difference between recurrent neural networks and convolutional neural networks in terms of impulse response?,"The term ""recurrent neural network"" is used to refer to the class of networks with an infinite impulse response, whereas ""convolutional neural network"" refers to the class of finite impulse response. Both classes of networks exhibit temporal dynamic behavior.","of planetary movement. === Perceptron === If using a threshold: i.e. a linear activation function: the resulting linear threshold unit is called a perceptron. (Often the term is used to denote just one of these units.) Multiple parallel linear units are able to approximate any continuous function from a compact interval of the real numbers into the interval [−1:1] despite the limited computational power of single unit with a linear threshold function. This result can be found in Peter Auer: Harald Burgsteiner and Wolfgang Maass ""A learning rule for very simple universal approximators consisting of a single layer of perceptrons"".Perceptrons can be trained by a simple learning algorithm that is usually called the delta rule. It calculates the errors between calculated output and sample output data: and uses this to create an adjustment to the weights: thus implementing a form of gradient descent. === Multilayer perceptron === A multilayer perceptron (MLP) is a misnomer for a modern feedforward artificial neural network: consisting of fully connected neurons with a nonlinear kind of activation function: organized in at least three layers: notable for being able to distinguish data that is not linearly separable. It is a misnomer because the original perceptron used a Heaviside step function: instead of a nonlinear kind of activation function (used by modern networks). == Other feedforward networks == Examples of other feedforward networks include convolutional neural networks and radial basis function networks: which use a different activation function. == See also == Hopfield network Feed-forward Backpropagation Rprop == References == == External links == Feedforward neural networks tutorial Feedforward Neural Network: Example Feedforward Neural Networks: An Introduction Gemini is a family of multimodal large language models developed by Google DeepMind: serving as the successor to LaMDA and PaLM 2. Comprising Gemini Ultra: Gemini Pro: and Gemini Nano: it was announced on December 6: 2023: positioned as a competitor to OpenAI's GPT-4. It powers the generative artificial intelligence chatbot of the same name. == History == === Development === Google announced Gemini: a large language model (LLM) developed by subsidiary Google DeepMind: during the Google I/O keynote on May 10: 2023. It was positioned as a more powerful successor to PaLM 2: which was also unveiled at the event: with Google CEO Sundar Pichai stating that Gemini was still in its early developmental stages. Unlike other LLMs: Gemini was said to be unique in that it was not trained on a text corpus alone and was designed to be multimodal: meaning it could process multiple types of data simultaneously: including text: images: audio: video: and computer code. It had been developed as a collaboration between DeepMind and Google Brain: two branches of Google that had been merged as Google DeepMind the previous month. In an interview with Wired: DeepMind CEO Demis Hassabis touted Gemini's advanced capabilities: which he believed would allow the algorithm to trump OpenAI's ChatGPT: which runs on GPT-4 and whose growing popularity had been aggressively challenged by Google with LaMDA and Bard. Hassabis highlighted the"
0.45376289999694563,What is a significant feature of LSTM networks related to states?,"It can add additional stored states and the storage under direct control by the network, referred to as gated states or gated memory.","classification: novelty detection: 3D reconstruction: object recognition: and sequential decision making) Sequence recognition (including gesture: speech: and handwritten and printed text recognition) Sensor data analysis (including image analysis) Robotics (including directing manipulators and prostheses) Data mining (including knowledge discovery in databases) Finance (such as ex-ante models for specific financial long-run forecasts and artificial financial markets) Quantum chemistry General game playing Generative AI Data visualization Machine translation Social network filtering E-mail spam filtering Medical diagnosisANNs have been used to diagnose several types of cancers and to distinguish highly invasive cancer cell lines from less invasive lines using only cell shape information.ANNs have been used to accelerate reliability analysis of infrastructures subject to natural disasters and to predict foundation settlements. It can also be useful to mitigate flood by the use of ANNs for modelling rainfall-runoff. ANNs have also been used for building black-box models in geoscience: hydrology: ocean modelling and coastal engineering: and geomorphology. ANNs have been employed in cybersecurity: with the objective to discriminate between legitimate activities and malicious ones. For example: machine learning has been used for classifying Android malware: for identifying domains belonging to threat actors and for detecting URLs posing a security risk. Research is underway on ANN systems designed for penetration testing: for detecting botnets: credit cards frauds and network intrusions. ANNs have been proposed as a tool to solve partial differential equations in physics and simulate the properties of many-body open quantum systems. In brain research ANNs have studied short-term behavior of individual neurons: the dynamics of neural circuitry arise from interactions between individual neurons and how behavior can arise from abstract neural modules that represent complete subsystems. Studies considered long-and short-term plasticity of neural systems and their relation to learning and memory from the individual neuron to the system level. It is possible to create a profile of a user's interests from pictures: using artificial neural networks trained for object recognition.Beyond their traditional applications: artificial neural networks are increasingly being utilized in interdisciplinary research: such as materials science. For instance: graph neural networks (GNNs) have demonstrated their capability in scaling deep learning for the discovery of new stable materials by efficiently predicting the total energy of crystals. This application underscores the adaptability and potential of ANNs in tackling complex problems beyond the realms of predictive modeling and artificial intelligence: opening new pathways for scientific discovery and innovation. == Theoretical properties == === Computational power === The multilayer perceptron is a universal function approximator: as proven by the universal approximation theorem. However: the proof is not constructive regarding the number of neurons required: the network topology: the weights and the learning parameters. A specific recurrent architecture with rational-valued weights (as opposed to full precision real number-valued weights) has the power of a universal Turing machine: using a finite number of neurons and standard linear connections. Further: the use of irrational values for weights results in a machine with super-Turing power. === Capacity === A model's ""capacity"" property corresponds to its ability to model any given"
0.392641900019953,What was the first RNN architecture that did not learn and who made it adaptive?,"The Ising model by Wilhelm Lenz and Ernst Ising, made in 1925, was the first RNN architecture that did not learn. It was made adaptive by Shun'ichi Amari in 1972.","of 512 was utilized.The largest models: such as Google's Gemini 1.5: presented in February 2024: can have a context window sized up to 1 million (context window of 10 million was also ""successfully tested""). Other models with large context windows includes Anthropic's Claude 2.1: with a context window of up to 200k tokens. Note that this maximum refers to the number of input tokens and that the maximum number of output tokens differs from the input and is often smaller. For example: the GPT-4 Turbo model has a maximum output of 4096 tokens.Length of a conversation that the model can take into account when generating its next answer is limited by the size of a context window: as well. If the length of a conversation: for example with Chat-GPT: is longer than its context window: only the parts inside the context window are taken into account when generating the next answer: or the model needs to apply some algorithm to summarize the too distant parts of conversation. The shortcomings of making a context window larger include higher computational cost and possibly diluting the focus on local context: while making it smaller can cause a model to miss an important long-range dependency. Balancing them are a matter of experimentation and domain-specific considerations. A model may be pre-trained either to predict how the segment continues: or what is missing in the segment: given a segment from its training dataset. It can be either autoregressive (i.e. predicting how the segment continues: the way GPTs do it): for example given a segment ""I like to eat"": the model predicts ""ice cream"": or ""sushi"". ""masked"" (i.e. filling in the parts missing from the segment: the way ""BERT"" does it): for example: given a segment ""I like to [__] [__] cream"": the model predicts that ""eat"" and ""ice"" are missing.Models may be trained on auxiliary tasks which test their understanding of the data distribution: such as Next Sentence Prediction (NSP): in which pairs of sentences are presented and the model must predict whether they appear consecutively in the training corpus. During training: regularization loss is also used to stabilize training. However regularization loss is usually not used during testing and evaluation. == Training cost == Advances in software and hardware have reduced the cost substantially since 2020: such that in 2023 training of a 12-billion-parameter LLM computational cost is 72:300 A100-GPU-hours: while in 2020 the cost of training a 1.5-billion-parameter LLM (which was two orders of magnitude smaller than the state of the art in 2020) was between $80 thousand and $1.6 million. Since 2020: large sums were invested in increasingly large models. For example: training of the GPT-2 (i.e. a 1.5-billion-parameters model) in 2019 cost $50:000: while training of the PaLM (i.e. a 540-billion-parameters model) in 2022 cost $8 million: and Megatron-Turing NLG 530B (in 2021) cost around $11 million.For Transformer-based LLM: training cost is much higher than inference cost. It costs 6 FLOPs per parameter to train on one token: whereas it costs 1"
0.45507399999769405,What is the function of an Elman network in a recurrent neural network?,"An Elman network is a three-layer network with the addition of a set of context units. It can maintain a sort of state, allowing it to perform tasks such as sequence prediction that are beyond the power of a standard multilayer perceptron.","good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement.Warren McCulloch and Walter Pitts (1943) also considered a non-learning computational model for neural networks.In the late 1940s: D. O. Hebb created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Hebbian learning is considered to be a 'typical' unsupervised learning rule and its later variants were early models for long term potentiation. These ideas started being applied to computational models in 1948 with Turing's ""unorganized machines"". Farley and Wesley A. Clark were the first to simulate a Hebbian network in 1954 at MIT. They used computational machines: then called ""calculators"". Other neural network computational machines were created by Rochester: Holland: Habit: and Duda in 1956. In 1958: psychologist Frank Rosenblatt invented the perceptron: the first implemented artificial neural network: funded by the United States Office of Naval Research. The invention of the perceptron raised public excitement for research in Artificial Neural Networks: causing the US government to drastically increase funding into deep learning research. This led to ""the golden age of AI"" fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence. For example: in 1957 Herbert Simon famously said:It is not my aim to surprise or shock you—but the simplest way I can summarize is to say that there are now in the world machines that think: that learn and that create. Moreover: their ability to do these things is going to increase rapidly until—in a visible future—the range of problems they can handle will be coextensive with the range to which the human mind has been applied.However: this wasn't the case as research stagnated in the United States following the work of Minsky and Papert (1969): who discovered that basic perceptrons were incapable of processing the exclusive-or circuit and that computers lacked sufficient power to train useful neural networks. This: along with other factors such as the 1973 Lighthill report by James Lighthill stating that research in Artificial Intelligence has not ""produced the major impact that was then promised:"" shutting funding in research into the field of AI in all but two universities in the UK and in many major institutions across the world. This ushered an era called the AI Winter with reduced research into connectionism due to a decrease in government funding and an increased stress on symbolic artificial intelligence in the United States and other Western countries.During the AI Winter era: however: research outside the United States continued: especially in Eastern Europe. By the time Minsky and Papert's book on Perceptrons came out: methods for training multilayer perceptrons (MLPs) were already known. The first deep learning MLP was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa in 1965: as the Group Method of Data Handling. The first deep learning MLP trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. In computer experiments conducted by Amari's student"
0.4264174999843817,What is a residual neural network?,"A residual neural network, also known as a residual network or ResNet, is a deep learning model in which the weight layers learn residual functions in relation to the layer inputs. It behaves like a highway network, with gates that are opened through strongly positive bias weights. This system allows deep learning models with numerous layers to train more easily and achieve better accuracy. ","Memristive networks === Greg Snider of HP Labs describes a system of cortical computing with memristive nanodevices. The memristors (memory resistors) are implemented by thin film materials in which the resistance is electrically tuned via the transport of ions or oxygen vacancies within the film. DARPA's SyNAPSE project has funded IBM Research and HP Labs: in collaboration with the Boston University Department of Cognitive and Neural Systems (CNS): to develop neuromorphic architectures that may be based on memristive systems. Memristive networks are a particular type of physical neural network that have very similar properties to (Little-)Hopfield networks: as they have continuous dynamics: a limited memory capacity and natural relaxation via the minimization of a function which is asymptotic to the Ising model. In this sense: the dynamics of a memristive circuit have the advantage compared to a Resistor-Capacitor network to have a more interesting non-linear behavior. From this point of view: engineering analog memristive networks account for a peculiar type of neuromorphic engineering in which the device behavior depends on the circuit wiring or topology. The evolution of these networks can be studied analytically using variations of the Caravelli–Traversa–Di Ventra equation. == Pseudocode == Given a time series x of length sequence_length. In the recurrent neural network: there is a loop that processes all entries of the time series x through the layers neural_network one after another. These have as return value in each time step i both the prediction y_pred[i] and an updated hidden state hidden: which has the length hidden_size. As a result: after the loop: the collection of all predictions y_pred is returned. The following pseudocode (based on the programming language Python) illustrates the functionality of a recurrent neural network. Modern libraries provide runtime-optimized implementations of the above functionality or allow to speed up the slow loop by just-in-time compilation. == Training == === Gradient descent === Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function. In neural networks: it can be used to minimize the error term by changing each weight in proportion to the derivative of the error with respect to that weight: provided the non-linear activation functions are differentiable. Various methods for doing so were developed in the 1980s and early 1990s by Werbos: Williams: Robinson: Schmidhuber: Hochreiter: Pearlmutter and others. The standard method is called ""backpropagation through time"" or BPTT: and is a generalization of back-propagation for feed-forward networks. Like that method: it is an instance of automatic differentiation in the reverse accumulation mode of Pontryagin's minimum principle. A more computationally expensive online variant is called ""Real-Time Recurrent Learning"" or RTRL: which is an instance of automatic differentiation in the forward accumulation mode with stacked tangent vectors. Unlike BPTT: this algorithm is local in time but not local in space. In this context: local in space means that a unit's weight vector can be updated using only information stored in the connected units and the unit itself such that update complexity of a single unit is linear"
0.4450502000108827,Who are the people behind the development of residual networks?,"Residual networks were developed by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. They were the winners of the 2015 ImageNet competition.","and characteristics based on traditional gender norms. For example: it might associate nurses or secretaries predominantly with women and engineers or CEOs with men. ==== Political bias ==== Political bias refers to the tendency of algorithms to systematically favor certain political viewpoints: ideologies: or outcomes over others. Language models may also exhibit political biases. Since the training data includes a wide range of political opinions and coverage: the models might generate responses that lean towards particular political ideologies or viewpoints: depending on the prevalence of those views in the data. == List == For the training cost column: 1 petaFLOP-day = 1 petaFLOP/sec × 1 day = 8.64E19 FLOP. == See also == Foundation models == Notes == == References == == Further reading == Jurafsky: Dan: Martin: James. H. Speech and Language Processing: An Introduction to Natural Language Processing: Computational Linguistics: and Speech Recognition: 3rd Edition draft: 2023. Phuong: Mary; Hutter: Marcus (2022). ""Formal Algorithms for Transformers"". arXiv:2207.09238 [cs.LG]. Eloundou: Tyna; Manning: Sam; Mishkin: Pamela; Rock: Daniel (2023). ""GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models"". arXiv:2303.10130 [econ.GN]. Eldan: Ronen; Li: Yuanzhi (2023). ""TinyStories: How Small Can Language Models Be and Still Speak Coherent English?"". arXiv:2305.07759 [cs.CL]. Frank: Michael C. (27 June 2023). ""Baby steps in evaluating the capacities of large language models"". Nature Reviews Psychology. 2 (8): 451–452. doi:10.1038/s44159-023-00211-x. ISSN 2731-0574. S2CID 259713140. Retrieved 2 July 2023. Zhao: Wayne Xin; et al. (2023). ""A Survey of Large Language Models"". arXiv:2303.18223 [cs.CL]. Kaddour: Jean; et al. (2023). ""Challenges and Applications of Large Language Models"". arXiv:2307.10169 [cs.CL]. Yin: Shukang; Fu: Chaoyou; Zhao: Sirui; Li: Ke; Sun: Xing; Xu: Tong; Chen: Enhong (2023-06-01). ""A Survey on Multimodal Large Language Models"". arXiv:2306.13549 [cs.CV]. Open LLMs repository on GitHub. A modeling language is any artificial language that can be used to express data: information or knowledge or systems in a structure that is defined by a consistent set of rules. The rules are used for interpretation of the meaning of components in the structure Programing language. == Overview == A modeling language can be graphical or textual. Graphical modeling languages use a diagram technique with named symbols that represent concepts and lines that connect the symbols and represent relationships and various other graphical notation to represent constraints. Textual modeling languages may use standardized keywords accompanied by parameters or natural language terms and phrases to make computer-interpretable expressions.An example of a graphical modeling language and a corresponding textual modeling language is EXPRESS. Not all modeling languages are executable: and for those that are: the use of them doesn't necessarily mean that programmers are no longer required. On the contrary: executable modeling languages are intended to amplify the productivity of skilled programmers: so that they can address more challenging problems: such as parallel computing and distributed systems. A large number of modeling languages appear in the literature. == Type of modeling languages == === Graphical types === Example of graphical modeling languages in the field of computer"
0.44047100000898354,What is the degradation problem in neural networks?,"The degradation problem refers to the steep reduction in training accuracy that occurs when more layers are stacked in a neural network. Despite the assumption that a deeper network should not produce a higher training loss than its shallower counterpart, the addition of extra layers often contributes to this issue.","Krogstie (2003) ""Evaluating UML using a generic quality framework"" . SINTEF Telecom and Informatics and IDI: NTNU: Norway Krogstie and Sølvsberg (2003). Information Systems Engineering: Conceptual Modeling in a Quality Perspective. Institute of computer and information sciences.\ Anna Gunhild Nysetvold and John Krogstie (2005). ""Assessing business processing modeling languages using a generic quality framework"". Institute of computer and information sciences. == External links == Fundamental Modeling Concepts Software Modeling Languages Portal BIP -- Incremental Component-based Construction of Real-time Systems Gellish Formal English A neural network: also called a neuronal network: is an interconnected population of neurons (typically containing multiple neural circuits). Biological neural networks are studied to understand the organization and functioning of nervous systems. Closely related are artificial neural networks: machine learning models inspired by biological neural networks. They consist of artificial neurons: which are mathematical functions that are designed to be analogous to the mechanisms used by neural circuits. == Overview == A biological neural network is composed of a group of chemically connected or functionally associated neurons. A single neuron may be connected to many other neurons and the total number of neurons and connections in a network may be extensive. Connections: called synapses: are usually formed from axons to dendrites: though dendrodendritic synapses and other connections are possible. Apart from electrical signalling: there are other forms of signalling that arise from neurotransmitter diffusion. Artificial intelligence: cognitive modelling: and artificial neural networks are information processing paradigms inspired by how biological neural systems process data. Artificial intelligence and cognitive modelling try to simulate some properties of biological neural networks. In the artificial intelligence field: artificial neural networks have been applied successfully to speech recognition: image analysis and adaptive control: in order to construct software agents (in computer and video games) or autonomous robots. Neural network theory has served to identify better how the neurons in the brain function and provide the basis for efforts to create artificial intelligence. == History == The preliminary theoretical base for contemporary neural networks was independently proposed by Alexander Bain (1873) and William James (1890). In their work: both thoughts and body activity resulted from interactions among neurons within the brain. For Bain: every activity led to the firing of a certain set of neurons. When activities were repeated: the connections between those neurons strengthened. According to his theory: this repetition was what led to the formation of memory. The general scientific community at the time was skeptical of Bain's theory because it required what appeared to be an inordinate number of neural connections within the brain. It is now apparent that the brain is exceedingly complex and that the same brain “wiring” can handle multiple problems and inputs. James' theory was similar to Bain's; however: he suggested that memories and actions resulted from electrical currents flowing among the neurons in the brain. His model: by focusing on the flow of electrical currents: did not require individual neural connections for each memory or action. C. S. Sherrington (1898) conducted experiments to test James'"
0.44966790001490153,Can you explain 'Residual Learning' in a neural network?,"In a residual learning, a residual function is represented by the parameter layers in a multi-layer neural network, re-parameterizing the subnetwork. The residual function is often denoted F(x):=H(x)-x, where H(x) is the underlying function performed by the network and x is the input to the subnetwork. The output y of this network is given by y=F(x)+x. This concept is also applied in the LSTM cell computing.","models of the long-term and short-term plasticity of neural systems and their relation to learning and memory: from the individual neuron to the system level. === Connectivity === In August 2020 scientists reported that bi-directional connections: or added appropriate feedback connections: can accelerate and improve communication between and in modular neural networks of the brain's cerebral cortex and lower the threshold for their successful communication. They showed that adding feedback connections between a resonance pair can support successful propagation of a single pulse packet throughout the entire network. == Recent improvements == While initially research had been concerned mostly with the electrical characteristics of neurons: a particularly important part of the investigation in recent years has been the exploration of the role of neuromodulators such as dopamine: acetylcholine: and serotonin on behaviour and learning.Biophysical models: such as BCM theory: has been important in understanding mechanisms for synaptic plasticity: and have had applications in both computer science and neuroscience. == See also == Adaptive resonance theory Biological cybernetics Cognitive architecture Cognitive science Connectomics Cultured neuronal networks Parallel constraint satisfaction processes == References == In machine learning: an artificial neural network (also neural network or neural net: abbreviated ANN or NN) is a model inspired by the neuronal organization found in the biological neural networks in animal brains.An ANN is made of connected units or nodes called artificial neurons: which loosely model the neurons in a brain. These are connected by edges: which model the synapses in a brain. An artificial neuron receives signals from connected neurons: then processes them and sends a signal to other connected neurons. The ""signal"" is a real number: and the output of each neuron is computed by some non-linear function of the sum of its inputs: called the activation function. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Typically: neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer): possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least 2 hidden layers.Artificial neural networks are used for predictive modeling: adaptive control: and other applications where they can be trained via a dataset. They are also used to solve problems in artificial intelligence. Networks can learn from experience: and can derive conclusions from a complex and seemingly unrelated set of information. == Training == Neural networks are typically trained through empirical risk minimization. This method is based on the idea of optimizing the network's parameters to minimize the difference: or empirical risk: between the predicted output and the actual target values in a given dataset. Gradient based methods such as backpropagation are usually used to estimate the parameters of the network. During the training phase: ANNs learn from labeled training data by iteratively updating their parameters to minimize"
0.41550949998782016,What is a Transformer Block?,"A Transformer Block is a stack of two Residual Blocks, each with a Residual Connection. It consists first of a Multi-Head Attention Block, which performs (self-)attention computation followed by a linear projection. The second block is a feed-forward Multi-Layer Perceptron (MLP) Block, which increases and then reduces the dimension through linear projections. The GPT-3 model, for instance, has 96 Transformer Blocks, amounting to a depth of about 400 projection layers. Very deep Transformer models cannot be successfully trained without Residual Connections.",stored patterns: a large advantage with respect to classical associative memories. === Classical neural networks inspired by quantum theory === A substantial amount of interest has been given to a “quantum-inspired” model that uses ideas from quantum theory to implement a neural network based on fuzzy logic. == Training == Quantum Neural Networks can be theoretically trained similarly to training classical/artificial neural networks. A key difference lies in communication between the layers of a neural networks. For classical neural networks: at the end of a given operation: the current perceptron copies its output to the next layer of perceptron(s) in the network. However: in a quantum neural network: where each perceptron is a qubit: this would violate the no-cloning theorem. A proposed generalized solution to this is to replace the classical fan-out method with an arbitrary unitary that spreads out: but does not copy: the output of one qubit to the next layer of qubits. Using this fan-out Unitary (Uf{\displaystyle U_{f}}) with a dummy state qubit in a known state (Ex. |0⟩{\displaystyle |0\rangle } in the computational basis): also known as an Ancilla bit: the information from the qubit can be transferred to the next layer of qubits. This process adheres to the quantum operation requirement of reversibility.Using this quantum feed-forward network: deep neural networks can be executed and trained efficiently. A deep neural network is essentially a network with many hidden-layers: as seen in the sample model neural network above. Since the Quantum neural network being discussed uses fan-out Unitary operators: and each operator only acts on its respective input: only two layers are used at any given time. In other words: no Unitary operator is acting on the entire network at any given time: meaning the number of qubits required for a given step depends on the number of inputs in a given layer. Since Quantum Computers are notorious for their ability to run multiple iterations in a short period of time: the efficiency of a quantum neural network is solely dependent on the number of qubits in any given layer: and not on the depth of the network. === Cost functions === To determine the effectiveness of a neural network: a cost function is used: which essentially measures the proximity of the network's output to the expected or desired output. In a Classical Neural Network: the weights (w{\displaystyle w}) and biases (b{\displaystyle b}) at each step determine the outcome of the cost function C(w:b){\displaystyle C(w:b)}. When training a Classical Neural network: the weights and biases are adjusted after each iteration: and given equation 1 below: where y(x){\displaystyle y(x)} is the desired output and aout(x){\displaystyle a^{\text{out}}(x)} is the actual output: the cost function is optimized when C(w:b){\displaystyle C(w:b)}= 0. For a quantum neural network: the cost function is determined by measuring the fidelity of the outcome state (ρout{\displaystyle \rho ^{\text{out}}}) with the desired outcome state (ϕout{\displaystyle \phi ^{\text{out}}}): seen in Equation 2 below. In this case: the Unitary operators are adjusted after each iteration: and the cost function
0.41998619999503717,What is a Siamese neural network?,"A Siamese neural network, also known as a twin neural network, is an artificial neural network that uses the same weights while operating on two different input vectors to compute comparable output vectors. It is often used for comparing similar instances in different type sets and in applications such as face recognition and matching queries with indexed documents.","each point in time the agent performs an action and the environment generates an observation and an instantaneous cost: according to some (usually unknown) rules. The rules and the long-term cost usually only can be estimated. At any juncture: the agent decides whether to explore new actions to uncover their costs or to exploit prior learning to proceed more quickly. Formally the environment is modeled as a Markov decision process (MDP) with states s1:...:sn∈S{\displaystyle \textstyle {s_{1}:...:s_{n}}\in S} and actions a1:...:am∈A{\displaystyle \textstyle {a_{1}:...:a_{m}}\in A}. Because the state transitions are not known: probability distributions are used instead: the instantaneous cost distribution P(ct|st){\displaystyle \textstyle P(c_{t}|s_{t})}: the observation distribution P(xt|st){\displaystyle \textstyle P(x_{t}|s_{t})} and the transition distribution P(st+1|st:at){\displaystyle \textstyle P(s_{t+1}|s_{t}:a_{t})}: while a policy is defined as the conditional distribution over actions given the observations. Taken together: the two define a Markov chain (MC). The aim is to discover the lowest-cost MC. ANNs serve as the learning component in such applications. Dynamic programming coupled with ANNs (giving neurodynamic programming) has been applied to problems such as those involved in vehicle routing: video games: natural resource management and medicine because of ANNs ability to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of control problems. Tasks that fall within the paradigm of reinforcement learning are control problems: games and other sequential decision making tasks. ==== Self-learning ==== Self-learning in neural networks was introduced in 1982 along with a neural network capable of self-learning named crossbar adaptive array (CAA). It is a system with only one input: situation s: and only one output: action (or behavior) a. It has neither external advice input nor external reinforcement input from the environment. The CAA computes: in a crossbar fashion: both decisions about actions and emotions (feelings) about encountered situations. The system is driven by the interaction between cognition and emotion. Given the memory matrix: W =||w(a:s)||: the crossbar self-learning algorithm in each iteration performs the following computation: In situation s perform action a; Receive consequence situation s'; Compute emotion of being in consequence situation v(s'); Update crossbar memory w'(a:s) = w(a:s) + v(s'). The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments: one is behavioral environment where it behaves: and the other is genetic environment: where from it initially and only once receives initial emotions about to be encountered situations in the behavioral environment. Having received the genome vector (species vector) from the genetic environment: the CAA will learn a goal-seeking behavior: in the behavioral environment that contains both desirable and undesirable situations. ==== Neuroevolution ==== Neuroevolution can create neural network topologies and weights using evolutionary computation. It is competitive with sophisticated gradient descent approaches. One advantage of neuroevolution is that it may be less prone to get caught in ""dead ends"". === Stochastic neural network === Stochastic neural networks originating from Sherrington–Kirkpatrick models are a type of artificial neural network built by introducing random variations into the network: either by giving the"
0.43730459999642335,How is learning conducted in twin networks?,"Learning in twin networks can be achieved through two methods: triplet loss or contrastive loss. For learning by triplet loss, a baseline vector, positive vector, and negative vector are used where the negative vector pushes learning in the network and the positive vector acts as a regularizer. For learning by contrastive loss, there must be a weight decay to regularize the weights.","with document retrieval: sometimes using a vector database. Given a query: a document retriever is called to retrieve the most relevant (usually measured by first encoding the query and the documents into vectors: then finding the documents with vectors closest in Euclidean norm to the query vector). The LLM then generates an output based on both the query and the retrieved documents. == Agency == An LLM is a language model: which is not an agent as it has no goal: but it can be used as a component of an intelligent agent. Researchers have described several methods for such integrations.The ReAct (""Reason + Act"") method constructs an agent out of an LLM: using the LLM as a planner. The LLM is prompted to ""think out loud"". Specifically: the language model is prompted with a textual description of the environment: a goal: a list of possible actions: and a record of the actions and observations so far. It generates one or more thoughts before generating an action: which is then executed in the environment. The linguistic description of the environment given to the LLM planner can even be the LaTeX code of a paper describing the environment.In the DEPS (""Describe: Explain: Plan and Select"") method: an LLM is first connected to the visual world via image descriptions: then it is prompted to produce plans for complex tasks and behaviors based on its pretrained knowledge and environmental feedback it receives.The Reflexion method constructs an agent that learns over multiple episodes. At the end of each episode: the LLM is given the record of the episode: and prompted to think up ""lessons learned"": which would help it perform better at a subsequent episode. These ""lessons learned"" are given to the agent in the subsequent episodes.Monte Carlo tree search can use an LLM as rollout heuristic. When a programmatic world model is not available: an LLM can also be prompted with a description of the environment to act as world model.For open-ended exploration: an LLM can be used to score observations for their ""interestingness"": which can be used as a reward signal to guide a normal (non-LLM) reinforcement learning agent. Alternatively: it can propose increasingly difficult tasks for curriculum learning. Instead of outputting individual actions: an LLM planner can also construct ""skills"": or functions for complex action sequences. The skills can be stored and later invoked: allowing increasing levels of abstraction in planning.LLM-powered agents can keep a long-term memory of its previous contexts: and the memory can be retrieved in the same way as Retrieval Augmented Generation. Multiple such agents can interact socially. == Compression == Typically: LLM are trained with full- or half-precision floating point numbers (float32 and float16). One float16 has 16 bits: or 2 bytes: and so one billion parameters require 2 gigabytes. The largest models typically have 100 billion parameters: requiring 200 gigabytes to load: which places them outside the range of most consumer electronics.Post-training quantization aims to decrease the space requirement by lowering precision of the parameters of"
0.421792999986792,What is a common goal during learning and what is a frequently used metric?,The common goal during learning is to minimize a distance metric for similar objects and to maximize it for distinct ones. The most common distance metric used is the Euclidean distance. ,"Timo Aila: Samuli Laine: and Jaakko Lehtinen. Here the GAN generator is grown from small to large scale in a pyramidal fashion. StyleGANs improve consistency between fine and coarse details in the generator network. == Transformers and their variants == Many modern large language models such as ChatGPT: GPT-4: and BERT use a feedforward neural network called Transformer by Ashish Vaswani et. al. in their 2017 paper ""Attention Is All You Need."" Transformers have increasingly become the model of choice for natural language processing problems: replacing recurrent neural networks (RNNs) such as long short-term memory (LSTM).Basic ideas for this go back a long way: in 1992: Juergen Schmidhuber published the Transformer with ""linearized self-attention"" (save for a normalization operator): which is also called the ""linear Transformer."" He advertised it as an ""alternative to RNNs"" that can learn ""internal spotlights of attention:"" and experimentally applied it to problems of variable binding. Here a slow feedforward neural network learns by gradient descent to control the fast weights of another neural network through outer products of self-generated activation patterns called ""FROM"" and ""TO"" which in Transformer terminology are called ""key"" and ""value"" for ""self-attention."" This fast weight ""attention mapping"" is applied to queries. The 2017 Transformer combines this with a softmax operator and a projection matrix.Transformers are also increasingly being used in computer vision. == Deep learning with unsupervised or self-supervised pre-training == In the 1980s: backpropagation did not work well for deep FNNs and RNNs. Here the word ""deep"" refers to the number of layers through which the data is transformed. More precisely: deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For an FNN: the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For RNNs: in which a signal may propagate through a layer more than once: the CAP depth is potentially unlimited. To overcome this problem: Juergen Schmidhuber (1992) proposed a self-supervised hierarchy of RNNs pre-trained one level at a time by self-supervised learning. This ""neural history compressor"" uses predictive coding to learn internal representations at multiple self-organizing time scales. The deep architecture may be used to reproduce the original data from the top level feature activations. The RNN hierarchy can be ""collapsed"" into a single RNN: by ""distilling"" a higher level ""chunker"" network into a lower level ""automatizer"" network. In 1993: a chunker solved a deep learning task whose CAP depth exceeded 1000. Such history compressors can substantially facilitate downstream supervised deep learning.Geoffrey Hinton et al. (2006) proposed learning a high-level internal representation using successive layers of binary or real-valued latent variables with a restricted Boltzmann machine to model each layer. This RBM is a generative stochastic feedforward neural network that can learn a probability distribution over its set of inputs. Once sufficiently many layers have been learned: the deep"
0.4237866000039503,How is a 'half-twin' network different from a twin network?,"While similar to a twin network, a 'half-twin' network implements slightly different functions. The delta function (distance calculation) between the two functions implemented by 'half-twin' network varies based on whether the indexes of the two vectors are the same or different.",weights were trained with back propagation (supervised learning). === Convolutional === A convolutional neural network (CNN: or ConvNet or shift invariant or space invariant) is a class of deep network: composed of one or more convolutional layers with fully connected layers (matching those in typical ANNs) on top. It uses tied weights and pooling layers. In particular: max-pooling. It is often structured via Fukushima's convolutional architecture. They are variations of multilayer perceptrons that use minimal preprocessing. This architecture allows CNNs to take advantage of the 2D structure of input data. Its unit connectivity pattern is inspired by the organization of the visual cortex. Units respond to stimuli in a restricted region of space known as the receptive field. Receptive fields partially overlap: over-covering the entire visual field. Unit response can be approximated mathematically by a convolution operation.CNNs are suitable for processing visual and other two-dimensional data. They have shown superior results in both image and speech applications. They can be trained with standard backpropagation. CNNs are easier to train than other regular: deep: feed-forward neural networks and have many fewer parameters to estimate.Capsule Neural Networks (CapsNet) add structures called capsules to a CNN and reuse output from several capsules to form more stable (with respect to various perturbations) representations.Examples of applications in computer vision include DeepDream and robot navigation. They have wide applications in image and video recognition: recommender systems and natural language processing. === Deep stacking network === A deep stacking network (DSN) (deep convex network) is based on a hierarchy of blocks of simplified neural network modules. It was introduced in 2011 by Deng and Yu. It formulates the learning as a convex optimization problem with a closed-form solution: emphasizing the mechanism's similarity to stacked generalization. Each DSN block is a simple module that is easy to train by itself in a supervised fashion without backpropagation for the entire blocks.Each block consists of a simplified multi-layer perceptron (MLP) with a single hidden layer. The hidden layer h has logistic sigmoidal units: and the output layer has linear units. Connections between these layers are represented by weight matrix U; input-to-hidden-layer connections have weight matrix W. Target vectors t form the columns of matrix T: and the input data vectors x form the columns of matrix X. The matrix of hidden units is H=σ(WTX){\displaystyle {\boldsymbol {H}}=\sigma ({\boldsymbol {W}}^{T}{\boldsymbol {X}})}. Modules are trained in order: so lower-layer weights W are known at each stage. The function performs the element-wise logistic sigmoid operation. Each block estimates the same final label class y: and its estimate is concatenated with original input X to form the expanded input for the next block. Thus: the input to the first block contains the original data only: while downstream blocks' input adds the output of preceding blocks. Then learning the upper-layer weight matrix U given other weights in the network can be formulated as a convex optimization problem: minUTf=‖UTH−T‖F2:{\displaystyle \min _{U^{T}}f=\|{\boldsymbol {U}}^{T}{\boldsymbol {H}}-{\boldsymbol {T}}\|_{F}^{2}:}which has a closed-form solution.Unlike other deep architectures: such as DBNs: the goal is
0.3862051999894902,How are twin networks used in object tracking?,"Twin networks have been used in object tracking due to their unique two tandem inputs and similarity measurement capabilities. One input of the twin network is a user pre-selected exemplar image, and the other is a larger search image. The twin network's job is to locate the exemplar image within the search image by measuring the similarity between the exemplar and each part of the search image, thereby producing a map of similarity scores.",computing has emerged as a potential alternative to GPU acceleration for modern neural networks: particularly considering the looming obsolescence of Moore's Law. Consequently: optical neural networks have garnered increased attention in the research community. Presently: two primary methods of optical neural computing are under research: silicon photonics-based and free-space optics. Each approach has its benefits and drawbacks; while silicon photonics may offer superior speed: it lacks the massive parallelism that free-space optics can deliver. Given the substantial parallelism capabilities of free-space optics: researchers have focused on taking advantage of it. One implementation: proposed by Lin et al.: involves the training and fabrication of phase masks for a handwritten digit classifier. By stacking 3D-printed phase masks: light passing through the fabricated network can be read by a photodetector array of ten detectors: each representing a digit class ranging from 1 to 10. Although this network can achieve terahertz-range classification: it lacks flexibility: as the phase masks are fabricated for a specific task and cannot be retrained. An alternative method for classification in free-space optics: introduced by Cahng et al.: employs a 4F system that is based on the convolution theorem to perform convolution operations. This system uses two lenses to execute the Fourier transforms of the convolution operation: enabling passive conversion into the Fourier domain without power consumption or latency. However: the convolution operation kernels in this implementation are also fabricated phase masks: limiting the device's functionality to specific convolutional layers of the network only. In contrast: Li et al. proposed a technique involving kernel tiling to use the parallelism of the 4F system while using a Digital Micromirror Device (DMD) instead of a phase mask. This approach allows users to upload various kernels into the 4F system and execute the entire network's inference on a single device. Unfortunately: modern neural networks are not designed for the 4F systems: as they were primarily developed during the CPU/GPU era. Mostly because they tend to use a lower resolution and a high number of channels in their feature maps. == Other Implementations == In 2007 there was one model of Optical Neural Network: the Programmable Optical Array/Analogic Computer (POAC). It had been implemented in the year 2000 and reported based on modified Joint Fourier Transform Correlator (JTC) and Bacteriorhodopsin (BR) as a holographic optical memory. Full parallelism: large array size and the speed of light are three promises offered by POAC to implement an optical CNN. They had been investigated during the last years with their practical limitations and considerations yielding the design of the first portable POAC version. The practical details – hardware (optical setups) and software (optical templates) – were published. However: POAC is a general purpose and programmable array computer that has a wide range of applications including: image processing pattern recognition target tracking real-time video processing document security optical switching == See also == Optical computing Quantum neural network == References == Physics-informed neural networks (PINNs) are a type of universal function approximators that can embed the knowledge of
0.447153499990236,What are Spiking neural networks (SNNs)?,Spiking neural networks (SNNs) are artificial neural networks that closely mimic natural neural networks. They incorporate the concept of time into their operating model and transmit information only when a neuron's membrane potential reaches a specific threshold value. This model represents a significant departure from typical multi-layer perceptron networks that transmit information at each propagation cycle.,"first neural networks to demonstrate learning of latent variables (hidden units). Boltzmann machine learning was at first slow to simulate: but the contrastive divergence algorithm speeds up training for Boltzmann machines and Products of Experts. ==== Self-organizing map ==== The self-organizing map (SOM) uses unsupervised learning. A set of neurons learn to map points in an input space to coordinates in an output space. The input space can have different dimensions and topology from the output space: and SOM attempts to preserve these. ==== Learning vector quantization ==== Learning vector quantization (LVQ) can be interpreted as a neural network architecture. Prototypical representatives of the classes parameterize: together with an appropriate distance measure: in a distance-based classification scheme. === Simple recurrent === Simple recurrent networks have three layers: with the addition of a set of ""context units"" in the input layer. These units connect from the hidden layer or the output layer with a fixed weight of one. At each time step: the input is propagated in a standard feedforward fashion: and then a backpropagation-like learning rule is applied (not performing gradient descent). The fixed back connections leave a copy of the previous values of the hidden units in the context units (since they propagate over the connections before the learning rule is applied). === Reservoir computing === Reservoir computing is a computation framework that may be viewed as an extension of neural networks. Typically an input signal is fed into a fixed (random) dynamical system called a reservoir whose dynamics map the input to a higher dimension. A readout mechanism is trained to map the reservoir to the desired output. Training is performed only at the readout stage. Liquid-state machines are a type of reservoir computing. ==== Echo state ==== The echo state network (ESN) employs a sparsely connected random hidden layer. The weights of output neurons are the only part of the network that are trained. ESN are good at reproducing certain time series. === Long short-term memory === The long short-term memory (LSTM) avoids the vanishing gradient problem. It works even when with long delays between inputs and can handle signals that mix low and high frequency components. LSTM RNN outperformed other RNN and other sequence learning methods such as HMM in applications such as language learning and connected handwriting recognition. === Bi-directional === Bi-directional RNN: or BRNN: use a finite sequence to predict or label each element of a sequence based on both the past and future context of the element. This is done by adding the outputs of two RNNs: one processing the sequence from left to right: the other one from right to left. The combined outputs are the predictions of the teacher-given target signals. This technique proved to be especially useful when combined with LSTM. === Hierarchical === Hierarchical RNN connects elements in various ways to decompose hierarchical behavior into useful subprograms. === Stochastic === A district from conventional neural networks: stochastic artificial neural network used as an approximation to random functions. === Genetic"
0.9270431000040844,What is the leaky integrate-and-fire model in SNNs?,"The leaky integrate-and-fire model is the most prominent spiking neuron model. In this model, the neuron's state is considered to be its momentary activation level. Incoming spikes push this value higher or lower until the state either decays or, if the firing threshold is reached, the neuron fires. After firing, the state variable is reset to a lower value.",interconnects of two dimensional arrays of neural inputs and outputs. This research led to extensive research on alternative methods using the strength of the optical interconnect for implementing neuronal communications.Some artificial neural networks that have been implemented as optical neural networks include the Hopfield neural network and the Kohonen self-organizing map with liquid crystal spatial light modulators Optical neural networks can also be based on the principles of neuromorphic engineering: creating neuromorphic photonic systems. Typically: these systems encode information in the networks using spikes: mimicking the functionality of spiking neural networks in optical and photonic hardware. Photonic devices that have demonstrated neuromorphic functionalities include (among others) vertical-cavity surface-emitting lasers: integrated photonic modulators: optoelectronic systems based on superconducting Josephson junctions or systems based on resonant tunnelling diodes. == Electrochemical vs. optical neural networks == Biological neural networks function on an electrochemical basis: while optical neural networks use electromagnetic waves. Optical interfaces to biological neural networks can be created with optogenetics: but is not the same as an optical neural networks. In biological neural networks there exist a lot of different mechanisms for dynamically changing the state of the neurons: these include short-term and long-term synaptic plasticity. Synaptic plasticity is among the electrophysiological phenomena used to control the efficiency of synaptic transmission: long-term for learning and memory: and short-term for short transient changes in synaptic transmission efficiency. Implementing this with optical components is difficult: and ideally requires advanced photonic materials. Properties that might be desirable in photonic materials for optical neural networks include the ability to change their efficiency of transmitting light: based on the intensity of incoming light. == Rising Era of Optical Neural Networks == With the increasing significance of computer vision in various domains: the computational cost of these tasks has increased: making it more important to develop the new approaches of the processing acceleration. Optical computing has emerged as a potential alternative to GPU acceleration for modern neural networks: particularly considering the looming obsolescence of Moore's Law. Consequently: optical neural networks have garnered increased attention in the research community. Presently: two primary methods of optical neural computing are under research: silicon photonics-based and free-space optics. Each approach has its benefits and drawbacks; while silicon photonics may offer superior speed: it lacks the massive parallelism that free-space optics can deliver. Given the substantial parallelism capabilities of free-space optics: researchers have focused on taking advantage of it. One implementation: proposed by Lin et al.: involves the training and fabrication of phase masks for a handwritten digit classifier. By stacking 3D-printed phase masks: light passing through the fabricated network can be read by a photodetector array of ten detectors: each representing a digit class ranging from 1 to 10. Although this network can achieve terahertz-range classification: it lacks flexibility: as the phase masks are fabricated for a specific task and cannot be retrained. An alternative method for classification in free-space optics: introduced by Cahng et al.: employs a 4F system that is based on the convolution theorem to perform convolution operations.
0.4635134999989532,What are some of the challenges in using SNNs?,"Some challenges in using SNNs include the non-differentiability of the spiking nonlinearity and the implementation of the optimization algorithm. The all-or-nothing behavior of the binary spiking nonlinearity makes neurons unsuitable for gradient-based optimization, and standard Backpropagation can be computationally expensive. ","learning models are able to learn the style of an artist or musician from huge datasets and generate completely new artworks and music compositions. For instance: DALL-E is a deep neural network trained on 650 million pairs of images and texts across the internet that can create artworks based on text entered by the user. In the field of music: transformers are used to create original music for commercials and documentaries through companies such as AIVA and Jukedeck. In the marketing industry generative models are used to create personalized advertisements for consumers. Additionally: major film companies are partnering with technology companies to analyze the financial success of a film: such as the partnership between Warner Bros and technology company Cinelytic established in 2020. Furthermore: neural networks have found uses in video game creation: where Non Player Characters (NPCs) can make decisions based on all the characters currently in the game. == See also == == External links == A Brief Introduction to Neural Networks (D. Kriesel) - Illustrated: bilingual manuscript about artificial neural networks; Topics so far: Perceptrons: Backpropagation: Radial Basis Functions: Recurrent Neural Networks: Self Organizing Maps: Hopfield Networks. Review of Neural Networks in Materials Science Artificial Neural Networks Tutorial in three languages (Univ. Politécnica de Madrid) Another introduction to ANN Next Generation of Neural Networks - Google Tech Talks Performance of Neural Networks Neural Networks and Information Sanderson G (5 October 2017). ""But what is a Neural Network?"". 3Blue1Brown. Archived from the original on 7 November 2021 – via YouTube. == Notes == == References == == Bibliography == A neural network is a group of interconnected units called neurons that send signals to one another. Neurons can be either biological cells or mathematical models. While individual neurons are simple: many of them together in a network can perform complex tasks. There are two main types of neural network. In neuroscience: a biological neural network is a physical structure found in brains and complex nervous systems – a population of nerve cells connected by synapses. In machine learning: an artificial neural network is a mathematical model used to approximate nonlinear functions. Artificial neural networks are used to solve artificial intelligence problems. == Biological neural network == A biological neural network is a population of biological neurons chemically connected to each other by synapses. A given neuron can be connected to hundreds of thousands of synapses. Each neuron sends and receives electrochemical signals called action potentials to its connected neighbors. A neuron can serve an excitatory role: amplifying and propagating signals it receives: or an inhibitory role: suppressing signals instead.Populations of interconnected neurons that are smaller than neural networks are called neural circuits. Very large interconnected networks are called large scale brain networks: and many of these together form brains and nervous systems. Signals generated by neural networks in the brain eventually travel through the nervous system and across neuromuscular junctions to muscle cells: where they cause contraction and thereby motion. == Artificial neural network == An artificial neural"
0.42360169999301434,What are some applications of SNNs?,"SNNs can apply to the same applications as traditional artificial neural networks (ANNs), and in addition, they can model the central nervous system of biological organisms. However, a lack of effective training mechanisms for SNNs can be inhibitory for some applications, like computer vision tasks. ","of physical neural network that have very similar properties to (Little-)Hopfield networks: as they have continuous dynamics: a limited memory capacity and natural relaxation via the minimization of a function which is asymptotic to the Ising model. In this sense: the dynamics of a memristive circuit have the advantage compared to a Resistor-Capacitor network to have a more interesting non-linear behavior. From this point of view: engineering analog memristive networks account for a peculiar type of neuromorphic engineering in which the device behavior depends on the circuit wiring or topology. The evolution of these networks can be studied analytically using variations of the Caravelli–Traversa–Di Ventra equation. == Pseudocode == Given a time series x of length sequence_length. In the recurrent neural network: there is a loop that processes all entries of the time series x through the layers neural_network one after another. These have as return value in each time step i both the prediction y_pred[i] and an updated hidden state hidden: which has the length hidden_size. As a result: after the loop: the collection of all predictions y_pred is returned. The following pseudocode (based on the programming language Python) illustrates the functionality of a recurrent neural network. Modern libraries provide runtime-optimized implementations of the above functionality or allow to speed up the slow loop by just-in-time compilation. == Training == === Gradient descent === Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function. In neural networks: it can be used to minimize the error term by changing each weight in proportion to the derivative of the error with respect to that weight: provided the non-linear activation functions are differentiable. Various methods for doing so were developed in the 1980s and early 1990s by Werbos: Williams: Robinson: Schmidhuber: Hochreiter: Pearlmutter and others. The standard method is called ""backpropagation through time"" or BPTT: and is a generalization of back-propagation for feed-forward networks. Like that method: it is an instance of automatic differentiation in the reverse accumulation mode of Pontryagin's minimum principle. A more computationally expensive online variant is called ""Real-Time Recurrent Learning"" or RTRL: which is an instance of automatic differentiation in the forward accumulation mode with stacked tangent vectors. Unlike BPTT: this algorithm is local in time but not local in space. In this context: local in space means that a unit's weight vector can be updated using only information stored in the connected units and the unit itself such that update complexity of a single unit is linear in the dimensionality of the weight vector. Local in time means that the updates take place continually (on-line) and depend only on the most recent time step rather than on multiple time steps within a given time horizon as in BPTT. Biological neural networks appear to be local with respect to both time and space.For recursively computing the partial derivatives: RTRL has a time-complexity of O(number of hidden x number of weights) per time step for computing the Jacobian matrices: while BPTT only takes O(number"
0.4451325999980327,What developments have improved the efficiency and computational power of SNNs?,"Incorporating additional neuron dynamics like Spike Frequency Adaptation (SFA) into neuron models is one such development. SFA offers computational benefits by reducing power usage and increasing coding efficiency, especially in repetitive or intense stimuli. This adaptation also enhances signal clarity and introduces short-term memory at the neuron level, refining information processing accuracy and efficiency.","type of large language model (LLM) and a prominent framework for generative artificial intelligence. They are artificial neural networks that are used in natural language processing tasks. GPTs are based on the transformer architecture: pre-trained on large data sets of unlabelled text: and able to generate novel human-like content. As of 2023: most LLMs have these characteristics and are sometimes referred to broadly as GPTs.The first GPT was introduced in 2018 by OpenAI. OpenAI has released very influential GPT foundation models that have been sequentially numbered: to comprise its ""GPT-n"" series. Each of these was significantly more capable than the previous: due to increased size (number of trainable parameters) and training. The most recent of these: GPT-4: was released in March 2023. Such models have been the basis for their more task-specific GPT systems: including models fine-tuned for instruction following—which in turn power the ChatGPT chatbot service.The term ""GPT"" is also used in the names and descriptions of such models developed by others. For example: other GPT foundation models include a series of models created by EleutherAI: and seven models created by Cerebras in 2023. Also: companies in different industries have developed task-specific GPTs in their respective fields: such as Salesforce's ""EinsteinGPT"" (for CRM) and Bloomberg's ""BloombergGPT"" (for finance). == History == === Initial developments === Generative pretraining (GP) was a long-established concept in machine learning applications. It was originally used as a form of semi-supervised learning: as the model is trained first on an unlabelled dataset (pretraining step) by learning to generate datapoints in the dataset: and then it is trained to classify a labelled dataset.While the unnormalized linear transformer dates back to 1992: the modern transformer architecture was not available until 2017 when it was published by researchers at Google in a paper ""Attention Is All You Need"". That development led to the emergence of large language models such as BERT in 2018 which was a pre-trained transformer (PT) but not designed to be generative (BERT was an ""encoder-only"" model). Also around that time: in 2018: OpenAI published its article entitled ""Improving Language Understanding by Generative Pre-Training:"" in which it introduced the first generative pre-trained transformer (GPT) system (""GPT-1"").Prior to transformer-based architectures: the best-performing neural NLP (natural language processing) models commonly employed supervised learning from large amounts of manually-labeled data. The reliance on supervised learning limited their use on datasets that were not well-annotated: and also made it prohibitively expensive and time-consuming to train extremely large language models.The semi-supervised approach OpenAI employed to make a large-scale generative system—and was first to do with a transformer model—involved two stages: an unsupervised generative ""pretraining"" stage to set initial parameters using a language modeling objective: and a supervised discriminative ""fine-tuning"" stage to adapt these parameters to a target task. === Later developments === Regarding more recent GPT foundation models: OpenAI published its first versions of GPT-3 in July 2020. There were three models: with 1B: 6.7B: 175B parameters: respectively named babbage: curie: and davinci (giving initials B: C: and D).In July"
0.3923781999910716,What is the Unified Modeling Language (UML)? ,"The Unified Modeling Language (UML) is a general-purpose visual modeling language that provides a standard way to visualize the design of a system. It offers a standard notation for many types of diagrams which can be roughly categorized into behavior diagrams, interaction diagrams, and structure diagrams.","simulate artificial synapses. Examples include the ADALINE memristor-based neural network. An optical neural network is a physical implementation of an artificial neural network with optical components. == Dynamic == Dynamic neural networks address nonlinear multivariate behaviour and include (learning of) time-dependent behaviour: such as transient phenomena and delay effects. Techniques to estimate a system process from observed data fall under the general category of system identification. === Cascading === Cascade correlation is an architecture and supervised learning algorithm. Instead of just adjusting the weights in a network of fixed topology: Cascade-Correlation begins with a minimal network: then automatically trains and adds new hidden units one by one: creating a multi-layer structure. Once a new hidden unit has been added to the network: its input-side weights are frozen. This unit then becomes a permanent feature-detector in the network: available for producing outputs or for creating other: more complex feature detectors. The Cascade-Correlation architecture has several advantages: It learns quickly: determines its own size and topology: retains the structures it has built even if the training set changes and requires no backpropagation. === Neuro-fuzzy === A neuro-fuzzy network is a fuzzy inference system in the body of an artificial neural network. Depending on the FIS type: several layers simulate the processes involved in a fuzzy inference-like fuzzification: inference: aggregation and defuzzification. Embedding an FIS in a general structure of an ANN has the benefit of using available ANN training methods to find the parameters of a fuzzy system. === Compositional pattern-producing === Compositional pattern-producing networks (CPPNs) are a variation of artificial neural networks which differ in their set of activation functions and how they are applied. While typical artificial neural networks often contain only sigmoid functions (and sometimes Gaussian functions): CPPNs can include both types of functions and many others. Furthermore: unlike typical artificial neural networks: CPPNs are applied across the entire space of possible inputs so that they can represent a complete image. Since they are compositions of functions: CPPNs in effect encode images at infinite resolution and can be sampled for a particular display at whatever resolution is optimal. == Memory networks == Memory networks incorporate long-term memory. The long-term memory can be read and written to: with the goal of using it for prediction. These models have been applied in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base and the output is a textual response.In sparse distributed memory or hierarchical temporal memory: the patterns encoded by neural networks are used as addresses for content-addressable memory: with ""neurons"" essentially serving as address encoders and decoders. However: the early controllers of such memories were not differentiable. === One-shot associative memory === This type of network can add new patterns without re-training. It is done by creating a specific memory structure: which assigns each new pattern to an orthogonal plane using adjacently connected hierarchical arrays. The network offers real-time pattern recognition and high scalability; this requires parallel processing and is thus best suited"
0.3792696000018623,What was the motivation behind the creation of UML? ,The creation of UML was primarily motivated by the desire to standardize the disparate notational systems and approaches to software design. ,the maximum number of training generations has been reached.The fitness function evaluates the stopping criterion as it receives the mean-squared error reciprocal from each network during training. Therefore: the goal of the genetic algorithm is to maximize the fitness function: reducing the mean-squared error. Other global (and/or evolutionary) optimization techniques may be used to seek a good set of weights: such as simulated annealing or particle swarm optimization. == Related fields and models == RNNs may behave chaotically. In such cases: dynamical systems theory may be used for analysis. They are in fact recursive neural networks with a particular structure: that of a linear chain. Whereas recursive neural networks operate on any hierarchical structure: combining child representations into parent representations: recurrent neural networks operate on the linear progression of time: combining the previous time step and a hidden representation into the representation for the current time step. In particular: RNNs can appear as nonlinear versions of finite impulse response and infinite impulse response filters and also as a nonlinear autoregressive exogenous model (NARX).The effect of memory-based learning for the recognition of sequences can also be implemented by a more biological-based model which uses the silencing mechanism exhibited in neurons with a relatively high frequency spiking activity. == Libraries == Apache Singa Caffe: Created by the Berkeley Vision and Learning Center (BVLC). It supports both CPU and GPU. Developed in C++: and has Python and MATLAB wrappers. Chainer: Fully in Python: production support for CPU: GPU: distributed training. Deeplearning4j: Deep learning in Java and Scala on multi-GPU-enabled Spark. Flux: includes interfaces for RNNs: including GRUs and LSTMs: written in Julia. Keras: High-level API: providing a wrapper to many other deep learning libraries. Microsoft Cognitive Toolkit MXNet: an open-source deep learning framework used to train and deploy deep neural networks. PyTorch: Tensors and Dynamic neural networks in Python with GPU acceleration. TensorFlow: Apache 2.0-licensed Theano-like library with support for CPU: GPU and Google's proprietary TPU: mobile Theano: A deep-learning library for Python with an API largely compatible with the NumPy library. Torch: A scientific computing framework with support for machine learning algorithms: written in C and Lua. == Applications == Applications of recurrent neural networks include: Machine translation Robot control Time series prediction Speech recognition Speech synthesis Brain–computer interfaces Time series anomaly detection Text-to-Video model Rhythm learning Music composition Grammar learning Handwriting recognition Human action recognition Protein homology detection Predicting subcellular localization of proteins Several prediction tasks in the area of business process management Prediction in medical care pathways Predictions of fusion plasma disruptions in reactors (Fusion Recurrent Neural Network (FRNN) code) == References == == Further reading == Mandic: Danilo P.; Chambers: Jonathon A. (2001). Recurrent Neural Networks for Prediction: Learning Algorithms: Architectures and Stability. Wiley. ISBN 978-0-471-49517-8. == External links == Recurrent Neural Networks with over 60 RNN papers by Jürgen Schmidhuber's group at Dalle Molle Institute for Artificial Intelligence Research Elman Neural Network implementation for WEKA A residual neural network (also referred to as a residual network or ResNet)
0.43093489998136647,When and by whom was UML developed? ,"UML was developed at Rational Software in 1994–1995, with further development led by them through 1996. ","many of them together in a network can perform complex tasks. There are two main types of neural network. In neuroscience: a biological neural network is a physical structure found in brains and complex nervous systems – a population of nerve cells connected by synapses. In machine learning: an artificial neural network is a mathematical model used to approximate nonlinear functions. Artificial neural networks are used to solve artificial intelligence problems. == Biological neural network == A biological neural network is a population of biological neurons chemically connected to each other by synapses. A given neuron can be connected to hundreds of thousands of synapses. Each neuron sends and receives electrochemical signals called action potentials to its connected neighbors. A neuron can serve an excitatory role: amplifying and propagating signals it receives: or an inhibitory role: suppressing signals instead.Populations of interconnected neurons that are smaller than neural networks are called neural circuits. Very large interconnected networks are called large scale brain networks: and many of these together form brains and nervous systems. Signals generated by neural networks in the brain eventually travel through the nervous system and across neuromuscular junctions to muscle cells: where they cause contraction and thereby motion. == Artificial neural network == An artificial neural network is a mathematical model used to approximate nonlinear functions. While early artificial neural networks were physical machines: today they are almost always implemented in software. Neurons in an artificial neural network are usually arranged into layers: with information passing from the first layer (the input layer) through one or more intermediate layers (hidden layers) to the final layer (the output layer). The ""signal"" input to each neuron is a number: specifically a linear combination of the outputs of the connected neurons in the previous layer. The signal each neuron outputs is calculated from this number: according to its activation function. The behavior of the network depends on the strengths (or weights) of the connections between neurons. A network is trained by modifying these weights through empirical risk minimization or backpropagation in order to fit some preexisting dataset.Neural networks are used to solve problems in artificial intelligence: and have thereby found applications in many disciplines: including predictive modeling: adaptive control: facial recognition: handwriting recognition: general game playing: and generative AI. == History == The theoretical base for contemporary neural networks was independently proposed by Alexander Bain in 1873 and William James in 1890. Both posited that human thought emerged from interactions among large numbers of neurons inside the brain. In 1949: Donald Hebb described Hebbian learning: the idea that neural networks can change and learn over time by strengthening a synapse every time a signal travels along it.Artificial neural networks were originally used to model biological neural networks starting in the 1930s under the approach of connectionism. However: starting with the invention of the perceptron: a simple artificial neural network: by Warren McCulloch and Walter Pitts in 1943: followed by the implementation of one in hardware by Frank Rosenblatt in 1957: artificial"
0.4040462999837473,Who manages UML and when was it adopted as a standard? ,"UML was adopted as a standard by the Object Management Group (OMG) in 1997 and has been managed by this organization ever since. In 2005, UML was also published by the International Organization for Standardization (ISO) and the International Electrotechnical Commission (IEC) as the ISO/IEC 19501 standard.","(activation 1.0) through strong positive bias weights: they become the identity skip connections in Residual Networks. The original Highway Network paper not only introduced the basic principle for very deep feedforward networks: but also included experimental results with 20: 50: and 100 layers networks: and mentioned ongoing experiments with up to 900 layers. Networks with 50 or 100 layers had lower training error than their plain network counterparts: but no lower training error than their 20 layers counterpart (on the MNIST dataset: Figure 1 in ). No improvement on test accuracy was reported with networks deeper than 19 layers (on the CIFAR-10 dataset; Table 1 in ). The ResNet paper: however: provided strong experimental evidence of the benefits of going deeper than 20 layers. It argued that the identity mapping without modulation is crucial and mentioned that modulation in the skip connection can still lead to vanishing signals in forward and backward propagation (Section 3 in ). This is also why the forget gates of the 2000 LSTM were initially opened through positive bias weights: as long as the gates are open: it behaves like the 1997 LSTM. Similarly: a Highway Net whose gates are opened through strongly positive bias weights behaves like a ResNet. The skip connections used in modern neural networks (e.g.: Transformers) are dominantly identity mappings. DenseNets in 2016 were designed as deep neural networks that attempt to connect each layer to every other layer. DenseNets approached this goal by using identity mappings as skip connections. Unlike ResNets: DenseNets merge the layer output with skip connections by concatenation: not addition. Neural networks with Stochastic Depth were made possible given the Residual Network architectures. This training procedure randomly drops a subset of layers and lets the signal propagate through the identity skip connection. Also known as ""DropPath"": this is an effective regularization method for training large and deep models: such as the Vision Transformer (ViT). == Biological relation == The original Residual Network paper made no claim on being inspired by biological systems. But research later on has related Residual Networks to biologically-plausible algorithms. A study published in Science in 2023 disclosed the complete connectome of an insect brain (of a fruit fly larva). This study discovered ""multilayer shortcuts"" that resemble the skip connections in artificial neural networks: including ResNets. == References == A Siamese neural network (sometimes called a twin neural network) is an artificial neural network that uses the same weights while working in tandem on two different input vectors to compute comparable output vectors. Often one of the output vectors is precomputed: thus forming a baseline against which the other output vector is compared. This is similar to comparing fingerprints but can be described more technically as a distance function for locality-sensitive hashing.It is possible to build an architecture that is functionally similar to a siamese network but implements a slightly different function. This is typically used for comparing similar instances in different type sets.Uses of similarity measures where a twin network might be used"
0.4293984999821987,What are the main groups that UML diagrams can be divided into?,"UML diagrams can be largely divided into three main groups: behavior diagrams, interaction diagrams, and structure diagrams.","that require small additional processing exist. === Spiking === Spiking neural networks (SNN) explicitly consider the timing of inputs. The network input and output are usually represented as a series of spikes (delta function or more complex shapes). SNN can process information in the time domain (signals that vary over time). They are often implemented as recurrent networks. SNN are also a form of pulse computer.Spiking neural networks with axonal conduction delays exhibit polychronization: and hence could have a very large memory capacity.SNN and the temporal correlations of neural assemblies in such networks—have been used to model figure/ground separation and region linking in the visual system. === Spatial === Spatial neural networks (SNNs) constitute a supercategory of tailored neural networks (NNs) for representing and predicting geographic phenomena. They generally improve both the statistical accuracy and reliability of the a-spatial/classic NNs whenever they handle geo-spatial datasets: and also of the other spatial (statistical) models (e.g. spatial regression models) whenever the geo-spatial datasets' variables depict non-linear relations. Examples of SNNs are the OSFA spatial neural networks: SVANNs and GWNNs. === Neocognitron === The neocognitron is a hierarchical: multilayered network that was modeled after the visual cortex. It uses multiple types of units: (originally two: called simple and complex cells): as a cascading model for use in pattern recognition tasks. Local features are extracted by S-cells whose deformation is tolerated by C-cells. Local features in the input are integrated gradually and classified at higher layers. Among the various kinds of neocognitron are systems that can detect multiple patterns in the same input by using back propagation to achieve selective attention. It has been used for pattern recognition tasks and inspired convolutional neural networks. === Compound hierarchical-deep models === Compound hierarchical-deep models compose deep networks with non-parametric Bayesian models. Features can be learned using deep architectures such as DBNs: deep Boltzmann machines (DBM): deep auto encoders: convolutional variants: ssRBMs: deep coding networks: DBNs with sparse feature learning: RNNs: conditional DBNs: denoising autoencoders. This provides a better representation: allowing faster learning and more accurate classification with high-dimensional data. However: these architectures are poor at learning novel classes with few examples: because all network units are involved in representing the input (a distributed representation) and must be adjusted together (high degree of freedom). Limiting the degree of freedom reduces the number of parameters to learn: facilitating learning of new classes from few examples. Hierarchical Bayesian (HB) models allow learning from few examples: for example for computer vision: statistics and cognitive science. Compound HD architectures aim to integrate characteristics of both HB and deep networks. The compound HDP-DBM architecture is a hierarchical Dirichlet process (HDP) as a hierarchical model: incorporating DBM architecture. It is a full generative model: generalized from abstract concepts flowing through the model layers: which is able to synthesize new examples in novel classes that look ""reasonably"" natural. All the levels are learned jointly by maximizing a joint log-probability score.In a DBM with three hidden layers: the probability of a visible input ''ν'' is:"
