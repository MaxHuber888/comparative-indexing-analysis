query_time,query,answer,response_0,response_1
0.4894077999924775,What is an AI accelerator?,"An AI accelerator, also referred to as deep learning processor, or neural processing unit (NPU), is a class of specialized hardware accelerator or computer system designed to accelerate artificial intelligence and machine learning applications, including artificial neural networks and machine vision.","Score: 0.8642034728443146 / An AI accelerator: deep learning processor: or neural processing unit (NPU) is a class of specialized hardware accelerator or computer system designed to accelerate artificial intelligence and machine learning applications: including artificial neural networks and machine vision. Typical applications include algorithms for robotics: Internet of Things: and other data-intensive or sensor-driven tasks. They are often manycore designs and generally focus on low-precision arithmetic: novel dataflow architectures or in-memory computing capability. As of 2024: a typical AI integrated circuit chip contains tens of billions of MOSFET transistors.AI accelerators are used in mobile devices: such as neural processing units (NPUs) in Apple iPhones or Huawei cellphones: and personal computers such as Apple silicon Macs: to cloud computing servers such as tensor processing units (TPU) in the Google Cloud Platform. A number of vendor-specific terms exist for devices in this category: and it is an emerging technology without a dominant design. / Graphics processing units designed by companies such as Nvidia and AMD often include AI-specific hardware: and are commonly used as AI accelerators: both for training and inference. /  /  / == History == / Computer systems have frequently complemented the CPU with special-purpose accelerators for specialized tasks: known as coprocessors. Notable application-specific hardware units include video cards for graphics: sound cards: graphics processing units and digital signal processors. As deep learning and artificial intelligence workloads rose in prominence in the 2010s: specialized hardware units were developed or adapted from existing products to accelerate these tasks. /  /  / === Early attempts === / First attempts like Intel's ETANN 80170NX incorporated analog circuits to compute neural functions.Later all-digital chips like the Nestor/Intel Ni1000 followed. As early as 1993: digital signal processors were used as neural network accelerators to accelerate optical character recognition software.By 1988: Wei Zhang et al. had discussed fast optical implementations of convolutional neural networks for alphabet recognition.In the 1990s: there were also attempts to create parallel high-throughput systems for workstations aimed at various applications: including neural network simulations.This presentation covers a past attempt at neural net accelerators: notes the similarity to the modern SLI GPGPU processor setup: and argues that general purpose vector accelerators are the way forward (in relation to RISC-V hwacha project. Argues that NN's are just dense and sparse matrices: one of several recurring algorithms)FPGA-based accelerators were also first explored in the 1990s for both inference and training.In 2014: Chen et al. proposed DianNao (Chinese for ""electric brain""): to accelerate deep neural networks especially. DianNao provides the 452 Gop/s peak performance (of key operations in deep neural networks) only in a small footprint of 3.02 mm2 and 485 mW. Later: the successors (DaDianNao: ShiDianNao: PuDianNao) are proposed by the same group: forming the DianNao FamilySmartphones began incorporating AI accelerators starting with the Qualcomm Snapdragon 820 in 2015. /  /  / === Heterogeneous computing === /  / Heterogeneous computing incorporates many specialized processors in a single system: or a single chip: each optimized for a specific type of task. Architectures such as the Cell microprocessor have features significantly overlapping with AI accelerators including: support for packed low precision arithmetic: dataflow architecture: and prioritizing throughput over latency. The Cell microprocessor has been applied to a number of tasks including AI.In the 2000s: CPUs also gained increasingly wide SIMD units: driven by video and gaming workloads; as well as support for packed low-precision data types. Due to the increasing performance of CPUs: they are also used for running AI workloads. CPUs are superior for DNNs with small or medium-scale parallelism: for sparse DNNs and in low-batch-size scenarios. /  /  / === Use of GPU === / Graphics processing units or GPUs are specialized hardware for the manipulation of images and calculation of local image properties. The mathematical basis of neural networks and image manipulation are similar: embarrassingly parallel tasks involving matrices: leading GPUs to become increasingly used for machine learning tasks.In 2012: Alex Krizhevsky adopted two GPUs to train a deep learning network: i.e.: AlexNet: which won the champion of the ISLVRC-2012 competition. During the 2010's: GPU manufacturers such as Nvidia added deep learning related features in both hardware (e.g.: INT8 operators) and software (e.g.: cuDNN Library). / GPUs continue to be used in large-scale AI applications.","Score: 0.8433146951589063 / Such an approach improves computational times drastically in comparison with digital algorithms. /  /  / === Atomically thin semiconductors === / In 2020: Marega et al. published experiments with a large-area active channel material for developing logic-in-memory devices and circuits based on floating-gate field-effect transistors (FGFETs). Such atomically thin semiconductors are considered promising for energy-efficient machine learning applications: where the same basic device structure is used for both logic operations and data storage. The authors used two-dimensional materials such as semiconducting molybdenum disulphide to precisely tune FGFETs as building blocks in which logic operations can be performed with the memory elements.  /  /  / === Integrated photonic tensor core === / In 1988: Wei Zhang et al. discussed fast optical implementations of convolutional neural networks for alphabet recognition. / In 2021: J. Feldmann et al. proposed an integrated photonic hardware accelerator for parallel convolutional processing. The authors identify two key advantages of integrated photonics over its electronic counterparts: (1) massively parallel data transfer through wavelength division multiplexing in conjunction with frequency combs: and (2) extremely high data modulation speeds. Their system can execute trillions of multiply-accumulate operations per second: indicating the potential of integrated photonics in data-heavy AI applications. Optical processors that can also perform backpropagation for artificial neural networks have been experimentally developed. /  /  / == Nomenclature == / As of 2016: the field is still in flux and vendors are pushing their own marketing term for what amounts to an ""AI accelerator"": in the hope that their designs and APIs will become the dominant design. There is no consensus on the boundary between these devices: nor the exact form they will take; however several examples clearly aim to fill this new space: with a fair amount of overlap in capabilities. / In the past when consumer graphics accelerators emerged: the industry eventually adopted Nvidia's self-assigned term: ""the GPU"": / as the collective noun for ""graphics accelerators"": which had taken many forms before settling on an overall pipeline implementing a model presented by Direct3D. / All models of Intel Meteor Lake processors have a Versatile Processor Unit (VPU) built-in for accelerating inference for computer vision and deep learning. /  /  / == Deep Learning Processors (DLP) == / Inspired from the pioneer work of DianNao Family: many DLPs are proposed in both academia and industry with design optimized to leverage the features of deep neural networks for high efficiency. Only at ISCA 2016: three sessions: 15% (!) of the accepted papers: are all architecture designs about deep learning. Such efforts include Eyeriss (MIT): EIE (Stanford): Minerva (Harvard): Stripes (University of Toronto) in academia: TPU (Google): and MLU (Cambricon) in industry. We listed several representative works in Table 1. /  /  / === Digital DLPs === / The major components of DLPs architecture usually include a computation component: the on-chip memory hierarchy: and the control logic that manages the data communication and computing flows. / Regarding the computation component: as most operations in deep learning can be aggregated into vector operations: the most common ways for building computation components in digital DLPs are the MAC-based (multiplier-accumulation) organization: either with vector MACs or scalar MACs. Rather than SIMD or SIMT in general processing devices: deep learning domain-specific parallelism is better explored on these MAC-based organizations. Regarding the memory hierarchy: as deep learning algorithms require high bandwidth to provide the computation component with sufficient data: DLPs usually employ a relatively larger size (tens of kilobytes or several megabytes) on-chip buffer but with dedicated on-chip data reuse strategy and data exchange strategy to alleviate the burden for memory bandwidth. For example: DianNao: 16 16-in vector MAC: requires 16 × 16 × 2 = 512 16-bit data: i.e.: almost 1024GB/s bandwidth requirements between computation components and buffers. With on-chip reuse: such bandwidth requirements are reduced drastically. Instead of the widely used cache in general processing devices: DLPs always use scratchpad memory as it could provide higher data reuse opportunities by leveraging the relatively regular data access pattern in deep learning algorithms. Regarding the control logic: as the deep learning algorithms keep evolving at a dramatic speed: DLPs start to leverage dedicated ISA (instruction set architecture) to support the deep learning domain flexibly. At first: DianNao used a VLIW-style instruction set where each instruction could finish a layer in a DNN."
0.16378169998642989,Where are AI accelerators often used?,"AI accelerators are used in various devices, including mobile devices such as neural processing units (NPUs) in Apple iPhones or Huawei cellphones, and personal computers such as Apple silicon Macs, to cloud computing servers such as tensor processing units (TPU) in the Google Cloud Platform. ","Score: 0.8631452599647175 / Due to the increasing performance of CPUs: they are also used for running AI workloads. CPUs are superior for DNNs with small or medium-scale parallelism: for sparse DNNs and in low-batch-size scenarios. /  /  / === Use of GPU === / Graphics processing units or GPUs are specialized hardware for the manipulation of images and calculation of local image properties. The mathematical basis of neural networks and image manipulation are similar: embarrassingly parallel tasks involving matrices: leading GPUs to become increasingly used for machine learning tasks.In 2012: Alex Krizhevsky adopted two GPUs to train a deep learning network: i.e.: AlexNet: which won the champion of the ISLVRC-2012 competition. During the 2010's: GPU manufacturers such as Nvidia added deep learning related features in both hardware (e.g.: INT8 operators) and software (e.g.: cuDNN Library). / GPUs continue to be used in large-scale AI applications. For example: Summit: a supercomputer from IBM for Oak Ridge National Laboratory: contains 27:648 Nvidia Tesla V100 cards: which can be used to accelerate deep learning algorithms. / Over the 2010's GPUs continued to evolve in a direction to facilitate deep learning: both for training and inference in devices such as self-driving cars. GPU developers such as Nvidia NVLink are developing additional connective capability for the kind of dataflow workloads AI benefits from. As GPUs have been increasingly applied to AI acceleration: GPU manufacturers have incorporated neural network-specific hardware to further accelerate these tasks. Tensor cores are intended to speed up the training of neural networks. /  /  / === Use of FPGAs === / Deep learning frameworks are still evolving: making it hard to design custom hardware. Reconfigurable devices such as field-programmable gate arrays (FPGA) make it easier to evolve hardware: frameworks: and software alongside each other.Microsoft has used FPGA chips to accelerate inference for real-time deep learning services. /  /  / === Emergence of dedicated AI accelerator ASICs === / While GPUs and FPGAs perform far better than CPUs for AI-related tasks: a factor of up to 10 in efficiency may be gained with a more specific design: via an application-specific integrated circuit (ASIC). These accelerators employ strategies such as optimized memory use and the use of lower precision arithmetic to accelerate calculation and increase throughput of computation. Some low-precision floating-point formats used for AI acceleration are half-precision and the bfloat16 floating-point format. Companies such as Google: Qualcomm: Amazon: Apple: Facebook: AMD and Samsung are all designing their own AI ASICs. Cerebras Systems has built a dedicated AI accelerator based on the largest processor in the industry: the second-generation Wafer Scale Engine (WSE-2): to support deep learning workloads. /  /  / == Ongoing research == /  /  / === In-memory computing architectures === / In June 2017: IBM researchers announced an architecture in contrast to the Von Neumann architecture based on in-memory computing and phase-change memory arrays applied to temporal correlation detection: intending to generalize the approach to heterogeneous computing and massively parallel systems. In October 2018: IBM researchers announced an architecture based on in-memory processing and modeled on the human brain's synaptic network to accelerate deep neural networks. The system is based on phase-change memory arrays. /  /  / === In-memory computing with analog resistive memories === / In 2019: researchers from Politecnico di Milano found a way to solve systems of linear equations in a few tens of nanoseconds via a single operation. Their algorithm is based on in-memory computing with analog resistive memories which performs with high efficiencies of time and energy: via conducting matrix–vector multiplication in one step using Ohm's law and Kirchhoff's law. The researchers showed that a feedback circuit with cross-point resistive memories can solve algebraic problems such as systems of linear equations: matrix eigenvectors: and differential equations in just one step. Such an approach improves computational times drastically in comparison with digital algorithms. /  /  / === Atomically thin semiconductors === / In 2020: Marega et al. published experiments with a large-area active channel material for developing logic-in-memory devices and circuits based on floating-gate field-effect transistors (FGFETs). Such atomically thin semiconductors are considered promising for energy-efficient machine learning applications: where the same basic device structure is used for both logic operations and data storage. The authors used two-dimensional materials such as semiconducting molybdenum disulphide to precisely tune FGFETs as building blocks in which logic operations can be performed with the memory elements.  /  /  / === Integrated photonic tensor core === / In 1988: Wei Zhang et al.","Score: 0.8623140436622234 / An AI accelerator: deep learning processor: or neural processing unit (NPU) is a class of specialized hardware accelerator or computer system designed to accelerate artificial intelligence and machine learning applications: including artificial neural networks and machine vision. Typical applications include algorithms for robotics: Internet of Things: and other data-intensive or sensor-driven tasks. They are often manycore designs and generally focus on low-precision arithmetic: novel dataflow architectures or in-memory computing capability. As of 2024: a typical AI integrated circuit chip contains tens of billions of MOSFET transistors.AI accelerators are used in mobile devices: such as neural processing units (NPUs) in Apple iPhones or Huawei cellphones: and personal computers such as Apple silicon Macs: to cloud computing servers such as tensor processing units (TPU) in the Google Cloud Platform. A number of vendor-specific terms exist for devices in this category: and it is an emerging technology without a dominant design. / Graphics processing units designed by companies such as Nvidia and AMD often include AI-specific hardware: and are commonly used as AI accelerators: both for training and inference. /  /  / == History == / Computer systems have frequently complemented the CPU with special-purpose accelerators for specialized tasks: known as coprocessors. Notable application-specific hardware units include video cards for graphics: sound cards: graphics processing units and digital signal processors. As deep learning and artificial intelligence workloads rose in prominence in the 2010s: specialized hardware units were developed or adapted from existing products to accelerate these tasks. /  /  / === Early attempts === / First attempts like Intel's ETANN 80170NX incorporated analog circuits to compute neural functions.Later all-digital chips like the Nestor/Intel Ni1000 followed. As early as 1993: digital signal processors were used as neural network accelerators to accelerate optical character recognition software.By 1988: Wei Zhang et al. had discussed fast optical implementations of convolutional neural networks for alphabet recognition.In the 1990s: there were also attempts to create parallel high-throughput systems for workstations aimed at various applications: including neural network simulations.This presentation covers a past attempt at neural net accelerators: notes the similarity to the modern SLI GPGPU processor setup: and argues that general purpose vector accelerators are the way forward (in relation to RISC-V hwacha project. Argues that NN's are just dense and sparse matrices: one of several recurring algorithms)FPGA-based accelerators were also first explored in the 1990s for both inference and training.In 2014: Chen et al. proposed DianNao (Chinese for ""electric brain""): to accelerate deep neural networks especially. DianNao provides the 452 Gop/s peak performance (of key operations in deep neural networks) only in a small footprint of 3.02 mm2 and 485 mW. Later: the successors (DaDianNao: ShiDianNao: PuDianNao) are proposed by the same group: forming the DianNao FamilySmartphones began incorporating AI accelerators starting with the Qualcomm Snapdragon 820 in 2015. /  /  / === Heterogeneous computing === /  / Heterogeneous computing incorporates many specialized processors in a single system: or a single chip: each optimized for a specific type of task. Architectures such as the Cell microprocessor have features significantly overlapping with AI accelerators including: support for packed low precision arithmetic: dataflow architecture: and prioritizing throughput over latency. The Cell microprocessor has been applied to a number of tasks including AI.In the 2000s: CPUs also gained increasingly wide SIMD units: driven by video and gaming workloads; as well as support for packed low-precision data types. Due to the increasing performance of CPUs: they are also used for running AI workloads. CPUs are superior for DNNs with small or medium-scale parallelism: for sparse DNNs and in low-batch-size scenarios. /  /  / === Use of GPU === / Graphics processing units or GPUs are specialized hardware for the manipulation of images and calculation of local image properties. The mathematical basis of neural networks and image manipulation are similar: embarrassingly parallel tasks involving matrices: leading GPUs to become increasingly used for machine learning tasks.In 2012: Alex Krizhevsky adopted two GPUs to train a deep learning network: i.e.: AlexNet: which won the champion of the ISLVRC-2012 competition. During the 2010's: GPU manufacturers such as Nvidia added deep learning related features in both hardware (e.g.: INT8 operators) and software (e.g.: cuDNN Library). / GPUs continue to be used in large-scale AI applications."
0.1796457000018563,How have GPUs been utilized for AI acceleration?,"Graphics processing units or GPUs, that are specialized hardware for the manipulation of images and calculation of local image properties, have been increasingly used for machine learning tasks. GPU developers have also incorporated neural network-specific hardware to further accelerate these tasks.","Score: 0.8819268051008881 / Due to the increasing performance of CPUs: they are also used for running AI workloads. CPUs are superior for DNNs with small or medium-scale parallelism: for sparse DNNs and in low-batch-size scenarios. /  /  / === Use of GPU === / Graphics processing units or GPUs are specialized hardware for the manipulation of images and calculation of local image properties. The mathematical basis of neural networks and image manipulation are similar: embarrassingly parallel tasks involving matrices: leading GPUs to become increasingly used for machine learning tasks.In 2012: Alex Krizhevsky adopted two GPUs to train a deep learning network: i.e.: AlexNet: which won the champion of the ISLVRC-2012 competition. During the 2010's: GPU manufacturers such as Nvidia added deep learning related features in both hardware (e.g.: INT8 operators) and software (e.g.: cuDNN Library). / GPUs continue to be used in large-scale AI applications. For example: Summit: a supercomputer from IBM for Oak Ridge National Laboratory: contains 27:648 Nvidia Tesla V100 cards: which can be used to accelerate deep learning algorithms. / Over the 2010's GPUs continued to evolve in a direction to facilitate deep learning: both for training and inference in devices such as self-driving cars. GPU developers such as Nvidia NVLink are developing additional connective capability for the kind of dataflow workloads AI benefits from. As GPUs have been increasingly applied to AI acceleration: GPU manufacturers have incorporated neural network-specific hardware to further accelerate these tasks. Tensor cores are intended to speed up the training of neural networks. /  /  / === Use of FPGAs === / Deep learning frameworks are still evolving: making it hard to design custom hardware. Reconfigurable devices such as field-programmable gate arrays (FPGA) make it easier to evolve hardware: frameworks: and software alongside each other.Microsoft has used FPGA chips to accelerate inference for real-time deep learning services. /  /  / === Emergence of dedicated AI accelerator ASICs === / While GPUs and FPGAs perform far better than CPUs for AI-related tasks: a factor of up to 10 in efficiency may be gained with a more specific design: via an application-specific integrated circuit (ASIC). These accelerators employ strategies such as optimized memory use and the use of lower precision arithmetic to accelerate calculation and increase throughput of computation. Some low-precision floating-point formats used for AI acceleration are half-precision and the bfloat16 floating-point format. Companies such as Google: Qualcomm: Amazon: Apple: Facebook: AMD and Samsung are all designing their own AI ASICs. Cerebras Systems has built a dedicated AI accelerator based on the largest processor in the industry: the second-generation Wafer Scale Engine (WSE-2): to support deep learning workloads. /  /  / == Ongoing research == /  /  / === In-memory computing architectures === / In June 2017: IBM researchers announced an architecture in contrast to the Von Neumann architecture based on in-memory computing and phase-change memory arrays applied to temporal correlation detection: intending to generalize the approach to heterogeneous computing and massively parallel systems. In October 2018: IBM researchers announced an architecture based on in-memory processing and modeled on the human brain's synaptic network to accelerate deep neural networks. The system is based on phase-change memory arrays. /  /  / === In-memory computing with analog resistive memories === / In 2019: researchers from Politecnico di Milano found a way to solve systems of linear equations in a few tens of nanoseconds via a single operation. Their algorithm is based on in-memory computing with analog resistive memories which performs with high efficiencies of time and energy: via conducting matrix–vector multiplication in one step using Ohm's law and Kirchhoff's law. The researchers showed that a feedback circuit with cross-point resistive memories can solve algebraic problems such as systems of linear equations: matrix eigenvectors: and differential equations in just one step. Such an approach improves computational times drastically in comparison with digital algorithms. /  /  / === Atomically thin semiconductors === / In 2020: Marega et al. published experiments with a large-area active channel material for developing logic-in-memory devices and circuits based on floating-gate field-effect transistors (FGFETs). Such atomically thin semiconductors are considered promising for energy-efficient machine learning applications: where the same basic device structure is used for both logic operations and data storage. The authors used two-dimensional materials such as semiconducting molybdenum disulphide to precisely tune FGFETs as building blocks in which logic operations can be performed with the memory elements.  /  /  / === Integrated photonic tensor core === / In 1988: Wei Zhang et al.","Score: 0.8582872324677233 / An AI accelerator: deep learning processor: or neural processing unit (NPU) is a class of specialized hardware accelerator or computer system designed to accelerate artificial intelligence and machine learning applications: including artificial neural networks and machine vision. Typical applications include algorithms for robotics: Internet of Things: and other data-intensive or sensor-driven tasks. They are often manycore designs and generally focus on low-precision arithmetic: novel dataflow architectures or in-memory computing capability. As of 2024: a typical AI integrated circuit chip contains tens of billions of MOSFET transistors.AI accelerators are used in mobile devices: such as neural processing units (NPUs) in Apple iPhones or Huawei cellphones: and personal computers such as Apple silicon Macs: to cloud computing servers such as tensor processing units (TPU) in the Google Cloud Platform. A number of vendor-specific terms exist for devices in this category: and it is an emerging technology without a dominant design. / Graphics processing units designed by companies such as Nvidia and AMD often include AI-specific hardware: and are commonly used as AI accelerators: both for training and inference. /  /  / == History == / Computer systems have frequently complemented the CPU with special-purpose accelerators for specialized tasks: known as coprocessors. Notable application-specific hardware units include video cards for graphics: sound cards: graphics processing units and digital signal processors. As deep learning and artificial intelligence workloads rose in prominence in the 2010s: specialized hardware units were developed or adapted from existing products to accelerate these tasks. /  /  / === Early attempts === / First attempts like Intel's ETANN 80170NX incorporated analog circuits to compute neural functions.Later all-digital chips like the Nestor/Intel Ni1000 followed. As early as 1993: digital signal processors were used as neural network accelerators to accelerate optical character recognition software.By 1988: Wei Zhang et al. had discussed fast optical implementations of convolutional neural networks for alphabet recognition.In the 1990s: there were also attempts to create parallel high-throughput systems for workstations aimed at various applications: including neural network simulations.This presentation covers a past attempt at neural net accelerators: notes the similarity to the modern SLI GPGPU processor setup: and argues that general purpose vector accelerators are the way forward (in relation to RISC-V hwacha project. Argues that NN's are just dense and sparse matrices: one of several recurring algorithms)FPGA-based accelerators were also first explored in the 1990s for both inference and training.In 2014: Chen et al. proposed DianNao (Chinese for ""electric brain""): to accelerate deep neural networks especially. DianNao provides the 452 Gop/s peak performance (of key operations in deep neural networks) only in a small footprint of 3.02 mm2 and 485 mW. Later: the successors (DaDianNao: ShiDianNao: PuDianNao) are proposed by the same group: forming the DianNao FamilySmartphones began incorporating AI accelerators starting with the Qualcomm Snapdragon 820 in 2015. /  /  / === Heterogeneous computing === /  / Heterogeneous computing incorporates many specialized processors in a single system: or a single chip: each optimized for a specific type of task. Architectures such as the Cell microprocessor have features significantly overlapping with AI accelerators including: support for packed low precision arithmetic: dataflow architecture: and prioritizing throughput over latency. The Cell microprocessor has been applied to a number of tasks including AI.In the 2000s: CPUs also gained increasingly wide SIMD units: driven by video and gaming workloads; as well as support for packed low-precision data types. Due to the increasing performance of CPUs: they are also used for running AI workloads. CPUs are superior for DNNs with small or medium-scale parallelism: for sparse DNNs and in low-batch-size scenarios. /  /  / === Use of GPU === / Graphics processing units or GPUs are specialized hardware for the manipulation of images and calculation of local image properties. The mathematical basis of neural networks and image manipulation are similar: embarrassingly parallel tasks involving matrices: leading GPUs to become increasingly used for machine learning tasks.In 2012: Alex Krizhevsky adopted two GPUs to train a deep learning network: i.e.: AlexNet: which won the champion of the ISLVRC-2012 competition. During the 2010's: GPU manufacturers such as Nvidia added deep learning related features in both hardware (e.g.: INT8 operators) and software (e.g.: cuDNN Library). / GPUs continue to be used in large-scale AI applications."
0.15006710001034662,What has been the progress of AI accelerator technology in the 1990s?,"In the 1990s, attempts were made to create high-throughput parallel systems for workstations aimed at various applications, including neural network simulations. FPGA-based accelerators for both inference and training were first explored in the 1990s.","Score: 0.8255060595548996 / Such an approach improves computational times drastically in comparison with digital algorithms. /  /  / === Atomically thin semiconductors === / In 2020: Marega et al. published experiments with a large-area active channel material for developing logic-in-memory devices and circuits based on floating-gate field-effect transistors (FGFETs). Such atomically thin semiconductors are considered promising for energy-efficient machine learning applications: where the same basic device structure is used for both logic operations and data storage. The authors used two-dimensional materials such as semiconducting molybdenum disulphide to precisely tune FGFETs as building blocks in which logic operations can be performed with the memory elements.  /  /  / === Integrated photonic tensor core === / In 1988: Wei Zhang et al. discussed fast optical implementations of convolutional neural networks for alphabet recognition. / In 2021: J. Feldmann et al. proposed an integrated photonic hardware accelerator for parallel convolutional processing. The authors identify two key advantages of integrated photonics over its electronic counterparts: (1) massively parallel data transfer through wavelength division multiplexing in conjunction with frequency combs: and (2) extremely high data modulation speeds. Their system can execute trillions of multiply-accumulate operations per second: indicating the potential of integrated photonics in data-heavy AI applications. Optical processors that can also perform backpropagation for artificial neural networks have been experimentally developed. /  /  / == Nomenclature == / As of 2016: the field is still in flux and vendors are pushing their own marketing term for what amounts to an ""AI accelerator"": in the hope that their designs and APIs will become the dominant design. There is no consensus on the boundary between these devices: nor the exact form they will take; however several examples clearly aim to fill this new space: with a fair amount of overlap in capabilities. / In the past when consumer graphics accelerators emerged: the industry eventually adopted Nvidia's self-assigned term: ""the GPU"": / as the collective noun for ""graphics accelerators"": which had taken many forms before settling on an overall pipeline implementing a model presented by Direct3D. / All models of Intel Meteor Lake processors have a Versatile Processor Unit (VPU) built-in for accelerating inference for computer vision and deep learning. /  /  / == Deep Learning Processors (DLP) == / Inspired from the pioneer work of DianNao Family: many DLPs are proposed in both academia and industry with design optimized to leverage the features of deep neural networks for high efficiency. Only at ISCA 2016: three sessions: 15% (!) of the accepted papers: are all architecture designs about deep learning. Such efforts include Eyeriss (MIT): EIE (Stanford): Minerva (Harvard): Stripes (University of Toronto) in academia: TPU (Google): and MLU (Cambricon) in industry. We listed several representative works in Table 1. /  /  / === Digital DLPs === / The major components of DLPs architecture usually include a computation component: the on-chip memory hierarchy: and the control logic that manages the data communication and computing flows. / Regarding the computation component: as most operations in deep learning can be aggregated into vector operations: the most common ways for building computation components in digital DLPs are the MAC-based (multiplier-accumulation) organization: either with vector MACs or scalar MACs. Rather than SIMD or SIMT in general processing devices: deep learning domain-specific parallelism is better explored on these MAC-based organizations. Regarding the memory hierarchy: as deep learning algorithms require high bandwidth to provide the computation component with sufficient data: DLPs usually employ a relatively larger size (tens of kilobytes or several megabytes) on-chip buffer but with dedicated on-chip data reuse strategy and data exchange strategy to alleviate the burden for memory bandwidth. For example: DianNao: 16 16-in vector MAC: requires 16 × 16 × 2 = 512 16-bit data: i.e.: almost 1024GB/s bandwidth requirements between computation components and buffers. With on-chip reuse: such bandwidth requirements are reduced drastically. Instead of the widely used cache in general processing devices: DLPs always use scratchpad memory as it could provide higher data reuse opportunities by leveraging the relatively regular data access pattern in deep learning algorithms. Regarding the control logic: as the deep learning algorithms keep evolving at a dramatic speed: DLPs start to leverage dedicated ISA (instruction set architecture) to support the deep learning domain flexibly. At first: DianNao used a VLIW-style instruction set where each instruction could finish a layer in a DNN.","Score: 0.8183737014598791 / For example: DianNao: 16 16-in vector MAC: requires 16 × 16 × 2 = 512 16-bit data: i.e.: almost 1024GB/s bandwidth requirements between computation components and buffers. With on-chip reuse: such bandwidth requirements are reduced drastically. Instead of the widely used cache in general processing devices: DLPs always use scratchpad memory as it could provide higher data reuse opportunities by leveraging the relatively regular data access pattern in deep learning algorithms. Regarding the control logic: as the deep learning algorithms keep evolving at a dramatic speed: DLPs start to leverage dedicated ISA (instruction set architecture) to support the deep learning domain flexibly. At first: DianNao used a VLIW-style instruction set where each instruction could finish a layer in a DNN. Cambricon introduces the first deep learning domain-specific ISA: which could support more than ten different deep learning algorithms. TPU also reveals five key instructions from the CISC-style ISA. /  /  / === Hybrid DLPs === / Hybrid DLPs emerge for DNN inference and training acceleration because of their high efficiency. Processing-in-memory (PIM) architectures are one most important type of hybrid DLP. The key design concept of PIM is to bridge the gap between computing and memory: with the following manners: 1) Moving computation components into memory cells: controllers: or memory chips to alleviate the memory wall issue. Such architectures significantly shorten data paths and leverage much higher internal bandwidth: hence resulting in attractive performance improvement. 2) Build high efficient DNN engines by adopting computational devices. In 2013: HP Lab demonstrated the astonishing capability of adopting ReRAM crossbar structure for computing. Inspiring by this work: tremendous work are proposed to explore the new architecture and system design based on ReRAM: phase change memory: etc. /  /  / == Benchmarks == / Benchmarks such as MLPerf and others may be used to evaluate the performance of AI accelerators. Table 2 lists several typical benchmarks for AI accelerators. /  /  / == Potential applications == / Agricultural robots: for example: herbicide-free weed control. / Autonomous vehicles: Nvidia has targeted their Drive PX-series boards at this application. / Computer-aided diagnosis / Industrial robots: increasing the range of tasks that can be automated: by adding adaptability to variable situations. / Machine translation / Military robots / Natural language processing / Search engines: increasing the energy efficiency of data centers and the ability to use increasingly advanced queries. / Unmanned aerial vehicles: e.g. navigation systems: e.g. the Movidius Myriad 2 has been demonstrated successfully guiding autonomous drones. / Voice user interface: e.g. in mobile phones: a target for Qualcomm Zeroth. /  /  / == See also == / Cognitive computer / Neuromorphic engineering / Optical neural network / Physical neural network / Cerebras Systems /  /  / == References == /  /  / == External links == / Nvidia Puts The Accelerator To The Metal With Pascal.htm: The Next Platform / Eyeriss Project: MIT / https://alphaics.ai/"
0.18568819999927655,What is the difference between CPUs and AI accelerators in terms of performing AI-related tasks?,"While CPUs are used for running AI workloads and are superior for DNNs with small or medium-scale parallelism, for sparse DNNs and in low-batch-size scenarios, specialized AI accelerators like GPUs and FPGAs perform far better due to their optimized memory use and the use of lower precision arithmetic to accelerate calculation and increase throughput of computation. In fact, ASICs, a specific design of accelerators, may provide up to 10 times the efficiency of CPUs for AI-related tasks.","Score: 0.8617697481252092 / Due to the increasing performance of CPUs: they are also used for running AI workloads. CPUs are superior for DNNs with small or medium-scale parallelism: for sparse DNNs and in low-batch-size scenarios. /  /  / === Use of GPU === / Graphics processing units or GPUs are specialized hardware for the manipulation of images and calculation of local image properties. The mathematical basis of neural networks and image manipulation are similar: embarrassingly parallel tasks involving matrices: leading GPUs to become increasingly used for machine learning tasks.In 2012: Alex Krizhevsky adopted two GPUs to train a deep learning network: i.e.: AlexNet: which won the champion of the ISLVRC-2012 competition. During the 2010's: GPU manufacturers such as Nvidia added deep learning related features in both hardware (e.g.: INT8 operators) and software (e.g.: cuDNN Library). / GPUs continue to be used in large-scale AI applications. For example: Summit: a supercomputer from IBM for Oak Ridge National Laboratory: contains 27:648 Nvidia Tesla V100 cards: which can be used to accelerate deep learning algorithms. / Over the 2010's GPUs continued to evolve in a direction to facilitate deep learning: both for training and inference in devices such as self-driving cars. GPU developers such as Nvidia NVLink are developing additional connective capability for the kind of dataflow workloads AI benefits from. As GPUs have been increasingly applied to AI acceleration: GPU manufacturers have incorporated neural network-specific hardware to further accelerate these tasks. Tensor cores are intended to speed up the training of neural networks. /  /  / === Use of FPGAs === / Deep learning frameworks are still evolving: making it hard to design custom hardware. Reconfigurable devices such as field-programmable gate arrays (FPGA) make it easier to evolve hardware: frameworks: and software alongside each other.Microsoft has used FPGA chips to accelerate inference for real-time deep learning services. /  /  / === Emergence of dedicated AI accelerator ASICs === / While GPUs and FPGAs perform far better than CPUs for AI-related tasks: a factor of up to 10 in efficiency may be gained with a more specific design: via an application-specific integrated circuit (ASIC). These accelerators employ strategies such as optimized memory use and the use of lower precision arithmetic to accelerate calculation and increase throughput of computation. Some low-precision floating-point formats used for AI acceleration are half-precision and the bfloat16 floating-point format. Companies such as Google: Qualcomm: Amazon: Apple: Facebook: AMD and Samsung are all designing their own AI ASICs. Cerebras Systems has built a dedicated AI accelerator based on the largest processor in the industry: the second-generation Wafer Scale Engine (WSE-2): to support deep learning workloads. /  /  / == Ongoing research == /  /  / === In-memory computing architectures === / In June 2017: IBM researchers announced an architecture in contrast to the Von Neumann architecture based on in-memory computing and phase-change memory arrays applied to temporal correlation detection: intending to generalize the approach to heterogeneous computing and massively parallel systems. In October 2018: IBM researchers announced an architecture based on in-memory processing and modeled on the human brain's synaptic network to accelerate deep neural networks. The system is based on phase-change memory arrays. /  /  / === In-memory computing with analog resistive memories === / In 2019: researchers from Politecnico di Milano found a way to solve systems of linear equations in a few tens of nanoseconds via a single operation. Their algorithm is based on in-memory computing with analog resistive memories which performs with high efficiencies of time and energy: via conducting matrix–vector multiplication in one step using Ohm's law and Kirchhoff's law. The researchers showed that a feedback circuit with cross-point resistive memories can solve algebraic problems such as systems of linear equations: matrix eigenvectors: and differential equations in just one step. Such an approach improves computational times drastically in comparison with digital algorithms. /  /  / === Atomically thin semiconductors === / In 2020: Marega et al. published experiments with a large-area active channel material for developing logic-in-memory devices and circuits based on floating-gate field-effect transistors (FGFETs). Such atomically thin semiconductors are considered promising for energy-efficient machine learning applications: where the same basic device structure is used for both logic operations and data storage. The authors used two-dimensional materials such as semiconducting molybdenum disulphide to precisely tune FGFETs as building blocks in which logic operations can be performed with the memory elements.  /  /  / === Integrated photonic tensor core === / In 1988: Wei Zhang et al.","Score: 0.8578059684333695 / An AI accelerator: deep learning processor: or neural processing unit (NPU) is a class of specialized hardware accelerator or computer system designed to accelerate artificial intelligence and machine learning applications: including artificial neural networks and machine vision. Typical applications include algorithms for robotics: Internet of Things: and other data-intensive or sensor-driven tasks. They are often manycore designs and generally focus on low-precision arithmetic: novel dataflow architectures or in-memory computing capability. As of 2024: a typical AI integrated circuit chip contains tens of billions of MOSFET transistors.AI accelerators are used in mobile devices: such as neural processing units (NPUs) in Apple iPhones or Huawei cellphones: and personal computers such as Apple silicon Macs: to cloud computing servers such as tensor processing units (TPU) in the Google Cloud Platform. A number of vendor-specific terms exist for devices in this category: and it is an emerging technology without a dominant design. / Graphics processing units designed by companies such as Nvidia and AMD often include AI-specific hardware: and are commonly used as AI accelerators: both for training and inference. /  /  / == History == / Computer systems have frequently complemented the CPU with special-purpose accelerators for specialized tasks: known as coprocessors. Notable application-specific hardware units include video cards for graphics: sound cards: graphics processing units and digital signal processors. As deep learning and artificial intelligence workloads rose in prominence in the 2010s: specialized hardware units were developed or adapted from existing products to accelerate these tasks. /  /  / === Early attempts === / First attempts like Intel's ETANN 80170NX incorporated analog circuits to compute neural functions.Later all-digital chips like the Nestor/Intel Ni1000 followed. As early as 1993: digital signal processors were used as neural network accelerators to accelerate optical character recognition software.By 1988: Wei Zhang et al. had discussed fast optical implementations of convolutional neural networks for alphabet recognition.In the 1990s: there were also attempts to create parallel high-throughput systems for workstations aimed at various applications: including neural network simulations.This presentation covers a past attempt at neural net accelerators: notes the similarity to the modern SLI GPGPU processor setup: and argues that general purpose vector accelerators are the way forward (in relation to RISC-V hwacha project. Argues that NN's are just dense and sparse matrices: one of several recurring algorithms)FPGA-based accelerators were also first explored in the 1990s for both inference and training.In 2014: Chen et al. proposed DianNao (Chinese for ""electric brain""): to accelerate deep neural networks especially. DianNao provides the 452 Gop/s peak performance (of key operations in deep neural networks) only in a small footprint of 3.02 mm2 and 485 mW. Later: the successors (DaDianNao: ShiDianNao: PuDianNao) are proposed by the same group: forming the DianNao FamilySmartphones began incorporating AI accelerators starting with the Qualcomm Snapdragon 820 in 2015. /  /  / === Heterogeneous computing === /  / Heterogeneous computing incorporates many specialized processors in a single system: or a single chip: each optimized for a specific type of task. Architectures such as the Cell microprocessor have features significantly overlapping with AI accelerators including: support for packed low precision arithmetic: dataflow architecture: and prioritizing throughput over latency. The Cell microprocessor has been applied to a number of tasks including AI.In the 2000s: CPUs also gained increasingly wide SIMD units: driven by video and gaming workloads; as well as support for packed low-precision data types. Due to the increasing performance of CPUs: they are also used for running AI workloads. CPUs are superior for DNNs with small or medium-scale parallelism: for sparse DNNs and in low-batch-size scenarios. /  /  / === Use of GPU === / Graphics processing units or GPUs are specialized hardware for the manipulation of images and calculation of local image properties. The mathematical basis of neural networks and image manipulation are similar: embarrassingly parallel tasks involving matrices: leading GPUs to become increasingly used for machine learning tasks.In 2012: Alex Krizhevsky adopted two GPUs to train a deep learning network: i.e.: AlexNet: which won the champion of the ISLVRC-2012 competition. During the 2010's: GPU manufacturers such as Nvidia added deep learning related features in both hardware (e.g.: INT8 operators) and software (e.g.: cuDNN Library). / GPUs continue to be used in large-scale AI applications."
0.17842439998639748,What is the BERT model?,"The Bidirectional Encoder Representations from Transformers (BERT) is a language model based on the transformer architecture, introduced in October 2018 by researchers at Google. It is known for its significant improvement over previous models. BERT was originally implemented in the English language and has become a ubiquitous baseline in Natural Language Processing (NLP) experiments.","Score: 0.8812586656246699 / Bidirectional Encoder Representations from Transformers (BERT) is a language model based on the transformer architecture: notable for its dramatic improvement over previous state of the art models. It was introduced in October 2018 by researchers at Google. A 2020 literature survey concluded that ""in a little over a year: BERT has become a ubiquitous baseline in Natural Language Processing (NLP) experiments counting over 150 research publications analyzing and improving the model.""BERT was originally implemented in the English language at two model sizes: (1) BERTBASE: 12 encoders with 12 bidirectional self-attention heads totaling 110 million parameters: and (2) BERTLARGE: 24 encoders with 16 bidirectional self-attention heads totaling 340 million parameters. Both models were pre-trained on the Toronto BookCorpus (800M words) and English Wikipedia  (2:500M words). /  /  / == Design == / BERT is an ""encoder-only"" transformer architecture.  / On a high level: BERT consists of three modules: /  / embedding. This module converts an array of one-hot encoded tokens into an array of vectors representing the tokens. / a stack of encoders. These encoders are the Transformer encoders. They perform transformations over the array of representation vectors. / un-embedding. This module converts the final representation vectors into one-hot encoded tokens again.The un-embedding module is necessary for pretraining: but it is often unnecessary for downstream tasks. Instead: one would take the representation vectors output at the end of the stack of encoders: and use those as a vector representation of the text input: and train a smaller model on top of that. / BERT uses WordPiece to convert each English word into an integer code. Its vocabulary has size 30:000. Any token not appearing in its vocabulary is replaced by [UNK] for ""unknown"". /  /  / === Pretraining === / BERT was pre-trained simultaneously on two tasks:language modeling: 15% of tokens were selected for prediction: and the training objective was to predict the selected token given its context. The selected token is  /  / replaced with a [MASK] token with probability 80%: / replaced with a random word token with probability 10%: / not replaced with probability 10%.For example: the sentence ""my dog is cute"" may have the 4-th token selected for prediction. The model would have input text /  / ""my dog is [MASK]"" with probability 80%: / ""my dog is happy"" with probability 10%: / ""my dog is cute"" with probability 10%.After processing the input text: the model's 4-th output vector is passed to a separate neural network: which outputs a probability distribution over its 30:000-large vocabulary. / next sentence prediction: Given two spans of text: the model predicts if these two spans appeared sequentially in the training corpus: outputting either [IsNext] or [NotNext]. The first span starts with a special token [CLS] (for ""classify""). The two spans are separated by a special token [SEP] (for ""separate""). After processing the two spans: the 1-st output vector (the vector coding for [CLS]) is passed to a separate neural network for the binary classification into [IsNext] and [NotNext]. /  / For example: given ""[CLS] my dog is cute [SEP] he likes playing"" the model should output token [IsNext]. / Given ""[CLS] my dog is cute [SEP] how do magnets work"" the model should output token [NotNext].As a result of this training process: BERT learns latent representations of words and sentences in context. After pre-training: BERT can be fine-tuned with fewer resources on smaller datasets to optimize its performance on specific tasks such as NLP tasks (language inference: text classification) and sequence-to-sequence based language generation tasks (question-answering: conversational response generation). The pre-training stage is significantly more computationally expensive than fine-tuning. /  /  / === Architecture details === / This section describes BERTBASE. The other one: BERTLARGE: is similar: just larger.  / The lowest layer is the embedding layer: which contains three components: word_embeddings: position_embeddings: token_type_embeddings. /  / word_embeddings takes in a one-hot vector of the input token. The one-hot vector input has dimension 30:000: because BERT has a vocabulary size that large. / position_embeddings performs absolute position embedding.","Score: 0.8699837470490341 / For example: if the task is a sentiment classification task on financial data: a pre-trained model for the analysis of sentiment of financial text should be chosen. The weights of the original pre-trained models were released on GitHub. /  /  / == History == / BERT was originally published by Google researchers Jacob Devlin: Ming-Wei Chang: Kenton Lee: and Kristina Toutanova. The design has its origins from pre-training contextual representations: including semi-supervised sequence learning: generative pre-training: ELMo: and ULMFit. Unlike previous models: BERT is a deeply bidirectional: unsupervised language representation: pre-trained using only a plain text corpus. Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary: whereas BERT takes into account the context for each occurrence of a given word. For instance: whereas the vector for ""running"" will have the same word2vec vector representation for both of its occurrences in the sentences ""He is running a company"" and ""He is running a marathon"": BERT will provide a contextualized embedding that will be different according to the sentence.On October 25: 2019: Google announced that they had started applying BERT models for English language search queries within the US. On December 9: 2019: it was reported that BERT had been adopted by Google Search for over 70 languages. In October 2020: almost every single English-based query was processed by a BERT model.A later paper proposes RoBERTa: which preserves BERT's architecture: but improves its training: changing key hyperparameters: removing the next-sentence prediction task: and using much larger mini-batch sizes. /  /  / == Recognition == / The research paper describing BERT won the Best Long Paper Award at the 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL). /  /  / == References == /  /  / == Further reading == / Rogers: Anna; Kovaleva: Olga; Rumshisky: Anna (2020). ""A Primer in BERTology: What we know about how BERT works"". arXiv:2002.12327 [cs.CL]. /  /  / == External links == / Official GitHub repository / BERT on Devopedia"
0.15497360000154004,How is BERT pretrained?,"BERT was pre-trained simultaneously on two tasks: language modeling and next sentence prediction. In language modeling, a portion of the tokens in the text were selected for prediction and were replaced with a [MASK] token, a random word token, or not replaced. The next sentence prediction task involved predicting if two spans of text appeared sequentially in the training corpus, outputting either [IsNext] or [NotNext].","Score: 0.8713366051232272 / After pre-training: BERT can be fine-tuned with fewer resources on smaller datasets to optimize its performance on specific tasks such as NLP tasks (language inference: text classification) and sequence-to-sequence based language generation tasks (question-answering: conversational response generation). The pre-training stage is significantly more computationally expensive than fine-tuning. /  /  / === Architecture details === / This section describes BERTBASE. The other one: BERTLARGE: is similar: just larger.  / The lowest layer is the embedding layer: which contains three components: word_embeddings: position_embeddings: token_type_embeddings. /  / word_embeddings takes in a one-hot vector of the input token. The one-hot vector input has dimension 30:000: because BERT has a vocabulary size that large. / position_embeddings performs absolute position embedding. It is like word_embeddings: but on a vocabulary consisting of just the time-stamps 0 to 511: since BERT has a context window of 512. / token_type_embeddings is like word_embeddings: but on a vocabulary consisting of just 0 and 1. The only type-1 tokens are those that appear after the [SEP]. All other tokens are type-0.The three outputs are added: then pushed through a LayerNorm (layer normalization): obtaining an array of representation vectors: each having 768 dimensions. / After this: the representation vectors move through 12 Transformer encoders: then they are un-embedded by an affine-Add & LayerNorm-linear. /  /  / == Performance == / When BERT was published: it achieved state-of-the-art performance on a number of natural language understanding tasks: / GLUE (General Language Understanding Evaluation) task set (consisting of 9 tasks) / SQuAD (Stanford Question Answering Dataset) v1.1 and v2.0 / SWAG (Situations With Adversarial Generations) /  /  / == Analysis == / The reasons for BERT's state-of-the-art performance on these natural language understanding tasks are not yet well understood. Current research has focused on investigating the relationship behind BERT's output as a result of carefully chosen input sequences: analysis of internal vector representations through probing classifiers: and the relationships represented by attention weights. / The high performance of the BERT model could also be attributed to the fact that it is bidirectionally trained. This means that BERT: based on the Transformer model architecture: applies its self-attention mechanism to learn information from a text from the left and right side during training: and consequently gains a deep understanding of the context. For example: the word fine can have two different meanings depending on the context (I feel fine today: She has fine blond hair).  BERT considers the words surrounding the target word fine from the left and right side. / However it comes at a cost: due to encoder-only architecture lacking a decoder: BERT can't be prompted and can't generate text: while bidirectional models in general do not work effectively without the right side: thus being difficult to prompt: with even short text generation requiring sophisticated computationally expensive techniques.In contrast to deep learning neural networks which require very large amounts of data: BERT has already been pre-trained which means that it has learnt the representations of the words and sentences as well as the underlying semantic relations that they are connected with. BERT can then be fine-tuned on smaller datasets for specific tasks such as sentiment classification. The pre-trained models are chosen according to the content of the given dataset one uses but also the goal of the task. For example: if the task is a sentiment classification task on financial data: a pre-trained model for the analysis of sentiment of financial text should be chosen. The weights of the original pre-trained models were released on GitHub. /  /  / == History == / BERT was originally published by Google researchers Jacob Devlin: Ming-Wei Chang: Kenton Lee: and Kristina Toutanova. The design has its origins from pre-training contextual representations: including semi-supervised sequence learning: generative pre-training: ELMo: and ULMFit. Unlike previous models: BERT is a deeply bidirectional: unsupervised language representation: pre-trained using only a plain text corpus. Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary: whereas BERT takes into account the context for each occurrence of a given word.","Score: 0.8703135360221761 / For example: if the task is a sentiment classification task on financial data: a pre-trained model for the analysis of sentiment of financial text should be chosen. The weights of the original pre-trained models were released on GitHub. /  /  / == History == / BERT was originally published by Google researchers Jacob Devlin: Ming-Wei Chang: Kenton Lee: and Kristina Toutanova. The design has its origins from pre-training contextual representations: including semi-supervised sequence learning: generative pre-training: ELMo: and ULMFit. Unlike previous models: BERT is a deeply bidirectional: unsupervised language representation: pre-trained using only a plain text corpus. Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary: whereas BERT takes into account the context for each occurrence of a given word. For instance: whereas the vector for ""running"" will have the same word2vec vector representation for both of its occurrences in the sentences ""He is running a company"" and ""He is running a marathon"": BERT will provide a contextualized embedding that will be different according to the sentence.On October 25: 2019: Google announced that they had started applying BERT models for English language search queries within the US. On December 9: 2019: it was reported that BERT had been adopted by Google Search for over 70 languages. In October 2020: almost every single English-based query was processed by a BERT model.A later paper proposes RoBERTa: which preserves BERT's architecture: but improves its training: changing key hyperparameters: removing the next-sentence prediction task: and using much larger mini-batch sizes. /  /  / == Recognition == / The research paper describing BERT won the Best Long Paper Award at the 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL). /  /  / == References == /  /  / == Further reading == / Rogers: Anna; Kovaleva: Olga; Rumshisky: Anna (2020). ""A Primer in BERTology: What we know about how BERT works"". arXiv:2002.12327 [cs.CL]. /  /  / == External links == / Official GitHub repository / BERT on Devopedia"
0.18992030000663362,Can you describe the architecture of BERT?,"BERT is an ""encoder-only"" transformer architecture. It consists of an embedding module, a stack of encoders, and an un-embedding module. The lowest layer is the embedding layer which contains word embeddings, position embeddings, and token type embeddings. The representation vectors then move through Transformer encoders and are then un-embedded.","Score: 0.8611244714301157 / Bidirectional Encoder Representations from Transformers (BERT) is a language model based on the transformer architecture: notable for its dramatic improvement over previous state of the art models. It was introduced in October 2018 by researchers at Google. A 2020 literature survey concluded that ""in a little over a year: BERT has become a ubiquitous baseline in Natural Language Processing (NLP) experiments counting over 150 research publications analyzing and improving the model.""BERT was originally implemented in the English language at two model sizes: (1) BERTBASE: 12 encoders with 12 bidirectional self-attention heads totaling 110 million parameters: and (2) BERTLARGE: 24 encoders with 16 bidirectional self-attention heads totaling 340 million parameters. Both models were pre-trained on the Toronto BookCorpus (800M words) and English Wikipedia  (2:500M words). /  /  / == Design == / BERT is an ""encoder-only"" transformer architecture.  / On a high level: BERT consists of three modules: /  / embedding. This module converts an array of one-hot encoded tokens into an array of vectors representing the tokens. / a stack of encoders. These encoders are the Transformer encoders. They perform transformations over the array of representation vectors. / un-embedding. This module converts the final representation vectors into one-hot encoded tokens again.The un-embedding module is necessary for pretraining: but it is often unnecessary for downstream tasks. Instead: one would take the representation vectors output at the end of the stack of encoders: and use those as a vector representation of the text input: and train a smaller model on top of that. / BERT uses WordPiece to convert each English word into an integer code. Its vocabulary has size 30:000. Any token not appearing in its vocabulary is replaced by [UNK] for ""unknown"". /  /  / === Pretraining === / BERT was pre-trained simultaneously on two tasks:language modeling: 15% of tokens were selected for prediction: and the training objective was to predict the selected token given its context. The selected token is  /  / replaced with a [MASK] token with probability 80%: / replaced with a random word token with probability 10%: / not replaced with probability 10%.For example: the sentence ""my dog is cute"" may have the 4-th token selected for prediction. The model would have input text /  / ""my dog is [MASK]"" with probability 80%: / ""my dog is happy"" with probability 10%: / ""my dog is cute"" with probability 10%.After processing the input text: the model's 4-th output vector is passed to a separate neural network: which outputs a probability distribution over its 30:000-large vocabulary. / next sentence prediction: Given two spans of text: the model predicts if these two spans appeared sequentially in the training corpus: outputting either [IsNext] or [NotNext]. The first span starts with a special token [CLS] (for ""classify""). The two spans are separated by a special token [SEP] (for ""separate""). After processing the two spans: the 1-st output vector (the vector coding for [CLS]) is passed to a separate neural network for the binary classification into [IsNext] and [NotNext]. /  / For example: given ""[CLS] my dog is cute [SEP] he likes playing"" the model should output token [IsNext]. / Given ""[CLS] my dog is cute [SEP] how do magnets work"" the model should output token [NotNext].As a result of this training process: BERT learns latent representations of words and sentences in context. After pre-training: BERT can be fine-tuned with fewer resources on smaller datasets to optimize its performance on specific tasks such as NLP tasks (language inference: text classification) and sequence-to-sequence based language generation tasks (question-answering: conversational response generation). The pre-training stage is significantly more computationally expensive than fine-tuning. /  /  / === Architecture details === / This section describes BERTBASE. The other one: BERTLARGE: is similar: just larger.  / The lowest layer is the embedding layer: which contains three components: word_embeddings: position_embeddings: token_type_embeddings. /  / word_embeddings takes in a one-hot vector of the input token. The one-hot vector input has dimension 30:000: because BERT has a vocabulary size that large. / position_embeddings performs absolute position embedding.","Score: 0.8588620631794401 / After pre-training: BERT can be fine-tuned with fewer resources on smaller datasets to optimize its performance on specific tasks such as NLP tasks (language inference: text classification) and sequence-to-sequence based language generation tasks (question-answering: conversational response generation). The pre-training stage is significantly more computationally expensive than fine-tuning. /  /  / === Architecture details === / This section describes BERTBASE. The other one: BERTLARGE: is similar: just larger.  / The lowest layer is the embedding layer: which contains three components: word_embeddings: position_embeddings: token_type_embeddings. /  / word_embeddings takes in a one-hot vector of the input token. The one-hot vector input has dimension 30:000: because BERT has a vocabulary size that large. / position_embeddings performs absolute position embedding. It is like word_embeddings: but on a vocabulary consisting of just the time-stamps 0 to 511: since BERT has a context window of 512. / token_type_embeddings is like word_embeddings: but on a vocabulary consisting of just 0 and 1. The only type-1 tokens are those that appear after the [SEP]. All other tokens are type-0.The three outputs are added: then pushed through a LayerNorm (layer normalization): obtaining an array of representation vectors: each having 768 dimensions. / After this: the representation vectors move through 12 Transformer encoders: then they are un-embedded by an affine-Add & LayerNorm-linear. /  /  / == Performance == / When BERT was published: it achieved state-of-the-art performance on a number of natural language understanding tasks: / GLUE (General Language Understanding Evaluation) task set (consisting of 9 tasks) / SQuAD (Stanford Question Answering Dataset) v1.1 and v2.0 / SWAG (Situations With Adversarial Generations) /  /  / == Analysis == / The reasons for BERT's state-of-the-art performance on these natural language understanding tasks are not yet well understood. Current research has focused on investigating the relationship behind BERT's output as a result of carefully chosen input sequences: analysis of internal vector representations through probing classifiers: and the relationships represented by attention weights. / The high performance of the BERT model could also be attributed to the fact that it is bidirectionally trained. This means that BERT: based on the Transformer model architecture: applies its self-attention mechanism to learn information from a text from the left and right side during training: and consequently gains a deep understanding of the context. For example: the word fine can have two different meanings depending on the context (I feel fine today: She has fine blond hair).  BERT considers the words surrounding the target word fine from the left and right side. / However it comes at a cost: due to encoder-only architecture lacking a decoder: BERT can't be prompted and can't generate text: while bidirectional models in general do not work effectively without the right side: thus being difficult to prompt: with even short text generation requiring sophisticated computationally expensive techniques.In contrast to deep learning neural networks which require very large amounts of data: BERT has already been pre-trained which means that it has learnt the representations of the words and sentences as well as the underlying semantic relations that they are connected with. BERT can then be fine-tuned on smaller datasets for specific tasks such as sentiment classification. The pre-trained models are chosen according to the content of the given dataset one uses but also the goal of the task. For example: if the task is a sentiment classification task on financial data: a pre-trained model for the analysis of sentiment of financial text should be chosen. The weights of the original pre-trained models were released on GitHub. /  /  / == History == / BERT was originally published by Google researchers Jacob Devlin: Ming-Wei Chang: Kenton Lee: and Kristina Toutanova. The design has its origins from pre-training contextual representations: including semi-supervised sequence learning: generative pre-training: ELMo: and ULMFit. Unlike previous models: BERT is a deeply bidirectional: unsupervised language representation: pre-trained using only a plain text corpus. Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary: whereas BERT takes into account the context for each occurrence of a given word."
0.1549867000139784,How does BERT perform in natural language understanding tasks?,"When BERT was published, it achieved state-of-the-art performance on a number of natural language understanding tasks including the GLUE (General Language Understanding Evaluation) task set, SQuAD (Stanford Question Answering Dataset), and SWAG (Situations With Adversarial Generations).","Score: 0.8521494833459088 / After pre-training: BERT can be fine-tuned with fewer resources on smaller datasets to optimize its performance on specific tasks such as NLP tasks (language inference: text classification) and sequence-to-sequence based language generation tasks (question-answering: conversational response generation). The pre-training stage is significantly more computationally expensive than fine-tuning. /  /  / === Architecture details === / This section describes BERTBASE. The other one: BERTLARGE: is similar: just larger.  / The lowest layer is the embedding layer: which contains three components: word_embeddings: position_embeddings: token_type_embeddings. /  / word_embeddings takes in a one-hot vector of the input token. The one-hot vector input has dimension 30:000: because BERT has a vocabulary size that large. / position_embeddings performs absolute position embedding. It is like word_embeddings: but on a vocabulary consisting of just the time-stamps 0 to 511: since BERT has a context window of 512. / token_type_embeddings is like word_embeddings: but on a vocabulary consisting of just 0 and 1. The only type-1 tokens are those that appear after the [SEP]. All other tokens are type-0.The three outputs are added: then pushed through a LayerNorm (layer normalization): obtaining an array of representation vectors: each having 768 dimensions. / After this: the representation vectors move through 12 Transformer encoders: then they are un-embedded by an affine-Add & LayerNorm-linear. /  /  / == Performance == / When BERT was published: it achieved state-of-the-art performance on a number of natural language understanding tasks: / GLUE (General Language Understanding Evaluation) task set (consisting of 9 tasks) / SQuAD (Stanford Question Answering Dataset) v1.1 and v2.0 / SWAG (Situations With Adversarial Generations) /  /  / == Analysis == / The reasons for BERT's state-of-the-art performance on these natural language understanding tasks are not yet well understood. Current research has focused on investigating the relationship behind BERT's output as a result of carefully chosen input sequences: analysis of internal vector representations through probing classifiers: and the relationships represented by attention weights. / The high performance of the BERT model could also be attributed to the fact that it is bidirectionally trained. This means that BERT: based on the Transformer model architecture: applies its self-attention mechanism to learn information from a text from the left and right side during training: and consequently gains a deep understanding of the context. For example: the word fine can have two different meanings depending on the context (I feel fine today: She has fine blond hair).  BERT considers the words surrounding the target word fine from the left and right side. / However it comes at a cost: due to encoder-only architecture lacking a decoder: BERT can't be prompted and can't generate text: while bidirectional models in general do not work effectively without the right side: thus being difficult to prompt: with even short text generation requiring sophisticated computationally expensive techniques.In contrast to deep learning neural networks which require very large amounts of data: BERT has already been pre-trained which means that it has learnt the representations of the words and sentences as well as the underlying semantic relations that they are connected with. BERT can then be fine-tuned on smaller datasets for specific tasks such as sentiment classification. The pre-trained models are chosen according to the content of the given dataset one uses but also the goal of the task. For example: if the task is a sentiment classification task on financial data: a pre-trained model for the analysis of sentiment of financial text should be chosen. The weights of the original pre-trained models were released on GitHub. /  /  / == History == / BERT was originally published by Google researchers Jacob Devlin: Ming-Wei Chang: Kenton Lee: and Kristina Toutanova. The design has its origins from pre-training contextual representations: including semi-supervised sequence learning: generative pre-training: ELMo: and ULMFit. Unlike previous models: BERT is a deeply bidirectional: unsupervised language representation: pre-trained using only a plain text corpus. Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary: whereas BERT takes into account the context for each occurrence of a given word.","Score: 0.8515243508824782 / For example: if the task is a sentiment classification task on financial data: a pre-trained model for the analysis of sentiment of financial text should be chosen. The weights of the original pre-trained models were released on GitHub. /  /  / == History == / BERT was originally published by Google researchers Jacob Devlin: Ming-Wei Chang: Kenton Lee: and Kristina Toutanova. The design has its origins from pre-training contextual representations: including semi-supervised sequence learning: generative pre-training: ELMo: and ULMFit. Unlike previous models: BERT is a deeply bidirectional: unsupervised language representation: pre-trained using only a plain text corpus. Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary: whereas BERT takes into account the context for each occurrence of a given word. For instance: whereas the vector for ""running"" will have the same word2vec vector representation for both of its occurrences in the sentences ""He is running a company"" and ""He is running a marathon"": BERT will provide a contextualized embedding that will be different according to the sentence.On October 25: 2019: Google announced that they had started applying BERT models for English language search queries within the US. On December 9: 2019: it was reported that BERT had been adopted by Google Search for over 70 languages. In October 2020: almost every single English-based query was processed by a BERT model.A later paper proposes RoBERTa: which preserves BERT's architecture: but improves its training: changing key hyperparameters: removing the next-sentence prediction task: and using much larger mini-batch sizes. /  /  / == Recognition == / The research paper describing BERT won the Best Long Paper Award at the 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL). /  /  / == References == /  /  / == Further reading == / Rogers: Anna; Kovaleva: Olga; Rumshisky: Anna (2020). ""A Primer in BERTology: What we know about how BERT works"". arXiv:2002.12327 [cs.CL]. /  /  / == External links == / Official GitHub repository / BERT on Devopedia"
0.15343999999458902,Who were the original researchers behind BERT and what innovation did it provide over previous models?,"BERT was originally published by Google researchers Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Unlike previous models such as word2vec or GloVe which generated a single word embedding representation for each word in the vocabulary, BERT takes into account the context for each occurrence of a given word, providing a contextualized embedding that varies according to the sentence.","Score: 0.8425378301844513 / For example: if the task is a sentiment classification task on financial data: a pre-trained model for the analysis of sentiment of financial text should be chosen. The weights of the original pre-trained models were released on GitHub. /  /  / == History == / BERT was originally published by Google researchers Jacob Devlin: Ming-Wei Chang: Kenton Lee: and Kristina Toutanova. The design has its origins from pre-training contextual representations: including semi-supervised sequence learning: generative pre-training: ELMo: and ULMFit. Unlike previous models: BERT is a deeply bidirectional: unsupervised language representation: pre-trained using only a plain text corpus. Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary: whereas BERT takes into account the context for each occurrence of a given word. For instance: whereas the vector for ""running"" will have the same word2vec vector representation for both of its occurrences in the sentences ""He is running a company"" and ""He is running a marathon"": BERT will provide a contextualized embedding that will be different according to the sentence.On October 25: 2019: Google announced that they had started applying BERT models for English language search queries within the US. On December 9: 2019: it was reported that BERT had been adopted by Google Search for over 70 languages. In October 2020: almost every single English-based query was processed by a BERT model.A later paper proposes RoBERTa: which preserves BERT's architecture: but improves its training: changing key hyperparameters: removing the next-sentence prediction task: and using much larger mini-batch sizes. /  /  / == Recognition == / The research paper describing BERT won the Best Long Paper Award at the 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL). /  /  / == References == /  /  / == Further reading == / Rogers: Anna; Kovaleva: Olga; Rumshisky: Anna (2020). ""A Primer in BERTology: What we know about how BERT works"". arXiv:2002.12327 [cs.CL]. /  /  / == External links == / Official GitHub repository / BERT on Devopedia","Score: 0.8271880656694269 / After pre-training: BERT can be fine-tuned with fewer resources on smaller datasets to optimize its performance on specific tasks such as NLP tasks (language inference: text classification) and sequence-to-sequence based language generation tasks (question-answering: conversational response generation). The pre-training stage is significantly more computationally expensive than fine-tuning. /  /  / === Architecture details === / This section describes BERTBASE. The other one: BERTLARGE: is similar: just larger.  / The lowest layer is the embedding layer: which contains three components: word_embeddings: position_embeddings: token_type_embeddings. /  / word_embeddings takes in a one-hot vector of the input token. The one-hot vector input has dimension 30:000: because BERT has a vocabulary size that large. / position_embeddings performs absolute position embedding. It is like word_embeddings: but on a vocabulary consisting of just the time-stamps 0 to 511: since BERT has a context window of 512. / token_type_embeddings is like word_embeddings: but on a vocabulary consisting of just 0 and 1. The only type-1 tokens are those that appear after the [SEP]. All other tokens are type-0.The three outputs are added: then pushed through a LayerNorm (layer normalization): obtaining an array of representation vectors: each having 768 dimensions. / After this: the representation vectors move through 12 Transformer encoders: then they are un-embedded by an affine-Add & LayerNorm-linear. /  /  / == Performance == / When BERT was published: it achieved state-of-the-art performance on a number of natural language understanding tasks: / GLUE (General Language Understanding Evaluation) task set (consisting of 9 tasks) / SQuAD (Stanford Question Answering Dataset) v1.1 and v2.0 / SWAG (Situations With Adversarial Generations) /  /  / == Analysis == / The reasons for BERT's state-of-the-art performance on these natural language understanding tasks are not yet well understood. Current research has focused on investigating the relationship behind BERT's output as a result of carefully chosen input sequences: analysis of internal vector representations through probing classifiers: and the relationships represented by attention weights. / The high performance of the BERT model could also be attributed to the fact that it is bidirectionally trained. This means that BERT: based on the Transformer model architecture: applies its self-attention mechanism to learn information from a text from the left and right side during training: and consequently gains a deep understanding of the context. For example: the word fine can have two different meanings depending on the context (I feel fine today: She has fine blond hair).  BERT considers the words surrounding the target word fine from the left and right side. / However it comes at a cost: due to encoder-only architecture lacking a decoder: BERT can't be prompted and can't generate text: while bidirectional models in general do not work effectively without the right side: thus being difficult to prompt: with even short text generation requiring sophisticated computationally expensive techniques.In contrast to deep learning neural networks which require very large amounts of data: BERT has already been pre-trained which means that it has learnt the representations of the words and sentences as well as the underlying semantic relations that they are connected with. BERT can then be fine-tuned on smaller datasets for specific tasks such as sentiment classification. The pre-trained models are chosen according to the content of the given dataset one uses but also the goal of the task. For example: if the task is a sentiment classification task on financial data: a pre-trained model for the analysis of sentiment of financial text should be chosen. The weights of the original pre-trained models were released on GitHub. /  /  / == History == / BERT was originally published by Google researchers Jacob Devlin: Ming-Wei Chang: Kenton Lee: and Kristina Toutanova. The design has its origins from pre-training contextual representations: including semi-supervised sequence learning: generative pre-training: ELMo: and ULMFit. Unlike previous models: BERT is a deeply bidirectional: unsupervised language representation: pre-trained using only a plain text corpus. Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary: whereas BERT takes into account the context for each occurrence of a given word."
0.17166089999955148,What is the BigScience Large Open-science Open-access Multilingual Language Model (BLOOM)?,"The BigScience Large Open-science Open-access Multilingual Language Model (BLOOM) is a 176-billion-parameter transformer-based autoregressive large language model (LLM), trained on approximately 366 billion tokens from March to July 2022. ","Score: 0.9087367468819577 / BigScience Large Open-science Open-access Multilingual Language Model (BLOOM) is a 176-billion-parameter transformer-based autoregressive large language model (LLM). The model: as well as the code base and the data used to train it: are distributed under free licences. BLOOM was trained on approximately 366 billion (1.6TB) tokens from March to July 2022.BLOOM is the main outcome of the BigScience collaborative initiative: a one-year-long research workshop that took place between May 2021 and May 2022. BigScience was led by HuggingFace and involved several hundreds of researchers and engineers from France and abroad representing both the academia and the private sector. BigScience was supported by a large-scale public compute grant on the French public supercomputer Jean Zay: managed by GENCI and IDRIS (CNRS): on which it was trained. / BLOOM's training corpus: named ROOTS: combines data extracted from the then-latest version of the web-based OSCAR corpus (38% of ROOTS) and newly collected data extracted from a manually selected and documented list of language data sources. It encompasses 46 natural languages (in amounts ranging from 30% of the whole dataset for English to 0.00002% for Chi Tumbuka) and 13 programming languages. /  /  / == References ==","Score: 0.8308961960076429 / A large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification. LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process. LLMs can be used for text generation: a form of generative AI: by taking an input text and repeatedly predicting the next token or word.LLMs are artificial neural networks. The largest and most capable: as of March 2024: are built with a decoder-only transformer-based architecture while some recent implementations are based on other architectures: such as recurrent neural network variants and Mamba (a state space model).Up to 2020: fine tuning was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models: such as GPT-3: however: can be prompt-engineered to achieve similar results. They are thought to acquire knowledge about syntax: semantics and ""ontology"" inherent in human language corpora: but also inaccuracies and biases present in the corpora.Some notable LLMs are OpenAI's GPT series of models (e.g.: GPT-3.5 and GPT-4: used in ChatGPT and Microsoft Copilot): Google's PaLM and Gemini (the latter of which is currently used in the chatbot of the same name): xAI's Grok: Meta's LLaMA family of open-source models: Anthropic's Claude models: and Mistral AI's open source models. /  /  / == History == / At the 2017 NeurIPS conference: Google researchers introduced the transformer architecture in their landmark paper ""Attention Is All You Need"". This paper's goal was to improve upon 2014 Seq2seq technology:  and was based mainly on the attention mechanism developed by Bahdanau et al. in 2014. The following year in 2018: BERT was introduced and quickly became ""ubiquitous"". Though the original transformer has both encoder and decoder blocks: BERT is an encoder-only model. / Although decoder-only GPT-1 was introduced in 2018: it was GPT-2 in 2019 that caught widespread attention because OpenAI at first deemed it too powerful to release publicly: out of fear of malicious use. GPT-3 in 2020 went a step further and as of 2024 is available only via API with no offering of downloading the model to execute locally. But it was the 2022 consumer-facing browser-based ChatGPT that captured the imaginations of the general population and caused some media hype and online buzz. The 2023 GPT-4 was praised for its increased accuracy and as a ""holy grail"" for its multimodal capabilities. OpenAI did not reveal high-level architecture and the number of parameters of GPT-4. / In the meantime: competing language models have for the most part been playing catch-up to the GPT series: at least in terms of number of parameters. Notable exceptions in terms of either number of parameters or measured accuracy include Google's 2019 T5-11B and 2022 PaLM-E: and Anthropic's 2024 Claude 3. In terms of Elo ratings: on January 26: 2024: Google's Bard (Gemini Pro) surpassed the regular GPT-4: but not the limited-availability GPT-4-Turbo.Since 2022: source-available models have been gaining popularity: especially at first with BLOOM and LLaMA: though both have restrictions on the field of use. Mistral AI's models Mistral 7B and Mixtral 8x7b have the more permissive Apache License. As of January 2024: Mixtral 8x7b is the most powerful open LLM according to the LMSYS Chatbot Arena Leaderboard: being more powerful than GPT-3.5 but not as powerful as GPT-4. /  /  / == Dataset preprocessing == /  /  / === Probabilistic tokenization === / Because machine learning algorithms process numbers rather than text: the text must be converted to numbers. In the first step: a vocabulary is decided upon: then integer indexes are arbitrarily but uniquely assigned to each vocabulary entry: and finally: an embedding is associated to the integer index. Algorithms include byte-pair encoding and WordPiece. / Probabilistic tokenization also compresses the datasets. Because LLMs generally require input to be an array that is not jagged: the shorter texts must be ""padded"" until they match the length of the longest one."
0.16692640000837855,What is unique about BLOOM's licensing?,"BLOOM, its code base, and the data used to train it are all distributed under free licenses, which makes the resources open and accessible to the general public.","Score: 0.7891063727740472 / BigScience Large Open-science Open-access Multilingual Language Model (BLOOM) is a 176-billion-parameter transformer-based autoregressive large language model (LLM). The model: as well as the code base and the data used to train it: are distributed under free licences. BLOOM was trained on approximately 366 billion (1.6TB) tokens from March to July 2022.BLOOM is the main outcome of the BigScience collaborative initiative: a one-year-long research workshop that took place between May 2021 and May 2022. BigScience was led by HuggingFace and involved several hundreds of researchers and engineers from France and abroad representing both the academia and the private sector. BigScience was supported by a large-scale public compute grant on the French public supercomputer Jean Zay: managed by GENCI and IDRIS (CNRS): on which it was trained. / BLOOM's training corpus: named ROOTS: combines data extracted from the then-latest version of the web-based OSCAR corpus (38% of ROOTS) and newly collected data extracted from a manually selected and documented list of language data sources. It encompasses 46 natural languages (in amounts ranging from 30% of the whole dataset for English to 0.00002% for Chi Tumbuka) and 13 programming languages. /  /  / == References ==","Score: 0.7344328173233213 / Such an approach improves computational times drastically in comparison with digital algorithms. /  /  / === Atomically thin semiconductors === / In 2020: Marega et al. published experiments with a large-area active channel material for developing logic-in-memory devices and circuits based on floating-gate field-effect transistors (FGFETs). Such atomically thin semiconductors are considered promising for energy-efficient machine learning applications: where the same basic device structure is used for both logic operations and data storage. The authors used two-dimensional materials such as semiconducting molybdenum disulphide to precisely tune FGFETs as building blocks in which logic operations can be performed with the memory elements.  /  /  / === Integrated photonic tensor core === / In 1988: Wei Zhang et al. discussed fast optical implementations of convolutional neural networks for alphabet recognition. / In 2021: J. Feldmann et al. proposed an integrated photonic hardware accelerator for parallel convolutional processing. The authors identify two key advantages of integrated photonics over its electronic counterparts: (1) massively parallel data transfer through wavelength division multiplexing in conjunction with frequency combs: and (2) extremely high data modulation speeds. Their system can execute trillions of multiply-accumulate operations per second: indicating the potential of integrated photonics in data-heavy AI applications. Optical processors that can also perform backpropagation for artificial neural networks have been experimentally developed. /  /  / == Nomenclature == / As of 2016: the field is still in flux and vendors are pushing their own marketing term for what amounts to an ""AI accelerator"": in the hope that their designs and APIs will become the dominant design. There is no consensus on the boundary between these devices: nor the exact form they will take; however several examples clearly aim to fill this new space: with a fair amount of overlap in capabilities. / In the past when consumer graphics accelerators emerged: the industry eventually adopted Nvidia's self-assigned term: ""the GPU"": / as the collective noun for ""graphics accelerators"": which had taken many forms before settling on an overall pipeline implementing a model presented by Direct3D. / All models of Intel Meteor Lake processors have a Versatile Processor Unit (VPU) built-in for accelerating inference for computer vision and deep learning. /  /  / == Deep Learning Processors (DLP) == / Inspired from the pioneer work of DianNao Family: many DLPs are proposed in both academia and industry with design optimized to leverage the features of deep neural networks for high efficiency. Only at ISCA 2016: three sessions: 15% (!) of the accepted papers: are all architecture designs about deep learning. Such efforts include Eyeriss (MIT): EIE (Stanford): Minerva (Harvard): Stripes (University of Toronto) in academia: TPU (Google): and MLU (Cambricon) in industry. We listed several representative works in Table 1. /  /  / === Digital DLPs === / The major components of DLPs architecture usually include a computation component: the on-chip memory hierarchy: and the control logic that manages the data communication and computing flows. / Regarding the computation component: as most operations in deep learning can be aggregated into vector operations: the most common ways for building computation components in digital DLPs are the MAC-based (multiplier-accumulation) organization: either with vector MACs or scalar MACs. Rather than SIMD or SIMT in general processing devices: deep learning domain-specific parallelism is better explored on these MAC-based organizations. Regarding the memory hierarchy: as deep learning algorithms require high bandwidth to provide the computation component with sufficient data: DLPs usually employ a relatively larger size (tens of kilobytes or several megabytes) on-chip buffer but with dedicated on-chip data reuse strategy and data exchange strategy to alleviate the burden for memory bandwidth. For example: DianNao: 16 16-in vector MAC: requires 16 × 16 × 2 = 512 16-bit data: i.e.: almost 1024GB/s bandwidth requirements between computation components and buffers. With on-chip reuse: such bandwidth requirements are reduced drastically. Instead of the widely used cache in general processing devices: DLPs always use scratchpad memory as it could provide higher data reuse opportunities by leveraging the relatively regular data access pattern in deep learning algorithms. Regarding the control logic: as the deep learning algorithms keep evolving at a dramatic speed: DLPs start to leverage dedicated ISA (instruction set architecture) to support the deep learning domain flexibly. At first: DianNao used a VLIW-style instruction set where each instruction could finish a layer in a DNN."
0.12264409998897463,What are the origins of BLOOM?,"BLOOM was the main result of the BigScience collaborative initiative, a one-year-long research workshop that took place between May 2021 and May 2022. It was led by HuggingFace and involved several hundreds of researchers and engineers from France and globally, including both academia and the private sector.","Score: 0.8223982769110291 / BigScience Large Open-science Open-access Multilingual Language Model (BLOOM) is a 176-billion-parameter transformer-based autoregressive large language model (LLM). The model: as well as the code base and the data used to train it: are distributed under free licences. BLOOM was trained on approximately 366 billion (1.6TB) tokens from March to July 2022.BLOOM is the main outcome of the BigScience collaborative initiative: a one-year-long research workshop that took place between May 2021 and May 2022. BigScience was led by HuggingFace and involved several hundreds of researchers and engineers from France and abroad representing both the academia and the private sector. BigScience was supported by a large-scale public compute grant on the French public supercomputer Jean Zay: managed by GENCI and IDRIS (CNRS): on which it was trained. / BLOOM's training corpus: named ROOTS: combines data extracted from the then-latest version of the web-based OSCAR corpus (38% of ROOTS) and newly collected data extracted from a manually selected and documented list of language data sources. It encompasses 46 natural languages (in amounts ranging from 30% of the whole dataset for English to 0.00002% for Chi Tumbuka) and 13 programming languages. /  /  / == References ==","Score: 0.7383645221415716 / They used computational machines: then called ""calculators"". Other neural network computational machines were created by Rochester: Holland: Habit: and Duda in 1956. In 1958: psychologist Frank Rosenblatt invented the perceptron: the first implemented artificial neural network: funded by the United States Office of Naval Research. / The invention of the perceptron raised public excitement for research in Artificial Neural Networks: causing the US government to drastically increase funding into deep learning research. This led to ""the golden age of AI"" fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence. For example: in 1957 Herbert Simon famously said:It is not my aim to surprise or shock you—but the simplest way I can summarize is to say that there are now in the world machines that think: that learn and that create. Moreover: their ability to do these things is going to increase rapidly until—in a visible future—the range of problems they can handle will be coextensive with the range to which the human mind has been applied.However: this wasn't the case as research stagnated in the United States following the work of Minsky and Papert (1969): who discovered that basic perceptrons were incapable of processing the exclusive-or circuit and that computers lacked sufficient power to train useful neural networks. This: along with other factors such as the 1973 Lighthill report by James Lighthill stating that research in Artificial Intelligence has not ""produced the major impact that was then promised:"" shutting funding in research into the field of AI in all but two universities in the UK and in many major institutions across the world. This ushered an era called the AI Winter with reduced research into connectionism due to a decrease in government funding and an increased stress on symbolic artificial intelligence in the United States and other Western countries.During the AI Winter era: however: research outside the United States continued: especially in Eastern Europe. By the time Minsky and Papert's book on Perceptrons came out: methods for training multilayer perceptrons (MLPs) were already known. The first deep learning MLP was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa in 1965: as the Group Method of Data Handling. The first deep learning MLP trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. In computer experiments conducted by Amari's student Saito: a five layer MLP with two modifiable layers learned useful internal representations to classify non-linearily separable pattern classes.Self-organizing maps (SOMs) were described by Teuvo Kohonen in 1982. SOMs are neurophysiologically inspired neural networks that learn low-dimensional representations of high-dimensional data while preserving the topological structure of the data. They are trained using competitive learning.The convolutional neural network (CNN) architecture with convolutional layers and downsampling layers was introduced by Kunihiko Fukushima in 1980. He called it the neocognitron. In 1969: he also introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for CNNs and  deep neural networks in general. CNNs have become an essential tool for computer vision. / A key in later advances in artificial neural network research was the backpropagation algorithm: an efficient application of the Leibniz chain rule (1673) to networks of differentiable nodes. It is also known as  / the reverse mode of automatic differentiation or reverse accumulation: due to Seppo Linnainmaa (1970). The term ""back-propagating errors"" was introduced in 1962 by Frank Rosenblatt: but he did not have an implementation of this procedure: although Henry J. Kelley and Bryson had dynamic programming based continuous precursors of backpropagation already in 1960–61 in the context of control theory.  / In 1973: Dreyfus used backpropagation to adapt parameters of controllers in proportion to error gradients.  / In 1982: Paul Werbos applied backpropagation to MLPs in the way that has become standard. In 1986 Rumelhart: Hinton and Williams showed that backpropagation learned interesting internal representations of words as feature vectors when trained to predict the next word in a sequence.In the late 1970s to early 1980s: interest briefly emerged in theoretically investigating the Ising model created by Wilhelm Lenz (1920) and Ernst Ising (1925) / in relation to Cayley tree topologies and large neural networks."
0.15429340000264347,On which supercomputer was BLOOM trained and who supported the training?,"BLOOM was trained on the French public supercomputer Jean Zay, managed by GENCI and IDRIS (CNRS). BigScience, the initiative behind BLOOM, was supported by a large-scale public compute grant on Jean Zay.","Score: 0.8172781436139775 / BigScience Large Open-science Open-access Multilingual Language Model (BLOOM) is a 176-billion-parameter transformer-based autoregressive large language model (LLM). The model: as well as the code base and the data used to train it: are distributed under free licences. BLOOM was trained on approximately 366 billion (1.6TB) tokens from March to July 2022.BLOOM is the main outcome of the BigScience collaborative initiative: a one-year-long research workshop that took place between May 2021 and May 2022. BigScience was led by HuggingFace and involved several hundreds of researchers and engineers from France and abroad representing both the academia and the private sector. BigScience was supported by a large-scale public compute grant on the French public supercomputer Jean Zay: managed by GENCI and IDRIS (CNRS): on which it was trained. / BLOOM's training corpus: named ROOTS: combines data extracted from the then-latest version of the web-based OSCAR corpus (38% of ROOTS) and newly collected data extracted from a manually selected and documented list of language data sources. It encompasses 46 natural languages (in amounts ranging from 30% of the whole dataset for English to 0.00002% for Chi Tumbuka) and 13 programming languages. /  /  / == References ==","Score: 0.7815484500204651 / This helps to exclude rare dependencies. Finally: data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.DNNs must consider many training parameters: such as the size (number of layers and number of units per layer): the learning rate: and initial weights. Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources. Various tricks: such as batching (computing the gradient on several training examples at once rather than individual examples) speed up computation. Large processing capabilities of many-core architectures (such as GPUs or the Intel Xeon Phi) have produced significant speedups in training: because of the suitability of such processing architectures for the matrix and vector computations.Alternatively: engineers may look for other types of neural networks with more straightforward and convergent training algorithms. CMAC (cerebellar model articulation controller) is one such kind of neural network. It doesn't require learning rates or randomized initial weights. The training process can be guaranteed to converge in one step with a new batch of data: and the computational complexity of the training algorithm is linear with respect to the number of neurons involved. /  /  / == Hardware == / Since the 2010s: advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer. By 2019: graphic processing units (GPUs): often with AI-specific enhancements: had displaced CPUs as the dominant method of training large-scale commercial cloud AI. OpenAI estimated the hardware computation used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017): and found a 300:000-fold increase in the amount of computation required: with a doubling-time trendline of 3.4 months.Special electronic circuits called deep learning processors were designed to speed up deep learning algorithms. Deep learning processors include neural processing units (NPUs) in Huawei cellphones and cloud computing servers such as tensor processing units (TPU) in the Google Cloud Platform. Cerebras Systems has also built a dedicated system to handle large deep learning models: the CS-2: based on the largest processor in the industry: the second-generation Wafer Scale Engine (WSE-2).Atomically thin semiconductors are considered promising for energy-efficient deep learning hardware where the same basic device structure is used for both logic operations and data storage. / In 2020: Marega et al. published experiments with a large-area active channel material for developing logic-in-memory devices and circuits based on floating-gate field-effect transistors (FGFETs).In 2021: J. Feldmann et al. proposed an integrated photonic hardware accelerator for parallel convolutional processing. The authors identify two key advantages of integrated photonics over its electronic counterparts: (1) massively parallel data transfer through wavelength division multiplexing in conjunction with frequency combs: and (2) extremely high data modulation speeds. Their system can execute trillions of multiply-accumulate operations per second: indicating the potential of integrated photonics in data-heavy AI applications. /  /  / == Applications == /  /  / === Automatic speech recognition === /  / Large-scale automatic speech recognition is the first and most convincing successful case of deep learning. LSTM RNNs can learn ""Very Deep Learning"" tasks that involve multi-second intervals containing speech events separated by thousands of discrete time steps: where one time step corresponds to about 10 ms. LSTM with forget gates is competitive with traditional speech recognizers on certain tasks.The initial success in speech recognition was based on small-scale recognition tasks based on TIMIT. The data set contains 630 speakers from eight major dialects of American English: where each speaker reads 10 sentences. Its small size lets many configurations be tried. More importantly: the TIMIT task concerns phone-sequence recognition: which: unlike word-sequence recognition: allows weak phone bigram language models. This lets the strength of the acoustic modeling aspects of speech recognition be more easily analyzed. The error rates listed below: including these early results and measured as percent phone error rates (PER): have been summarized since 1991."
0.15366149999317713,"Can you tell me more about BLOOM's training corpus, ROOTS?","ROOTS is the training corpus for BLOOM and combines data extracted from the web-based OSCAR corpus (38% of ROOTS) and newly collected data from a manually selected list of language data sources. It includes data in 46 natural languages, with English forming 30% of the whole dataset to as little as 0.00002% for Chi Tumbuka, as well as data in 13 different programming languages.","Score: 0.8465099157139081 / BigScience Large Open-science Open-access Multilingual Language Model (BLOOM) is a 176-billion-parameter transformer-based autoregressive large language model (LLM). The model: as well as the code base and the data used to train it: are distributed under free licences. BLOOM was trained on approximately 366 billion (1.6TB) tokens from March to July 2022.BLOOM is the main outcome of the BigScience collaborative initiative: a one-year-long research workshop that took place between May 2021 and May 2022. BigScience was led by HuggingFace and involved several hundreds of researchers and engineers from France and abroad representing both the academia and the private sector. BigScience was supported by a large-scale public compute grant on the French public supercomputer Jean Zay: managed by GENCI and IDRIS (CNRS): on which it was trained. / BLOOM's training corpus: named ROOTS: combines data extracted from the then-latest version of the web-based OSCAR corpus (38% of ROOTS) and newly collected data extracted from a manually selected and documented list of language data sources. It encompasses 46 natural languages (in amounts ranging from 30% of the whole dataset for English to 0.00002% for Chi Tumbuka) and 13 programming languages. /  /  / == References ==","Score: 0.7829487787503323 / This helps to exclude rare dependencies. Finally: data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.DNNs must consider many training parameters: such as the size (number of layers and number of units per layer): the learning rate: and initial weights. Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources. Various tricks: such as batching (computing the gradient on several training examples at once rather than individual examples) speed up computation. Large processing capabilities of many-core architectures (such as GPUs or the Intel Xeon Phi) have produced significant speedups in training: because of the suitability of such processing architectures for the matrix and vector computations.Alternatively: engineers may look for other types of neural networks with more straightforward and convergent training algorithms. CMAC (cerebellar model articulation controller) is one such kind of neural network. It doesn't require learning rates or randomized initial weights. The training process can be guaranteed to converge in one step with a new batch of data: and the computational complexity of the training algorithm is linear with respect to the number of neurons involved. /  /  / == Hardware == / Since the 2010s: advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer. By 2019: graphic processing units (GPUs): often with AI-specific enhancements: had displaced CPUs as the dominant method of training large-scale commercial cloud AI. OpenAI estimated the hardware computation used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017): and found a 300:000-fold increase in the amount of computation required: with a doubling-time trendline of 3.4 months.Special electronic circuits called deep learning processors were designed to speed up deep learning algorithms. Deep learning processors include neural processing units (NPUs) in Huawei cellphones and cloud computing servers such as tensor processing units (TPU) in the Google Cloud Platform. Cerebras Systems has also built a dedicated system to handle large deep learning models: the CS-2: based on the largest processor in the industry: the second-generation Wafer Scale Engine (WSE-2).Atomically thin semiconductors are considered promising for energy-efficient deep learning hardware where the same basic device structure is used for both logic operations and data storage. / In 2020: Marega et al. published experiments with a large-area active channel material for developing logic-in-memory devices and circuits based on floating-gate field-effect transistors (FGFETs).In 2021: J. Feldmann et al. proposed an integrated photonic hardware accelerator for parallel convolutional processing. The authors identify two key advantages of integrated photonics over its electronic counterparts: (1) massively parallel data transfer through wavelength division multiplexing in conjunction with frequency combs: and (2) extremely high data modulation speeds. Their system can execute trillions of multiply-accumulate operations per second: indicating the potential of integrated photonics in data-heavy AI applications. /  /  / == Applications == /  /  / === Automatic speech recognition === /  / Large-scale automatic speech recognition is the first and most convincing successful case of deep learning. LSTM RNNs can learn ""Very Deep Learning"" tasks that involve multi-second intervals containing speech events separated by thousands of discrete time steps: where one time step corresponds to about 10 ms. LSTM with forget gates is competitive with traditional speech recognizers on certain tasks.The initial success in speech recognition was based on small-scale recognition tasks based on TIMIT. The data set contains 630 speakers from eight major dialects of American English: where each speaker reads 10 sentences. Its small size lets many configurations be tried. More importantly: the TIMIT task concerns phone-sequence recognition: which: unlike word-sequence recognition: allows weak phone bigram language models. This lets the strength of the acoustic modeling aspects of speech recognition be more easily analyzed. The error rates listed below: including these early results and measured as percent phone error rates (PER): have been summarized since 1991."
0.19974660000298172,What is Chinchilla and when was it presented?,"Chinchilla is a family of large language models developed by the research team at DeepMind, presented in March 2022.","Score: 0.8054764250264677 / Chinchilla is a family of large language models developed by the research team at DeepMind: presented in March 2022. It is named ""chinchilla"" because it is a further development over a previous model family named Gopher. Both model families were trained in order to investigate the scaling laws of large language models.It claimed to outperform GPT-3. It considerably simplifies downstream utilization because it requires much less computer power for inference and fine-tuning. Based on the training of previously employed language models: it has been determined that if one doubles the model size: one must also have twice the number of training tokens. This hypothesis has been used to train Chinchilla by DeepMind. Similar to Gopher in terms of cost: Chinchilla has 70B parameters and four times as much data.Chinchilla has an average accuracy of 67.5% on the MMLU benchmark (Measuring Massive Multitask Language Understanding): which is 7% higher than Gopher's performance. Chinchilla was still in the testing phase as of January 12: 2023.Chinchilla contributes to developing an effective training paradigm for large autoregressive language models with limited compute resources. The Chinchilla team recommends that the number of training tokens is twice for every model size doubling: meaning that using larger: higher-quality training datasets can lead to better results on downstream tasks. /  /  / == Architecture == / Both the Gopher family and Chinchilla family are families of transformer models.  / In particular: they are essentially the same as GPT-2: with different sizes and minor modifications. Gopher family uses RMSNorm instead of LayerNorm; relative positional encoding rather than absolute positional encoding. The Chinchilla family is the same as the Gopher family: but trained with AdamW instead of Adam optimizer. / The Gopher family contains six models of increasing size: from 44 million parameters to 280 billion parameters. They refer to the largest one as ""Gopher"" by default. Similar naming conventions apply for the Chinchilla family. / Table 1 of  shows the entire Gopher family: /  / Table 4 of  compares the 70-billion-parameter Chinchilla with Gopher 280B. /  /  / == See also == / LaMDA /  /  / == References ==","Score: 0.7180134833068675 / The RNN hierarchy can be ""collapsed"" into a single RNN: by ""distilling"" a higher level ""chunker"" network into a lower level ""automatizer"" network. In 1993: a chunker solved a deep learning task whose CAP depth exceeded 1000. / Such history compressors can substantially facilitate downstream supervised deep learning.Geoffrey Hinton et al. (2006) proposed learning a high-level internal representation using successive layers of binary or real-valued latent variables with a restricted Boltzmann machine to model each layer. This RBM is a generative stochastic feedforward neural network that can learn a probability distribution over its set of inputs. Once sufficiently many layers have been learned: the deep architecture may be used as a generative model by reproducing the data when sampling down the model (an ""ancestral pass"") from the top level feature activations. In 2012: Andrew Ng and Jeff Dean created an FNN that learned to recognize higher-level concepts: such as cats: only from watching unlabeled images taken from YouTube videos. /  /  / == The vanishing gradient problem and its solutions == /  / Sepp Hochreiter's diploma thesis (1991) was called ""one of the most important documents in the history of machine learning"" by his supervisor Juergen Schmidhuber. Hochreiter not only tested the neural history compressor: but also identified and analyzed the vanishing gradient problem. He proposed recurrent residual connections to solve this problem. This led to the deep learning method called long short-term memory (LSTM): published in 1997. LSTM recurrent neural networks can learn ""very deep learning"" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. The ""vanilla LSTM"" with forget gate was introduced in 1999 by Felix Gers: Schmidhuber and Fred Cummins. LSTM has become the  most cited neural network of the 20th century.In 2015: Rupesh Kumar Srivastava: Klaus Greff: and Schmidhuber used LSTM principles to create the Highway network: a feedforward neural network with hundreds of layers: much deeper than previous networks. 7 months later: Kaiming He: Xiangyu Zhang;  Shaoqing Ren: and Jian Sun won the ImageNet 2015 competition with an open-gated or gateless Highway network variant called Residual neural network. This has become the most cited neural network of the 21st century.In 2011: Xavier Glorot: Antoine Bordes and Yoshua Bengio found that the ReLU of Kunihiko Fukushima also helps to overcome the vanishing gradient problem: compared to widely used activation functions prior to 2011. /  /  / == Hardware-based designs == / The development of metal–oxide–semiconductor (MOS) very-large-scale integration (VLSI): combining millions or billions of MOS transistors onto a single chip in the form of complementary MOS (CMOS) technology: enabled the development of practical artificial neural networks in the 1980s.Computational devices were created in CMOS: for both biophysical simulation and neuromorphic computing inspired by the structure and function of the human brain. Nanodevices for very large scale principal components analyses and convolution may create a new class of neural computing because they are fundamentally analog rather than digital (even though the first implementations may use digital devices). Ciresan and colleagues (2010) in Schmidhuber's group showed that despite the vanishing gradient problem: GPUs make backpropagation feasible for many-layered feedforward neural networks. /  /  / == Contests == / Between 2009 and 2012: recurrent neural networks and deep feedforward neural networks developed in Schmidhuber's research group won eight international competitions in pattern recognition and machine learning. For example: the bi-directional and multi-dimensional long short-term memory (LSTM) of Graves et al. won three competitions in connected handwriting recognition at the 2009 International Conference on Document Analysis and Recognition (ICDAR): without any prior knowledge about the three languages to be learned.Ciresan and colleagues won pattern recognition contests: including the IJCNN 2011 Traffic Sign Recognition Competition: the ISBI 2012 Segmentation of Neuronal Structures in Electron Microscopy Stacks challenge and others. Their neural networks were the first pattern recognizers to achieve human-competitive/superhuman performance on benchmarks such as traffic sign recognition (IJCNN 2012): or the MNIST handwritten digits problem."
0.17769310000585392,How does Chinchilla compare to the previous model named Gopher?,"Chinchilla is a further development over Gopher. It requires less computer power for inference and fine-tuning, has an average accuracy of 67.5% on the MMLU benchmark which is 7% higher than Gopher, and has 70 billion parameters with four times as much data.","Score: 0.8574502810850578 / Chinchilla is a family of large language models developed by the research team at DeepMind: presented in March 2022. It is named ""chinchilla"" because it is a further development over a previous model family named Gopher. Both model families were trained in order to investigate the scaling laws of large language models.It claimed to outperform GPT-3. It considerably simplifies downstream utilization because it requires much less computer power for inference and fine-tuning. Based on the training of previously employed language models: it has been determined that if one doubles the model size: one must also have twice the number of training tokens. This hypothesis has been used to train Chinchilla by DeepMind. Similar to Gopher in terms of cost: Chinchilla has 70B parameters and four times as much data.Chinchilla has an average accuracy of 67.5% on the MMLU benchmark (Measuring Massive Multitask Language Understanding): which is 7% higher than Gopher's performance. Chinchilla was still in the testing phase as of January 12: 2023.Chinchilla contributes to developing an effective training paradigm for large autoregressive language models with limited compute resources. The Chinchilla team recommends that the number of training tokens is twice for every model size doubling: meaning that using larger: higher-quality training datasets can lead to better results on downstream tasks. /  /  / == Architecture == / Both the Gopher family and Chinchilla family are families of transformer models.  / In particular: they are essentially the same as GPT-2: with different sizes and minor modifications. Gopher family uses RMSNorm instead of LayerNorm; relative positional encoding rather than absolute positional encoding. The Chinchilla family is the same as the Gopher family: but trained with AdamW instead of Adam optimizer. / The Gopher family contains six models of increasing size: from 44 million parameters to 280 billion parameters. They refer to the largest one as ""Gopher"" by default. Similar naming conventions apply for the Chinchilla family. / Table 1 of  shows the entire Gopher family: /  / Table 4 of  compares the 70-billion-parameter Chinchilla with Gopher 280B. /  /  / == See also == / LaMDA /  /  / == References ==","Score: 0.7483241958325789 / amount of neurons in its layers: amount of weights between them and biases): / size of its (pre-)training dataset (i.e. number of tokens in corpus: D{\displaystyle D}): / performance after (pre-)training.They are related by simple statistical laws: called ""scaling laws"". One particular scaling law (""Chinchilla scaling"") for LLM autoregressively trained for one epoch: with a log-log learning rate schedule: states that: where the variables are /  / C{\displaystyle C} is the cost of training the model: in FLOPs. / N{\displaystyle N} is the number of parameters in the model. / D{\displaystyle D} is the number of tokens in the training set. / L{\displaystyle L} is the average negative log-likelihood loss per token (nats/token): achieved by the trained LLM on the test dataset.and the statistical hyper-parameters are /  / C0=6{\displaystyle C_{0}=6}: meaning that it costs 6 FLOPs per parameter to train on one token. Note that training cost is much higher than inference cost: where it costs 1 to 2 FLOPs per parameter to infer on one token. / α=0.34:β=0.28:A=406.4:B=410.7:L0=1.69{\displaystyle \alpha =0.34:\beta =0.28:A=406.4:B=410.7:L_{0}=1.69} /  /  / === Emergent abilities === / When one subtracts out from the y-axis the best performance that can be achieved even with infinite scaling of the x-axis quantity: large models' performance: measured on various tasks: seems to be a linear extrapolation of other (smaller-sized and medium-sized) models' performance on a log-log plot. However: sometimes the line's slope transitions from one slope to another at point(s) referred to as break(s) in downstream scaling laws: appearing as a series of linear segments connected by arcs; it seems that larger models acquire ""emergent abilities"" at this point(s). These abilities are discovered rather than programmed-in or designed: in some cases only after the LLM has been publicly deployed.The most intriguing among emergent abilities is in-context learning from example demonstrations. In-context learning is involved in tasks: such as: /  / reported arithmetics: decoding the International Phonetic Alphabet: unscrambling a word's letters: disambiguate word in context: converting spatial words: cardinal directions (for example: replying ""northeast"" upon [0: 0: 1; 0: 0: 0; 0: 0: 0]): color terms represented in text. / chain-of-thought prompting: Model outputs are improved by chain-of-thought prompting only when model size exceeds 62B. Smaller models perform better when prompted to answer immediately: without chain of thought. / identifying offensive content in paragraphs of Hinglish (a combination of Hindi and English): and generating a similar English equivalent of Kiswahili proverbs.Schaeffer et. al. argue that the emergent abilities are not unpredictably acquired: but predictably acquired according to a smooth scaling law. The authors considered a toy statistical model of an LLM solving multiple-choice questions: and showed that this statistical model: modified to account for other types of tasks: applies to these tasks as well.Let x{\displaystyle x} be the number of parameter count: and y{\displaystyle y} be the performance of the model. /  /  / == Interpretation == / Large language models by themselves are ""black boxes"": and it is not clear how they can perform linguistic tasks. There are several methods for understanding how LLM work. / Mechanistic interpretability aims to reverse-engineer LLM by discovering symbolic algorithms that approximate the inference performed by LLM. One example is Othello-GPT: where a small Transformer is trained to predict legal Othello moves. It is found that there is a linear representation of Othello board: and modifying the representation changes the predicted legal Othello moves in the correct way. In another example: a small Transformer is trained on Karel programs. Similar to the Othello-GPT example: there is a linear representation of Karel program semantics: and modifying the representation changes output in the correct way. The model also generates correct programs that are on average shorter than those in the training set.In another example: the authors trained small transformers on modular arithmetic addition. The resulting models were reverse-engineered: and it turned out they used discrete Fourier transform."
0.12554000000818633,What hypothesis was used to train the Chinchilla by DeepMind?,"The hypothesis used to train Chinchilla by DeepMind is that if one doubles the model size, one must also have twice the number of training tokens.","Score: 0.8462944183940431 / Chinchilla is a family of large language models developed by the research team at DeepMind: presented in March 2022. It is named ""chinchilla"" because it is a further development over a previous model family named Gopher. Both model families were trained in order to investigate the scaling laws of large language models.It claimed to outperform GPT-3. It considerably simplifies downstream utilization because it requires much less computer power for inference and fine-tuning. Based on the training of previously employed language models: it has been determined that if one doubles the model size: one must also have twice the number of training tokens. This hypothesis has been used to train Chinchilla by DeepMind. Similar to Gopher in terms of cost: Chinchilla has 70B parameters and four times as much data.Chinchilla has an average accuracy of 67.5% on the MMLU benchmark (Measuring Massive Multitask Language Understanding): which is 7% higher than Gopher's performance. Chinchilla was still in the testing phase as of January 12: 2023.Chinchilla contributes to developing an effective training paradigm for large autoregressive language models with limited compute resources. The Chinchilla team recommends that the number of training tokens is twice for every model size doubling: meaning that using larger: higher-quality training datasets can lead to better results on downstream tasks. /  /  / == Architecture == / Both the Gopher family and Chinchilla family are families of transformer models.  / In particular: they are essentially the same as GPT-2: with different sizes and minor modifications. Gopher family uses RMSNorm instead of LayerNorm; relative positional encoding rather than absolute positional encoding. The Chinchilla family is the same as the Gopher family: but trained with AdamW instead of Adam optimizer. / The Gopher family contains six models of increasing size: from 44 million parameters to 280 billion parameters. They refer to the largest one as ""Gopher"" by default. Similar naming conventions apply for the Chinchilla family. / Table 1 of  shows the entire Gopher family: /  / Table 4 of  compares the 70-billion-parameter Chinchilla with Gopher 280B. /  /  / == See also == / LaMDA /  /  / == References ==","Score: 0.8145495036487026 / Such techniques lack ways of representing causal relationships (...) have no obvious ways of performing logical inferences: and they are also still a long way from integrating abstract knowledge: such as information about what objects are: what they are for: and how they are typically used. The most powerful A.I. systems: like Watson (...) use techniques like deep learning as just one element in a very complicated ensemble of techniques: ranging from the statistical technique of Bayesian inference to deductive reasoning. /  / In further reference to the idea that artistic sensitivity might be inherent in relatively low levels of the cognitive hierarchy: a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained demonstrate a visual appeal: the original research notice received well over 1:000 comments: and was the subject of what was for a time the most frequently accessed article on The Guardian's website. /  /  / === Errors === / Some deep learning architectures display problematic behaviors: such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images (2014) and misclassifying minuscule perturbations of correctly classified images (2013). Goertzel hypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-component artificial general intelligence (AGI) architectures. These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar decompositions of observed entities and events. Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic goal of both human language acquisition and artificial intelligence (AI). /  /  / === Cyber threat === / As deep learning moves from the lab into the world: research and experience show that artificial neural networks are vulnerable to hacks and deception. By identifying patterns that these systems use to function: attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize. For example: an attacker can make subtle changes to an image such that the ANN finds a match even though the image looks to a human nothing like the search target. Such manipulation is termed an ""adversarial attack"".In 2016 researchers used one ANN to doctor images in trial and error fashion: identify another's focal points: and thereby generate images that deceived it. The modified images looked no different to human eyes. Another group showed that printouts of doctored images then photographed successfully tricked an image classification system. One defense is reverse image search: in which a possible fake image is submitted to a site such as TinEye that can then find other instances of it. A refinement is to search using only parts of the image: to identify images from which that piece may have been taken.Another group showed that certain psychedelic spectacles could fool a facial recognition system into thinking ordinary people were celebrities: potentially allowing one person to impersonate another. In 2017 researchers added stickers to stop signs and caused an ANN to misclassify them.ANNs can however be further trained to detect attempts at deception: potentially leading attackers and defenders into an arms race similar to the kind that already defines the malware defense industry. ANNs have been trained to defeat ANN-based anti-malware software by repeatedly attacking a defense with malware that was continually altered by a genetic algorithm until it tricked the anti-malware while retaining its ability to damage the target.In 2016: another group demonstrated that certain sounds could make the Google Now voice command system open a particular web address: and hypothesized that this could ""serve as a stepping stone for further attacks (e.g.: opening a web page hosting drive-by malware)"".In ""data poisoning"": false data is continually smuggled into a machine learning system's training set to prevent it from achieving mastery. /  /  / === Data collection ethics === / Most Deep Learning systems rely on training and verification data that is generated and/or annotated by humans. It has been argued in media philosophy that not only low-paid clickwork (e.g. on Amazon Mechanical Turk) is regularly deployed for this purpose: but also implicit forms of human microwork that are often not recognized as such. The philosopher Rainer Mühlhoff distinguishes five types of ""machinic capture"" of human microwork to generate training data: (1) gamification (the embedding of annotation or computation tasks in the flow of a game): (2) ""trapping and tracking"" (e.g. CAPTCHAs for image recognition or click-tracking on Google search results pages): (3) exploitation of social motivations (e.g."
0.3115527000045404,How are the Chinchilla and Gopher families similar in terms of architecture?,"Both the Gopher and Chinchilla families are transformer models. They are essentially the same as GPT-2, with different sizes and minor modifications. Gopher uses RMSNorm instead of LayerNorm, and relative positional encoding rather than absolute. Chinchilla is the same as Gopher but trained with AdamW instead of Adam optimizer.","Score: 0.827214069970992 / Chinchilla is a family of large language models developed by the research team at DeepMind: presented in March 2022. It is named ""chinchilla"" because it is a further development over a previous model family named Gopher. Both model families were trained in order to investigate the scaling laws of large language models.It claimed to outperform GPT-3. It considerably simplifies downstream utilization because it requires much less computer power for inference and fine-tuning. Based on the training of previously employed language models: it has been determined that if one doubles the model size: one must also have twice the number of training tokens. This hypothesis has been used to train Chinchilla by DeepMind. Similar to Gopher in terms of cost: Chinchilla has 70B parameters and four times as much data.Chinchilla has an average accuracy of 67.5% on the MMLU benchmark (Measuring Massive Multitask Language Understanding): which is 7% higher than Gopher's performance. Chinchilla was still in the testing phase as of January 12: 2023.Chinchilla contributes to developing an effective training paradigm for large autoregressive language models with limited compute resources. The Chinchilla team recommends that the number of training tokens is twice for every model size doubling: meaning that using larger: higher-quality training datasets can lead to better results on downstream tasks. /  /  / == Architecture == / Both the Gopher family and Chinchilla family are families of transformer models.  / In particular: they are essentially the same as GPT-2: with different sizes and minor modifications. Gopher family uses RMSNorm instead of LayerNorm; relative positional encoding rather than absolute positional encoding. The Chinchilla family is the same as the Gopher family: but trained with AdamW instead of Adam optimizer. / The Gopher family contains six models of increasing size: from 44 million parameters to 280 billion parameters. They refer to the largest one as ""Gopher"" by default. Similar naming conventions apply for the Chinchilla family. / Table 1 of  shows the entire Gopher family: /  / Table 4 of  compares the 70-billion-parameter Chinchilla with Gopher 280B. /  /  / == See also == / LaMDA /  /  / == References ==","Score: 0.7459930709181791 / The Long short-term memory architecture overcomes these problems.In reinforcement learning settings: no teacher provides target signals. Instead a fitness function or reward function or utility function is occasionally used to evaluate performance: which influences its input stream through output units connected to actuators that affect the environment. Variants of evolutionary computation are often used to optimize the weight matrix. /  /  / ==== Hopfield ==== /  / The Hopfield network (like similar attractor-based networks) is of historic interest although it is not a general RNN: as it is not designed to process sequences of patterns. Instead it requires stationary inputs. It is an RNN in which all connections are symmetric. It guarantees that it will converge. If the connections are trained using Hebbian learning the Hopfield network can perform as robust content-addressable memory: resistant to connection alteration. /  /  / ==== Boltzmann machine ==== /  / The Boltzmann machine can be thought of as a noisy Hopfield network. It is one of the first neural networks to demonstrate learning of latent variables (hidden units). Boltzmann machine learning was at first slow to simulate: but the contrastive divergence algorithm speeds up training for Boltzmann machines and Products of Experts. /  /  / ==== Self-organizing map ==== /  / The self-organizing map (SOM) uses unsupervised learning. A set of neurons learn to map points in an input space to coordinates in an output space. The input space can have different dimensions and topology from the output space: and SOM attempts to preserve these. /  /  / ==== Learning vector quantization ==== /  / Learning vector quantization (LVQ) can be interpreted as a neural network architecture. Prototypical representatives of the classes parameterize: together with an appropriate distance measure: in a distance-based classification scheme. /  /  / === Simple recurrent === / Simple recurrent networks have three layers: with the addition of a set of ""context units"" in the input layer. These units connect from the hidden layer or the output layer with a fixed weight of one. At each time step: the input is propagated in a standard feedforward fashion: and then a backpropagation-like learning rule is applied (not performing gradient descent). The fixed back connections leave a copy of the previous values of the hidden units in the context units (since they propagate over the connections before the learning rule is applied). /  /  / === Reservoir computing === /  / Reservoir computing is a computation framework that may be viewed as an extension of neural networks. Typically an input signal is fed into a fixed (random) dynamical system called a reservoir whose dynamics map the input to a higher dimension. A readout mechanism is trained to map the reservoir to the desired output. Training is performed only at the readout stage. Liquid-state machines are a type of reservoir computing. /  /  / ==== Echo state ==== /  / The echo state network (ESN) employs a sparsely connected random hidden layer. The weights of output neurons are the only part of the network that are trained. ESN are good at reproducing certain time series. /  /  / === Long short-term memory === /  / The long short-term memory (LSTM) avoids the vanishing gradient problem. It works even when with long delays between inputs and can handle signals that mix low and high frequency components. LSTM RNN outperformed other RNN and other sequence learning methods such as HMM in applications such as language learning and connected handwriting recognition. /  /  / === Bi-directional === /  / Bi-directional RNN: or BRNN: use a finite sequence to predict or label each element of a sequence based on both the past and future context of the element. This is done by adding the outputs of two RNNs: one processing the sequence from left to right: the other one from right to left. The combined outputs are the predictions of the teacher-given target signals. This technique proved to be especially useful when combined with LSTM. /  /  / === Hierarchical === /  / Hierarchical RNN connects elements in various ways to decompose hierarchical behavior into useful subprograms. /  /  / === Stochastic === /  / A district from conventional neural networks: stochastic artificial neural network used as an approximation to  / random functions. /  /  / === Genetic Scale === / A RNN (often a LSTM) where a series is decomposed into a number of scales where every scale informs the primary length between two consecutive points. A first order scale consists of a normal RNN: a second order consists of all points separated by two indices and so on. The Nth order RNN connects the first and last node. The outputs from all the various scales are treated as a Committee of Machines and the associated scores are used genetically for the next iteration. /  /  / == Modular == /  / Biological studies have shown that the human brain operates as a collection of small networks."
0.1584549999970477,"What is the status of Chinchilla as of January 12, 2023?","As of January 12, 2023, Chinchilla was still in the testing phase.","Score: 0.7879403163685674 / Chinchilla is a family of large language models developed by the research team at DeepMind: presented in March 2022. It is named ""chinchilla"" because it is a further development over a previous model family named Gopher. Both model families were trained in order to investigate the scaling laws of large language models.It claimed to outperform GPT-3. It considerably simplifies downstream utilization because it requires much less computer power for inference and fine-tuning. Based on the training of previously employed language models: it has been determined that if one doubles the model size: one must also have twice the number of training tokens. This hypothesis has been used to train Chinchilla by DeepMind. Similar to Gopher in terms of cost: Chinchilla has 70B parameters and four times as much data.Chinchilla has an average accuracy of 67.5% on the MMLU benchmark (Measuring Massive Multitask Language Understanding): which is 7% higher than Gopher's performance. Chinchilla was still in the testing phase as of January 12: 2023.Chinchilla contributes to developing an effective training paradigm for large autoregressive language models with limited compute resources. The Chinchilla team recommends that the number of training tokens is twice for every model size doubling: meaning that using larger: higher-quality training datasets can lead to better results on downstream tasks. /  /  / == Architecture == / Both the Gopher family and Chinchilla family are families of transformer models.  / In particular: they are essentially the same as GPT-2: with different sizes and minor modifications. Gopher family uses RMSNorm instead of LayerNorm; relative positional encoding rather than absolute positional encoding. The Chinchilla family is the same as the Gopher family: but trained with AdamW instead of Adam optimizer. / The Gopher family contains six models of increasing size: from 44 million parameters to 280 billion parameters. They refer to the largest one as ""Gopher"" by default. Similar naming conventions apply for the Chinchilla family. / Table 1 of  shows the entire Gopher family: /  / Table 4 of  compares the 70-billion-parameter Chinchilla with Gopher 280B. /  /  / == See also == / LaMDA /  /  / == References ==","Score: 0.7185012195096792 / H. Speech and Language Processing: An Introduction to Natural Language Processing: Computational Linguistics: and Speech Recognition: 3rd Edition draft: 2023. / Phuong: Mary; Hutter: Marcus (2022). ""Formal Algorithms for Transformers"". arXiv:2207.09238 [cs.LG]. / Eloundou: Tyna; Manning: Sam; Mishkin: Pamela; Rock: Daniel (2023). ""GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models"". arXiv:2303.10130 [econ.GN]. / Eldan: Ronen; Li: Yuanzhi (2023). ""TinyStories: How Small Can Language Models Be and Still Speak Coherent English?"". arXiv:2305.07759 [cs.CL]. / Frank: Michael C. (27 June 2023). ""Baby steps in evaluating the capacities of large language models"". Nature Reviews Psychology. 2 (8): 451–452. doi:10.1038/s44159-023-00211-x. ISSN 2731-0574. S2CID 259713140. Retrieved 2 July 2023. / Zhao: Wayne Xin; et al. (2023). ""A Survey of Large Language Models"". arXiv:2303.18223 [cs.CL]. / Kaddour: Jean; et al. (2023). ""Challenges and Applications of Large Language Models"". arXiv:2307.10169 [cs.CL]. / Yin: Shukang; Fu: Chaoyou; Zhao: Sirui; Li: Ke; Sun: Xing; Xu: Tong; Chen: Enhong (2023-06-01). ""A Survey on Multimodal Large Language Models"". arXiv:2306.13549 [cs.CV]. / Open LLMs repository on GitHub."
0.17657979999785312,What are dilution and dropout techniques in artificial neural networks?,"Dilution and dropout techniques in artificial neural networks are regularization techniques aimed at reducing overfitting by preventing complex co-adaptations on training data. In these processes, dilution refers to thinning weights, while dropout refers to randomly ""dropping out"", or omitting, units during the training process of a neural network.","Score: 0.8896514930683008 / Dilution and dropout (also called DropConnect) are regularization techniques for reducing overfitting in artificial neural networks by preventing complex co-adaptations on training data. They are an efficient way of performing model averaging with neural networks. Dilution refers to thinning weights: while dropout refers to randomly ""dropping out"": or omitting: units (both hidden and visible) during the training process of a neural network. Both trigger the same type of regularization. /  /  / == Types and uses == / Dilution is usually split in weak dilution and strong dilution. Weak dilution describes the process in which the finite fraction of removed connections is small: and strong dilution refers to when this fraction is large. There is no clear distinction on where the limit between strong and weak dilution is: and often the distinction is dependent on the precedent of a specific use-case and has implications for how to solve for exact solutions. / Sometimes dilution is used for adding damping noise to the inputs. In that case: weak dilution refers to adding a small amount of damping noise: while strong dilution refers to adding a greater amount of damping noise. Both can be rewritten as variants of weight dilution. / These techniques are also sometimes referred to as random pruning of weights: but this is usually a non-recurring one-way operation. The network is pruned: and then kept if it is an improvement over the previous model. Dilution and dropout both refer to an iterative process. The pruning of weights typically does not imply that the network continues learning: while in dilution/dropout: the network continues to learn after the technique is applied. /  /  / == Generalized linear network == / Output from a layer of linear nodes: in an artificial neural net can be described as /  / yi{\displaystyle y_{i}} – output from node i{\displaystyle i} / wij{\displaystyle w_{ij}} – real weight before dilution: also called the Hebb connection strength / xj{\displaystyle x_{j}} – input from node j{\displaystyle j}This can be written in vector notation as /  / y{\displaystyle \mathbf {y} } – output vector / W{\displaystyle \mathbf {W} } – weight matrix / x{\displaystyle \mathbf {x} } – input vectorEquations (1) and (2) are used in the subsequent sections. /  /  / == Weak dilution == / During weak dilution: the finite fraction of removed connections (the weights) is small: giving rise to a tiny uncertainty. This edge-case can be solved exactly with mean field theory. In weak dilution the impact on the weights can be described as /  / wij^{\displaystyle {\hat {w_{ij}}}} – diluted weight / wij{\displaystyle w_{ij}} – real weight before dilution / P(c){\displaystyle P(c)} – the probability of c{\displaystyle c}: the probability of keeping a weightThe interpretation of probability P(c){\displaystyle P(c)} can also be changed from keeping a weight into pruning a weight. / In vector notation this can be written as /  / where the function g⁡(⋅){\displaystyle \operatorname {g} (\cdot )} imposes the previous dilution. / In weak dilution only a small and fixed fraction of the weights are diluted. When the number of terms in the sum goes to infinite (the weights for each node) it is still infinite (the fraction is fixed): thus mean field theory can be applied. In the notation from Hertz et al. this would be written as /  / ⟨hi⟩{\displaystyle \left\langle h_{i}\right\rangle } the mean field temperature / c{\displaystyle c} – a scaling factor for the temperature from the probability of keeping the weight / wij{\displaystyle w_{ij}} – real weight before dilution: also called the Hebb connection strength / ⟨Sj⟩{\displaystyle \left\langle S_{j}\right\rangle } – the mean stable equilibrium statesThere are some assumptions for this to hold: which are not listed here. /  /  / == Strong dilution == / When the dilution is strong: the finite fraction of removed connections (the weights) is large: giving rise to a huge uncertainty.","Score: 0.8590656940629414 / When the number of terms in the sum goes to infinite (the weights for each node) it is still infinite (the fraction is fixed): thus mean field theory can be applied. In the notation from Hertz et al. this would be written as /  / ⟨hi⟩{\displaystyle \left\langle h_{i}\right\rangle } the mean field temperature / c{\displaystyle c} – a scaling factor for the temperature from the probability of keeping the weight / wij{\displaystyle w_{ij}} – real weight before dilution: also called the Hebb connection strength / ⟨Sj⟩{\displaystyle \left\langle S_{j}\right\rangle } – the mean stable equilibrium statesThere are some assumptions for this to hold: which are not listed here. /  /  / == Strong dilution == / When the dilution is strong: the finite fraction of removed connections (the weights) is large: giving rise to a huge uncertainty. /  /  / == Dropout == / Dropout is a special case of the previous weight equation (3): where the aforementioned equation is adjusted to remove a whole row in the vector matrix: and not only random weights /  / P(c){\displaystyle P(c)} – the probability c{\displaystyle c} to keep a row in the weight matrix / wj{\displaystyle \mathbf {w} _{j}} – real row in the weight matrix before dropout / wj^{\displaystyle {\hat {\mathbf {w} _{j}}}} – diluted row in the weight matrixBecause dropout removes a whole row from the vector matrix: the previous (unlisted) assumptions for weak dilution and the use of mean field theory are not applicable. / The process by which the node is driven to zero: whether by setting the weights to zero: by “removing the node”: or by some other means: does not impact the end result and does not create a new and unique case. If the neural net is processed by a high-performance digital array-multiplicator: then it is likely more effective to drive the value to zero late in the process graph. If the net is processed by a constrained processor: perhaps even an analog neuromorph processor: then it is likely a more power-efficient solution is to drive the value to zero early in the process graph. /  /  / == Google's patent == / Although there have been examples of randomly removing connections between neurons in a neural network to improve models: this technique was first introduced with the name dropout by Geoffrey Hinton: et al. in 2012. Google currently holds the patent for the dropout technique. /  /  / == See also == / AlexNet / Convolutional neural network § Dropout /  /  / == Notes == /  /  / == References =="
0.17969109999830835,How are weak dilution and strong dilution differentiated in neural networks?,"In neural networks, weak dilution is when the finite fraction of removed connections is small, often adding a small amount of damping noise to the inputs. Conversely, strong dilution refers to when this fraction of removed connections is large, often accompanied by adding a greater amount of damping noise.","Score: 0.8700663462170092 / Dilution and dropout (also called DropConnect) are regularization techniques for reducing overfitting in artificial neural networks by preventing complex co-adaptations on training data. They are an efficient way of performing model averaging with neural networks. Dilution refers to thinning weights: while dropout refers to randomly ""dropping out"": or omitting: units (both hidden and visible) during the training process of a neural network. Both trigger the same type of regularization. /  /  / == Types and uses == / Dilution is usually split in weak dilution and strong dilution. Weak dilution describes the process in which the finite fraction of removed connections is small: and strong dilution refers to when this fraction is large. There is no clear distinction on where the limit between strong and weak dilution is: and often the distinction is dependent on the precedent of a specific use-case and has implications for how to solve for exact solutions. / Sometimes dilution is used for adding damping noise to the inputs. In that case: weak dilution refers to adding a small amount of damping noise: while strong dilution refers to adding a greater amount of damping noise. Both can be rewritten as variants of weight dilution. / These techniques are also sometimes referred to as random pruning of weights: but this is usually a non-recurring one-way operation. The network is pruned: and then kept if it is an improvement over the previous model. Dilution and dropout both refer to an iterative process. The pruning of weights typically does not imply that the network continues learning: while in dilution/dropout: the network continues to learn after the technique is applied. /  /  / == Generalized linear network == / Output from a layer of linear nodes: in an artificial neural net can be described as /  / yi{\displaystyle y_{i}} – output from node i{\displaystyle i} / wij{\displaystyle w_{ij}} – real weight before dilution: also called the Hebb connection strength / xj{\displaystyle x_{j}} – input from node j{\displaystyle j}This can be written in vector notation as /  / y{\displaystyle \mathbf {y} } – output vector / W{\displaystyle \mathbf {W} } – weight matrix / x{\displaystyle \mathbf {x} } – input vectorEquations (1) and (2) are used in the subsequent sections. /  /  / == Weak dilution == / During weak dilution: the finite fraction of removed connections (the weights) is small: giving rise to a tiny uncertainty. This edge-case can be solved exactly with mean field theory. In weak dilution the impact on the weights can be described as /  / wij^{\displaystyle {\hat {w_{ij}}}} – diluted weight / wij{\displaystyle w_{ij}} – real weight before dilution / P(c){\displaystyle P(c)} – the probability of c{\displaystyle c}: the probability of keeping a weightThe interpretation of probability P(c){\displaystyle P(c)} can also be changed from keeping a weight into pruning a weight. / In vector notation this can be written as /  / where the function g⁡(⋅){\displaystyle \operatorname {g} (\cdot )} imposes the previous dilution. / In weak dilution only a small and fixed fraction of the weights are diluted. When the number of terms in the sum goes to infinite (the weights for each node) it is still infinite (the fraction is fixed): thus mean field theory can be applied. In the notation from Hertz et al. this would be written as /  / ⟨hi⟩{\displaystyle \left\langle h_{i}\right\rangle } the mean field temperature / c{\displaystyle c} – a scaling factor for the temperature from the probability of keeping the weight / wij{\displaystyle w_{ij}} – real weight before dilution: also called the Hebb connection strength / ⟨Sj⟩{\displaystyle \left\langle S_{j}\right\rangle } – the mean stable equilibrium statesThere are some assumptions for this to hold: which are not listed here. /  /  / == Strong dilution == / When the dilution is strong: the finite fraction of removed connections (the weights) is large: giving rise to a huge uncertainty.","Score: 0.8447777334604402 / When the number of terms in the sum goes to infinite (the weights for each node) it is still infinite (the fraction is fixed): thus mean field theory can be applied. In the notation from Hertz et al. this would be written as /  / ⟨hi⟩{\displaystyle \left\langle h_{i}\right\rangle } the mean field temperature / c{\displaystyle c} – a scaling factor for the temperature from the probability of keeping the weight / wij{\displaystyle w_{ij}} – real weight before dilution: also called the Hebb connection strength / ⟨Sj⟩{\displaystyle \left\langle S_{j}\right\rangle } – the mean stable equilibrium statesThere are some assumptions for this to hold: which are not listed here. /  /  / == Strong dilution == / When the dilution is strong: the finite fraction of removed connections (the weights) is large: giving rise to a huge uncertainty. /  /  / == Dropout == / Dropout is a special case of the previous weight equation (3): where the aforementioned equation is adjusted to remove a whole row in the vector matrix: and not only random weights /  / P(c){\displaystyle P(c)} – the probability c{\displaystyle c} to keep a row in the weight matrix / wj{\displaystyle \mathbf {w} _{j}} – real row in the weight matrix before dropout / wj^{\displaystyle {\hat {\mathbf {w} _{j}}}} – diluted row in the weight matrixBecause dropout removes a whole row from the vector matrix: the previous (unlisted) assumptions for weak dilution and the use of mean field theory are not applicable. / The process by which the node is driven to zero: whether by setting the weights to zero: by “removing the node”: or by some other means: does not impact the end result and does not create a new and unique case. If the neural net is processed by a high-performance digital array-multiplicator: then it is likely more effective to drive the value to zero late in the process graph. If the net is processed by a constrained processor: perhaps even an analog neuromorph processor: then it is likely a more power-efficient solution is to drive the value to zero early in the process graph. /  /  / == Google's patent == / Although there have been examples of randomly removing connections between neurons in a neural network to improve models: this technique was first introduced with the name dropout by Geoffrey Hinton: et al. in 2012. Google currently holds the patent for the dropout technique. /  /  / == See also == / AlexNet / Convolutional neural network § Dropout /  /  / == Notes == /  /  / == References =="
0.17072180000832304,How is dilution related to adding damping noise to inputs?,"Dilution can be used for adding damping noise to the inputs within a neural network. Weak dilution refers to adding a small amount of damping noise, while strong dilution refers to adding a larger amount of damping noise. These can be considered as variants of weight dilution.","Score: 0.812391945008429 / When the number of terms in the sum goes to infinite (the weights for each node) it is still infinite (the fraction is fixed): thus mean field theory can be applied. In the notation from Hertz et al. this would be written as /  / ⟨hi⟩{\displaystyle \left\langle h_{i}\right\rangle } the mean field temperature / c{\displaystyle c} – a scaling factor for the temperature from the probability of keeping the weight / wij{\displaystyle w_{ij}} – real weight before dilution: also called the Hebb connection strength / ⟨Sj⟩{\displaystyle \left\langle S_{j}\right\rangle } – the mean stable equilibrium statesThere are some assumptions for this to hold: which are not listed here. /  /  / == Strong dilution == / When the dilution is strong: the finite fraction of removed connections (the weights) is large: giving rise to a huge uncertainty. /  /  / == Dropout == / Dropout is a special case of the previous weight equation (3): where the aforementioned equation is adjusted to remove a whole row in the vector matrix: and not only random weights /  / P(c){\displaystyle P(c)} – the probability c{\displaystyle c} to keep a row in the weight matrix / wj{\displaystyle \mathbf {w} _{j}} – real row in the weight matrix before dropout / wj^{\displaystyle {\hat {\mathbf {w} _{j}}}} – diluted row in the weight matrixBecause dropout removes a whole row from the vector matrix: the previous (unlisted) assumptions for weak dilution and the use of mean field theory are not applicable. / The process by which the node is driven to zero: whether by setting the weights to zero: by “removing the node”: or by some other means: does not impact the end result and does not create a new and unique case. If the neural net is processed by a high-performance digital array-multiplicator: then it is likely more effective to drive the value to zero late in the process graph. If the net is processed by a constrained processor: perhaps even an analog neuromorph processor: then it is likely a more power-efficient solution is to drive the value to zero early in the process graph. /  /  / == Google's patent == / Although there have been examples of randomly removing connections between neurons in a neural network to improve models: this technique was first introduced with the name dropout by Geoffrey Hinton: et al. in 2012. Google currently holds the patent for the dropout technique. /  /  / == See also == / AlexNet / Convolutional neural network § Dropout /  /  / == Notes == /  /  / == References ==","Score: 0.8083018554518828 / Dilution and dropout (also called DropConnect) are regularization techniques for reducing overfitting in artificial neural networks by preventing complex co-adaptations on training data. They are an efficient way of performing model averaging with neural networks. Dilution refers to thinning weights: while dropout refers to randomly ""dropping out"": or omitting: units (both hidden and visible) during the training process of a neural network. Both trigger the same type of regularization. /  /  / == Types and uses == / Dilution is usually split in weak dilution and strong dilution. Weak dilution describes the process in which the finite fraction of removed connections is small: and strong dilution refers to when this fraction is large. There is no clear distinction on where the limit between strong and weak dilution is: and often the distinction is dependent on the precedent of a specific use-case and has implications for how to solve for exact solutions. / Sometimes dilution is used for adding damping noise to the inputs. In that case: weak dilution refers to adding a small amount of damping noise: while strong dilution refers to adding a greater amount of damping noise. Both can be rewritten as variants of weight dilution. / These techniques are also sometimes referred to as random pruning of weights: but this is usually a non-recurring one-way operation. The network is pruned: and then kept if it is an improvement over the previous model. Dilution and dropout both refer to an iterative process. The pruning of weights typically does not imply that the network continues learning: while in dilution/dropout: the network continues to learn after the technique is applied. /  /  / == Generalized linear network == / Output from a layer of linear nodes: in an artificial neural net can be described as /  / yi{\displaystyle y_{i}} – output from node i{\displaystyle i} / wij{\displaystyle w_{ij}} – real weight before dilution: also called the Hebb connection strength / xj{\displaystyle x_{j}} – input from node j{\displaystyle j}This can be written in vector notation as /  / y{\displaystyle \mathbf {y} } – output vector / W{\displaystyle \mathbf {W} } – weight matrix / x{\displaystyle \mathbf {x} } – input vectorEquations (1) and (2) are used in the subsequent sections. /  /  / == Weak dilution == / During weak dilution: the finite fraction of removed connections (the weights) is small: giving rise to a tiny uncertainty. This edge-case can be solved exactly with mean field theory. In weak dilution the impact on the weights can be described as /  / wij^{\displaystyle {\hat {w_{ij}}}} – diluted weight / wij{\displaystyle w_{ij}} – real weight before dilution / P(c){\displaystyle P(c)} – the probability of c{\displaystyle c}: the probability of keeping a weightThe interpretation of probability P(c){\displaystyle P(c)} can also be changed from keeping a weight into pruning a weight. / In vector notation this can be written as /  / where the function g⁡(⋅){\displaystyle \operatorname {g} (\cdot )} imposes the previous dilution. / In weak dilution only a small and fixed fraction of the weights are diluted. When the number of terms in the sum goes to infinite (the weights for each node) it is still infinite (the fraction is fixed): thus mean field theory can be applied. In the notation from Hertz et al. this would be written as /  / ⟨hi⟩{\displaystyle \left\langle h_{i}\right\rangle } the mean field temperature / c{\displaystyle c} – a scaling factor for the temperature from the probability of keeping the weight / wij{\displaystyle w_{ij}} – real weight before dilution: also called the Hebb connection strength / ⟨Sj⟩{\displaystyle \left\langle S_{j}\right\rangle } – the mean stable equilibrium statesThere are some assumptions for this to hold: which are not listed here. /  /  / == Strong dilution == / When the dilution is strong: the finite fraction of removed connections (the weights) is large: giving rise to a huge uncertainty."
0.1368038000073284,How does the dropout technique differ from dilution in terms of implementation?,"The dropout technique can be considered a special case of dilution where the equation is adjusted to remove a whole row in the vector matrix, rather than just random weights. This process doesn’t rely on whether the weights are set to zero, the node is removed, or any other means. The end result remains the same.","Score: 0.8598978314139705 / Dilution and dropout (also called DropConnect) are regularization techniques for reducing overfitting in artificial neural networks by preventing complex co-adaptations on training data. They are an efficient way of performing model averaging with neural networks. Dilution refers to thinning weights: while dropout refers to randomly ""dropping out"": or omitting: units (both hidden and visible) during the training process of a neural network. Both trigger the same type of regularization. /  /  / == Types and uses == / Dilution is usually split in weak dilution and strong dilution. Weak dilution describes the process in which the finite fraction of removed connections is small: and strong dilution refers to when this fraction is large. There is no clear distinction on where the limit between strong and weak dilution is: and often the distinction is dependent on the precedent of a specific use-case and has implications for how to solve for exact solutions. / Sometimes dilution is used for adding damping noise to the inputs. In that case: weak dilution refers to adding a small amount of damping noise: while strong dilution refers to adding a greater amount of damping noise. Both can be rewritten as variants of weight dilution. / These techniques are also sometimes referred to as random pruning of weights: but this is usually a non-recurring one-way operation. The network is pruned: and then kept if it is an improvement over the previous model. Dilution and dropout both refer to an iterative process. The pruning of weights typically does not imply that the network continues learning: while in dilution/dropout: the network continues to learn after the technique is applied. /  /  / == Generalized linear network == / Output from a layer of linear nodes: in an artificial neural net can be described as /  / yi{\displaystyle y_{i}} – output from node i{\displaystyle i} / wij{\displaystyle w_{ij}} – real weight before dilution: also called the Hebb connection strength / xj{\displaystyle x_{j}} – input from node j{\displaystyle j}This can be written in vector notation as /  / y{\displaystyle \mathbf {y} } – output vector / W{\displaystyle \mathbf {W} } – weight matrix / x{\displaystyle \mathbf {x} } – input vectorEquations (1) and (2) are used in the subsequent sections. /  /  / == Weak dilution == / During weak dilution: the finite fraction of removed connections (the weights) is small: giving rise to a tiny uncertainty. This edge-case can be solved exactly with mean field theory. In weak dilution the impact on the weights can be described as /  / wij^{\displaystyle {\hat {w_{ij}}}} – diluted weight / wij{\displaystyle w_{ij}} – real weight before dilution / P(c){\displaystyle P(c)} – the probability of c{\displaystyle c}: the probability of keeping a weightThe interpretation of probability P(c){\displaystyle P(c)} can also be changed from keeping a weight into pruning a weight. / In vector notation this can be written as /  / where the function g⁡(⋅){\displaystyle \operatorname {g} (\cdot )} imposes the previous dilution. / In weak dilution only a small and fixed fraction of the weights are diluted. When the number of terms in the sum goes to infinite (the weights for each node) it is still infinite (the fraction is fixed): thus mean field theory can be applied. In the notation from Hertz et al. this would be written as /  / ⟨hi⟩{\displaystyle \left\langle h_{i}\right\rangle } the mean field temperature / c{\displaystyle c} – a scaling factor for the temperature from the probability of keeping the weight / wij{\displaystyle w_{ij}} – real weight before dilution: also called the Hebb connection strength / ⟨Sj⟩{\displaystyle \left\langle S_{j}\right\rangle } – the mean stable equilibrium statesThere are some assumptions for this to hold: which are not listed here. /  /  / == Strong dilution == / When the dilution is strong: the finite fraction of removed connections (the weights) is large: giving rise to a huge uncertainty.","Score: 0.8255277975878965 / When the number of terms in the sum goes to infinite (the weights for each node) it is still infinite (the fraction is fixed): thus mean field theory can be applied. In the notation from Hertz et al. this would be written as /  / ⟨hi⟩{\displaystyle \left\langle h_{i}\right\rangle } the mean field temperature / c{\displaystyle c} – a scaling factor for the temperature from the probability of keeping the weight / wij{\displaystyle w_{ij}} – real weight before dilution: also called the Hebb connection strength / ⟨Sj⟩{\displaystyle \left\langle S_{j}\right\rangle } – the mean stable equilibrium statesThere are some assumptions for this to hold: which are not listed here. /  /  / == Strong dilution == / When the dilution is strong: the finite fraction of removed connections (the weights) is large: giving rise to a huge uncertainty. /  /  / == Dropout == / Dropout is a special case of the previous weight equation (3): where the aforementioned equation is adjusted to remove a whole row in the vector matrix: and not only random weights /  / P(c){\displaystyle P(c)} – the probability c{\displaystyle c} to keep a row in the weight matrix / wj{\displaystyle \mathbf {w} _{j}} – real row in the weight matrix before dropout / wj^{\displaystyle {\hat {\mathbf {w} _{j}}}} – diluted row in the weight matrixBecause dropout removes a whole row from the vector matrix: the previous (unlisted) assumptions for weak dilution and the use of mean field theory are not applicable. / The process by which the node is driven to zero: whether by setting the weights to zero: by “removing the node”: or by some other means: does not impact the end result and does not create a new and unique case. If the neural net is processed by a high-performance digital array-multiplicator: then it is likely more effective to drive the value to zero late in the process graph. If the net is processed by a constrained processor: perhaps even an analog neuromorph processor: then it is likely a more power-efficient solution is to drive the value to zero early in the process graph. /  /  / == Google's patent == / Although there have been examples of randomly removing connections between neurons in a neural network to improve models: this technique was first introduced with the name dropout by Geoffrey Hinton: et al. in 2012. Google currently holds the patent for the dropout technique. /  /  / == See also == / AlexNet / Convolutional neural network § Dropout /  /  / == Notes == /  /  / == References =="
0.22462880000239238,Who introduced the dropout technique and who currently holds the patent?,"The dropout technique was first introduced by Geoffrey Hinton and others in 2012 for neural networks. Currently, Google holds the patent for this technique.","Score: 0.7857707925644656 / In 1973: Dreyfus used backpropagation to adapt parameters of controllers in proportion to error gradients.  / In 1982: Paul Werbos applied backpropagation to MLPs in the way that has become standard. In 1986 Rumelhart: Hinton and Williams showed that backpropagation learned interesting internal representations of words as feature vectors when trained to predict the next word in a sequence.In the late 1970s to early 1980s: interest briefly emerged in theoretically investigating the Ising model created by Wilhelm Lenz (1920) and Ernst Ising (1925) / in relation to Cayley tree topologies and large neural networks. / The Ising model is essentially a non-learning artificial recurrent neural network (RNN) consisting of neuron-like threshold elements. / In 1972: Shun'ichi Amari described an adaptive version of this architecture: / In 1981: the Ising model was solved exactly by Peter Barth for the general case of closed Cayley trees (with loops) with an arbitrary branching ratio / and found to exhibit unusual phase transition behavior in its local-apex and long-range site-site correlations.John Hopfield popularised this architecture in 1982: / and it is now known as a Hopfield network. / The time delay neural network (TDNN) of Alex Waibel (1987) combined convolutions and weight sharing and backpropagation.  In 1988: Wei Zhang et al. applied backpropagation to a CNN (a simplified Neocognitron with convolutional interconnections between the image feature layers and the last fully connected layer) for alphabet recognition. In 1989: Yann LeCun et al. trained a CNN to recognize handwritten ZIP codes on mail.  / In 1992: max-pooling for CNNs was introduced by Juan Weng et al. to help with least-shift invariance and tolerance to deformation to aid 3D object recognition.  / LeNet-5 (1998): a 7-level CNN by Yann LeCun et al.: that classifies digits: was applied by several banks to recognize hand-written numbers on checks digitized in 32x32 pixel images. / From 1988 onward: the use of neural networks transformed the field of protein structure prediction: in particular when the first cascading networks were trained on profiles (matrices) produced by multiple sequence alignments.In 1991: Sepp Hochreiter's diploma thesis  identified and analyzed the vanishing gradient problem and proposed recurrent residual connections to solve it. His thesis was called ""one of the most important documents in the history of machine learning"" by his supervisor Juergen Schmidhuber.In 1991: Juergen Schmidhuber  published adversarial neural networks that contest with each other in the form of a zero-sum game: where one network's gain is the other network's loss. The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called ""artificial curiosity."" / In 1992: Juergen Schmidhuber proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning. It uses predictive coding  to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN: by distilling a higher level chunker network into a lower level automatizer network.  In the same year he also published an alternative to RNNs which is a precursor of a linear Transformer. It introduces the concept internal spotlights of attention: a slow feedforward neural network learns by gradient descent to control the fast weights of another neural network through outer products of self-generated activation patterns. / The development of metal–oxide–semiconductor (MOS) very-large-scale integration (VLSI): in the form of complementary MOS (CMOS) technology: enabled increasing MOS transistor counts in digital electronics. This provided more processing power for the development of practical artificial neural networks in the 1980s.Neural networks' early successes included predicting the stock market and in 1995 a (mostly) self-driving car.1997:  Sepp Hochreite and Juergen Schmidhuber introduced the deep learning method called long short-term memory (LSTM): published in Neural Computation. LSTM recurrent neural networks can learn ""very deep learning"" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before.","Score: 0.7796993372221649 / Dilution and dropout (also called DropConnect) are regularization techniques for reducing overfitting in artificial neural networks by preventing complex co-adaptations on training data. They are an efficient way of performing model averaging with neural networks. Dilution refers to thinning weights: while dropout refers to randomly ""dropping out"": or omitting: units (both hidden and visible) during the training process of a neural network. Both trigger the same type of regularization. /  /  / == Types and uses == / Dilution is usually split in weak dilution and strong dilution. Weak dilution describes the process in which the finite fraction of removed connections is small: and strong dilution refers to when this fraction is large. There is no clear distinction on where the limit between strong and weak dilution is: and often the distinction is dependent on the precedent of a specific use-case and has implications for how to solve for exact solutions. / Sometimes dilution is used for adding damping noise to the inputs. In that case: weak dilution refers to adding a small amount of damping noise: while strong dilution refers to adding a greater amount of damping noise. Both can be rewritten as variants of weight dilution. / These techniques are also sometimes referred to as random pruning of weights: but this is usually a non-recurring one-way operation. The network is pruned: and then kept if it is an improvement over the previous model. Dilution and dropout both refer to an iterative process. The pruning of weights typically does not imply that the network continues learning: while in dilution/dropout: the network continues to learn after the technique is applied. /  /  / == Generalized linear network == / Output from a layer of linear nodes: in an artificial neural net can be described as /  / yi{\displaystyle y_{i}} – output from node i{\displaystyle i} / wij{\displaystyle w_{ij}} – real weight before dilution: also called the Hebb connection strength / xj{\displaystyle x_{j}} – input from node j{\displaystyle j}This can be written in vector notation as /  / y{\displaystyle \mathbf {y} } – output vector / W{\displaystyle \mathbf {W} } – weight matrix / x{\displaystyle \mathbf {x} } – input vectorEquations (1) and (2) are used in the subsequent sections. /  /  / == Weak dilution == / During weak dilution: the finite fraction of removed connections (the weights) is small: giving rise to a tiny uncertainty. This edge-case can be solved exactly with mean field theory. In weak dilution the impact on the weights can be described as /  / wij^{\displaystyle {\hat {w_{ij}}}} – diluted weight / wij{\displaystyle w_{ij}} – real weight before dilution / P(c){\displaystyle P(c)} – the probability of c{\displaystyle c}: the probability of keeping a weightThe interpretation of probability P(c){\displaystyle P(c)} can also be changed from keeping a weight into pruning a weight. / In vector notation this can be written as /  / where the function g⁡(⋅){\displaystyle \operatorname {g} (\cdot )} imposes the previous dilution. / In weak dilution only a small and fixed fraction of the weights are diluted. When the number of terms in the sum goes to infinite (the weights for each node) it is still infinite (the fraction is fixed): thus mean field theory can be applied. In the notation from Hertz et al. this would be written as /  / ⟨hi⟩{\displaystyle \left\langle h_{i}\right\rangle } the mean field temperature / c{\displaystyle c} – a scaling factor for the temperature from the probability of keeping the weight / wij{\displaystyle w_{ij}} – real weight before dilution: also called the Hebb connection strength / ⟨Sj⟩{\displaystyle \left\langle S_{j}\right\rangle } – the mean stable equilibrium statesThere are some assumptions for this to hold: which are not listed here. /  /  / == Strong dilution == / When the dilution is strong: the finite fraction of removed connections (the weights) is large: giving rise to a huge uncertainty."
0.158263999997871,What is a Feedforward Neural Network (FNN)?,"A feedforward neural network (FNN) is one of two broad types of artificial neural networks, characterized by the direction of the flow of information between its layers. Its flow is uni-directional, meaning that the information in the model flows in only one direction—forward—from the input nodes, through the hidden nodes (if any) and to the output nodes, without any cycles or loops. This is in contrast to recurrent neural networks, which have a bi-directional flow.","Score: 0.9096016222880674 / A feedforward neural network (FNN) is one of the two broad types of artificial neural network: characterized by direction of the flow of information between its layers. Its flow is uni-directional: meaning that the information in the model flows in only one direction—forward—from the input nodes: through the hidden nodes (if any) and to the output nodes: without any cycles or loops: in contrast to recurrent neural networks: which have a bi-directional flow. Modern feedforward networks are trained using the backpropagation method and are colloquially referred to as the ""vanilla"" neural networks. /  /  / == Timeline == / In 1958: a layered network of perceptrons: consisting of an input layer: a hidden layer with randomized weights that did not learn: and an output layer with learning connections: was introduced already by Frank Rosenblatt in his book Perceptron. This extreme learning machine was not yet a deep learning network.In 1965: the first  deep-learning feedforward network: not yet using stochastic gradient descent: was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa: at the time called the Group Method of Data Handling.In 1967: a deep-learning network: using stochastic gradient descent for the first time: was able to classify non-linearily separable pattern classes: as reported Shun'ichi Amari. Amari's student Saito conducted the computer experiments: using a five-layered feedforward network with two learning layers.In 1970: modern backpropagation method: an efficient application of a chain-rule-based supervised learning: was for the first time published by the Finnish researcher Seppo Linnainmaa. The term (i.e. ""back-propagating errors"") itself has been used by Rosenblatt himself: but he did not know how to implement it: although a continuous precursor of backpropagation was already used in the context of control theory in 1960 by Henry J. Kelley. It is known also as a reverse mode of automatic differentiation.In 1982: backpropagation was applied in the way that has become standard: for the first time by Paul Werbos.In 1985: an experimental analysis of the technique was conducted by David E. Rumelhart et al.. Many improvements to the approach have been made in subsequent decades.In 1987: using a stochastic gradient descent within a (wide 12-layer nonlinear) feed-forward network: Matthew Brand has trained it to reproduce logic functions of nontrivial circuit depth: using small batches of random input/output samples. He: however: concluded that on hardware (sub-megaflop computers) available at the time it was impractical: and proposed using fixed random early layers as an input hash for a single modifiable layer.In 1990s: an (much simpler) alternative to using neural networks: although still related support vector machine approach was developed by Vladimir Vapnik and his colleagues. In addition to performing linear classification: they were able to efficiently perform a non-linear classification using what is called the kernel trick: using high-dimensional feature spaces.In 2003: interest in backpropagation networks returned due to the successes of deep learning being applied to language modelling by Yoshua Bengio with co-authors.In 2017: modern transformer architectures were introduced. /  /  / == Mathematical foundations == /  /  / === Activation function === / The two historically common activation functions are both sigmoids: and are described by /  / y(vi)=tanh⁡(vi)  and  y(vi)=(1+e−vi)−1{\displaystyle y(v_{i})=\tanh(v_{i})~~{\textrm {and}}~~y(v_{i})=(1+e^{-v_{i}})^{-1}}.The first is a hyperbolic tangent that ranges from -1 to 1: while the other is the logistic function: which is similar in shape but ranges from 0 to 1. Here yi{\displaystyle y_{i}} is the output of the i{\displaystyle i}th node (neuron) and vi{\displaystyle v_{i}} is the weighted sum of the input connections. Alternative activation functions have been proposed: including the rectifier and softplus functions. More specialized activation functions include radial basis functions (used in radial basis networks: another class of supervised neural network models). / In recent developments of deep learning the rectified linear unit (ReLU) is more frequently used as one of the possible ways to overcome the numerical problems related to the sigmoids. /  /  / === Learning === / Learning occurs by changing connection weights after each piece of data is processed: based on the amount of error in the output compared to the expected result.","Score: 0.8482582605637538 / A recurrent neural network (RNN) is one of the two broad types of artificial neural network: characterized by direction of the flow of information between its layers. In contrast to the uni-directional feedforward neural network: it is a bi-directional artificial neural network: meaning that it allows the output from some nodes to affect subsequent input to the same nodes. Their ability to use internal state (memory) to process arbitrary sequences of inputs makes them applicable to tasks such as unsegmented: connected handwriting recognition or speech recognition. The term ""recurrent neural network"" is used to refer to the class of networks with an infinite impulse response: whereas ""convolutional neural network"" refers to the class of finite impulse response. Both classes of networks exhibit temporal dynamic behavior. A finite impulse recurrent network is a directed acyclic graph that can be unrolled and replaced with a strictly feedforward neural network: while an infinite impulse recurrent network is a directed cyclic graph that can not be unrolled. / Additional stored states and the storage under direct control by the network can be added to both infinite-impulse and finite-impulse networks. Another network or graph can also replace the storage if that incorporates time delays or has feedback loops. Such controlled states are referred to as gated states or gated memory and are part of long short-term memory networks (LSTMs) and gated recurrent units. This is also called Feedforward Neural Network (FNN). Recurrent neural networks are theoretically Turing complete and can run arbitrary programs to process arbitrary sequences of inputs. /  /  / == History == / The Ising model (1925) by Wilhelm Lenz and Ernst Ising / was the first RNN architecture that did not learn. Shun'ichi Amari made it adaptive in 1972. This was also called the Hopfield network (1982). See also David Rumelhart's work in 1986.  In 1993: a neural history compressor system solved a ""Very Deep Learning"" task that required more than 1000 subsequent layers in an RNN unfolded in time. /  /  / === LSTM === / Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1997 and set accuracy records in multiple applications domains.Around 2007: LSTM started to revolutionize speech recognition: outperforming traditional models in certain speech applications. In 2009: a Connectionist Temporal Classification (CTC)-trained LSTM network was the first RNN to win pattern recognition contests when it won several competitions in connected handwriting recognition. In 2014: the Chinese company Baidu used CTC-trained RNNs to break the 2S09 Switchboard Hub5'00 speech recognition dataset benchmark without using any traditional speech processing methods.LSTM also improved large-vocabulary speech recognition and text-to-speech synthesis and was used in Google Android. In 2015: Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM.LSTM broke records for improved machine translation: Language Modeling and Multilingual Language Processing. LSTM combined with convolutional neural networks (CNNs) improved automatic image captioning. /  /  / == Architectures == /  / RNNs come in many variants. /  /  / === Fully recurrent === / Fully recurrent neural networks (FRNN) connect the outputs of all neurons to the inputs of all neurons.  This is the most general neural network topology because all other topologies can be represented by setting some connection weights to zero to simulate the lack of connections between those neurons. The illustration to the right may be misleading to many because practical neural network topologies are frequently organized in ""layers"" and the drawing gives that appearance. However: what appears to be layers are: in fact: different steps in time of the same fully recurrent neural network. The left-most item in the illustration shows the recurrent connections as the arc labeled 'v'.  It is ""unfolded"" in time to produce the appearance of layers. /  /  / === Elman networks and Jordan networks === / An Elman network is a three-layer network (arranged horizontally as x: y: and z in the illustration) with the addition of a set of context units (u in the illustration). The middle (hidden) layer is connected to these context units fixed with a weight of one. At each time step: the input is fed forward and a learning rule is applied. The fixed back-connections save a copy of the previous values of the hidden units in the context units (since they propagate over the connections before the learning rule is applied). Thus the network can maintain a sort of state: allowing it to perform such tasks as sequence-prediction that are beyond the power of a standard multilayer perceptron."
0.16315500001655892,What is the main method of training modern feedforward networks?,Modern feedforward networks are trained using the backpropagation method.,"Score: 0.8542054310623739 / A feedforward neural network (FNN) is one of the two broad types of artificial neural network: characterized by direction of the flow of information between its layers. Its flow is uni-directional: meaning that the information in the model flows in only one direction—forward—from the input nodes: through the hidden nodes (if any) and to the output nodes: without any cycles or loops: in contrast to recurrent neural networks: which have a bi-directional flow. Modern feedforward networks are trained using the backpropagation method and are colloquially referred to as the ""vanilla"" neural networks. /  /  / == Timeline == / In 1958: a layered network of perceptrons: consisting of an input layer: a hidden layer with randomized weights that did not learn: and an output layer with learning connections: was introduced already by Frank Rosenblatt in his book Perceptron. This extreme learning machine was not yet a deep learning network.In 1965: the first  deep-learning feedforward network: not yet using stochastic gradient descent: was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa: at the time called the Group Method of Data Handling.In 1967: a deep-learning network: using stochastic gradient descent for the first time: was able to classify non-linearily separable pattern classes: as reported Shun'ichi Amari. Amari's student Saito conducted the computer experiments: using a five-layered feedforward network with two learning layers.In 1970: modern backpropagation method: an efficient application of a chain-rule-based supervised learning: was for the first time published by the Finnish researcher Seppo Linnainmaa. The term (i.e. ""back-propagating errors"") itself has been used by Rosenblatt himself: but he did not know how to implement it: although a continuous precursor of backpropagation was already used in the context of control theory in 1960 by Henry J. Kelley. It is known also as a reverse mode of automatic differentiation.In 1982: backpropagation was applied in the way that has become standard: for the first time by Paul Werbos.In 1985: an experimental analysis of the technique was conducted by David E. Rumelhart et al.. Many improvements to the approach have been made in subsequent decades.In 1987: using a stochastic gradient descent within a (wide 12-layer nonlinear) feed-forward network: Matthew Brand has trained it to reproduce logic functions of nontrivial circuit depth: using small batches of random input/output samples. He: however: concluded that on hardware (sub-megaflop computers) available at the time it was impractical: and proposed using fixed random early layers as an input hash for a single modifiable layer.In 1990s: an (much simpler) alternative to using neural networks: although still related support vector machine approach was developed by Vladimir Vapnik and his colleagues. In addition to performing linear classification: they were able to efficiently perform a non-linear classification using what is called the kernel trick: using high-dimensional feature spaces.In 2003: interest in backpropagation networks returned due to the successes of deep learning being applied to language modelling by Yoshua Bengio with co-authors.In 2017: modern transformer architectures were introduced. /  /  / == Mathematical foundations == /  /  / === Activation function === / The two historically common activation functions are both sigmoids: and are described by /  / y(vi)=tanh⁡(vi)  and  y(vi)=(1+e−vi)−1{\displaystyle y(v_{i})=\tanh(v_{i})~~{\textrm {and}}~~y(v_{i})=(1+e^{-v_{i}})^{-1}}.The first is a hyperbolic tangent that ranges from -1 to 1: while the other is the logistic function: which is similar in shape but ranges from 0 to 1. Here yi{\displaystyle y_{i}} is the output of the i{\displaystyle i}th node (neuron) and vi{\displaystyle v_{i}} is the weighted sum of the input connections. Alternative activation functions have been proposed: including the rectifier and softplus functions. More specialized activation functions include radial basis functions (used in radial basis networks: another class of supervised neural network models). / In recent developments of deep learning the rectified linear unit (ReLU) is more frequently used as one of the possible ways to overcome the numerical problems related to the sigmoids. /  /  / === Learning === / Learning occurs by changing connection weights after each piece of data is processed: based on the amount of error in the output compared to the expected result.","Score: 0.8344480317433856 / It introduces the concept internal spotlights of attention: a slow feedforward neural network learns by gradient descent to control the fast weights of another neural network through outer products of self-generated activation patterns. / The development of metal–oxide–semiconductor (MOS) very-large-scale integration (VLSI): in the form of complementary MOS (CMOS) technology: enabled increasing MOS transistor counts in digital electronics. This provided more processing power for the development of practical artificial neural networks in the 1980s.Neural networks' early successes included predicting the stock market and in 1995 a (mostly) self-driving car.1997:  Sepp Hochreite and Juergen Schmidhuber introduced the deep learning method called long short-term memory (LSTM): published in Neural Computation. LSTM recurrent neural networks can learn ""very deep learning"" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. The ""vanilla LSTM"" with forget gate was introduced in 1999 by Felix Gers: Schmidhuber and Fred Cummins.Geoffrey Hinton et al. (2006) proposed learning a high-level representation using successive layers of binary or real-valued latent variables with a restricted Boltzmann machine to model each layer. In 2012: Ng and Dean created a network that learned to recognize higher-level concepts: such as cats: only from watching unlabeled images. Unsupervised pre-training and increased computing power from GPUs and distributed computing allowed the use of larger networks: particularly in image and visual recognition problems: which became known as ""deep learning"".Variants of the back-propagation algorithm: as well as unsupervised methods by Geoff Hinton and colleagues at the University of Toronto: can be used to train deep: highly nonlinear neural architectures: similar to the 1980 Neocognitron by Kunihiko Fukushima: and the ""standard architecture of vision"": inspired by the simple and complex cells identified by David H. Hubel and Torsten Wiesel in the primary visual cortex. / Computational devices have been created in CMOS for both biophysical simulation and neuromorphic computing. More recent efforts show promise for creating nanodevices for very large scale principal components analyses and convolution. If successful: these efforts could usher in a new era of neural computing that is a step beyond digital computing: because it depends on learning rather than programming and because it is fundamentally analog rather than digital even though the first instantiations may in fact be with CMOS digital devices. / Ciresan and colleagues (2010) showed that despite the vanishing gradient problem: GPUs make backpropagation feasible for many-layered feedforward neural networks. Between 2009 and 2012: ANNs began winning prizes in image recognition contests: approaching human level performance on various tasks: initially in pattern recognition and handwriting recognition. For example: the bi-directional and multi-dimensional long short-term memory (LSTM) of Graves et al. won three competitions in connected handwriting recognition in 2009 without any prior knowledge about the three languages to be learned.Ciresan and colleagues built the first pattern recognizers to achieve human-competitive/superhuman performance on benchmarks such as traffic sign recognition (IJCNN 2012). / Radial basis function and wavelet networks were introduced in 2013. These can be shown to offer best approximation properties and have been applied in nonlinear system identification and classification applications.In 2014: the adversarial network principle was used in a generative adversarial network (GAN) by Ian Goodfellow et al. Here the adversarial network (discriminator) outputs a value between 1 and 0 depending on the likelihood of the first network's (generator) output is in a given set. This can be used to create realistic deepfakes. / Excellent image quality is achieved by Nvidia's StyleGAN (2018) based on the Progressive GAN by Tero Karras: Timo Aila: Samuli  Laine: and Jaakko Lehtinen. Here the GAN generator is grown from small to large scale in a pyramidal fashion. / In 2015: Rupesh Kumar Srivastava: Klaus Greff: and Schmidhuber used the LSTM principle to create the Highway network: a feedforward neural network with hundreds of layers: much deeper than previous networks. 7 months later: Kaiming He: Xiangyu Zhang;  Shaoqing Ren: and Jian Sun won the ImageNet 2015 competition with an open-gated or gateless Highway network variant called Residual neural network.In 2017: Ashish Vaswani et al."
0.17564990001847036,Who first utilized a deep-learning feedforward neural network?,The first deep-learning feedforward network was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa in 1965.,"Score: 0.8777238753104978 / In 2015: Rupesh Kumar Srivastava: Klaus Greff: and Schmidhuber used LSTM principles to create the Highway network: a feedforward neural network with hundreds of layers: much deeper than previous networks. 7 months later: Kaiming He: Xiangyu Zhang;  Shaoqing Ren: and Jian Sun won the ImageNet 2015 competition with an open-gated or gateless Highway network variant called Residual neural network. This has become the most cited neural network of the 21st century.In 1994: André de Carvalho: together with Mike Fairhurst and David Bisset: published experimental results of a multi-layer boolean neural network: also known as a weightless neural network: composed of a 3-layers self-organising feature extraction neural network module (SOFT) followed by a multi-layer classification neural network module (GSN): which were independently trained. Each layer in the feature extraction module extracted features with growing complexity regarding the previous layer.In 1995: Brendan Frey demonstrated that it was possible to train (over two days) a network containing six fully connected layers and several hundred hidden units using the wake-sleep algorithm: co-developed with Peter Dayan and Hinton.Since 1997: Sven Behnke extended the feed-forward hierarchical convolutional approach in the Neural Abstraction Pyramid by lateral and backward connections in order to flexibly incorporate context into decisions and iteratively resolve local ambiguities. / Simpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) were a popular choice in the 1990s and 2000s: because of artificial neural networks' computational cost and a lack of understanding of how the brain wires its biological networks. / Both shallow and deep learning (e.g.: recurrent nets) of ANNs for speech recognition have been explored for many years. These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively. Key difficulties have been analyzed: including gradient diminishing and weak temporal correlation structure in neural predictive models. Additional difficulties were the lack of training data and limited computing power. Most speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government's NSA and DARPA: SRI studied deep neural networks (DNNs) in speech and speaker recognition. The speaker recognition team led by Larry Heck reported significant success with deep neural networks in speech processing in the 1998 National Institute of Standards and Technology Speaker Recognition evaluation. The SRI deep neural network was then deployed in the Nuance Verifier: representing the first major industrial application of deep learning. The principle of elevating ""raw"" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the ""raw"" spectrogram or linear filter-bank features in the late 1990s: showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech: waveforms: later produced excellent larger-scale results.Speech recognition was taken over by LSTM. In 2003: LSTM started to become competitive with traditional speech recognizers on certain tasks. In 2006: Alex Graves: Santiago Fernández: Faustino Gomez: and Schmidhuber combined it with connectionist temporal classification (CTC) in stacks of LSTM RNNs. In 2015: Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM: which they made available through Google Voice Search.The impact of deep learning in industry began in the early 2000s: when CNNs already processed an estimated 10% to 20% of all the checks written in the US: according to Yann LeCun. Industrial applications of deep learning to large-scale speech recognition started around 2010. / In 2006: publications by Geoff Hinton: Ruslan Salakhutdinov: Osindero and Teh showed how a many-layered feedforward neural network could be effectively pre-trained one layer at a time: treating each layer in turn as an unsupervised restricted Boltzmann machine: then fine-tuning it using supervised backpropagation. The papers referred to learning for deep belief nets. / The 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech: and the possibility that given more capable hardware and large-scale data sets that deep neural nets might become practical.","Score: 0.8733745665019036 / Industrial applications of deep learning to large-scale speech recognition started around 2010. / In 2006: publications by Geoff Hinton: Ruslan Salakhutdinov: Osindero and Teh showed how a many-layered feedforward neural network could be effectively pre-trained one layer at a time: treating each layer in turn as an unsupervised restricted Boltzmann machine: then fine-tuning it using supervised backpropagation. The papers referred to learning for deep belief nets. / The 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech: and the possibility that given more capable hardware and large-scale data sets that deep neural nets might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets. However: it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large: context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems. The nature of the recognition errors produced by the two types of systems was characteristically different: offering technical insights into how to integrate deep learning into the existing highly efficient: run-time speech decoding system deployed by all major speech recognition systems. Analysis around 2009–2010: contrasting the GMM (and other generative speech models) vs. DNN models: stimulated early industrial investment in deep learning for speech recognition.  That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models. / In 2010: researchers extended deep learning from TIMIT to large vocabulary speech recognition: by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees.Deep learning is part of state-of-the-art systems in various disciplines: particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification): as well as a range of large-vocabulary speech recognition tasks have steadily improved. Convolutional neural networks were superseded for ASR by CTC for LSTM. but are more successful in computer vision. / Advances in hardware have driven renewed interest in deep learning. In 2009: Nvidia was involved in what was called the ""big bang"" of deep learning: ""as deep-learning neural networks were trained with Nvidia graphics processing units (GPUs)"". That year: Andrew Ng determined that GPUs could increase the speed of deep-learning systems by about 100 times. In particular: GPUs are well-suited for the matrix/vector computations involved in machine learning. GPUs speed up training algorithms by orders of magnitude: reducing running times from weeks to days. Further: specialized hardware and algorithm optimizations can be used for efficient processing of deep learning models. /  /  / === Deep learning revolution === / In the late 2000s: deep learning started to outperform other methods in machine learning competitions. / In 2009: a long short-term memory trained by connectionist temporal classification (Alex Graves: Santiago Fernández: Faustino Gomez: and Jürgen Schmidhuber: 2006) was the first RNN to win pattern recognition contests: winning three competitions in connected handwriting recognition. Google later used CTC-trained LSTM for speech recognition on the smartphone.Significant impacts in image or object recognition were felt from 2011 to 2012. Although CNNs trained by backpropagation had been around for decades: and GPU implementations of NNs for years: including CNNs: faster implementations of CNNs on GPUs were needed to progress on computer vision. In 2011: the DanNet by Dan Ciresan: Ueli Meier: Jonathan Masci: Luca Maria Gambardella: and Jürgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest: outperforming traditional methods by a factor of 3. Also in 2011: DanNet won the ICDAR Chinese handwriting contest: and in May 2012: it won the ISBI image segmentation contest. Until 2011: CNNs did not play a major role at computer vision conferences: but in June 2012: a paper by Ciresan et al. at the leading conference CVPR showed how max-pooling CNNs on GPU can dramatically improve many vision benchmark records."
0.1292198000010103,What are the functions of the feedforward neural network?,"In a feedforward neural network, information flows in only one direction—forward—from the input nodes, through the hidden nodes (if any) and to the output nodes, without any cycles or loops.","Score: 0.855972706752919 / A feedforward neural network (FNN) is one of the two broad types of artificial neural network: characterized by direction of the flow of information between its layers. Its flow is uni-directional: meaning that the information in the model flows in only one direction—forward—from the input nodes: through the hidden nodes (if any) and to the output nodes: without any cycles or loops: in contrast to recurrent neural networks: which have a bi-directional flow. Modern feedforward networks are trained using the backpropagation method and are colloquially referred to as the ""vanilla"" neural networks. /  /  / == Timeline == / In 1958: a layered network of perceptrons: consisting of an input layer: a hidden layer with randomized weights that did not learn: and an output layer with learning connections: was introduced already by Frank Rosenblatt in his book Perceptron. This extreme learning machine was not yet a deep learning network.In 1965: the first  deep-learning feedforward network: not yet using stochastic gradient descent: was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa: at the time called the Group Method of Data Handling.In 1967: a deep-learning network: using stochastic gradient descent for the first time: was able to classify non-linearily separable pattern classes: as reported Shun'ichi Amari. Amari's student Saito conducted the computer experiments: using a five-layered feedforward network with two learning layers.In 1970: modern backpropagation method: an efficient application of a chain-rule-based supervised learning: was for the first time published by the Finnish researcher Seppo Linnainmaa. The term (i.e. ""back-propagating errors"") itself has been used by Rosenblatt himself: but he did not know how to implement it: although a continuous precursor of backpropagation was already used in the context of control theory in 1960 by Henry J. Kelley. It is known also as a reverse mode of automatic differentiation.In 1982: backpropagation was applied in the way that has become standard: for the first time by Paul Werbos.In 1985: an experimental analysis of the technique was conducted by David E. Rumelhart et al.. Many improvements to the approach have been made in subsequent decades.In 1987: using a stochastic gradient descent within a (wide 12-layer nonlinear) feed-forward network: Matthew Brand has trained it to reproduce logic functions of nontrivial circuit depth: using small batches of random input/output samples. He: however: concluded that on hardware (sub-megaflop computers) available at the time it was impractical: and proposed using fixed random early layers as an input hash for a single modifiable layer.In 1990s: an (much simpler) alternative to using neural networks: although still related support vector machine approach was developed by Vladimir Vapnik and his colleagues. In addition to performing linear classification: they were able to efficiently perform a non-linear classification using what is called the kernel trick: using high-dimensional feature spaces.In 2003: interest in backpropagation networks returned due to the successes of deep learning being applied to language modelling by Yoshua Bengio with co-authors.In 2017: modern transformer architectures were introduced. /  /  / == Mathematical foundations == /  /  / === Activation function === / The two historically common activation functions are both sigmoids: and are described by /  / y(vi)=tanh⁡(vi)  and  y(vi)=(1+e−vi)−1{\displaystyle y(v_{i})=\tanh(v_{i})~~{\textrm {and}}~~y(v_{i})=(1+e^{-v_{i}})^{-1}}.The first is a hyperbolic tangent that ranges from -1 to 1: while the other is the logistic function: which is similar in shape but ranges from 0 to 1. Here yi{\displaystyle y_{i}} is the output of the i{\displaystyle i}th node (neuron) and vi{\displaystyle v_{i}} is the weighted sum of the input connections. Alternative activation functions have been proposed: including the rectifier and softplus functions. More specialized activation functions include radial basis functions (used in radial basis networks: another class of supervised neural network models). / In recent developments of deep learning the rectified linear unit (ReLU) is more frequently used as one of the possible ways to overcome the numerical problems related to the sigmoids. /  /  / === Learning === / Learning occurs by changing connection weights after each piece of data is processed: based on the amount of error in the output compared to the expected result.","Score: 0.8334624762123363 / The analysis is more difficult for the change in weights to a hidden node: but it can be shown that the relevant derivative is /  / −∂E(n)∂vj(n)=ϕ′(vj(n))∑k−∂E(n)∂vk(n)wkj(n){\displaystyle -{\frac {\partial {\mathcal {E}}(n)}{\partial v_{j}(n)}}=\phi ^{\prime }(v_{j}(n))\sum _{k}-{\frac {\partial {\mathcal {E}}(n)}{\partial v_{k}(n)}}w_{kj}(n)}.This depends on the change in weights of the k{\displaystyle k}th nodes: which represent the output layer. So to change the hidden layer weights: the output layer weights change according to the derivative of the activation function: and so this algorithm represents a backpropagation of the activation function. /  /  / == History == /  /  / === Linear neural network === / The simplest kind of feedforward neural network is a linear network: which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated in each node. The mean squared errors between these calculated outputs and a given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement. /  /  / === Perceptron === /  / If using a threshold: i.e. a linear activation function:  the resulting linear threshold unit is called a perceptron. (Often the term is used to denote just one of these units.) Multiple parallel linear units are able to approximate any continuous function from a compact interval of the real numbers into the interval [−1:1] despite the limited computational power of single unit with a linear threshold function. This result can be found in Peter Auer: Harald Burgsteiner and Wolfgang Maass ""A learning rule for very simple universal approximators consisting of a single layer of perceptrons"".Perceptrons can be trained by a simple learning algorithm that is usually called the delta rule. It calculates the errors between calculated output and sample output data: and uses this to create an adjustment to the weights: thus implementing a form of gradient descent. /  /  / === Multilayer perceptron === / A multilayer perceptron (MLP) is a misnomer for a modern feedforward artificial neural network: consisting of fully connected neurons with a nonlinear kind of activation function: organized in at least three layers: notable for being able to distinguish data that is not linearly separable. It is a misnomer because the original perceptron used a Heaviside step function: instead of a nonlinear kind of activation function (used by modern networks). /  /  / == Other feedforward networks == / Examples of other feedforward networks include convolutional neural networks and radial basis function networks: which use a different activation function. /  /  / == See also == / Hopfield network / Feed-forward / Backpropagation / Rprop /  /  / == References == /  /  / == External links == / Feedforward neural networks tutorial / Feedforward Neural Network: Example / Feedforward Neural Networks: An Introduction"
0.1665363000065554,How does learning occur in a feedforward neural network?,"Learning occurs in a feedforward neural network by changing connection weights after each piece of data is processed. This is based on the amount of error in the output compared to the expected result. This is an example of supervised learning, and is carried out through backpropagation.","Score: 0.8569395920549222 / A feedforward neural network (FNN) is one of the two broad types of artificial neural network: characterized by direction of the flow of information between its layers. Its flow is uni-directional: meaning that the information in the model flows in only one direction—forward—from the input nodes: through the hidden nodes (if any) and to the output nodes: without any cycles or loops: in contrast to recurrent neural networks: which have a bi-directional flow. Modern feedforward networks are trained using the backpropagation method and are colloquially referred to as the ""vanilla"" neural networks. /  /  / == Timeline == / In 1958: a layered network of perceptrons: consisting of an input layer: a hidden layer with randomized weights that did not learn: and an output layer with learning connections: was introduced already by Frank Rosenblatt in his book Perceptron. This extreme learning machine was not yet a deep learning network.In 1965: the first  deep-learning feedforward network: not yet using stochastic gradient descent: was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa: at the time called the Group Method of Data Handling.In 1967: a deep-learning network: using stochastic gradient descent for the first time: was able to classify non-linearily separable pattern classes: as reported Shun'ichi Amari. Amari's student Saito conducted the computer experiments: using a five-layered feedforward network with two learning layers.In 1970: modern backpropagation method: an efficient application of a chain-rule-based supervised learning: was for the first time published by the Finnish researcher Seppo Linnainmaa. The term (i.e. ""back-propagating errors"") itself has been used by Rosenblatt himself: but he did not know how to implement it: although a continuous precursor of backpropagation was already used in the context of control theory in 1960 by Henry J. Kelley. It is known also as a reverse mode of automatic differentiation.In 1982: backpropagation was applied in the way that has become standard: for the first time by Paul Werbos.In 1985: an experimental analysis of the technique was conducted by David E. Rumelhart et al.. Many improvements to the approach have been made in subsequent decades.In 1987: using a stochastic gradient descent within a (wide 12-layer nonlinear) feed-forward network: Matthew Brand has trained it to reproduce logic functions of nontrivial circuit depth: using small batches of random input/output samples. He: however: concluded that on hardware (sub-megaflop computers) available at the time it was impractical: and proposed using fixed random early layers as an input hash for a single modifiable layer.In 1990s: an (much simpler) alternative to using neural networks: although still related support vector machine approach was developed by Vladimir Vapnik and his colleagues. In addition to performing linear classification: they were able to efficiently perform a non-linear classification using what is called the kernel trick: using high-dimensional feature spaces.In 2003: interest in backpropagation networks returned due to the successes of deep learning being applied to language modelling by Yoshua Bengio with co-authors.In 2017: modern transformer architectures were introduced. /  /  / == Mathematical foundations == /  /  / === Activation function === / The two historically common activation functions are both sigmoids: and are described by /  / y(vi)=tanh⁡(vi)  and  y(vi)=(1+e−vi)−1{\displaystyle y(v_{i})=\tanh(v_{i})~~{\textrm {and}}~~y(v_{i})=(1+e^{-v_{i}})^{-1}}.The first is a hyperbolic tangent that ranges from -1 to 1: while the other is the logistic function: which is similar in shape but ranges from 0 to 1. Here yi{\displaystyle y_{i}} is the output of the i{\displaystyle i}th node (neuron) and vi{\displaystyle v_{i}} is the weighted sum of the input connections. Alternative activation functions have been proposed: including the rectifier and softplus functions. More specialized activation functions include radial basis functions (used in radial basis networks: another class of supervised neural network models). / In recent developments of deep learning the rectified linear unit (ReLU) is more frequently used as one of the possible ways to overcome the numerical problems related to the sigmoids. /  /  / === Learning === / Learning occurs by changing connection weights after each piece of data is processed: based on the amount of error in the output compared to the expected result.","Score: 0.8437444337644121 / It introduces the concept internal spotlights of attention: a slow feedforward neural network learns by gradient descent to control the fast weights of another neural network through outer products of self-generated activation patterns. / The development of metal–oxide–semiconductor (MOS) very-large-scale integration (VLSI): in the form of complementary MOS (CMOS) technology: enabled increasing MOS transistor counts in digital electronics. This provided more processing power for the development of practical artificial neural networks in the 1980s.Neural networks' early successes included predicting the stock market and in 1995 a (mostly) self-driving car.1997:  Sepp Hochreite and Juergen Schmidhuber introduced the deep learning method called long short-term memory (LSTM): published in Neural Computation. LSTM recurrent neural networks can learn ""very deep learning"" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. The ""vanilla LSTM"" with forget gate was introduced in 1999 by Felix Gers: Schmidhuber and Fred Cummins.Geoffrey Hinton et al. (2006) proposed learning a high-level representation using successive layers of binary or real-valued latent variables with a restricted Boltzmann machine to model each layer. In 2012: Ng and Dean created a network that learned to recognize higher-level concepts: such as cats: only from watching unlabeled images. Unsupervised pre-training and increased computing power from GPUs and distributed computing allowed the use of larger networks: particularly in image and visual recognition problems: which became known as ""deep learning"".Variants of the back-propagation algorithm: as well as unsupervised methods by Geoff Hinton and colleagues at the University of Toronto: can be used to train deep: highly nonlinear neural architectures: similar to the 1980 Neocognitron by Kunihiko Fukushima: and the ""standard architecture of vision"": inspired by the simple and complex cells identified by David H. Hubel and Torsten Wiesel in the primary visual cortex. / Computational devices have been created in CMOS for both biophysical simulation and neuromorphic computing. More recent efforts show promise for creating nanodevices for very large scale principal components analyses and convolution. If successful: these efforts could usher in a new era of neural computing that is a step beyond digital computing: because it depends on learning rather than programming and because it is fundamentally analog rather than digital even though the first instantiations may in fact be with CMOS digital devices. / Ciresan and colleagues (2010) showed that despite the vanishing gradient problem: GPUs make backpropagation feasible for many-layered feedforward neural networks. Between 2009 and 2012: ANNs began winning prizes in image recognition contests: approaching human level performance on various tasks: initially in pattern recognition and handwriting recognition. For example: the bi-directional and multi-dimensional long short-term memory (LSTM) of Graves et al. won three competitions in connected handwriting recognition in 2009 without any prior knowledge about the three languages to be learned.Ciresan and colleagues built the first pattern recognizers to achieve human-competitive/superhuman performance on benchmarks such as traffic sign recognition (IJCNN 2012). / Radial basis function and wavelet networks were introduced in 2013. These can be shown to offer best approximation properties and have been applied in nonlinear system identification and classification applications.In 2014: the adversarial network principle was used in a generative adversarial network (GAN) by Ian Goodfellow et al. Here the adversarial network (discriminator) outputs a value between 1 and 0 depending on the likelihood of the first network's (generator) output is in a given set. This can be used to create realistic deepfakes. / Excellent image quality is achieved by Nvidia's StyleGAN (2018) based on the Progressive GAN by Tero Karras: Timo Aila: Samuli  Laine: and Jaakko Lehtinen. Here the GAN generator is grown from small to large scale in a pyramidal fashion. / In 2015: Rupesh Kumar Srivastava: Klaus Greff: and Schmidhuber used the LSTM principle to create the Highway network: a feedforward neural network with hundreds of layers: much deeper than previous networks. 7 months later: Kaiming He: Xiangyu Zhang;  Shaoqing Ren: and Jian Sun won the ImageNet 2015 competition with an open-gated or gateless Highway network variant called Residual neural network.In 2017: Ashish Vaswani et al."
0.12605160000384785,What is the Gemini family and who developed it?,"The Gemini family comprises of multimodal large language models developed by Google DeepMind. It serves as the successor to LaMDA and PaLM 2 and includes Gemini Ultra, Gemini Pro, and Gemini Nano. ","Score: 0.8226608714577094 / Gemini is a family of multimodal large language models developed by Google DeepMind: serving as the successor to LaMDA and PaLM 2. Comprising Gemini Ultra: Gemini Pro: and Gemini Nano: it was announced on December 6: 2023: positioned as a competitor to OpenAI's GPT-4. It powers the generative artificial intelligence chatbot of the same name. /  /  / == History == /  /  / === Development === /  / Google announced Gemini: a large language model (LLM) developed by subsidiary Google DeepMind: during the Google I/O keynote on May 10: 2023. It was positioned as a more powerful successor to PaLM 2: which was also unveiled at the event: with Google CEO Sundar Pichai stating that Gemini was still in its early developmental stages. Unlike other LLMs: Gemini was said to be unique in that it was not trained on a text corpus alone and was designed to be multimodal: meaning it could process multiple types of data simultaneously: including text: images: audio: video: and computer code. It had been developed as a collaboration between DeepMind and Google Brain: two branches of Google that had been merged as Google DeepMind the previous month. In an interview with Wired: DeepMind CEO Demis Hassabis touted Gemini's advanced capabilities: which he believed would allow the algorithm to trump OpenAI's ChatGPT: which runs on GPT-4 and whose growing popularity had been aggressively challenged by Google with LaMDA and Bard. Hassabis highlighted the strengths of DeepMind's AlphaGo program: which gained worldwide attention in 2016 when it defeated Go champion Lee Sedol: saying that Gemini would combine the power of AlphaGo and other Google–DeepMind LLMs.In August 2023: The Information published a report outlining Google's roadmap for Gemini: revealing that the company was targeting a launch date of late 2023. According to the report: Google hoped to surpass OpenAI and other competitors by combining conversational text capabilities present in most LLMs with artificial intelligence–powered image generation: allowing it to create contextual images and be adapted for a wider range of use cases. Like Bard: Google co-founder Sergey Brin was summoned out of retirement to assist in the development of Gemini: along with hundreds of other engineers from Google Brain and DeepMind; he was later credited as a ""core contributor"" to Gemini. Because Gemini was being trained on transcripts of YouTube videos: lawyers were brought in to filter out any potentially copyrighted materials.With news of Gemini's impending launch: OpenAI hastened its work on integrating GPT-4 with multimodal features similar to those of Gemini. The Information reported in September that several companies had been granted early access to ""an early version"" of the LLM: which Google intended to make available to clients through Google Cloud's Vertex AI service. The publication also stated that Google was arming Gemini to compete with both GPT-4 and Microsoft's GitHub Copilot. /  /  / === Launch === / On December 6: 2023: Pichai and Hassabis announced ""Gemini 1.0"" at a virtual press conference. It comprised three models: Gemini Ultra: designed for ""highly complex tasks""; Gemini Pro: designed for ""a wide range of tasks""; and Gemini Nano: designed for ""on-device tasks"". At launch: Gemini Pro and Nano were integrated into Bard and the Pixel 8 Pro smartphone: respectively: while Gemini Ultra was set to power ""Bard Advanced"" and become available to software developers in early 2024. Other products that Google intended to incorporate Gemini into included Search: Ads: Chrome: Duet AI on Google Workspace: and AlphaCode 2. It was made available only in English. Touted as Google's ""largest and most capable AI model"" and designed to emulate human behavior: the company stated that Gemini would not be made widely available until the following year due to the need for ""extensive safety testing"". Gemini was trained on and powered by Google's Tensor Processing Units (TPUs): and the name is in reference to the DeepMind–Google Brain merger as well as NASA's Project Gemini.Gemini Ultra was said to have outperformed GPT-4: Anthropic's Claude 2: Inflection AI's Inflection-2: Meta's LLaMA 2: and xAI's Grok 1 on a variety of industry benchmarks: while Gemini Pro was said to have outperformed GPT-3.5. Gemini Ultra was also the first language model to outperform human experts on the 57-subject Massive Multitask Language Understanding (MMLU) test: obtaining a score of 90%.","Score: 0.8040459643159501 / Touted as Google's ""largest and most capable AI model"" and designed to emulate human behavior: the company stated that Gemini would not be made widely available until the following year due to the need for ""extensive safety testing"". Gemini was trained on and powered by Google's Tensor Processing Units (TPUs): and the name is in reference to the DeepMind–Google Brain merger as well as NASA's Project Gemini.Gemini Ultra was said to have outperformed GPT-4: Anthropic's Claude 2: Inflection AI's Inflection-2: Meta's LLaMA 2: and xAI's Grok 1 on a variety of industry benchmarks: while Gemini Pro was said to have outperformed GPT-3.5. Gemini Ultra was also the first language model to outperform human experts on the 57-subject Massive Multitask Language Understanding (MMLU) test: obtaining a score of 90%. Gemini Pro was made available to Google Cloud customers on AI Studio and Vertex AI on December 13: while Gemini Nano will be made available to Android developers as well. Hassabis further revealed that DeepMind was exploring how Gemini could be ""combined with robotics to physically interact with the world"". In accordance with an executive order signed by U.S. President Joe Biden in October: Google stated that it would share testing results of Gemini Ultra with the federal government of the United States. Similarly: the company was engaged in discussions with the government of the United Kingdom to comply with the principles laid out at the AI Safety Summit at Bletchley Park in November. /  /  / === Updates === / Google partnered with Samsung to integrate Gemini Nano and Gemini Pro into its Galaxy S24 smartphone lineup in January 2024. The following month: Bard and Duet AI were unified under the Gemini brand: with ""Gemini Advanced with Ultra 1.0"" debuting via a new ""AI Premium"" tier of the Google One subscription service. Gemini Pro also received a global launch.In February: Google launched ""Gemini 1.5"" in a limited capacity: positioned as a more powerful and capable model than 1.0 Ultra. This ""step change"" was achieved through various technical advancements: including a new architecture: a mixture-of-experts approach: and a larger one-million-token context window: which equates to roughly an hour of silent video: 11 hours of audio: 30:000 lines of code: or 700:000 words. The same month: Google debuted Gemma: a family of free and open-source LLMs that serve as a lightweight version of Gemini. They come in two sizes: with a neural network with two and seven billion parameters: respectively. Multiple publications viewed this as an response to Meta and others open-sourcing their AI models: and a stark reversal from Google's longstanding practice of keeping its AI proprietary. /  /  / == Technical specifications == / The first generation of Gemini (""Gemini 1"") has three models: with the same software architecture. They are decoder-only transformers: with modifications to allow efficient training and inference on TPUs. They have a context length of 32:768 tokens: with multi-query attention. Two versions of Gemini Nano: Nano-1 (1.8 billion parameters) and Nano-2 (3.25 billion parameters): are distilled from larger Gemini models: designed for use by edge devices such as smartphones. As Gemini is multimodal: each context window can contain multiple forms of input. The different modes can be interleaved and do not have to be presented in a fixed order: allowing for a multimodal conversation. For example: the user might open the conversation with a mix of text: picture: video: and audio: presented in any order: and Gemini might reply with the same free ordering. / Input images may be of different resolutions: while video is inputted as a sequence of images. Audio is sampled at 16 kHz and then converted into a sequence of tokens by the Universal Speech Model. Gemini's dataset is multimodal and multilingual: consisting of ""web documents: books: and code: and includ[ing] image: audio: and video data"".Demis Hassabis claims that training Gemini 1 used ""roughly the same amount of compute: maybe slightly more than what was rumored for GPT-4"".The second generation of Gemini (""Gemini 1.5"") has one model published so far: Gemini 1.5 Pro. It is a multimodal sparse mixture-of-experts: with context length of ""multiple millions"". /  /  / == Reception == / Gemini's launch was preluded by months of intense speculation and anticipation: which MIT Technology Review described as ""peak AI hype""."
0.1691444999887608,When was the Gemini family announced and who were they positioned as a competitor to?,"The Gemini family was announced on December 6, 2023, and they were positioned as a competitor to OpenAI's GPT-4. ","Score: 0.8122592696524931 / Gemini is a family of multimodal large language models developed by Google DeepMind: serving as the successor to LaMDA and PaLM 2. Comprising Gemini Ultra: Gemini Pro: and Gemini Nano: it was announced on December 6: 2023: positioned as a competitor to OpenAI's GPT-4. It powers the generative artificial intelligence chatbot of the same name. /  /  / == History == /  /  / === Development === /  / Google announced Gemini: a large language model (LLM) developed by subsidiary Google DeepMind: during the Google I/O keynote on May 10: 2023. It was positioned as a more powerful successor to PaLM 2: which was also unveiled at the event: with Google CEO Sundar Pichai stating that Gemini was still in its early developmental stages. Unlike other LLMs: Gemini was said to be unique in that it was not trained on a text corpus alone and was designed to be multimodal: meaning it could process multiple types of data simultaneously: including text: images: audio: video: and computer code. It had been developed as a collaboration between DeepMind and Google Brain: two branches of Google that had been merged as Google DeepMind the previous month. In an interview with Wired: DeepMind CEO Demis Hassabis touted Gemini's advanced capabilities: which he believed would allow the algorithm to trump OpenAI's ChatGPT: which runs on GPT-4 and whose growing popularity had been aggressively challenged by Google with LaMDA and Bard. Hassabis highlighted the strengths of DeepMind's AlphaGo program: which gained worldwide attention in 2016 when it defeated Go champion Lee Sedol: saying that Gemini would combine the power of AlphaGo and other Google–DeepMind LLMs.In August 2023: The Information published a report outlining Google's roadmap for Gemini: revealing that the company was targeting a launch date of late 2023. According to the report: Google hoped to surpass OpenAI and other competitors by combining conversational text capabilities present in most LLMs with artificial intelligence–powered image generation: allowing it to create contextual images and be adapted for a wider range of use cases. Like Bard: Google co-founder Sergey Brin was summoned out of retirement to assist in the development of Gemini: along with hundreds of other engineers from Google Brain and DeepMind; he was later credited as a ""core contributor"" to Gemini. Because Gemini was being trained on transcripts of YouTube videos: lawyers were brought in to filter out any potentially copyrighted materials.With news of Gemini's impending launch: OpenAI hastened its work on integrating GPT-4 with multimodal features similar to those of Gemini. The Information reported in September that several companies had been granted early access to ""an early version"" of the LLM: which Google intended to make available to clients through Google Cloud's Vertex AI service. The publication also stated that Google was arming Gemini to compete with both GPT-4 and Microsoft's GitHub Copilot. /  /  / === Launch === / On December 6: 2023: Pichai and Hassabis announced ""Gemini 1.0"" at a virtual press conference. It comprised three models: Gemini Ultra: designed for ""highly complex tasks""; Gemini Pro: designed for ""a wide range of tasks""; and Gemini Nano: designed for ""on-device tasks"". At launch: Gemini Pro and Nano were integrated into Bard and the Pixel 8 Pro smartphone: respectively: while Gemini Ultra was set to power ""Bard Advanced"" and become available to software developers in early 2024. Other products that Google intended to incorporate Gemini into included Search: Ads: Chrome: Duet AI on Google Workspace: and AlphaCode 2. It was made available only in English. Touted as Google's ""largest and most capable AI model"" and designed to emulate human behavior: the company stated that Gemini would not be made widely available until the following year due to the need for ""extensive safety testing"". Gemini was trained on and powered by Google's Tensor Processing Units (TPUs): and the name is in reference to the DeepMind–Google Brain merger as well as NASA's Project Gemini.Gemini Ultra was said to have outperformed GPT-4: Anthropic's Claude 2: Inflection AI's Inflection-2: Meta's LLaMA 2: and xAI's Grok 1 on a variety of industry benchmarks: while Gemini Pro was said to have outperformed GPT-3.5. Gemini Ultra was also the first language model to outperform human experts on the 57-subject Massive Multitask Language Understanding (MMLU) test: obtaining a score of 90%.","Score: 0.8019867312548267 / Touted as Google's ""largest and most capable AI model"" and designed to emulate human behavior: the company stated that Gemini would not be made widely available until the following year due to the need for ""extensive safety testing"". Gemini was trained on and powered by Google's Tensor Processing Units (TPUs): and the name is in reference to the DeepMind–Google Brain merger as well as NASA's Project Gemini.Gemini Ultra was said to have outperformed GPT-4: Anthropic's Claude 2: Inflection AI's Inflection-2: Meta's LLaMA 2: and xAI's Grok 1 on a variety of industry benchmarks: while Gemini Pro was said to have outperformed GPT-3.5. Gemini Ultra was also the first language model to outperform human experts on the 57-subject Massive Multitask Language Understanding (MMLU) test: obtaining a score of 90%. Gemini Pro was made available to Google Cloud customers on AI Studio and Vertex AI on December 13: while Gemini Nano will be made available to Android developers as well. Hassabis further revealed that DeepMind was exploring how Gemini could be ""combined with robotics to physically interact with the world"". In accordance with an executive order signed by U.S. President Joe Biden in October: Google stated that it would share testing results of Gemini Ultra with the federal government of the United States. Similarly: the company was engaged in discussions with the government of the United Kingdom to comply with the principles laid out at the AI Safety Summit at Bletchley Park in November. /  /  / === Updates === / Google partnered with Samsung to integrate Gemini Nano and Gemini Pro into its Galaxy S24 smartphone lineup in January 2024. The following month: Bard and Duet AI were unified under the Gemini brand: with ""Gemini Advanced with Ultra 1.0"" debuting via a new ""AI Premium"" tier of the Google One subscription service. Gemini Pro also received a global launch.In February: Google launched ""Gemini 1.5"" in a limited capacity: positioned as a more powerful and capable model than 1.0 Ultra. This ""step change"" was achieved through various technical advancements: including a new architecture: a mixture-of-experts approach: and a larger one-million-token context window: which equates to roughly an hour of silent video: 11 hours of audio: 30:000 lines of code: or 700:000 words. The same month: Google debuted Gemma: a family of free and open-source LLMs that serve as a lightweight version of Gemini. They come in two sizes: with a neural network with two and seven billion parameters: respectively. Multiple publications viewed this as an response to Meta and others open-sourcing their AI models: and a stark reversal from Google's longstanding practice of keeping its AI proprietary. /  /  / == Technical specifications == / The first generation of Gemini (""Gemini 1"") has three models: with the same software architecture. They are decoder-only transformers: with modifications to allow efficient training and inference on TPUs. They have a context length of 32:768 tokens: with multi-query attention. Two versions of Gemini Nano: Nano-1 (1.8 billion parameters) and Nano-2 (3.25 billion parameters): are distilled from larger Gemini models: designed for use by edge devices such as smartphones. As Gemini is multimodal: each context window can contain multiple forms of input. The different modes can be interleaved and do not have to be presented in a fixed order: allowing for a multimodal conversation. For example: the user might open the conversation with a mix of text: picture: video: and audio: presented in any order: and Gemini might reply with the same free ordering. / Input images may be of different resolutions: while video is inputted as a sequence of images. Audio is sampled at 16 kHz and then converted into a sequence of tokens by the Universal Speech Model. Gemini's dataset is multimodal and multilingual: consisting of ""web documents: books: and code: and includ[ing] image: audio: and video data"".Demis Hassabis claims that training Gemini 1 used ""roughly the same amount of compute: maybe slightly more than what was rumored for GPT-4"".The second generation of Gemini (""Gemini 1.5"") has one model published so far: Gemini 1.5 Pro. It is a multimodal sparse mixture-of-experts: with context length of ""multiple millions"". /  /  / == Reception == / Gemini's launch was preluded by months of intense speculation and anticipation: which MIT Technology Review described as ""peak AI hype""."
0.16143080001347698,What is unique about the Gemini family of language models?,"Unlike other Large Language Models, Gemini is unique in that it is not trained on a text corpus alone. It is designed to be multimodal, meaning it can process multiple types of data simultaneously, including text, images, audio, video, and computer code.","Score: 0.8777351781320659 / Gemini is a family of multimodal large language models developed by Google DeepMind: serving as the successor to LaMDA and PaLM 2. Comprising Gemini Ultra: Gemini Pro: and Gemini Nano: it was announced on December 6: 2023: positioned as a competitor to OpenAI's GPT-4. It powers the generative artificial intelligence chatbot of the same name. /  /  / == History == /  /  / === Development === /  / Google announced Gemini: a large language model (LLM) developed by subsidiary Google DeepMind: during the Google I/O keynote on May 10: 2023. It was positioned as a more powerful successor to PaLM 2: which was also unveiled at the event: with Google CEO Sundar Pichai stating that Gemini was still in its early developmental stages. Unlike other LLMs: Gemini was said to be unique in that it was not trained on a text corpus alone and was designed to be multimodal: meaning it could process multiple types of data simultaneously: including text: images: audio: video: and computer code. It had been developed as a collaboration between DeepMind and Google Brain: two branches of Google that had been merged as Google DeepMind the previous month. In an interview with Wired: DeepMind CEO Demis Hassabis touted Gemini's advanced capabilities: which he believed would allow the algorithm to trump OpenAI's ChatGPT: which runs on GPT-4 and whose growing popularity had been aggressively challenged by Google with LaMDA and Bard. Hassabis highlighted the strengths of DeepMind's AlphaGo program: which gained worldwide attention in 2016 when it defeated Go champion Lee Sedol: saying that Gemini would combine the power of AlphaGo and other Google–DeepMind LLMs.In August 2023: The Information published a report outlining Google's roadmap for Gemini: revealing that the company was targeting a launch date of late 2023. According to the report: Google hoped to surpass OpenAI and other competitors by combining conversational text capabilities present in most LLMs with artificial intelligence–powered image generation: allowing it to create contextual images and be adapted for a wider range of use cases. Like Bard: Google co-founder Sergey Brin was summoned out of retirement to assist in the development of Gemini: along with hundreds of other engineers from Google Brain and DeepMind; he was later credited as a ""core contributor"" to Gemini. Because Gemini was being trained on transcripts of YouTube videos: lawyers were brought in to filter out any potentially copyrighted materials.With news of Gemini's impending launch: OpenAI hastened its work on integrating GPT-4 with multimodal features similar to those of Gemini. The Information reported in September that several companies had been granted early access to ""an early version"" of the LLM: which Google intended to make available to clients through Google Cloud's Vertex AI service. The publication also stated that Google was arming Gemini to compete with both GPT-4 and Microsoft's GitHub Copilot. /  /  / === Launch === / On December 6: 2023: Pichai and Hassabis announced ""Gemini 1.0"" at a virtual press conference. It comprised three models: Gemini Ultra: designed for ""highly complex tasks""; Gemini Pro: designed for ""a wide range of tasks""; and Gemini Nano: designed for ""on-device tasks"". At launch: Gemini Pro and Nano were integrated into Bard and the Pixel 8 Pro smartphone: respectively: while Gemini Ultra was set to power ""Bard Advanced"" and become available to software developers in early 2024. Other products that Google intended to incorporate Gemini into included Search: Ads: Chrome: Duet AI on Google Workspace: and AlphaCode 2. It was made available only in English. Touted as Google's ""largest and most capable AI model"" and designed to emulate human behavior: the company stated that Gemini would not be made widely available until the following year due to the need for ""extensive safety testing"". Gemini was trained on and powered by Google's Tensor Processing Units (TPUs): and the name is in reference to the DeepMind–Google Brain merger as well as NASA's Project Gemini.Gemini Ultra was said to have outperformed GPT-4: Anthropic's Claude 2: Inflection AI's Inflection-2: Meta's LLaMA 2: and xAI's Grok 1 on a variety of industry benchmarks: while Gemini Pro was said to have outperformed GPT-3.5. Gemini Ultra was also the first language model to outperform human experts on the 57-subject Massive Multitask Language Understanding (MMLU) test: obtaining a score of 90%.","Score: 0.8674940796431498 / Touted as Google's ""largest and most capable AI model"" and designed to emulate human behavior: the company stated that Gemini would not be made widely available until the following year due to the need for ""extensive safety testing"". Gemini was trained on and powered by Google's Tensor Processing Units (TPUs): and the name is in reference to the DeepMind–Google Brain merger as well as NASA's Project Gemini.Gemini Ultra was said to have outperformed GPT-4: Anthropic's Claude 2: Inflection AI's Inflection-2: Meta's LLaMA 2: and xAI's Grok 1 on a variety of industry benchmarks: while Gemini Pro was said to have outperformed GPT-3.5. Gemini Ultra was also the first language model to outperform human experts on the 57-subject Massive Multitask Language Understanding (MMLU) test: obtaining a score of 90%. Gemini Pro was made available to Google Cloud customers on AI Studio and Vertex AI on December 13: while Gemini Nano will be made available to Android developers as well. Hassabis further revealed that DeepMind was exploring how Gemini could be ""combined with robotics to physically interact with the world"". In accordance with an executive order signed by U.S. President Joe Biden in October: Google stated that it would share testing results of Gemini Ultra with the federal government of the United States. Similarly: the company was engaged in discussions with the government of the United Kingdom to comply with the principles laid out at the AI Safety Summit at Bletchley Park in November. /  /  / === Updates === / Google partnered with Samsung to integrate Gemini Nano and Gemini Pro into its Galaxy S24 smartphone lineup in January 2024. The following month: Bard and Duet AI were unified under the Gemini brand: with ""Gemini Advanced with Ultra 1.0"" debuting via a new ""AI Premium"" tier of the Google One subscription service. Gemini Pro also received a global launch.In February: Google launched ""Gemini 1.5"" in a limited capacity: positioned as a more powerful and capable model than 1.0 Ultra. This ""step change"" was achieved through various technical advancements: including a new architecture: a mixture-of-experts approach: and a larger one-million-token context window: which equates to roughly an hour of silent video: 11 hours of audio: 30:000 lines of code: or 700:000 words. The same month: Google debuted Gemma: a family of free and open-source LLMs that serve as a lightweight version of Gemini. They come in two sizes: with a neural network with two and seven billion parameters: respectively. Multiple publications viewed this as an response to Meta and others open-sourcing their AI models: and a stark reversal from Google's longstanding practice of keeping its AI proprietary. /  /  / == Technical specifications == / The first generation of Gemini (""Gemini 1"") has three models: with the same software architecture. They are decoder-only transformers: with modifications to allow efficient training and inference on TPUs. They have a context length of 32:768 tokens: with multi-query attention. Two versions of Gemini Nano: Nano-1 (1.8 billion parameters) and Nano-2 (3.25 billion parameters): are distilled from larger Gemini models: designed for use by edge devices such as smartphones. As Gemini is multimodal: each context window can contain multiple forms of input. The different modes can be interleaved and do not have to be presented in a fixed order: allowing for a multimodal conversation. For example: the user might open the conversation with a mix of text: picture: video: and audio: presented in any order: and Gemini might reply with the same free ordering. / Input images may be of different resolutions: while video is inputted as a sequence of images. Audio is sampled at 16 kHz and then converted into a sequence of tokens by the Universal Speech Model. Gemini's dataset is multimodal and multilingual: consisting of ""web documents: books: and code: and includ[ing] image: audio: and video data"".Demis Hassabis claims that training Gemini 1 used ""roughly the same amount of compute: maybe slightly more than what was rumored for GPT-4"".The second generation of Gemini (""Gemini 1.5"") has one model published so far: Gemini 1.5 Pro. It is a multimodal sparse mixture-of-experts: with context length of ""multiple millions"". /  /  / == Reception == / Gemini's launch was preluded by months of intense speculation and anticipation: which MIT Technology Review described as ""peak AI hype""."
0.15781190001871437,How has Gemini been integrated into Google's products and services?,"Upon launch, Gemini Pro and Nano were integrated into Bard and the Pixel 8 Pro smartphone, respectively, while Gemini Ultra was set to power ""Bard Advanced"". Google also intended to incorporate Gemini into other products including Search, Ads, Chrome, Duet AI on Google Workspace, and AlphaCode 2.","Score: 0.8418431272534059 / Touted as Google's ""largest and most capable AI model"" and designed to emulate human behavior: the company stated that Gemini would not be made widely available until the following year due to the need for ""extensive safety testing"". Gemini was trained on and powered by Google's Tensor Processing Units (TPUs): and the name is in reference to the DeepMind–Google Brain merger as well as NASA's Project Gemini.Gemini Ultra was said to have outperformed GPT-4: Anthropic's Claude 2: Inflection AI's Inflection-2: Meta's LLaMA 2: and xAI's Grok 1 on a variety of industry benchmarks: while Gemini Pro was said to have outperformed GPT-3.5. Gemini Ultra was also the first language model to outperform human experts on the 57-subject Massive Multitask Language Understanding (MMLU) test: obtaining a score of 90%. Gemini Pro was made available to Google Cloud customers on AI Studio and Vertex AI on December 13: while Gemini Nano will be made available to Android developers as well. Hassabis further revealed that DeepMind was exploring how Gemini could be ""combined with robotics to physically interact with the world"". In accordance with an executive order signed by U.S. President Joe Biden in October: Google stated that it would share testing results of Gemini Ultra with the federal government of the United States. Similarly: the company was engaged in discussions with the government of the United Kingdom to comply with the principles laid out at the AI Safety Summit at Bletchley Park in November. /  /  / === Updates === / Google partnered with Samsung to integrate Gemini Nano and Gemini Pro into its Galaxy S24 smartphone lineup in January 2024. The following month: Bard and Duet AI were unified under the Gemini brand: with ""Gemini Advanced with Ultra 1.0"" debuting via a new ""AI Premium"" tier of the Google One subscription service. Gemini Pro also received a global launch.In February: Google launched ""Gemini 1.5"" in a limited capacity: positioned as a more powerful and capable model than 1.0 Ultra. This ""step change"" was achieved through various technical advancements: including a new architecture: a mixture-of-experts approach: and a larger one-million-token context window: which equates to roughly an hour of silent video: 11 hours of audio: 30:000 lines of code: or 700:000 words. The same month: Google debuted Gemma: a family of free and open-source LLMs that serve as a lightweight version of Gemini. They come in two sizes: with a neural network with two and seven billion parameters: respectively. Multiple publications viewed this as an response to Meta and others open-sourcing their AI models: and a stark reversal from Google's longstanding practice of keeping its AI proprietary. /  /  / == Technical specifications == / The first generation of Gemini (""Gemini 1"") has three models: with the same software architecture. They are decoder-only transformers: with modifications to allow efficient training and inference on TPUs. They have a context length of 32:768 tokens: with multi-query attention. Two versions of Gemini Nano: Nano-1 (1.8 billion parameters) and Nano-2 (3.25 billion parameters): are distilled from larger Gemini models: designed for use by edge devices such as smartphones. As Gemini is multimodal: each context window can contain multiple forms of input. The different modes can be interleaved and do not have to be presented in a fixed order: allowing for a multimodal conversation. For example: the user might open the conversation with a mix of text: picture: video: and audio: presented in any order: and Gemini might reply with the same free ordering. / Input images may be of different resolutions: while video is inputted as a sequence of images. Audio is sampled at 16 kHz and then converted into a sequence of tokens by the Universal Speech Model. Gemini's dataset is multimodal and multilingual: consisting of ""web documents: books: and code: and includ[ing] image: audio: and video data"".Demis Hassabis claims that training Gemini 1 used ""roughly the same amount of compute: maybe slightly more than what was rumored for GPT-4"".The second generation of Gemini (""Gemini 1.5"") has one model published so far: Gemini 1.5 Pro. It is a multimodal sparse mixture-of-experts: with context length of ""multiple millions"". /  /  / == Reception == / Gemini's launch was preluded by months of intense speculation and anticipation: which MIT Technology Review described as ""peak AI hype"".","Score: 0.8408344772410451 / Input images may be of different resolutions: while video is inputted as a sequence of images. Audio is sampled at 16 kHz and then converted into a sequence of tokens by the Universal Speech Model. Gemini's dataset is multimodal and multilingual: consisting of ""web documents: books: and code: and includ[ing] image: audio: and video data"".Demis Hassabis claims that training Gemini 1 used ""roughly the same amount of compute: maybe slightly more than what was rumored for GPT-4"".The second generation of Gemini (""Gemini 1.5"") has one model published so far: Gemini 1.5 Pro. It is a multimodal sparse mixture-of-experts: with context length of ""multiple millions"". /  /  / == Reception == / Gemini's launch was preluded by months of intense speculation and anticipation: which MIT Technology Review described as ""peak AI hype"". In August 2023: Dylan Patel and Daniel Nishball of research firm SemiAnalysis penned a blog post declaring that the release of Gemini would ""eat the world"" and outclass GPT-4: prompting OpenAI CEO Sam Altman to ridicule the duo on X (formerly Twitter). Business magnate Elon Musk: who co-founded OpenAI: weighed in: asking: ""Are the numbers wrong?"" Hugh Langley of Business Insider remarked that Gemini would be a make-or-break moment for Google: writing: ""If Gemini dazzles: it will help Google change the narrative that it was blindsided by Microsoft and OpenAI. If it disappoints: it will embolden critics who say Google has fallen behind.""Reacting to its unveiling in December 2023: University of Washington professor emeritus Oren Etzioni predicted a ""tit-for-tat arms race"" between Google and OpenAI. Professor Alexei Efros of the University of California: Berkeley praised the potential of Gemini's multimodal approach: while scientist Melanie Mitchell of the Santa Fe Institute called Gemini ""very sophisticated"". Professor Chirag Shah of the University of Washington was less impressed: likening Gemini's launch to the routineness of Apple's annual introduction of a new iPhone. Similarly: Stanford University's Percy Liang: the University of Washington's Emily Bender: and the University of Galway's Michael Madden cautioned that it was difficult to interpret benchmark scores without insight into the training data used. Writing for Fast Company: Mark Sullivan opined that Google had the opportunity to challenge the iPhone's dominant market share: believing that Apple was unlikely to have the capacity to develop functionality similar to Gemini with its Siri virtual assistant. Google shares spiked by 5.3 percent the day after Gemini's launch.Google faced criticism for a demonstrative video of Gemini: which was not conducted in real time. /  /  / == See also == / Gato: a multimodal neural network developed by DeepMind /  /  / == References == /  /  / == Further reading == /  /  / == External links == / Official website  / Press release via The Keyword / Announcement and demo on YouTube / White paper for 1.0 and 1.5"
0.1119436000008136,What were some notable achievements of Gemini Ultra and Gemini Pro?,"Gemini Ultra has outperformed GPT-4, Anthropic's Claude 2, Inflection AI's Inflection-2, Meta's LLaMA 2, and xAI's Grok 1 on a variety of industry benchmarks, and was the first language model to outperform human experts on the 57-subject Massive Multitask Language Understanding (MMLU) test, obtaining a score of 90%. Gemini Pro was also said to have outperformed GPT-3.5.","Score: 0.8461293579121905 / Gemini is a family of multimodal large language models developed by Google DeepMind: serving as the successor to LaMDA and PaLM 2. Comprising Gemini Ultra: Gemini Pro: and Gemini Nano: it was announced on December 6: 2023: positioned as a competitor to OpenAI's GPT-4. It powers the generative artificial intelligence chatbot of the same name. /  /  / == History == /  /  / === Development === /  / Google announced Gemini: a large language model (LLM) developed by subsidiary Google DeepMind: during the Google I/O keynote on May 10: 2023. It was positioned as a more powerful successor to PaLM 2: which was also unveiled at the event: with Google CEO Sundar Pichai stating that Gemini was still in its early developmental stages. Unlike other LLMs: Gemini was said to be unique in that it was not trained on a text corpus alone and was designed to be multimodal: meaning it could process multiple types of data simultaneously: including text: images: audio: video: and computer code. It had been developed as a collaboration between DeepMind and Google Brain: two branches of Google that had been merged as Google DeepMind the previous month. In an interview with Wired: DeepMind CEO Demis Hassabis touted Gemini's advanced capabilities: which he believed would allow the algorithm to trump OpenAI's ChatGPT: which runs on GPT-4 and whose growing popularity had been aggressively challenged by Google with LaMDA and Bard. Hassabis highlighted the strengths of DeepMind's AlphaGo program: which gained worldwide attention in 2016 when it defeated Go champion Lee Sedol: saying that Gemini would combine the power of AlphaGo and other Google–DeepMind LLMs.In August 2023: The Information published a report outlining Google's roadmap for Gemini: revealing that the company was targeting a launch date of late 2023. According to the report: Google hoped to surpass OpenAI and other competitors by combining conversational text capabilities present in most LLMs with artificial intelligence–powered image generation: allowing it to create contextual images and be adapted for a wider range of use cases. Like Bard: Google co-founder Sergey Brin was summoned out of retirement to assist in the development of Gemini: along with hundreds of other engineers from Google Brain and DeepMind; he was later credited as a ""core contributor"" to Gemini. Because Gemini was being trained on transcripts of YouTube videos: lawyers were brought in to filter out any potentially copyrighted materials.With news of Gemini's impending launch: OpenAI hastened its work on integrating GPT-4 with multimodal features similar to those of Gemini. The Information reported in September that several companies had been granted early access to ""an early version"" of the LLM: which Google intended to make available to clients through Google Cloud's Vertex AI service. The publication also stated that Google was arming Gemini to compete with both GPT-4 and Microsoft's GitHub Copilot. /  /  / === Launch === / On December 6: 2023: Pichai and Hassabis announced ""Gemini 1.0"" at a virtual press conference. It comprised three models: Gemini Ultra: designed for ""highly complex tasks""; Gemini Pro: designed for ""a wide range of tasks""; and Gemini Nano: designed for ""on-device tasks"". At launch: Gemini Pro and Nano were integrated into Bard and the Pixel 8 Pro smartphone: respectively: while Gemini Ultra was set to power ""Bard Advanced"" and become available to software developers in early 2024. Other products that Google intended to incorporate Gemini into included Search: Ads: Chrome: Duet AI on Google Workspace: and AlphaCode 2. It was made available only in English. Touted as Google's ""largest and most capable AI model"" and designed to emulate human behavior: the company stated that Gemini would not be made widely available until the following year due to the need for ""extensive safety testing"". Gemini was trained on and powered by Google's Tensor Processing Units (TPUs): and the name is in reference to the DeepMind–Google Brain merger as well as NASA's Project Gemini.Gemini Ultra was said to have outperformed GPT-4: Anthropic's Claude 2: Inflection AI's Inflection-2: Meta's LLaMA 2: and xAI's Grok 1 on a variety of industry benchmarks: while Gemini Pro was said to have outperformed GPT-3.5. Gemini Ultra was also the first language model to outperform human experts on the 57-subject Massive Multitask Language Understanding (MMLU) test: obtaining a score of 90%.","Score: 0.8395814738487644 / Touted as Google's ""largest and most capable AI model"" and designed to emulate human behavior: the company stated that Gemini would not be made widely available until the following year due to the need for ""extensive safety testing"". Gemini was trained on and powered by Google's Tensor Processing Units (TPUs): and the name is in reference to the DeepMind–Google Brain merger as well as NASA's Project Gemini.Gemini Ultra was said to have outperformed GPT-4: Anthropic's Claude 2: Inflection AI's Inflection-2: Meta's LLaMA 2: and xAI's Grok 1 on a variety of industry benchmarks: while Gemini Pro was said to have outperformed GPT-3.5. Gemini Ultra was also the first language model to outperform human experts on the 57-subject Massive Multitask Language Understanding (MMLU) test: obtaining a score of 90%. Gemini Pro was made available to Google Cloud customers on AI Studio and Vertex AI on December 13: while Gemini Nano will be made available to Android developers as well. Hassabis further revealed that DeepMind was exploring how Gemini could be ""combined with robotics to physically interact with the world"". In accordance with an executive order signed by U.S. President Joe Biden in October: Google stated that it would share testing results of Gemini Ultra with the federal government of the United States. Similarly: the company was engaged in discussions with the government of the United Kingdom to comply with the principles laid out at the AI Safety Summit at Bletchley Park in November. /  /  / === Updates === / Google partnered with Samsung to integrate Gemini Nano and Gemini Pro into its Galaxy S24 smartphone lineup in January 2024. The following month: Bard and Duet AI were unified under the Gemini brand: with ""Gemini Advanced with Ultra 1.0"" debuting via a new ""AI Premium"" tier of the Google One subscription service. Gemini Pro also received a global launch.In February: Google launched ""Gemini 1.5"" in a limited capacity: positioned as a more powerful and capable model than 1.0 Ultra. This ""step change"" was achieved through various technical advancements: including a new architecture: a mixture-of-experts approach: and a larger one-million-token context window: which equates to roughly an hour of silent video: 11 hours of audio: 30:000 lines of code: or 700:000 words. The same month: Google debuted Gemma: a family of free and open-source LLMs that serve as a lightweight version of Gemini. They come in two sizes: with a neural network with two and seven billion parameters: respectively. Multiple publications viewed this as an response to Meta and others open-sourcing their AI models: and a stark reversal from Google's longstanding practice of keeping its AI proprietary. /  /  / == Technical specifications == / The first generation of Gemini (""Gemini 1"") has three models: with the same software architecture. They are decoder-only transformers: with modifications to allow efficient training and inference on TPUs. They have a context length of 32:768 tokens: with multi-query attention. Two versions of Gemini Nano: Nano-1 (1.8 billion parameters) and Nano-2 (3.25 billion parameters): are distilled from larger Gemini models: designed for use by edge devices such as smartphones. As Gemini is multimodal: each context window can contain multiple forms of input. The different modes can be interleaved and do not have to be presented in a fixed order: allowing for a multimodal conversation. For example: the user might open the conversation with a mix of text: picture: video: and audio: presented in any order: and Gemini might reply with the same free ordering. / Input images may be of different resolutions: while video is inputted as a sequence of images. Audio is sampled at 16 kHz and then converted into a sequence of tokens by the Universal Speech Model. Gemini's dataset is multimodal and multilingual: consisting of ""web documents: books: and code: and includ[ing] image: audio: and video data"".Demis Hassabis claims that training Gemini 1 used ""roughly the same amount of compute: maybe slightly more than what was rumored for GPT-4"".The second generation of Gemini (""Gemini 1.5"") has one model published so far: Gemini 1.5 Pro. It is a multimodal sparse mixture-of-experts: with context length of ""multiple millions"". /  /  / == Reception == / Gemini's launch was preluded by months of intense speculation and anticipation: which MIT Technology Review described as ""peak AI hype""."
1.2318589999922551,What are Generative pre-trained transformers (GPT)?,"Generative pre-trained transformers (GPT) are a type of large language model (LLM) and a prominent framework for generative artificial intelligence. They are artificial neural networks used in natural language processing tasks. GPTs are based on the transformer architecture, pre-trained on large data sets of unlabelled text, and capable of generating novel human-like content.","Score: 0.9235119239810708 / Generative pre-trained transformers (GPT) are a type of large language model (LLM) and a prominent framework for generative artificial intelligence. They are artificial neural networks that are used in natural language processing tasks. GPTs are based on the transformer architecture: pre-trained on large data sets of unlabelled text: and able to generate novel human-like content. As of 2023: most LLMs have these characteristics and are sometimes referred to broadly as GPTs.The first GPT was introduced in 2018 by OpenAI. OpenAI has released very influential GPT foundation models that have been sequentially numbered: to comprise its ""GPT-n"" series. Each of these was significantly more capable than the previous: due to increased size (number of trainable parameters) and training. The most recent of these: GPT-4: was released in March 2023. Such models have been the basis for their more task-specific GPT systems: including models fine-tuned for instruction following—which in turn power the ChatGPT chatbot service.The term ""GPT"" is also used in the names and descriptions of such models developed by others. For example: other GPT foundation models include a series of models created by EleutherAI: and seven models created by Cerebras in 2023. Also: companies in different industries have developed task-specific GPTs in their respective fields: such as Salesforce's ""EinsteinGPT"" (for CRM) and Bloomberg's ""BloombergGPT"" (for finance). /  /  / == History == /  /  / === Initial developments === / Generative pretraining (GP) was a long-established concept in machine learning applications. It was originally used as a form of semi-supervised learning: as the model is trained first on an unlabelled dataset (pretraining step) by learning to generate datapoints in the dataset: and then it is trained to classify a labelled dataset.While the unnormalized linear transformer dates back to 1992: the modern transformer architecture was not available until 2017 when it was published by researchers at Google in a paper ""Attention Is All You Need"". That development led to the emergence of large language models such as BERT in 2018 which was a pre-trained transformer (PT) but not designed to be generative (BERT was an ""encoder-only"" model). Also around that time: in 2018: OpenAI published its article entitled ""Improving Language Understanding by Generative Pre-Training:"" in which it introduced the first generative pre-trained transformer (GPT) system (""GPT-1"").Prior to transformer-based architectures: the best-performing neural NLP (natural language processing) models commonly employed supervised learning from large amounts of manually-labeled data. The reliance on supervised learning limited their use on datasets that were not well-annotated: and also made it prohibitively expensive and time-consuming to train extremely large language models.The semi-supervised approach OpenAI employed to make a large-scale generative system—and was first to do with a transformer model—involved two stages: an unsupervised generative ""pretraining"" stage to set initial parameters using a language modeling objective: and a supervised discriminative ""fine-tuning"" stage to adapt these parameters to a target task. /  /  / === Later developments === / Regarding more recent GPT foundation models: OpenAI published its first versions of GPT-3 in July 2020. There were three models: with 1B: 6.7B: 175B parameters: respectively named babbage: curie: and davinci (giving initials B: C: and D).In July 2021: OpenAI published Codex: a task-specific GPT model targeted for programming applications. This was developed by fine-tuning a 12B parameter version of GPT-3 (different from previous GPT-3 models) using code from GitHub.In March 2022: OpenAI published two versions of GPT-3 that were fine-tuned for instruction-following (instruction-tuned): named davinci-instruct-beta (175B) and text-davinci-001: and then started beta testing code-davinci-002. text-davinci-002 was instruction-tuned from code-davinci-002. Both text-davinci-003 and ChatGPT were released in November 2022: with both building upon text-davinci-002 via reinforcement learning from human feedback (RLHF).","Score: 0.8884007082161312 / Advantages this had over the bare foundational models included higher accuracy: less negative/toxic sentiment: and generally better alignment with user needs. Hence: OpenAI began using this as the basis for its API service offerings. Other instruction-tuned models have been released by others: including a fully open version.Another (related) kind of task-specific models are chatbots: which engage in human-like conversation. In November 2022: OpenAI launched ChatGPT—an online chat interface powered by an instruction-tuned language model trained in a similar fashion to InstructGPT. They trained this model using RLHF: with human AI trainers providing conversations in which they played both the user and the AI: and mixed this new dialogue dataset with the InstructGPT dataset for a conversational format suitable for a chatbot. Other major chatbots currently include Microsoft's Bing Chat: which uses OpenAI's GPT-4 (as part of a broader close collaboration between OpenAI and Microsoft): and Google's competing chatbot Bard (initially based on their LaMDA family of conversation-trained language models: with plans to switch to PaLM).Yet another kind of task that a GPT can be used for is the meta-task of generating its own instructions: like developing a series of prompts for 'itself' to be able to effectuate a more general goal given by a human user. This is known as an AI agent: and more specifically a recursive one because it uses results from its previous self-instructions to help it form its subsequent prompts; the first major example of this was Auto-GPT (which uses OpenAI's GPT models): and others have since been developed as well. /  /  / === Multimodality === / Generative transformer-based systems can also be targeted to tasks involving modalities beyond text.  / For example: Microsoft’s “Visual ChatGPT” combines ChatGPT with visual foundation models (VFMs) to enable input or output comprising images as well as text. Also: advances in text-to-speech technology offer powerful tools for audio content creation when used in conjunction with foundational GPT language models. /  /  / === Domain-specificity === / GPT systems can be directed toward particular fields or domains. Some reported examples of such models and apps are as follows:  /  / EinsteinGPT – for sales and marketing domains: to aid with customer relationship management (uses GPT-3.5) / BloombergGPT – for the financial domain: to aid with financial news and information (uses ""freely available"" AI methods: combined with their proprietary data) / Khanmigo – described as a GPT version for tutoring: in the education domain: it aids students using Khan Academy by guiding them through their studies without directly providing answers (powered by GPT-4) / SlackGPT – for the Slack instant-messaging service: to aid with navigating and summarizing discussions on it (uses OpenAI's API) / BioGPT – for the biomedical domain: to aid with biomedical literature text generation and mining (uses GPT-2)Sometimes domain-specificity is accomplished via software plug-ins or add-ons. For example: several different companies have developed particular plugins that interact directly with OpenAI's ChatGPT interface: and Google Workspace has available add-ons such as “GPT for Sheets and Docs”—which is reported to aid use of spreadsheet functionality in Google Sheets.In November 2023: OpenAI announced that it's enabling ChatGPT Plus subscribers to create custom versions of ChatGPT (being called GPTs). These can be tailored for specific domains via prompt engineering: curated datasets: and/or targeted interaction with external tools. Users who register as verified builders are able to publish their custom GPTs for other users: with monetization potential. (This is notably distinct from OpenAI's API service: as this is based internally within OpenAI's platform.) /  /  / == Brand issues == / OpenAI: which created the first generative pre-trained transformer (GPT) in 2018: has recently asserted that “GPT” should be regarded as a brand of OpenAI. In April 2023: OpenAI revised the brand guidelines in its terms of service to indicate that other businesses using its API to run their artificial intelligence (AI) services would no longer be able to include “GPT” in such names or branding. In May 2023: OpenAI engaged a brand management service to notify its API customers of this policy: although these notifications stopped short of making overt legal claims (such as allegations of trademark infringement or demands to cease and desist)."
0.15546560002258047,Who introduced the first GPT and when?,The first GPT was introduced by OpenAI in 2018.,"Score: 0.7909468594368281 / Generative pre-trained transformers (GPT) are a type of large language model (LLM) and a prominent framework for generative artificial intelligence. They are artificial neural networks that are used in natural language processing tasks. GPTs are based on the transformer architecture: pre-trained on large data sets of unlabelled text: and able to generate novel human-like content. As of 2023: most LLMs have these characteristics and are sometimes referred to broadly as GPTs.The first GPT was introduced in 2018 by OpenAI. OpenAI has released very influential GPT foundation models that have been sequentially numbered: to comprise its ""GPT-n"" series. Each of these was significantly more capable than the previous: due to increased size (number of trainable parameters) and training. The most recent of these: GPT-4: was released in March 2023. Such models have been the basis for their more task-specific GPT systems: including models fine-tuned for instruction following—which in turn power the ChatGPT chatbot service.The term ""GPT"" is also used in the names and descriptions of such models developed by others. For example: other GPT foundation models include a series of models created by EleutherAI: and seven models created by Cerebras in 2023. Also: companies in different industries have developed task-specific GPTs in their respective fields: such as Salesforce's ""EinsteinGPT"" (for CRM) and Bloomberg's ""BloombergGPT"" (for finance). /  /  / == History == /  /  / === Initial developments === / Generative pretraining (GP) was a long-established concept in machine learning applications. It was originally used as a form of semi-supervised learning: as the model is trained first on an unlabelled dataset (pretraining step) by learning to generate datapoints in the dataset: and then it is trained to classify a labelled dataset.While the unnormalized linear transformer dates back to 1992: the modern transformer architecture was not available until 2017 when it was published by researchers at Google in a paper ""Attention Is All You Need"". That development led to the emergence of large language models such as BERT in 2018 which was a pre-trained transformer (PT) but not designed to be generative (BERT was an ""encoder-only"" model). Also around that time: in 2018: OpenAI published its article entitled ""Improving Language Understanding by Generative Pre-Training:"" in which it introduced the first generative pre-trained transformer (GPT) system (""GPT-1"").Prior to transformer-based architectures: the best-performing neural NLP (natural language processing) models commonly employed supervised learning from large amounts of manually-labeled data. The reliance on supervised learning limited their use on datasets that were not well-annotated: and also made it prohibitively expensive and time-consuming to train extremely large language models.The semi-supervised approach OpenAI employed to make a large-scale generative system—and was first to do with a transformer model—involved two stages: an unsupervised generative ""pretraining"" stage to set initial parameters using a language modeling objective: and a supervised discriminative ""fine-tuning"" stage to adapt these parameters to a target task. /  /  / === Later developments === / Regarding more recent GPT foundation models: OpenAI published its first versions of GPT-3 in July 2020. There were three models: with 1B: 6.7B: 175B parameters: respectively named babbage: curie: and davinci (giving initials B: C: and D).In July 2021: OpenAI published Codex: a task-specific GPT model targeted for programming applications. This was developed by fine-tuning a 12B parameter version of GPT-3 (different from previous GPT-3 models) using code from GitHub.In March 2022: OpenAI published two versions of GPT-3 that were fine-tuned for instruction-following (instruction-tuned): named davinci-instruct-beta (175B) and text-davinci-001: and then started beta testing code-davinci-002. text-davinci-002 was instruction-tuned from code-davinci-002. Both text-davinci-003 and ChatGPT were released in November 2022: with both building upon text-davinci-002 via reinforcement learning from human feedback (RLHF).","Score: 0.7893402212393734 / Users who register as verified builders are able to publish their custom GPTs for other users: with monetization potential. (This is notably distinct from OpenAI's API service: as this is based internally within OpenAI's platform.) /  /  / == Brand issues == / OpenAI: which created the first generative pre-trained transformer (GPT) in 2018: has recently asserted that “GPT” should be regarded as a brand of OpenAI. In April 2023: OpenAI revised the brand guidelines in its terms of service to indicate that other businesses using its API to run their artificial intelligence (AI) services would no longer be able to include “GPT” in such names or branding. In May 2023: OpenAI engaged a brand management service to notify its API customers of this policy: although these notifications stopped short of making overt legal claims (such as allegations of trademark infringement or demands to cease and desist). As of November 2023: OpenAI still prohibits its API licensees from naming their own products with ""GPT:"" but it has begun enabling its ChatGPT Plus subscribers to make ""custom versions of ChatGPT"" that are being called GPTs on the OpenAI site. OpenAI's terms of service says that its subscribers may use ""GPT"" in the names of these: although it's ""discouraged.""Relatedly: OpenAI has applied to the United States Patent and Trademark Office (USPTO) to seek domestic trademark registration for the term “GPT” in the field of AI. OpenAI sought to expedite handling of its application: but the USPTO declined that request in April 2023. In May 2023: the USPTO responded to the application with a determination that ""GPT"" was both descriptive and generic. As of November 2023: OpenAI continues to pursue its argument through the available processes. Regardless: failure to obtain a registered U.S. trademark does not preclude some level of common-law trademark rights in the U.S.: and/or trademark rights in other countries.For any given type or scope of trademark protection in the U.S.: OpenAI would need to establish that the term is actually “distinctive” to their specific offerings in addition to being a broader technical term for the kind of technology. Some media reports suggested that OpenAI may be able to obtain trademark registration based indirectly on the fame of its GPT-based chatbot product: ChatGPT: for which OpenAI has separately sought protection (and which it has sought to enforce more strongly). Other reports have indicated that registration for the bare term “GPT” seems unlikely to be granted: as it is used frequently as a common term to refer simply to AI systems that involve generative pre-trained transformers. In any event: to whatever extent exclusive rights in the term may occur the U.S.: others would need to avoid using it for similar products or services in ways likely to cause confusion. If such rights ever became broad enough to implicate other well-established uses in the field: the trademark doctrine of descriptive fair use could still preserve some room to continue non-brand-related usage. /  /  / == Selected bibliography == / This section lists the main official publications from OpenAI and Microsoft on their GPT models. /  / GPT-1: report: GitHub release. / GPT-2: blog announcement: report on its decision of ""staged release"": GitHub release. / GPT-3: report. No GitHub or any other form of code release thenceforth. / webGPT: blog announcement: report: / InstructGPT: blog announcement: report. / ChatGPT: blog announcement (no report). / GPT-4: blog announcement: reports: model card. /  /  / == See also == / Cyc / Gemini /  /  / == References =="
0.17053659999510273,What are some task-specific GPT systems developed by OpenAI? ,"OpenAI has developed task-specific GPT systems including models fine-tuned for instruction following, which power the ChatGPT chatbot service.","Score: 0.8688603836252451 / Advantages this had over the bare foundational models included higher accuracy: less negative/toxic sentiment: and generally better alignment with user needs. Hence: OpenAI began using this as the basis for its API service offerings. Other instruction-tuned models have been released by others: including a fully open version.Another (related) kind of task-specific models are chatbots: which engage in human-like conversation. In November 2022: OpenAI launched ChatGPT—an online chat interface powered by an instruction-tuned language model trained in a similar fashion to InstructGPT. They trained this model using RLHF: with human AI trainers providing conversations in which they played both the user and the AI: and mixed this new dialogue dataset with the InstructGPT dataset for a conversational format suitable for a chatbot. Other major chatbots currently include Microsoft's Bing Chat: which uses OpenAI's GPT-4 (as part of a broader close collaboration between OpenAI and Microsoft): and Google's competing chatbot Bard (initially based on their LaMDA family of conversation-trained language models: with plans to switch to PaLM).Yet another kind of task that a GPT can be used for is the meta-task of generating its own instructions: like developing a series of prompts for 'itself' to be able to effectuate a more general goal given by a human user. This is known as an AI agent: and more specifically a recursive one because it uses results from its previous self-instructions to help it form its subsequent prompts; the first major example of this was Auto-GPT (which uses OpenAI's GPT models): and others have since been developed as well. /  /  / === Multimodality === / Generative transformer-based systems can also be targeted to tasks involving modalities beyond text.  / For example: Microsoft’s “Visual ChatGPT” combines ChatGPT with visual foundation models (VFMs) to enable input or output comprising images as well as text. Also: advances in text-to-speech technology offer powerful tools for audio content creation when used in conjunction with foundational GPT language models. /  /  / === Domain-specificity === / GPT systems can be directed toward particular fields or domains. Some reported examples of such models and apps are as follows:  /  / EinsteinGPT – for sales and marketing domains: to aid with customer relationship management (uses GPT-3.5) / BloombergGPT – for the financial domain: to aid with financial news and information (uses ""freely available"" AI methods: combined with their proprietary data) / Khanmigo – described as a GPT version for tutoring: in the education domain: it aids students using Khan Academy by guiding them through their studies without directly providing answers (powered by GPT-4) / SlackGPT – for the Slack instant-messaging service: to aid with navigating and summarizing discussions on it (uses OpenAI's API) / BioGPT – for the biomedical domain: to aid with biomedical literature text generation and mining (uses GPT-2)Sometimes domain-specificity is accomplished via software plug-ins or add-ons. For example: several different companies have developed particular plugins that interact directly with OpenAI's ChatGPT interface: and Google Workspace has available add-ons such as “GPT for Sheets and Docs”—which is reported to aid use of spreadsheet functionality in Google Sheets.In November 2023: OpenAI announced that it's enabling ChatGPT Plus subscribers to create custom versions of ChatGPT (being called GPTs). These can be tailored for specific domains via prompt engineering: curated datasets: and/or targeted interaction with external tools. Users who register as verified builders are able to publish their custom GPTs for other users: with monetization potential. (This is notably distinct from OpenAI's API service: as this is based internally within OpenAI's platform.) /  /  / == Brand issues == / OpenAI: which created the first generative pre-trained transformer (GPT) in 2018: has recently asserted that “GPT” should be regarded as a brand of OpenAI. In April 2023: OpenAI revised the brand guidelines in its terms of service to indicate that other businesses using its API to run their artificial intelligence (AI) services would no longer be able to include “GPT” in such names or branding. In May 2023: OpenAI engaged a brand management service to notify its API customers of this policy: although these notifications stopped short of making overt legal claims (such as allegations of trademark infringement or demands to cease and desist).","Score: 0.8538032318631482 / This was developed by fine-tuning a 12B parameter version of GPT-3 (different from previous GPT-3 models) using code from GitHub.In March 2022: OpenAI published two versions of GPT-3 that were fine-tuned for instruction-following (instruction-tuned): named davinci-instruct-beta (175B) and text-davinci-001: and then started beta testing code-davinci-002. text-davinci-002 was instruction-tuned from code-davinci-002. Both text-davinci-003 and ChatGPT were released in November 2022: with both building upon text-davinci-002 via reinforcement learning from human feedback (RLHF). text-davinci-003 is trained for following instructions (like its predecessors): whereas ChatGPT is further trained for conversational interaction with a human user.OpenAI's most recent GPT foundation model: GPT-4: was released on March 14: 2023. It can be accessed directly by users via a premium version of ChatGPT: and is available to developers for incorporation into other products and services via OpenAI's API. Other producers of GPT foundation models include EleutherAI (with a series of models starting in March 2021) and Cerebras (with seven models released in March 2023). /  /  / == Foundational models == / A foundational model is an AI model trained on broad data at scale such that it can be adapted to a wide range of downstream tasks.Thus far: the most notable GPT foundation models have been from OpenAI's GPT-n series. The most recent from that is GPT-4: for which OpenAI declined to publish the size or training details (citing ""the competitive landscape and the safety implications of large-scale models""). / Other such models include Google's PaLM: a broad foundation model that has been compared to GPT-3 and has recently been made available to developers via an API: and Together's GPT-JT: which has been reported as the closest-performing open-source alternative to GPT-3 (and is derived from earlier open-source GPTs). Meta AI (formerly Facebook) also has a generative transformer-based foundational large language model: known as LLaMA.Foundational GPTs can also employ modalities other than text: for input and/or output. GPT-4 is a multi-modal LLM that is capable of processing text and image input (though its output is limited to text). Regarding multimodal output: some generative transformer-based models are used for text-to-image technologies such as diffusion and parallel decoding. Such kinds of models can serve as visual foundation models (VFMs) for developing downstream systems that can work with images. /  /  / == Task-specific models == / A foundational GPT model can be further adapted to produce more targeted systems directed to specific tasks and/or subject-matter domains. Methods for such adaptation can include additional fine-tuning (beyond that done for the foundation model) as well as certain forms of prompt engineering.An important example of this is fine-tuning models to follow instructions: which is of course a fairly broad task but more targeted than a foundation model. In January 2022: OpenAI introduced ""InstructGPT""—a series of models which were fine-tuned to follow instructions using a combination of supervised training and reinforcement learning from human feedback (RLHF) on base GPT-3 language models. Advantages this had over the bare foundational models included higher accuracy: less negative/toxic sentiment: and generally better alignment with user needs. Hence: OpenAI began using this as the basis for its API service offerings. Other instruction-tuned models have been released by others: including a fully open version.Another (related) kind of task-specific models are chatbots: which engage in human-like conversation. In November 2022: OpenAI launched ChatGPT—an online chat interface powered by an instruction-tuned language model trained in a similar fashion to InstructGPT. They trained this model using RLHF: with human AI trainers providing conversations in which they played both the user and the AI: and mixed this new dialogue dataset with the InstructGPT dataset for a conversational format suitable for a chatbot."
0.1263401000178419,What companies have developed GPT foundation models other than OpenAI?,EleutherAI and Cerebras have developed GPT foundation models aside from OpenAI.,"Score: 0.8342423873544814 / Advantages this had over the bare foundational models included higher accuracy: less negative/toxic sentiment: and generally better alignment with user needs. Hence: OpenAI began using this as the basis for its API service offerings. Other instruction-tuned models have been released by others: including a fully open version.Another (related) kind of task-specific models are chatbots: which engage in human-like conversation. In November 2022: OpenAI launched ChatGPT—an online chat interface powered by an instruction-tuned language model trained in a similar fashion to InstructGPT. They trained this model using RLHF: with human AI trainers providing conversations in which they played both the user and the AI: and mixed this new dialogue dataset with the InstructGPT dataset for a conversational format suitable for a chatbot. Other major chatbots currently include Microsoft's Bing Chat: which uses OpenAI's GPT-4 (as part of a broader close collaboration between OpenAI and Microsoft): and Google's competing chatbot Bard (initially based on their LaMDA family of conversation-trained language models: with plans to switch to PaLM).Yet another kind of task that a GPT can be used for is the meta-task of generating its own instructions: like developing a series of prompts for 'itself' to be able to effectuate a more general goal given by a human user. This is known as an AI agent: and more specifically a recursive one because it uses results from its previous self-instructions to help it form its subsequent prompts; the first major example of this was Auto-GPT (which uses OpenAI's GPT models): and others have since been developed as well. /  /  / === Multimodality === / Generative transformer-based systems can also be targeted to tasks involving modalities beyond text.  / For example: Microsoft’s “Visual ChatGPT” combines ChatGPT with visual foundation models (VFMs) to enable input or output comprising images as well as text. Also: advances in text-to-speech technology offer powerful tools for audio content creation when used in conjunction with foundational GPT language models. /  /  / === Domain-specificity === / GPT systems can be directed toward particular fields or domains. Some reported examples of such models and apps are as follows:  /  / EinsteinGPT – for sales and marketing domains: to aid with customer relationship management (uses GPT-3.5) / BloombergGPT – for the financial domain: to aid with financial news and information (uses ""freely available"" AI methods: combined with their proprietary data) / Khanmigo – described as a GPT version for tutoring: in the education domain: it aids students using Khan Academy by guiding them through their studies without directly providing answers (powered by GPT-4) / SlackGPT – for the Slack instant-messaging service: to aid with navigating and summarizing discussions on it (uses OpenAI's API) / BioGPT – for the biomedical domain: to aid with biomedical literature text generation and mining (uses GPT-2)Sometimes domain-specificity is accomplished via software plug-ins or add-ons. For example: several different companies have developed particular plugins that interact directly with OpenAI's ChatGPT interface: and Google Workspace has available add-ons such as “GPT for Sheets and Docs”—which is reported to aid use of spreadsheet functionality in Google Sheets.In November 2023: OpenAI announced that it's enabling ChatGPT Plus subscribers to create custom versions of ChatGPT (being called GPTs). These can be tailored for specific domains via prompt engineering: curated datasets: and/or targeted interaction with external tools. Users who register as verified builders are able to publish their custom GPTs for other users: with monetization potential. (This is notably distinct from OpenAI's API service: as this is based internally within OpenAI's platform.) /  /  / == Brand issues == / OpenAI: which created the first generative pre-trained transformer (GPT) in 2018: has recently asserted that “GPT” should be regarded as a brand of OpenAI. In April 2023: OpenAI revised the brand guidelines in its terms of service to indicate that other businesses using its API to run their artificial intelligence (AI) services would no longer be able to include “GPT” in such names or branding. In May 2023: OpenAI engaged a brand management service to notify its API customers of this policy: although these notifications stopped short of making overt legal claims (such as allegations of trademark infringement or demands to cease and desist).","Score: 0.8333889719651951 / This was developed by fine-tuning a 12B parameter version of GPT-3 (different from previous GPT-3 models) using code from GitHub.In March 2022: OpenAI published two versions of GPT-3 that were fine-tuned for instruction-following (instruction-tuned): named davinci-instruct-beta (175B) and text-davinci-001: and then started beta testing code-davinci-002. text-davinci-002 was instruction-tuned from code-davinci-002. Both text-davinci-003 and ChatGPT were released in November 2022: with both building upon text-davinci-002 via reinforcement learning from human feedback (RLHF). text-davinci-003 is trained for following instructions (like its predecessors): whereas ChatGPT is further trained for conversational interaction with a human user.OpenAI's most recent GPT foundation model: GPT-4: was released on March 14: 2023. It can be accessed directly by users via a premium version of ChatGPT: and is available to developers for incorporation into other products and services via OpenAI's API. Other producers of GPT foundation models include EleutherAI (with a series of models starting in March 2021) and Cerebras (with seven models released in March 2023). /  /  / == Foundational models == / A foundational model is an AI model trained on broad data at scale such that it can be adapted to a wide range of downstream tasks.Thus far: the most notable GPT foundation models have been from OpenAI's GPT-n series. The most recent from that is GPT-4: for which OpenAI declined to publish the size or training details (citing ""the competitive landscape and the safety implications of large-scale models""). / Other such models include Google's PaLM: a broad foundation model that has been compared to GPT-3 and has recently been made available to developers via an API: and Together's GPT-JT: which has been reported as the closest-performing open-source alternative to GPT-3 (and is derived from earlier open-source GPTs). Meta AI (formerly Facebook) also has a generative transformer-based foundational large language model: known as LLaMA.Foundational GPTs can also employ modalities other than text: for input and/or output. GPT-4 is a multi-modal LLM that is capable of processing text and image input (though its output is limited to text). Regarding multimodal output: some generative transformer-based models are used for text-to-image technologies such as diffusion and parallel decoding. Such kinds of models can serve as visual foundation models (VFMs) for developing downstream systems that can work with images. /  /  / == Task-specific models == / A foundational GPT model can be further adapted to produce more targeted systems directed to specific tasks and/or subject-matter domains. Methods for such adaptation can include additional fine-tuning (beyond that done for the foundation model) as well as certain forms of prompt engineering.An important example of this is fine-tuning models to follow instructions: which is of course a fairly broad task but more targeted than a foundation model. In January 2022: OpenAI introduced ""InstructGPT""—a series of models which were fine-tuned to follow instructions using a combination of supervised training and reinforcement learning from human feedback (RLHF) on base GPT-3 language models. Advantages this had over the bare foundational models included higher accuracy: less negative/toxic sentiment: and generally better alignment with user needs. Hence: OpenAI began using this as the basis for its API service offerings. Other instruction-tuned models have been released by others: including a fully open version.Another (related) kind of task-specific models are chatbots: which engage in human-like conversation. In November 2022: OpenAI launched ChatGPT—an online chat interface powered by an instruction-tuned language model trained in a similar fashion to InstructGPT. They trained this model using RLHF: with human AI trainers providing conversations in which they played both the user and the AI: and mixed this new dialogue dataset with the InstructGPT dataset for a conversational format suitable for a chatbot."
0.15797220001695678,What are the foundational models?,"Foundational models are AI models trained on broad data at scale so that they can be adapted to a wide range of downstream tasks. Notable GPT foundational models have been from OpenAI's GPT-n series. Other such models include Google's PaLM, Together's GPT-JT, and Meta AI's transformer-based large language model known as LLaMA.","Score: 0.7960874079668868 / This was developed by fine-tuning a 12B parameter version of GPT-3 (different from previous GPT-3 models) using code from GitHub.In March 2022: OpenAI published two versions of GPT-3 that were fine-tuned for instruction-following (instruction-tuned): named davinci-instruct-beta (175B) and text-davinci-001: and then started beta testing code-davinci-002. text-davinci-002 was instruction-tuned from code-davinci-002. Both text-davinci-003 and ChatGPT were released in November 2022: with both building upon text-davinci-002 via reinforcement learning from human feedback (RLHF). text-davinci-003 is trained for following instructions (like its predecessors): whereas ChatGPT is further trained for conversational interaction with a human user.OpenAI's most recent GPT foundation model: GPT-4: was released on March 14: 2023. It can be accessed directly by users via a premium version of ChatGPT: and is available to developers for incorporation into other products and services via OpenAI's API. Other producers of GPT foundation models include EleutherAI (with a series of models starting in March 2021) and Cerebras (with seven models released in March 2023). /  /  / == Foundational models == / A foundational model is an AI model trained on broad data at scale such that it can be adapted to a wide range of downstream tasks.Thus far: the most notable GPT foundation models have been from OpenAI's GPT-n series. The most recent from that is GPT-4: for which OpenAI declined to publish the size or training details (citing ""the competitive landscape and the safety implications of large-scale models""). / Other such models include Google's PaLM: a broad foundation model that has been compared to GPT-3 and has recently been made available to developers via an API: and Together's GPT-JT: which has been reported as the closest-performing open-source alternative to GPT-3 (and is derived from earlier open-source GPTs). Meta AI (formerly Facebook) also has a generative transformer-based foundational large language model: known as LLaMA.Foundational GPTs can also employ modalities other than text: for input and/or output. GPT-4 is a multi-modal LLM that is capable of processing text and image input (though its output is limited to text). Regarding multimodal output: some generative transformer-based models are used for text-to-image technologies such as diffusion and parallel decoding. Such kinds of models can serve as visual foundation models (VFMs) for developing downstream systems that can work with images. /  /  / == Task-specific models == / A foundational GPT model can be further adapted to produce more targeted systems directed to specific tasks and/or subject-matter domains. Methods for such adaptation can include additional fine-tuning (beyond that done for the foundation model) as well as certain forms of prompt engineering.An important example of this is fine-tuning models to follow instructions: which is of course a fairly broad task but more targeted than a foundation model. In January 2022: OpenAI introduced ""InstructGPT""—a series of models which were fine-tuned to follow instructions using a combination of supervised training and reinforcement learning from human feedback (RLHF) on base GPT-3 language models. Advantages this had over the bare foundational models included higher accuracy: less negative/toxic sentiment: and generally better alignment with user needs. Hence: OpenAI began using this as the basis for its API service offerings. Other instruction-tuned models have been released by others: including a fully open version.Another (related) kind of task-specific models are chatbots: which engage in human-like conversation. In November 2022: OpenAI launched ChatGPT—an online chat interface powered by an instruction-tuned language model trained in a similar fashion to InstructGPT. They trained this model using RLHF: with human AI trainers providing conversations in which they played both the user and the AI: and mixed this new dialogue dataset with the InstructGPT dataset for a conversational format suitable for a chatbot.","Score: 0.7840695600388427 / The NTL Model outlines how specific neural structures of the human brain shape the nature of thought and language and in turn what are the computational properties of such neural systems that can be applied to model thought and language in a computer system. After a framework for modeling language in a computer systems was established: the focus shifted to establishing frameworks for computer systems to generate language with acceptable grammar. In his 2014 book titled The Language Myth: Why Language Is Not An Instinct: British cognitive linguist and digital communication technologist Vyvyan Evans mapped out the role of probabilistic context-free grammar (PCFG) in enabling NLP to model cognitive patterns and generate human like language.  /  /  / == Evaluation == /  /  / === Perplexity === / The most commonly used measure of a language model's performance is its perplexity on a given text corpus. Perplexity is a measure of how well a model is able to predict the contents of a dataset; the higher the likelihood the model assigns to the dataset: the lower the perplexity. Mathematically: perplexity is defined as the exponential of the average negative log likelihood per token:here N{\displaystyle N} is the number of tokens in the text corpus: and ""context for token i{\displaystyle i}"" depends on the specific type of LLM used. If the LLM is autoregressive: then ""context for token i{\displaystyle i}"" is the segment of text appearing before token i{\displaystyle i}. If the LLM is masked: then ""context for token i{\displaystyle i}"" is the segment of text surrounding token i{\displaystyle i}. / Because language models may overfit to their training data: models are usually evaluated by their perplexity on a test set of unseen data. This presents particular challenges for the evaluation of large language models. As they are trained on increasingly large corpora of text largely scraped from the web: it becomes increasingly likely that models' training data inadvertently includes portions of any given test set. /  /  / ==== BPW: BPC: and BPT ==== / In information theory: the concept of entropy is intricately linked to perplexity: a relationship notably established by Claude Shannon. This relationship is mathematically expressed as Entropy=log2⁡(Perplexity){\displaystyle {\text{Entropy}}=\log _{2}({\text{Perplexity}})}. / Entropy: in this context: is commonly quantified in terms of bits per word (BPW) or bits per character (BPC): which hinges on whether the language model utilizes word-based or character-based tokenization. / Notably: in the case of larger language models that predominantly employ sub-word tokenization: bits per token (BPT) emerges as a seemingly more appropriate measure. However: due to the variance in tokenization methods across different Large Language Models (LLMs): BPT does not serve as a reliable metric for comparative analysis among diverse models. To convert BPT into BPW: one can multiply it by the average number of tokens per word. / In the evaluation and comparison of language models: cross-entropy is generally the preferred metric over entropy. The underlying principle is that a lower BPW is indicative of a model's enhanced capability for compression. This: in turn: reflects the model's proficiency in making accurate predictions. /  /  / === Task-specific datasets and benchmarks === / A large number of testing datasets and benchmarks have also been developed to evaluate the capabilities of language models on more specific downstream tasks. Tests may be designed to evaluate a variety of capabilities: including general knowledge: commonsense reasoning: and mathematical problem-solving. / One broad category of evaluation dataset is question answering datasets: consisting of pairs of questions and correct answers: for example: (""Have the San Jose Sharks won the Stanley Cup?"": ""No""). A question answering task is considered ""open book"" if the model's prompt includes text from which the expected answer can be derived (for example: the previous question could be adjoined with some text which includes the sentence ""The Sharks have advanced to the Stanley Cup finals once: losing to the Pittsburgh Penguins in 2016.""). Otherwise: the task is considered ""closed book"": and the model must draw on knowledge retained during training. Some examples of commonly used question answering datasets include TruthfulQA: Web Questions: TriviaQA: and SQuAD.Evaluation datasets may also take the form of text completion: having the model select the most likely word or sentence to complete a prompt: for example: ""Alice was friends with Bob. Alice went to visit her friend: ____"".Some composite benchmarks have also been developed which combine a diversity of different evaluation datasets and tasks."
0.12305890000425279,What is a Graph Neural Network (GNN)?,"A Graph Neural Network (GNN) is a type of artificial neural network designed for processing data that can be represented as graphs. It falls under the broader field of ""geometric deep learning"". GNNs use pairwise message passing as the key design element, enabling graph nodes to iteratively update their representations by exchanging information with their neighbors.","Score: 0.9034233793135122 / A graph neural network (GNN) belongs to a class of artificial neural networks for processing data that can be represented as graphs. / In the more general subject of ""geometric deep learning"": certain existing neural network architectures can be interpreted as GNNs operating on suitably defined graphs. A convolutional neural network layer: in the context of computer vision: can be seen as a GNN applied to graphs whose nodes are pixels and only adjacent pixels are connected by edges in the graph. A transformer layer: in natural language processing: can be seen as a GNN applied to complete graphs whose nodes are words or tokens in a passage of natural language text. / The key design element of GNNs is the use of pairwise message passing: such that graph nodes iteratively update their representations by exchanging information with their neighbors. Since their inception: several different GNN architectures have been proposed: which implement different flavors of message passing:  started by recursive or convolutional constructive approaches. As of 2022: whether it is possible to define GNN architectures ""going beyond"" message passing: or if every GNN can be built on message passing over suitably defined graphs: is an open research question.Relevant application domains for GNNs include Natural Language Processing: social networks: citation networks: molecular biology: chemistry: physics and NP-hard combinatorial optimization problems.Several open source libraries implementing graph neural networks are available: such as PyTorch Geometric (PyTorch): TensorFlow GNN (TensorFlow): jraph (Google JAX): and GraphNeuralNetworks.jl/GeometricFlux.jl (Julia: Flux). /  /  / == Architecture == / The architecture of a generic GNN implements the following fundamental layers: / Permutation equivariant: a permutation equivariant layer maps a representation of a graph into an updated representation of the same graph. In the literature: permutation equivariant layers are implemented via pairwise message passing between graph nodes. Intuitively: in a message passing layer: nodes update their representations by aggregating the messages received from their immediate neighbours. As such: each message passing layer increases the receptive field of the GNN by one hop. / Local pooling: a local pooling layer coarsens the graph via downsampling. Local pooling is used to increase the receptive field of a GNN: in a similar fashion to pooling layers in convolutional neural networks. Examples include k-nearest neighbours pooling: top-k pooling: and self-attention pooling. / Global pooling: a global pooling layer: also known as readout layer: provides fixed-size representation of the whole graph. The global pooling layer must be permutation invariant: such that permutations in the ordering of graph nodes and edges do not alter the final output. Examples include element-wise sum: mean or maximum.It has been demonstrated that GNNs cannot be more expressive than the Weisfeiler–Leman Graph Isomorphism Test. In practice: this means that there exist different graph structures (e.g.: molecules with the same atoms but different bonds) that cannot be distinguished by GNNs. More powerful GNNs operating on higher-dimension geometries such as simplicial complexes can be designed. As of 2022: whether or not future architectures will overcome the message passing primitive is an open research question. /  /  / == Message passing layers == / Message passing layers are permutation-equivariant layers mapping a graph into an updated representation of the same graph. Formally: they can be expressed as message passing neural networks (MPNNs).Let G=(V:E){\displaystyle G=(V:E)} be a graph: where V{\displaystyle V} is the node set and E{\displaystyle E} is the edge set. Let Nu{\displaystyle N_{u}} be the neighbourhood of some node u∈V{\displaystyle u\in V}. Additionally: let xu{\displaystyle \mathbf {x} _{u}} be the features of node u∈V{\displaystyle u\in V}: and euv{\displaystyle \mathbf {e} _{uv}} be the features of edge (u:v)∈E{\displaystyle (u:v)\in E}.","Score: 0.8654598055554211 / A limitation of GCNs is that they do not allow multidimensional edge features euv{\displaystyle \mathbf {e} _{uv}}. It is however possible to associate scalar weights wuv{\displaystyle w_{uv}} to each edge by imposing Auv=wuv{\displaystyle A_{uv}=w_{uv}}: i.e.: by setting each nonzero entry in the adjacency matrix equal to the weight of the corresponding edge. /  /  / === Graph attention network === / The graph attention network (GAT) was introduced by Petar Veličković et al. in 2018.Graph attention network is a combination of a graph neural network and an attention layer. / The implementation of attention layer in graphical neural networks helps provide attention or focus to the important information from the data instead of focusing on the whole data. / A multi-head GAT layer can be expressed as follows: /  / hu=‖k=1Kσ(∑v∈NuαuvWkxv){\displaystyle \mathbf {h} _{u}={\overset {K}{\underset {k=1}{\Big \Vert }}}\sigma \left(\sum _{v\in N_{u}}\alpha _{uv}\mathbf {W} ^{k}\mathbf {x} _{v}\right)}where K{\displaystyle K} is the number of attention heads: ‖{\displaystyle {\Big \Vert }} denotes vector concatenation: σ(⋅){\displaystyle \sigma (\cdot )} is an activation function (e.g.: ReLU): αij{\displaystyle \alpha _{ij}} are attention coefficients: and Wk{\displaystyle W^{k}} is a matrix of trainable parameters for the k{\displaystyle k}-th attention head. / For the final GAT layer: the outputs from each attention head are averaged before the application of the activation function. Formally: the final GAT layer can be written as: /  / hu=σ(1K∑k=1K∑v∈NuαuvWkxv){\displaystyle \mathbf {h} _{u}=\sigma \left({\frac {1}{K}}\sum _{k=1}^{K}\sum _{v\in N_{u}}\alpha _{uv}\mathbf {W} ^{k}\mathbf {x} _{v}\right)}Attention in Machine Learning is a technique that mimics cognitive attention. In the context of learning on graphs: the attention coefficient αuv{\displaystyle \alpha _{uv}} measures how important is node u∈V{\displaystyle u\in V} to node v∈V{\displaystyle v\in V}. / Normalized attention coefficients are computed as follows: /  / αuv=exp⁡(LeakyReLU(aT[Whu‖Whv‖euv]))∑z∈Nuexp⁡(LeakyReLU(aT[Whu‖Whz‖euz])){\displaystyle \alpha _{uv}={\frac {\exp({\text{LeakyReLU}}\left(\mathbf {a} ^{T}[\mathbf {W} \mathbf {h} _{u}\Vert \mathbf {W} \mathbf {h} _{v}\Vert \mathbf {e} _{uv}]\right))}{\sum _{z\in N_{u}}\exp({\text{LeakyReLU}}\left(\mathbf {a} ^{T}[\mathbf {W} \mathbf {h} _{u}\Vert \mathbf {W} \mathbf {h} _{z}\Vert \mathbf {e} _{uz}]\right))}}}where a{\displaystyle \mathbf {a} } is a vector of learnable weights: ⋅T{\displaystyle \cdot ^{T}} indicates transposition: and LeakyReLU{\displaystyle {\text{LeakyReLU}}} is a modified ReLU activation function. Attention coefficients are normalized to make them easily comparable across different nodes.A GCN can be seen as a special case of a GAT where attention coefficients are not learnable: but fixed and equal to the edge weights wuv{\displaystyle w_{uv}}. /  /  / === Gated graph sequence neural network === / The gated graph sequence neural network (GGS-NN) was introduced by Yujia Li et al. in 2015. The GGS-NN extends the GNN formulation by Scarselli et al. to output sequences."
0.12996859999839216,How can convolutional neural networks and transformer layers be interpreted in the context of GNNs?,"In geometric deep learning, convolutional neural networks and transformer layers can be interpreted as GNNs operating on specifically defined graphs. For instance, a convolutional neural network layer, in computer vision context, can be seen as a GNN applied to graphs whose nodes are pixels and where only adjacent pixels are connected by edges. Similarly, a transformer layer in natural language processing can be seen as a GNN applied to complete graphs whose nodes are words or tokens in a text passage.","Score: 0.8785587663066172 / A graph neural network (GNN) belongs to a class of artificial neural networks for processing data that can be represented as graphs. / In the more general subject of ""geometric deep learning"": certain existing neural network architectures can be interpreted as GNNs operating on suitably defined graphs. A convolutional neural network layer: in the context of computer vision: can be seen as a GNN applied to graphs whose nodes are pixels and only adjacent pixels are connected by edges in the graph. A transformer layer: in natural language processing: can be seen as a GNN applied to complete graphs whose nodes are words or tokens in a passage of natural language text. / The key design element of GNNs is the use of pairwise message passing: such that graph nodes iteratively update their representations by exchanging information with their neighbors. Since their inception: several different GNN architectures have been proposed: which implement different flavors of message passing:  started by recursive or convolutional constructive approaches. As of 2022: whether it is possible to define GNN architectures ""going beyond"" message passing: or if every GNN can be built on message passing over suitably defined graphs: is an open research question.Relevant application domains for GNNs include Natural Language Processing: social networks: citation networks: molecular biology: chemistry: physics and NP-hard combinatorial optimization problems.Several open source libraries implementing graph neural networks are available: such as PyTorch Geometric (PyTorch): TensorFlow GNN (TensorFlow): jraph (Google JAX): and GraphNeuralNetworks.jl/GeometricFlux.jl (Julia: Flux). /  /  / == Architecture == / The architecture of a generic GNN implements the following fundamental layers: / Permutation equivariant: a permutation equivariant layer maps a representation of a graph into an updated representation of the same graph. In the literature: permutation equivariant layers are implemented via pairwise message passing between graph nodes. Intuitively: in a message passing layer: nodes update their representations by aggregating the messages received from their immediate neighbours. As such: each message passing layer increases the receptive field of the GNN by one hop. / Local pooling: a local pooling layer coarsens the graph via downsampling. Local pooling is used to increase the receptive field of a GNN: in a similar fashion to pooling layers in convolutional neural networks. Examples include k-nearest neighbours pooling: top-k pooling: and self-attention pooling. / Global pooling: a global pooling layer: also known as readout layer: provides fixed-size representation of the whole graph. The global pooling layer must be permutation invariant: such that permutations in the ordering of graph nodes and edges do not alter the final output. Examples include element-wise sum: mean or maximum.It has been demonstrated that GNNs cannot be more expressive than the Weisfeiler–Leman Graph Isomorphism Test. In practice: this means that there exist different graph structures (e.g.: molecules with the same atoms but different bonds) that cannot be distinguished by GNNs. More powerful GNNs operating on higher-dimension geometries such as simplicial complexes can be designed. As of 2022: whether or not future architectures will overcome the message passing primitive is an open research question. /  /  / == Message passing layers == / Message passing layers are permutation-equivariant layers mapping a graph into an updated representation of the same graph. Formally: they can be expressed as message passing neural networks (MPNNs).Let G=(V:E){\displaystyle G=(V:E)} be a graph: where V{\displaystyle V} is the node set and E{\displaystyle E} is the edge set. Let Nu{\displaystyle N_{u}} be the neighbourhood of some node u∈V{\displaystyle u\in V}. Additionally: let xu{\displaystyle \mathbf {x} _{u}} be the features of node u∈V{\displaystyle u\in V}: and euv{\displaystyle \mathbf {e} _{uv}} be the features of edge (u:v)∈E{\displaystyle (u:v)\in E}.","Score: 0.8484270519126517 / == Message passing layers == / Message passing layers are permutation-equivariant layers mapping a graph into an updated representation of the same graph. Formally: they can be expressed as message passing neural networks (MPNNs).Let G=(V:E){\displaystyle G=(V:E)} be a graph: where V{\displaystyle V} is the node set and E{\displaystyle E} is the edge set. Let Nu{\displaystyle N_{u}} be the neighbourhood of some node u∈V{\displaystyle u\in V}. Additionally: let xu{\displaystyle \mathbf {x} _{u}} be the features of node u∈V{\displaystyle u\in V}: and euv{\displaystyle \mathbf {e} _{uv}} be the features of edge (u:v)∈E{\displaystyle (u:v)\in E}. An MPNN layer can be expressed as follows: / hu=ϕ(xu:⨁v∈Nuψ(xu:xv:euv)){\displaystyle \mathbf {h} _{u}=\phi \left(\mathbf {x} _{u}:\bigoplus _{v\in N_{u}}\psi (\mathbf {x} _{u}:\mathbf {x} _{v}:\mathbf {e} _{uv})\right)}where ϕ{\displaystyle \phi } and ψ{\displaystyle \psi } are differentiable functions (e.g.: artificial neural networks): and ⨁{\displaystyle \bigoplus } is a permutation invariant aggregation operator that can accept an arbitrary number of inputs (e.g.: element-wise sum: mean: or max). In particular: ϕ{\displaystyle \phi } and ψ{\displaystyle \psi } are referred to as update and message functions: respectively. Intuitively: in an MPNN computational block: graph nodes update their representations by aggregating the messages received from their neighbours. / The outputs of one or more MPNN layers are node representations hu{\displaystyle \mathbf {h} _{u}} for each node u∈V{\displaystyle u\in V} in the graph. Node representations can be employed for any downstream task: such as node/graph classification or edge prediction. / Graph nodes in an MPNN update their representation aggregating information from their immediate neighbours. As such: stacking n{\displaystyle n} MPNN layers means that one node will be able to communicate with nodes that are at most n{\displaystyle n} ""hops"" away. In principle: to ensure that every node receives information from every other node: one would need to stack a number of MPNN layers equal to the graph diameter. However: stacking many MPNN layers may cause issues such as oversmoothing and oversquashing. Oversmoothing refers to the issue of node representations becoming indistinguishable. Oversquashing refers to the bottleneck that is created by squeezing long-range dependencies into fixed-size representations. Countermeasures such as skip connections (as in residual neural networks): gated update rules and jumping knowledge can mitigate oversmoothing. Modifying the final layer to be a fully-adjacent layer: i.e.: by considering the graph as a complete graph: can mitigate oversquashing in problems where long-range dependencies are required.Other ""flavours"" of MPNN have been developed in the literature: such as graph convolutional networks and graph attention networks: whose definitions can be expressed in terms of the MPNN formalism. /  /  / === Graph convolutional network === / The graph convolutional network (GCN) was first introduced by Thomas Kipf and Max Welling in 2017.A GCN layer defines a first-order approximation of a localized spectral filter on graphs. GCNs can be understood as a generalization of convolutional neural networks to graph-structured data."
0.16292830000747927,What is the role of message passing in GNNs?,"In GNNs, message passing is the key design element wherein graph nodes update their representations by exchanging information with their neighbors iteratively. Different GNN architectures implement different kinds of message passing. This aspect of defining GNN architectures ""going beyond"" message passing or basing every GNN on message passing over appropriately defined graphs is still a subject of open research.","Score: 0.8583475242747136 / == Message passing layers == / Message passing layers are permutation-equivariant layers mapping a graph into an updated representation of the same graph. Formally: they can be expressed as message passing neural networks (MPNNs).Let G=(V:E){\displaystyle G=(V:E)} be a graph: where V{\displaystyle V} is the node set and E{\displaystyle E} is the edge set. Let Nu{\displaystyle N_{u}} be the neighbourhood of some node u∈V{\displaystyle u\in V}. Additionally: let xu{\displaystyle \mathbf {x} _{u}} be the features of node u∈V{\displaystyle u\in V}: and euv{\displaystyle \mathbf {e} _{uv}} be the features of edge (u:v)∈E{\displaystyle (u:v)\in E}. An MPNN layer can be expressed as follows: / hu=ϕ(xu:⨁v∈Nuψ(xu:xv:euv)){\displaystyle \mathbf {h} _{u}=\phi \left(\mathbf {x} _{u}:\bigoplus _{v\in N_{u}}\psi (\mathbf {x} _{u}:\mathbf {x} _{v}:\mathbf {e} _{uv})\right)}where ϕ{\displaystyle \phi } and ψ{\displaystyle \psi } are differentiable functions (e.g.: artificial neural networks): and ⨁{\displaystyle \bigoplus } is a permutation invariant aggregation operator that can accept an arbitrary number of inputs (e.g.: element-wise sum: mean: or max). In particular: ϕ{\displaystyle \phi } and ψ{\displaystyle \psi } are referred to as update and message functions: respectively. Intuitively: in an MPNN computational block: graph nodes update their representations by aggregating the messages received from their neighbours. / The outputs of one or more MPNN layers are node representations hu{\displaystyle \mathbf {h} _{u}} for each node u∈V{\displaystyle u\in V} in the graph. Node representations can be employed for any downstream task: such as node/graph classification or edge prediction. / Graph nodes in an MPNN update their representation aggregating information from their immediate neighbours. As such: stacking n{\displaystyle n} MPNN layers means that one node will be able to communicate with nodes that are at most n{\displaystyle n} ""hops"" away. In principle: to ensure that every node receives information from every other node: one would need to stack a number of MPNN layers equal to the graph diameter. However: stacking many MPNN layers may cause issues such as oversmoothing and oversquashing. Oversmoothing refers to the issue of node representations becoming indistinguishable. Oversquashing refers to the bottleneck that is created by squeezing long-range dependencies into fixed-size representations. Countermeasures such as skip connections (as in residual neural networks): gated update rules and jumping knowledge can mitigate oversmoothing. Modifying the final layer to be a fully-adjacent layer: i.e.: by considering the graph as a complete graph: can mitigate oversquashing in problems where long-range dependencies are required.Other ""flavours"" of MPNN have been developed in the literature: such as graph convolutional networks and graph attention networks: whose definitions can be expressed in terms of the MPNN formalism. /  /  / === Graph convolutional network === / The graph convolutional network (GCN) was first introduced by Thomas Kipf and Max Welling in 2017.A GCN layer defines a first-order approximation of a localized spectral filter on graphs. GCNs can be understood as a generalization of convolutional neural networks to graph-structured data.","Score: 0.8346803489731082 / A graph neural network (GNN) belongs to a class of artificial neural networks for processing data that can be represented as graphs. / In the more general subject of ""geometric deep learning"": certain existing neural network architectures can be interpreted as GNNs operating on suitably defined graphs. A convolutional neural network layer: in the context of computer vision: can be seen as a GNN applied to graphs whose nodes are pixels and only adjacent pixels are connected by edges in the graph. A transformer layer: in natural language processing: can be seen as a GNN applied to complete graphs whose nodes are words or tokens in a passage of natural language text. / The key design element of GNNs is the use of pairwise message passing: such that graph nodes iteratively update their representations by exchanging information with their neighbors. Since their inception: several different GNN architectures have been proposed: which implement different flavors of message passing:  started by recursive or convolutional constructive approaches. As of 2022: whether it is possible to define GNN architectures ""going beyond"" message passing: or if every GNN can be built on message passing over suitably defined graphs: is an open research question.Relevant application domains for GNNs include Natural Language Processing: social networks: citation networks: molecular biology: chemistry: physics and NP-hard combinatorial optimization problems.Several open source libraries implementing graph neural networks are available: such as PyTorch Geometric (PyTorch): TensorFlow GNN (TensorFlow): jraph (Google JAX): and GraphNeuralNetworks.jl/GeometricFlux.jl (Julia: Flux). /  /  / == Architecture == / The architecture of a generic GNN implements the following fundamental layers: / Permutation equivariant: a permutation equivariant layer maps a representation of a graph into an updated representation of the same graph. In the literature: permutation equivariant layers are implemented via pairwise message passing between graph nodes. Intuitively: in a message passing layer: nodes update their representations by aggregating the messages received from their immediate neighbours. As such: each message passing layer increases the receptive field of the GNN by one hop. / Local pooling: a local pooling layer coarsens the graph via downsampling. Local pooling is used to increase the receptive field of a GNN: in a similar fashion to pooling layers in convolutional neural networks. Examples include k-nearest neighbours pooling: top-k pooling: and self-attention pooling. / Global pooling: a global pooling layer: also known as readout layer: provides fixed-size representation of the whole graph. The global pooling layer must be permutation invariant: such that permutations in the ordering of graph nodes and edges do not alter the final output. Examples include element-wise sum: mean or maximum.It has been demonstrated that GNNs cannot be more expressive than the Weisfeiler–Leman Graph Isomorphism Test. In practice: this means that there exist different graph structures (e.g.: molecules with the same atoms but different bonds) that cannot be distinguished by GNNs. More powerful GNNs operating on higher-dimension geometries such as simplicial complexes can be designed. As of 2022: whether or not future architectures will overcome the message passing primitive is an open research question. /  /  / == Message passing layers == / Message passing layers are permutation-equivariant layers mapping a graph into an updated representation of the same graph. Formally: they can be expressed as message passing neural networks (MPNNs).Let G=(V:E){\displaystyle G=(V:E)} be a graph: where V{\displaystyle V} is the node set and E{\displaystyle E} is the edge set. Let Nu{\displaystyle N_{u}} be the neighbourhood of some node u∈V{\displaystyle u\in V}. Additionally: let xu{\displaystyle \mathbf {x} _{u}} be the features of node u∈V{\displaystyle u\in V}: and euv{\displaystyle \mathbf {e} _{uv}} be the features of edge (u:v)∈E{\displaystyle (u:v)\in E}."
0.17287529999157414,What are the possible application domains for GNNs?,"GNNs are applicable in various fields such as Natural Language Processing, social networks, citation networks, molecular biology, chemistry, physics, and NP-hard combinatorial optimization problems.","Score: 0.8421636110923667 / A graph neural network (GNN) belongs to a class of artificial neural networks for processing data that can be represented as graphs. / In the more general subject of ""geometric deep learning"": certain existing neural network architectures can be interpreted as GNNs operating on suitably defined graphs. A convolutional neural network layer: in the context of computer vision: can be seen as a GNN applied to graphs whose nodes are pixels and only adjacent pixels are connected by edges in the graph. A transformer layer: in natural language processing: can be seen as a GNN applied to complete graphs whose nodes are words or tokens in a passage of natural language text. / The key design element of GNNs is the use of pairwise message passing: such that graph nodes iteratively update their representations by exchanging information with their neighbors. Since their inception: several different GNN architectures have been proposed: which implement different flavors of message passing:  started by recursive or convolutional constructive approaches. As of 2022: whether it is possible to define GNN architectures ""going beyond"" message passing: or if every GNN can be built on message passing over suitably defined graphs: is an open research question.Relevant application domains for GNNs include Natural Language Processing: social networks: citation networks: molecular biology: chemistry: physics and NP-hard combinatorial optimization problems.Several open source libraries implementing graph neural networks are available: such as PyTorch Geometric (PyTorch): TensorFlow GNN (TensorFlow): jraph (Google JAX): and GraphNeuralNetworks.jl/GeometricFlux.jl (Julia: Flux). /  /  / == Architecture == / The architecture of a generic GNN implements the following fundamental layers: / Permutation equivariant: a permutation equivariant layer maps a representation of a graph into an updated representation of the same graph. In the literature: permutation equivariant layers are implemented via pairwise message passing between graph nodes. Intuitively: in a message passing layer: nodes update their representations by aggregating the messages received from their immediate neighbours. As such: each message passing layer increases the receptive field of the GNN by one hop. / Local pooling: a local pooling layer coarsens the graph via downsampling. Local pooling is used to increase the receptive field of a GNN: in a similar fashion to pooling layers in convolutional neural networks. Examples include k-nearest neighbours pooling: top-k pooling: and self-attention pooling. / Global pooling: a global pooling layer: also known as readout layer: provides fixed-size representation of the whole graph. The global pooling layer must be permutation invariant: such that permutations in the ordering of graph nodes and edges do not alter the final output. Examples include element-wise sum: mean or maximum.It has been demonstrated that GNNs cannot be more expressive than the Weisfeiler–Leman Graph Isomorphism Test. In practice: this means that there exist different graph structures (e.g.: molecules with the same atoms but different bonds) that cannot be distinguished by GNNs. More powerful GNNs operating on higher-dimension geometries such as simplicial complexes can be designed. As of 2022: whether or not future architectures will overcome the message passing primitive is an open research question. /  /  / == Message passing layers == / Message passing layers are permutation-equivariant layers mapping a graph into an updated representation of the same graph. Formally: they can be expressed as message passing neural networks (MPNNs).Let G=(V:E){\displaystyle G=(V:E)} be a graph: where V{\displaystyle V} is the node set and E{\displaystyle E} is the edge set. Let Nu{\displaystyle N_{u}} be the neighbourhood of some node u∈V{\displaystyle u\in V}. Additionally: let xu{\displaystyle \mathbf {x} _{u}} be the features of node u∈V{\displaystyle u\in V}: and euv{\displaystyle \mathbf {e} _{uv}} be the features of edge (u:v)∈E{\displaystyle (u:v)\in E}.","Score: 0.820396554467662 / Recursive auto-encoders built atop word embeddings can assess sentence similarity and detect paraphrasing. Deep neural architectures provide the best results for constituency parsing: sentiment analysis: information retrieval: spoken language understanding: machine translation: contextual entity linking: writing style recognition: named-entity recognition (token classification): text classification: and others.Recent developments generalize word embedding to sentence embedding. / Google Translate (GT) uses a large end-to-end long short-term memory (LSTM) network. Google Neural Machine Translation (GNMT) uses an example-based machine translation method in which the system ""learns from millions of examples"". It translates ""whole sentences at a time: rather than pieces"". Google Translate supports over one hundred languages. The network encodes the ""semantics of the sentence rather than simply memorizing phrase-to-phrase translations"". GT uses English as an intermediate between most language pairs. /  /  / === Drug discovery and toxicology === /  / A large percentage of candidate drugs fail to win regulatory approval. These failures are caused by insufficient efficacy (on-target effect): undesired interactions (off-target effects): or unanticipated toxic effects. Research has explored use of deep learning to predict the biomolecular targets: off-targets: and toxic effects of environmental chemicals in nutrients: household products and drugs.AtomNet is a deep learning system for structure-based rational drug design. AtomNet was used to predict novel candidate biomolecules for disease targets such as the Ebola virus and multiple sclerosis.In 2017 graph neural networks were used for the first time to predict various properties of molecules in a large toxicology data set. In 2019: generative neural networks were used to produce molecules that were validated experimentally all the way into mice. /  /  / === Customer relationship management === /  / Deep reinforcement learning has been used to approximate the value of possible direct marketing actions: defined in terms of RFM variables. The estimated value function was shown to have a natural interpretation as customer lifetime value. /  /  / === Recommendation systems === /  / Recommendation systems have used deep learning to extract meaningful features for a latent factor model for content-based music and journal recommendations. Multi-view deep learning has been applied for learning user preferences from multiple domains. The model uses a hybrid collaborative and content-based approach and enhances recommendations in multiple tasks. /  /  / === Bioinformatics === /  / An autoencoder ANN was used in bioinformatics: to predict gene ontology annotations and gene-function relationships.In medical informatics: deep learning was used to predict sleep quality based on data from wearables and predictions of health complications from electronic health record data. /  /  / === Deep Neural Network Estimations === / Deep neural networks can be used to estimate the entropy of a stochastic process and called Neural Joint Entropy Estimator (NJEE). Such an estimation provides insights on the effects of input random variables on an independent random variable. Practically: the DNN is trained as a classifier that maps an input vector or matrix X to an output probability distribution over the possible classes of random variable Y: given input X. For example: in image classification tasks: the NJEE maps a vector of pixels' color values to probabilities over possible image classes. In practice: the probability distribution of Y is obtained by a Softmax layer with number of nodes that is equal to the alphabet size of Y. NJEE uses continuously differentiable activation functions: such that the conditions for the universal approximation theorem holds. It is shown that this method provides a strongly consistent estimator and outperforms other methods in case of large alphabet sizes. /  /  / === Medical image analysis === / Deep learning has been shown to produce competitive results in medical application such as cancer cell classification: lesion detection: organ segmentation and image enhancement. Modern deep learning tools demonstrate the high accuracy of detecting various diseases and the helpfulness of their use by specialists to improve the diagnosis efficiency. /  /  / === Mobile advertising === / Finding the appropriate mobile audience for mobile advertising is always challenging: since many data points must be considered and analyzed before a target segment can be created and used in ad serving by any ad server. Deep learning has been used to interpret large: many-dimensioned advertising datasets. Many data points are collected during the request/serve/click internet advertising cycle. This information can form the basis of machine learning to improve ad selection. /  /  / === Image restoration === / Deep learning has been successfully applied to inverse problems such as denoising: super-resolution: inpainting: and film colorization. These applications include learning methods such as ""Shrinkage Fields for Effective Image Restoration"" which trains on an image dataset: and Deep Image Prior: which trains on the image that needs restoration. /  /  / === Financial fraud detection === / Deep learning is being successfully applied to financial fraud detection: tax evasion detection: and anti-money laundering."
0.16505339997820556,What are some examples of libraries implementing graph neural networks?,"Several open-source libraries implement graph neural networks. Some examples include PyTorch Geometric (PyTorch), TensorFlow GNN (TensorFlow), jraph (Google JAX), and GraphNeuralNetworks.jl/GeometricFlux.jl (Julia, Flux).","Score: 0.8152985447827953 / A graph neural network (GNN) belongs to a class of artificial neural networks for processing data that can be represented as graphs. / In the more general subject of ""geometric deep learning"": certain existing neural network architectures can be interpreted as GNNs operating on suitably defined graphs. A convolutional neural network layer: in the context of computer vision: can be seen as a GNN applied to graphs whose nodes are pixels and only adjacent pixels are connected by edges in the graph. A transformer layer: in natural language processing: can be seen as a GNN applied to complete graphs whose nodes are words or tokens in a passage of natural language text. / The key design element of GNNs is the use of pairwise message passing: such that graph nodes iteratively update their representations by exchanging information with their neighbors. Since their inception: several different GNN architectures have been proposed: which implement different flavors of message passing:  started by recursive or convolutional constructive approaches. As of 2022: whether it is possible to define GNN architectures ""going beyond"" message passing: or if every GNN can be built on message passing over suitably defined graphs: is an open research question.Relevant application domains for GNNs include Natural Language Processing: social networks: citation networks: molecular biology: chemistry: physics and NP-hard combinatorial optimization problems.Several open source libraries implementing graph neural networks are available: such as PyTorch Geometric (PyTorch): TensorFlow GNN (TensorFlow): jraph (Google JAX): and GraphNeuralNetworks.jl/GeometricFlux.jl (Julia: Flux). /  /  / == Architecture == / The architecture of a generic GNN implements the following fundamental layers: / Permutation equivariant: a permutation equivariant layer maps a representation of a graph into an updated representation of the same graph. In the literature: permutation equivariant layers are implemented via pairwise message passing between graph nodes. Intuitively: in a message passing layer: nodes update their representations by aggregating the messages received from their immediate neighbours. As such: each message passing layer increases the receptive field of the GNN by one hop. / Local pooling: a local pooling layer coarsens the graph via downsampling. Local pooling is used to increase the receptive field of a GNN: in a similar fashion to pooling layers in convolutional neural networks. Examples include k-nearest neighbours pooling: top-k pooling: and self-attention pooling. / Global pooling: a global pooling layer: also known as readout layer: provides fixed-size representation of the whole graph. The global pooling layer must be permutation invariant: such that permutations in the ordering of graph nodes and edges do not alter the final output. Examples include element-wise sum: mean or maximum.It has been demonstrated that GNNs cannot be more expressive than the Weisfeiler–Leman Graph Isomorphism Test. In practice: this means that there exist different graph structures (e.g.: molecules with the same atoms but different bonds) that cannot be distinguished by GNNs. More powerful GNNs operating on higher-dimension geometries such as simplicial complexes can be designed. As of 2022: whether or not future architectures will overcome the message passing primitive is an open research question. /  /  / == Message passing layers == / Message passing layers are permutation-equivariant layers mapping a graph into an updated representation of the same graph. Formally: they can be expressed as message passing neural networks (MPNNs).Let G=(V:E){\displaystyle G=(V:E)} be a graph: where V{\displaystyle V} is the node set and E{\displaystyle E} is the edge set. Let Nu{\displaystyle N_{u}} be the neighbourhood of some node u∈V{\displaystyle u\in V}. Additionally: let xu{\displaystyle \mathbf {x} _{u}} be the features of node u∈V{\displaystyle u\in V}: and euv{\displaystyle \mathbf {e} _{uv}} be the features of edge (u:v)∈E{\displaystyle (u:v)\in E}.","Score: 0.8131554956805273 / Recursive auto-encoders built atop word embeddings can assess sentence similarity and detect paraphrasing. Deep neural architectures provide the best results for constituency parsing: sentiment analysis: information retrieval: spoken language understanding: machine translation: contextual entity linking: writing style recognition: named-entity recognition (token classification): text classification: and others.Recent developments generalize word embedding to sentence embedding. / Google Translate (GT) uses a large end-to-end long short-term memory (LSTM) network. Google Neural Machine Translation (GNMT) uses an example-based machine translation method in which the system ""learns from millions of examples"". It translates ""whole sentences at a time: rather than pieces"". Google Translate supports over one hundred languages. The network encodes the ""semantics of the sentence rather than simply memorizing phrase-to-phrase translations"". GT uses English as an intermediate between most language pairs. /  /  / === Drug discovery and toxicology === /  / A large percentage of candidate drugs fail to win regulatory approval. These failures are caused by insufficient efficacy (on-target effect): undesired interactions (off-target effects): or unanticipated toxic effects. Research has explored use of deep learning to predict the biomolecular targets: off-targets: and toxic effects of environmental chemicals in nutrients: household products and drugs.AtomNet is a deep learning system for structure-based rational drug design. AtomNet was used to predict novel candidate biomolecules for disease targets such as the Ebola virus and multiple sclerosis.In 2017 graph neural networks were used for the first time to predict various properties of molecules in a large toxicology data set. In 2019: generative neural networks were used to produce molecules that were validated experimentally all the way into mice. /  /  / === Customer relationship management === /  / Deep reinforcement learning has been used to approximate the value of possible direct marketing actions: defined in terms of RFM variables. The estimated value function was shown to have a natural interpretation as customer lifetime value. /  /  / === Recommendation systems === /  / Recommendation systems have used deep learning to extract meaningful features for a latent factor model for content-based music and journal recommendations. Multi-view deep learning has been applied for learning user preferences from multiple domains. The model uses a hybrid collaborative and content-based approach and enhances recommendations in multiple tasks. /  /  / === Bioinformatics === /  / An autoencoder ANN was used in bioinformatics: to predict gene ontology annotations and gene-function relationships.In medical informatics: deep learning was used to predict sleep quality based on data from wearables and predictions of health complications from electronic health record data. /  /  / === Deep Neural Network Estimations === / Deep neural networks can be used to estimate the entropy of a stochastic process and called Neural Joint Entropy Estimator (NJEE). Such an estimation provides insights on the effects of input random variables on an independent random variable. Practically: the DNN is trained as a classifier that maps an input vector or matrix X to an output probability distribution over the possible classes of random variable Y: given input X. For example: in image classification tasks: the NJEE maps a vector of pixels' color values to probabilities over possible image classes. In practice: the probability distribution of Y is obtained by a Softmax layer with number of nodes that is equal to the alphabet size of Y. NJEE uses continuously differentiable activation functions: such that the conditions for the universal approximation theorem holds. It is shown that this method provides a strongly consistent estimator and outperforms other methods in case of large alphabet sizes. /  /  / === Medical image analysis === / Deep learning has been shown to produce competitive results in medical application such as cancer cell classification: lesion detection: organ segmentation and image enhancement. Modern deep learning tools demonstrate the high accuracy of detecting various diseases and the helpfulness of their use by specialists to improve the diagnosis efficiency. /  /  / === Mobile advertising === / Finding the appropriate mobile audience for mobile advertising is always challenging: since many data points must be considered and analyzed before a target segment can be created and used in ad serving by any ad server. Deep learning has been used to interpret large: many-dimensioned advertising datasets. Many data points are collected during the request/serve/click internet advertising cycle. This information can form the basis of machine learning to improve ad selection. /  /  / === Image restoration === / Deep learning has been successfully applied to inverse problems such as denoising: super-resolution: inpainting: and film colorization. These applications include learning methods such as ""Shrinkage Fields for Effective Image Restoration"" which trains on an image dataset: and Deep Image Prior: which trains on the image that needs restoration. /  /  / === Financial fraud detection === / Deep learning is being successfully applied to financial fraud detection: tax evasion detection: and anti-money laundering."
0.16260479998891242,Who was behind the first implementation of artificial neural networks (ANNs)?,The first implementation of ANNs was by psychologist Frank Rosenblatt.,"Score: 0.881255801095049 / Artificial neural networks (ANNs) are models created using machine learning to perform a number of tasks. Their creation was inspired by neural circuitry. While some of the computational implementations ANNs relate to earlier discoveries in mathematics: the first implementation of ANNs was by psychologist Frank Rosenblatt: who developed the perceptron. Little research was conducted on ANNs in the 1970s and 1980s: with the AAAI calling that period an ""AI winter"".Later: advances in hardware and the development of the backpropagation algorithm as well as recurrent neural networks and convolutional neural networks: renewed interest in ANNs. The 2010s: saw the development of a deep neural network (a neural network with many layers) called AlexNet. It greatly outperformed other image recognition models: and is thought to have launched the ongoing AI spring: and further increasing interest in ANNs. The transformer architecture was first described in 2017 as a method to teach ANNs grammatical dependencies in language: and is the predominant architecture used by large language models: such as GPT-4. Diffusion models were first described in 2015: and began to be used by image generation models such as DALL-E in the 2020s. /  /  / == Linear neural network == / The simplest kind of feedforward neural network is a linear network: which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated in each node. The mean squared errors between these calculated outputs and a given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement. /  /  / == Perceptrons and other early neural networks == / Warren McCulloch and Walter Pitts (1943) also considered a non-learning computational model for neural networks. This model paved the way for research to split into two approaches. One approach focused on biological processes while the other focused on the application of neural networks to artificial intelligence. This work led to work on nerve networks and their link to finite automata.In the early 1940s: D. O. Hebb created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Hebbian learning is unsupervised learning. This evolved into models for long-term potentiation. Researchers started applying these ideas to computational models in 1948 with Turing's B-type machines. Farley and Clark (1954) first used computational machines: then called ""calculators"": to simulate a Hebbian network. Other neural network computational machines were created by Rochester: Holland: Habit and Duda (1956).Rosenblatt (1958) created the perceptron: an algorithm for pattern recognition. With mathematical notation: Rosenblatt described circuitry not in the basic perceptron: such as the exclusive-or circuit that could not be processed by neural networks at the time. In 1959: a biological model proposed by Nobel laureates Hubel and Wiesel was based on their discovery of two types of cells in the primary visual cortex: simple cells and complex cells.Some say that research stagnated following Minsky and Papert (1969): who discovered that basic perceptrons were incapable of processing the exclusive-or circuit and that computers lacked sufficient power to process useful neural networks. However: by the time this book came out: methods for training multilayer perceptrons (MLPs) by deep learning were already known. /  /  / == First deep learning == / The first deep learning MLP was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa in 1965: as the Group Method of Data Handling. This method employs incremental layer by layer training based on regression analysis: where useless units in hidden layers are pruned with the help of a validation set. / The first deep learning MLP trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. / In computer experiments conducted by Amari's student Saito: a five layer MLP with two modifiable layers learned  useful internal representations to classify non-linearily separable pattern classes. /  /  / == Backpropagation == /  / The backpropagation algorithm is an efficient application of the Leibniz chain rule (1673) to networks of differentiable nodes.","Score: 0.872114841233052 / They used computational machines: then called ""calculators"". Other neural network computational machines were created by Rochester: Holland: Habit: and Duda in 1956. In 1958: psychologist Frank Rosenblatt invented the perceptron: the first implemented artificial neural network: funded by the United States Office of Naval Research. / The invention of the perceptron raised public excitement for research in Artificial Neural Networks: causing the US government to drastically increase funding into deep learning research. This led to ""the golden age of AI"" fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence. For example: in 1957 Herbert Simon famously said:It is not my aim to surprise or shock you—but the simplest way I can summarize is to say that there are now in the world machines that think: that learn and that create. Moreover: their ability to do these things is going to increase rapidly until—in a visible future—the range of problems they can handle will be coextensive with the range to which the human mind has been applied.However: this wasn't the case as research stagnated in the United States following the work of Minsky and Papert (1969): who discovered that basic perceptrons were incapable of processing the exclusive-or circuit and that computers lacked sufficient power to train useful neural networks. This: along with other factors such as the 1973 Lighthill report by James Lighthill stating that research in Artificial Intelligence has not ""produced the major impact that was then promised:"" shutting funding in research into the field of AI in all but two universities in the UK and in many major institutions across the world. This ushered an era called the AI Winter with reduced research into connectionism due to a decrease in government funding and an increased stress on symbolic artificial intelligence in the United States and other Western countries.During the AI Winter era: however: research outside the United States continued: especially in Eastern Europe. By the time Minsky and Papert's book on Perceptrons came out: methods for training multilayer perceptrons (MLPs) were already known. The first deep learning MLP was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa in 1965: as the Group Method of Data Handling. The first deep learning MLP trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. In computer experiments conducted by Amari's student Saito: a five layer MLP with two modifiable layers learned useful internal representations to classify non-linearily separable pattern classes.Self-organizing maps (SOMs) were described by Teuvo Kohonen in 1982. SOMs are neurophysiologically inspired neural networks that learn low-dimensional representations of high-dimensional data while preserving the topological structure of the data. They are trained using competitive learning.The convolutional neural network (CNN) architecture with convolutional layers and downsampling layers was introduced by Kunihiko Fukushima in 1980. He called it the neocognitron. In 1969: he also introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for CNNs and  deep neural networks in general. CNNs have become an essential tool for computer vision. / A key in later advances in artificial neural network research was the backpropagation algorithm: an efficient application of the Leibniz chain rule (1673) to networks of differentiable nodes. It is also known as  / the reverse mode of automatic differentiation or reverse accumulation: due to Seppo Linnainmaa (1970). The term ""back-propagating errors"" was introduced in 1962 by Frank Rosenblatt: but he did not have an implementation of this procedure: although Henry J. Kelley and Bryson had dynamic programming based continuous precursors of backpropagation already in 1960–61 in the context of control theory.  / In 1973: Dreyfus used backpropagation to adapt parameters of controllers in proportion to error gradients.  / In 1982: Paul Werbos applied backpropagation to MLPs in the way that has become standard. In 1986 Rumelhart: Hinton and Williams showed that backpropagation learned interesting internal representations of words as feature vectors when trained to predict the next word in a sequence.In the late 1970s to early 1980s: interest briefly emerged in theoretically investigating the Ising model created by Wilhelm Lenz (1920) and Ernst Ising (1925) / in relation to Cayley tree topologies and large neural networks."
0.1311608999967575,What is the AlexNet and what is its significance in the field of artificial neural networks?,"AlexNet is a deep neural network developed in the 2010s. It greatly outperformed other image recognition models and is thought to have launched the ongoing AI spring, contributing to further increased interest in ANNs.","Score: 0.8499544217616845 / This allows simple statistical association (the basic function of artificial neural networks) to be described as learning or recognition. In 1997: Alexander Dewdney: a former Scientific American columnist: commented that as a result: artificial neural networks have a ""something-for-nothing quality: one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are. No human hand (or mind) intervenes; solutions are found as if by magic; and no one: it seems: has learned anything"". One response to Dewdney is that neural networks have been successfully used to handle many complex and diverse tasks: ranging from autonomously flying aircraft to detecting credit card fraud to mastering the game of Go. / Technology writer Roger Bridgman commented: /  / Neural networks: for instance: are in the dock not only because they have been hyped to high heaven: (what hasn't?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be ""an opaque: unreadable table...valueless as a scientific resource"". / In spite of his emphatic declaration that science is not technology: Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having. /  / Although it is true that analyzing what has been learned by an artificial neural network is difficult: it is much easier to do so than to analyze what has been learned by a biological neural network. Moreover: recent emphasis on the explainability of AI has contributed towards the development of methods: notably those based on attention mechanisms: for visualizing and explaining learned neural networks. Furthermore: researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles that allow a learning machine to be successful. For example: Bengio and LeCun (2007) wrote an article regarding local vs non-local learning: as well as shallow vs deep architecture.Biological brains use both shallow and deep circuits as reported by brain anatomy: displaying a wide variety of invariance. Weng argued that the brain self-wires largely according to signal statistics and therefore: a serial cascade cannot catch all major statistical dependencies. /  /  / === Hardware === / Large and effective neural networks require considerable computing resources. While the brain has hardware tailored to the task of processing signals through a graph of neurons: simulating even a simplified neuron on von Neumann architecture may consume vast amounts of memory and storage. Furthermore: the designer often needs to transmit signals through many of these connections and their associated neurons –  which require enormous CPU power and time. / Schmidhuber noted that the resurgence of neural networks in the twenty-first century is largely attributable to advances in hardware: from 1991 to 2015: computing power: especially as delivered by GPGPUs (on GPUs): has increased around a million-fold: making the standard backpropagation algorithm feasible for training networks that are several layers deeper than before. The use of accelerators such as FPGAs and GPUs can reduce training times from months to days.Neuromorphic engineering or a physical neural network addresses the hardware difficulty directly: by constructing non-von-Neumann chips to directly implement neural networks in circuitry. Another type of chip optimized for neural network processing is called a Tensor Processing Unit: or TPU. /  /  / === Practical counterexamples === / Analyzing what has been learned by an ANN is much easier than analyzing what has been learned by a biological neural network. Furthermore: researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a learning machine to be successful. For example: local vs. non-local learning and shallow vs. deep architecture. /  /  / === Hybrid approaches === / Advocates of hybrid models (combining neural networks and symbolic approaches) say that such a mixture can better capture the mechanisms of the human mind. /  /  / === Dataset bias === / Neural networks are dependent on the quality of the data they are trained on: thus low quality data with imbalanced representativeness can lead to the model learning and perpetuating societal biases.  These inherited biases become especially critical when the ANNs are integrated into real-world scenarios where the training data may be imbalanced due to the scarcity of data for a specific race: gender or other attribute. This imbalance can result in the model having inadequate representation and understanding of underrepresented groups: leading to discriminatory outcomes that exasperate societal inequalities: especially in applications like facial recognition: hiring processes: and law enforcement.","Score: 0.8496585777302238 / In 2011: the DanNet by Dan Ciresan: Ueli Meier: Jonathan Masci: Luca Maria Gambardella: and Jürgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest: outperforming traditional methods by a factor of 3. Also in 2011: DanNet won the ICDAR Chinese handwriting contest: and in May 2012: it won the ISBI image segmentation contest. Until 2011: CNNs did not play a major role at computer vision conferences: but in June 2012: a paper by Ciresan et al. at the leading conference CVPR showed how max-pooling CNNs on GPU can dramatically improve many vision benchmark records.  In September 2012: DanNet also won the ICPR contest on analysis of large medical images for cancer detection: and in the following year also the MICCAI Grand Challenge on the same topic. In October 2012: the similar AlexNet by Alex Krizhevsky: Ilya Sutskever: and Geoffrey Hinton won the large-scale ImageNet competition by a significant margin over shallow machine learning methods.  / The VGG-16 network by Karen Simonyan and Andrew Zisserman further reduced the error rate and / won the ImageNet 2014 competition: following a similar trend in large-scale speech recognition. / Image classification was then extended to the more challenging task of generating descriptions (captions) for images: often as a combination of CNNs and LSTMs.In 2012: a team led by George E. Dahl won the ""Merck Molecular Activity Challenge"" using multi-task deep neural networks to predict the biomolecular target of one drug. In 2014: Sepp Hochreiter's group used deep learning to detect off-target and toxic effects of environmental chemicals in nutrients: household products and drugs and won the ""Tox21 Data Challenge"" of NIH: FDA and NCATS.In 2016: Roger Parloff mentioned a ""deep learning revolution"" that has transformed the AI industry.In March 2019: Yoshua Bengio: Geoffrey Hinton and Yann LeCun were awarded the Turing Award for conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing. /  /  / == Neural networks == /  / Artificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve their ability) to do tasks by considering examples: generally without task-specific programming. For example: in image recognition: they might learn to identify images that contain cats by analyzing example images that have been manually labeled as ""cat"" or ""no cat"" and using the analytic results to identify cats in other images. They have found most use in applications difficult to express with a traditional computer algorithm using rule-based programming. / An ANN is based on a collection of connected units called artificial neurons: (analogous to biological neurons in a biological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it. Neurons may have state: generally represented by real numbers: typically between 0 and 1. Neurons and synapses may also have a weight that varies as learning proceeds: which can increase or decrease the strength of the signal that it sends downstream. / Typically: neurons are organized in layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first (input): to the last (output) layer: possibly after traversing the layers multiple times. / The original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time: attention focused on matching specific mental abilities: leading to deviations from biology such as backpropagation: or passing information in the reverse direction and adjusting the network to reflect that information. / Neural networks have been used on a variety of tasks: including computer vision: speech recognition: machine translation: social network filtering: playing board and video games and medical diagnosis. / As of 2017: neural networks typically have a few thousand to a few million units and millions of connections. Despite this number being several order of magnitude less than the number of neurons on a human brain: these networks can perform many tasks at a level beyond that of humans (e.g.: recognizing faces: or playing ""Go""). /  /  / === Deep neural networks === / A deep neural network (DNN) is an artificial neural network with multiple layers between the input and output layers."
0.17407569999340922,How does a linear neural network work?,A linear network consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated in each node. The mean squared errors between these calculated outputs and a given target values are minimized by creating an adjustment to the weights.,"Score: 0.8368919147798366 / The original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time: attention focused on matching specific mental abilities: leading to deviations from biology such as backpropagation: or passing information in the reverse direction and adjusting the network to reflect that information. / Neural networks have been used on a variety of tasks: including computer vision: speech recognition: machine translation: social network filtering: playing board and video games and medical diagnosis. / As of 2017: neural networks typically have a few thousand to a few million units and millions of connections. Despite this number being several order of magnitude less than the number of neurons on a human brain: these networks can perform many tasks at a level beyond that of humans (e.g.: recognizing faces: or playing ""Go""). /  /  / === Deep neural networks === / A deep neural network (DNN) is an artificial neural network with multiple layers between the input and output layers. There are different types of neural networks but they always consist of the same components: neurons: synapses: weights: biases: and functions. These components as a whole function in a way that mimics functions of the human brain: and can be trained like any other ML algorithm.For example: a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed. The user can review the results and select which probabilities the network should display (above a certain threshold: etc.) and return the proposed label. Each mathematical manipulation as such is considered a layer: and complex DNN have many layers: hence the name ""deep"" networks. / DNNs can model complex non-linear relationships. DNN architectures generate compositional models where the object is expressed as a layered composition of primitives. The extra layers enable composition of features from lower layers: potentially modeling complex data with fewer units than a similarly performing shallow network. For instance: it was proved that sparse multivariate polynomials are exponentially easier to approximate with DNNs than with shallow networks.Deep architectures include many variants of a few basic approaches. Each architecture has found success in specific domains. It is not always possible to compare the performance of multiple architectures: unless they have been evaluated on the same data sets. / DNNs are typically feedforward networks in which data flows from the input layer to the output layer without looping back. At first: the DNN creates a map of virtual neurons and assigns random numerical values: or ""weights"": to connections between them. The weights and inputs are multiplied and return an output between 0 and 1. If the network did not accurately recognize a particular pattern: an algorithm would adjust the weights. That way the algorithm can make certain parameters more influential: until it determines the correct mathematical manipulation to fully process the data. / Recurrent neural networks: in which data can flow in any direction: are used for applications such as language modeling. Long short-term memory is particularly effective for this use.Convolutional neural networks (CNNs) are used in computer vision. CNNs also have been applied to acoustic modeling for automatic speech recognition (ASR). /  /  / ==== Challenges ==== / As with ANNs: many issues can arise with naively trained DNNs. Two common issues are overfitting and computation time. / DNNs are prone to overfitting because of the added layers of abstraction: which allow them to model rare dependencies in the training data. Regularization methods such as Ivakhnenko's unit pruning or weight decay (ℓ2{\displaystyle \ell _{2}}-regularization) or sparsity (ℓ1{\displaystyle \ell _{1}}-regularization) can be applied during training to combat overfitting. Alternatively dropout regularization randomly omits units from the hidden layers during training. This helps to exclude rare dependencies. Finally: data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.DNNs must consider many training parameters: such as the size (number of layers and number of units per layer): the learning rate: and initial weights. Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources. Various tricks: such as batching (computing the gradient on several training examples at once rather than individual examples) speed up computation. Large processing capabilities of many-core architectures (such as GPUs or the Intel Xeon Phi) have produced significant speedups in training: because of the suitability of such processing architectures for the matrix and vector computations.Alternatively: engineers may look for other types of neural networks with more straightforward and convergent training algorithms.","Score: 0.8315241071264641 / Convolutional layers convolve the input and pass its result to the next layer. This is similar to the response of a neuron in the visual cortex to a specific stimulus. Each convolutional neuron processes data only for its receptive field.  /  / Although fully connected feedforward neural networks can be used to learn features and classify data: this architecture is generally impractical for larger inputs (e.g.: high-resolution images): which would require massive numbers of neurons because each pixel is a relevant input feature. A fully connected layer for an image of size 100 × 100 has 10:000 weights for each neuron in the second layer. Convolution reduces the number of free parameters: allowing the network to be deeper. For example: using a 5 × 5 tiling region: each with the same shared weights: requires only 25 neurons. Using regularized weights over fewer parameters avoids the vanishing gradients and exploding gradients problems seen during backpropagation in earlier neural networks.To speed processing: standard convolutional layers can be replaced by depthwise separable convolutional layers: which are based on a depthwise convolution followed by a pointwise convolution. The depthwise convolution is a spatial convolution applied independently over each channel of the input tensor: while the pointwise convolution is a standard convolution restricted to the use of 1×1{\displaystyle 1\times 1}  kernels. /  /  / === Pooling layers === / Convolutional networks may include local and/or global pooling layers along with traditional convolutional layers. Pooling layers reduce the dimensions of data by combining the outputs of neuron clusters at one layer into a single neuron in the next layer. Local pooling combines small clusters: tiling sizes such as 2 × 2 are commonly used. Global pooling acts on all the neurons of the feature map. There are two common types of pooling in popular use: max and average. Max pooling uses the maximum value of each local cluster of neurons in the feature map: while average pooling takes the average value. /  /  / === Fully connected layers === / Fully connected layers connect every neuron in one layer to every neuron in another layer. It is the same as a traditional multilayer perceptron neural network (MLP). The flattened matrix goes through a fully connected layer to classify the images. /  /  / === Receptive field === / In neural networks: each neuron receives input from some number of locations in the previous layer. In a convolutional layer: each neuron receives input from only a restricted area of the previous layer called the neuron's receptive field. Typically the area is a square (e.g. 5 by 5 neurons). Whereas: in a fully connected layer: the receptive field is the entire previous layer. Thus: in each convolutional layer: each neuron takes input from a larger area in the input than previous layers. This is due to applying the convolution over and over: which takes the value of a pixel into account: as well as its surrounding pixels. When using dilated layers: the number of pixels in the receptive field remains constant: but the field is more sparsely populated as its dimensions grow when combining the effect of several layers. / To manipulate the receptive field size as desired: there are some alternatives to the standard convolutional layer. For example: atrous or dilated convolution expands the receptive field size without increasing the number of parameters by interleaving visible and blind regions. Moreover: a single dilated convolutional layer can comprise filters with multiple dilation ratios: thus having a variable receptive field size. /  /  / === Weights === / Each neuron in a neural network computes an output value by applying a specific function to the input values received from the receptive field in the previous layer. The function that is applied to the input values is determined by a vector of weights and a bias (typically real numbers). Learning consists of iteratively adjusting these biases and weights. / The vectors of weights and biases are called filters and represent particular features of the input (e.g.: a particular shape). A distinguishing feature of CNNs is that many neurons can share the same filter. This reduces the memory footprint because a single bias and a single vector of weights are used across all receptive fields that share that filter: as opposed to each receptive field having its own bias and vector weighting. /  /  / == History == / CNN are often compared to the way the brain achieves vision processing in living organisms. /  /  / === Receptive fields in the visual cortex === / Work by Hubel and Wiesel in the 1950s and 1960s showed that cat visual cortices contain neurons that individually respond to small regions of the visual field. Provided the eyes are not moving: the region of visual space within which visual stimuli affect the firing of a single neuron is known as its receptive field."
0.17498400001204573,Who developed the perceptron and what was its function?,"The perceptron, an algorithm for pattern recognition, was developed by Rosenblatt. It described circuitries not in the basic perceptron, including the exclusive-or circuit, which could not be processed by neural networks at the time.","Score: 0.8596029464152469 / They used computational machines: then called ""calculators"". Other neural network computational machines were created by Rochester: Holland: Habit: and Duda in 1956. In 1958: psychologist Frank Rosenblatt invented the perceptron: the first implemented artificial neural network: funded by the United States Office of Naval Research. / The invention of the perceptron raised public excitement for research in Artificial Neural Networks: causing the US government to drastically increase funding into deep learning research. This led to ""the golden age of AI"" fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence. For example: in 1957 Herbert Simon famously said:It is not my aim to surprise or shock you—but the simplest way I can summarize is to say that there are now in the world machines that think: that learn and that create. Moreover: their ability to do these things is going to increase rapidly until—in a visible future—the range of problems they can handle will be coextensive with the range to which the human mind has been applied.However: this wasn't the case as research stagnated in the United States following the work of Minsky and Papert (1969): who discovered that basic perceptrons were incapable of processing the exclusive-or circuit and that computers lacked sufficient power to train useful neural networks. This: along with other factors such as the 1973 Lighthill report by James Lighthill stating that research in Artificial Intelligence has not ""produced the major impact that was then promised:"" shutting funding in research into the field of AI in all but two universities in the UK and in many major institutions across the world. This ushered an era called the AI Winter with reduced research into connectionism due to a decrease in government funding and an increased stress on symbolic artificial intelligence in the United States and other Western countries.During the AI Winter era: however: research outside the United States continued: especially in Eastern Europe. By the time Minsky and Papert's book on Perceptrons came out: methods for training multilayer perceptrons (MLPs) were already known. The first deep learning MLP was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa in 1965: as the Group Method of Data Handling. The first deep learning MLP trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. In computer experiments conducted by Amari's student Saito: a five layer MLP with two modifiable layers learned useful internal representations to classify non-linearily separable pattern classes.Self-organizing maps (SOMs) were described by Teuvo Kohonen in 1982. SOMs are neurophysiologically inspired neural networks that learn low-dimensional representations of high-dimensional data while preserving the topological structure of the data. They are trained using competitive learning.The convolutional neural network (CNN) architecture with convolutional layers and downsampling layers was introduced by Kunihiko Fukushima in 1980. He called it the neocognitron. In 1969: he also introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for CNNs and  deep neural networks in general. CNNs have become an essential tool for computer vision. / A key in later advances in artificial neural network research was the backpropagation algorithm: an efficient application of the Leibniz chain rule (1673) to networks of differentiable nodes. It is also known as  / the reverse mode of automatic differentiation or reverse accumulation: due to Seppo Linnainmaa (1970). The term ""back-propagating errors"" was introduced in 1962 by Frank Rosenblatt: but he did not have an implementation of this procedure: although Henry J. Kelley and Bryson had dynamic programming based continuous precursors of backpropagation already in 1960–61 in the context of control theory.  / In 1973: Dreyfus used backpropagation to adapt parameters of controllers in proportion to error gradients.  / In 1982: Paul Werbos applied backpropagation to MLPs in the way that has become standard. In 1986 Rumelhart: Hinton and Williams showed that backpropagation learned interesting internal representations of words as feature vectors when trained to predict the next word in a sequence.In the late 1970s to early 1980s: interest briefly emerged in theoretically investigating the Ising model created by Wilhelm Lenz (1920) and Ernst Ising (1925) / in relation to Cayley tree topologies and large neural networks.","Score: 0.8317720833468482 / It also introduced variants: including a version with four-layer perceptrons where the last two layers have learned weights (and thus a proper multilayer perceptron).: section 16  In addition: term deep learning was proposed in 1986 by Rina Dechter although the history of its appearance is apparently more complicated.The first general: working learning algorithm for supervised: deep: feedforward: multilayer perceptrons was published by Alexey Ivakhnenko and Lapa in 1967. A 1971 paper described a deep network with eight layers trained by the group method of data handling.The first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. In computer experiments conducted by Amari's student Saito: a five layer MLP with two modifiable layers learned  internal representations to classify non-linearily separable pattern classes. In 1987 Matthew Brand reported that wide 12-layer nonlinear perceptrons could be fully end-to-end trained to reproduce logic functions of nontrivial circuit depth via gradient descent on small batches of random input/output samples: but concluded that training time on contemporary hardware (sub-megaflop computers) made the technique impractical: and proposed using fixed random early layers as an input hash for a single modifiable layer.  Instead: subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique. / In 1970: Seppo Linnainmaa published the reverse mode of automatic differentiation of discrete connected networks of nested differentiable functions. This became known as backpropagation. It is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes.  / The terminology ""back-propagating errors"" was actually introduced in 1962 by Rosenblatt: but he did not know how to implement this: although Henry J. Kelley had a continuous precursor of backpropagation already in 1960 in the context of control theory. In 1982: Paul Werbos applied backpropagation to MLPs in the way that has become standard. In 1985: David E. Rumelhart et al. published an experimental analysis of the technique.Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers began with the Neocognitron introduced by Kunihiko Fukushima in 1980. In 1969: he also introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for CNNs and deep learning in general. CNNs have become an essential tool for computer vision. / The term Deep Learning was introduced to the machine learning community by Rina Dechter in 1986: and to artificial neural networks by Igor Aizenberg and colleagues in 2000: in the context of Boolean threshold neurons.In 1988: Wei Zhang et al. applied the backpropagation algorithm  / to a convolutional neural network (a simplified Neocognitron with convolutional interconnections between the image feature layers and the last fully connected layer) for alphabet recognition. They also proposed an implementation of the CNN with an optical computing system.  / In 1989: Yann LeCun et al. applied backpropagation to a CNN with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked: training required 3 days. Subsequently: Wei Zhang: et al. modified their model by removing the last fully connected layer and applied it for medical image object segmentation in 1991 and breast cancer detection in mammograms in 1994. LeNet-5 (1998): a 7-level CNN by Yann LeCun et al.: that classifies digits: was applied by several banks to recognize hand-written numbers on checks  digitized in 32x32 pixel images. / In the 1980s: backpropagation did not work well for deep learning with long credit assignment paths. To overcome this problem: Jürgen Schmidhuber (1992) proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning. It uses predictive coding  to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN: by distilling a higher level chunker network into a lower level automatizer network."
0.16090950000216253,What is the role of the backpropagation algorithm in artificial neural networks?,The backpropagation algorithm is an efficient application of the Leibniz chain rule to networks of differentiable nodes. It's crucial for adjusting the weight of various nodes in the network to improve prediction accuracy.,"Score: 0.8668442948230799 / In 1973: Dreyfus used backpropagation to adapt parameters of controllers in proportion to error gradients.  / In 1982: Paul Werbos applied backpropagation to MLPs in the way that has become standard. In 1986 Rumelhart: Hinton and Williams showed that backpropagation learned interesting internal representations of words as feature vectors when trained to predict the next word in a sequence.In the late 1970s to early 1980s: interest briefly emerged in theoretically investigating the Ising model created by Wilhelm Lenz (1920) and Ernst Ising (1925) / in relation to Cayley tree topologies and large neural networks. / The Ising model is essentially a non-learning artificial recurrent neural network (RNN) consisting of neuron-like threshold elements. / In 1972: Shun'ichi Amari described an adaptive version of this architecture: / In 1981: the Ising model was solved exactly by Peter Barth for the general case of closed Cayley trees (with loops) with an arbitrary branching ratio / and found to exhibit unusual phase transition behavior in its local-apex and long-range site-site correlations.John Hopfield popularised this architecture in 1982: / and it is now known as a Hopfield network. / The time delay neural network (TDNN) of Alex Waibel (1987) combined convolutions and weight sharing and backpropagation.  In 1988: Wei Zhang et al. applied backpropagation to a CNN (a simplified Neocognitron with convolutional interconnections between the image feature layers and the last fully connected layer) for alphabet recognition. In 1989: Yann LeCun et al. trained a CNN to recognize handwritten ZIP codes on mail.  / In 1992: max-pooling for CNNs was introduced by Juan Weng et al. to help with least-shift invariance and tolerance to deformation to aid 3D object recognition.  / LeNet-5 (1998): a 7-level CNN by Yann LeCun et al.: that classifies digits: was applied by several banks to recognize hand-written numbers on checks digitized in 32x32 pixel images. / From 1988 onward: the use of neural networks transformed the field of protein structure prediction: in particular when the first cascading networks were trained on profiles (matrices) produced by multiple sequence alignments.In 1991: Sepp Hochreiter's diploma thesis  identified and analyzed the vanishing gradient problem and proposed recurrent residual connections to solve it. His thesis was called ""one of the most important documents in the history of machine learning"" by his supervisor Juergen Schmidhuber.In 1991: Juergen Schmidhuber  published adversarial neural networks that contest with each other in the form of a zero-sum game: where one network's gain is the other network's loss. The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called ""artificial curiosity."" / In 1992: Juergen Schmidhuber proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning. It uses predictive coding  to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN: by distilling a higher level chunker network into a lower level automatizer network.  In the same year he also published an alternative to RNNs which is a precursor of a linear Transformer. It introduces the concept internal spotlights of attention: a slow feedforward neural network learns by gradient descent to control the fast weights of another neural network through outer products of self-generated activation patterns. / The development of metal–oxide–semiconductor (MOS) very-large-scale integration (VLSI): in the form of complementary MOS (CMOS) technology: enabled increasing MOS transistor counts in digital electronics. This provided more processing power for the development of practical artificial neural networks in the 1980s.Neural networks' early successes included predicting the stock market and in 1995 a (mostly) self-driving car.1997:  Sepp Hochreite and Juergen Schmidhuber introduced the deep learning method called long short-term memory (LSTM): published in Neural Computation. LSTM recurrent neural networks can learn ""very deep learning"" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before.","Score: 0.8583506675766654 / It did so by utilizing weight sharing in combination with backpropagation training. Thus: while also using a pyramidal structure as in the neocognitron: it performed a global optimization of the weights instead of a local one.In 1988: Wei Zhang et al. applied backpropagation  / to a CNN (a simplified Neocognitron with convolutional interconnections between the image feature layers and the last fully connected layer) for alphabet recognition. They also proposed an implementation of the CNN with an optical computing system.In 1989: Yann LeCun et al. trained a CNN with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked: training required 3 days. Learning was fully automatic: performed better than manual coefficient design: and was suited to a broader range of image recognition problems and image types. / Subsequently: Wei Zhang: et al. modified their model by removing the last fully connected layer and applied it for medical image object segmentation in 1991 and breast cancer detection in mammograms in 1994.In 1990 Yamaguchi et al. introduced max-pooling: a fixed filtering operation that calculates and propagates the maximum value of a given region. They combined TDNNs with max-pooling in order to realize a speaker independent isolated word recognition system.  / In a variant of the neocognitron called the cresceptron: instead of using Fukushima's spatial averaging: J. Weng et al. also used max-pooling where a downsampling unit computes the maximum of the activations of the units in its patch. Max-pooling is often used in modern CNNs.LeNet-5: a 7-level CNN by Yann LeCun et al. in 1998: that classifies digits: was applied by several banks to recognize hand-written numbers on checks (British English: cheques) digitized in 32x32 pixel images. The ability to process higher-resolution images requires larger and more layers of CNNs: so this technique is constrained by the availability of computing resources. / In 2010: Backpropagation training through max-pooling was accelerated by GPUs and shown to perform better than other pooling variants. / Behnke (2003) relied only on the sign of the gradient (Rprop) on problems such as image reconstruction and face localization. Rprop is a first-order optimization algorithm created by Martin Riedmiller and Heinrich Braun in 1992.In 2011: a deep GPU-based CNN called ""DanNet"" by Dan Ciresan: Ueli Meier: and Juergen Schmidhuber achieved human-competitive performance for the first time in computer vision contests. Subsequently: a similar GPU-based CNN by Alex Krizhevsky: Ilya Sutskever: and Geoffrey Hinton won the ImageNet Large Scale Visual Recognition Challenge 2012. A very deep CNN with over 100 layers by Kaiming He: Xiangyu Zhang: Shaoqing Ren: and Jian Sun of Microsoft won the ImageNet 2015 contest.ANNs were able to guarantee shift invariance to deal with small and large natural objects in large cluttered scenes: only when invariance extended beyond shift: to all ANN-learned concepts: such as location: type (object class label): scale: lighting and others. This was realized in Developmental Networks (DNs) whose embodiments are Where-What Networks: WWN-1 (2008) through WWN-7 (2013). /  /  / == Artificial curiosity and generative adversarial networks == /  / In 1991: Juergen Schmidhuber published adversarial neural networks that contest with each other in the form of a zero-sum game: where one network's gain is the other network's loss. The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called ""artificial curiosity."" Earlier adversarial machine learning systems ""neither involved unsupervised neural networks nor were about modeling data nor used gradient descent.""In 2014: this adversarial principle was used in a generative adversarial network (GAN) by Ian Goodfellow et al. Here the environmental reaction is 1 or 0 depending on whether the first network's output is in a given set. This can be used to create realistic deepfakes.In 1992: Schmidhuber also published another type of gradient-based adversarial neural networks where the goal of the zero-sum game is to create disentangled representations of input patterns."
0.13054600000032224,What is a modeling language?,"A modeling language is an artificial language that can be used to express data, information or knowledge or systems in a structure defined by a consistent set of rules. These rules help interpret the meaning of components in the structure. They can be graphical or textual, and can be used in various fields including computer science, information management, business process modeling, software engineering, and systems engineering.","Score: 0.8960032860445437 / A modeling language is any artificial language that can be used to express data: information or knowledge or systems in a structure that is defined by a consistent set of rules. The rules are used for interpretation of the meaning of components in the structure Programing language. /  /  / == Overview == / A modeling language can be graphical or textual. / Graphical modeling languages use a diagram technique with named symbols that represent concepts and lines that connect the symbols and represent relationships and various other graphical notation to represent constraints. / Textual modeling languages may use standardized keywords accompanied by parameters or natural language terms and phrases to make computer-interpretable expressions.An example of a graphical modeling language and a corresponding textual modeling language is EXPRESS. / Not all modeling languages are executable: and for those that are: the use of them doesn't necessarily mean that programmers are no longer required. On the contrary: executable modeling languages are intended to amplify the productivity of skilled programmers: so that they can address more challenging problems: such as parallel computing and distributed systems. / A large number of modeling languages appear in the literature. /  /  / == Type of modeling languages == /  /  / === Graphical types === / Example of graphical modeling languages in the field of computer science: project management and systems engineering: /  / Behavior Trees are a formal: graphical modeling language used primarily in systems and software engineering. Commonly used to unambiguously represent the hundreds or even thousands of natural language requirements that are typically used to express the stakeholder needs for a large-scale software-integrated system. / Business Process Modeling Notation (BPMN: and the XML form BPML) is an example of a Process Modeling language. / C-K theory consists of a modeling language for design processes. / DRAKON is a general-purpose algorithmic modeling language for specifying software-intensive systems: a schematic representation of an algorithm or a stepwise process: and a family of programming languages. / EXPRESS and EXPRESS-G (ISO 10303-11) is an international standard general-purpose data modeling language. / Extended Enterprise Modeling Language (EEML) is commonly used for business process modeling across a number of layers. / Flowchart is a schematic representation of an algorithm or a stepwise process. / Fundamental Modeling Concepts (FMC) modeling language for software-intensive systems. / IDEF is a family of modeling languages: which include IDEF0 for functional modeling: IDEF1X for information modeling: IDEF3 for business process modeling: IDEF4 for Object-Oriented Design and IDEF5 for modeling ontologies. / Jackson Structured Programming (JSP) is a method for structured programming based on correspondences between data stream structure and program structure. / LePUS3 is an object-oriented visual Design Description Language and a formal specification language that is suitable primarily for modeling large object-oriented (Java: C++: C#) programs and design patterns. / Lifecycle Modeling Language is an open-standard language for systems engineering that supports the full system lifecycle: conceptual: utilization: support and retirement stages. / Object-Role Modeling (ORM) in the field of software engineering is a method for conceptual modeling: and can be used as a tool for information and rules analysis. / Petri nets use variations on exactly one diagramming technique and topology: namely the bipartite graph.  The simplicity of its basic user interface easily enabled extensive tool support over the years: particularly in the areas of model checking: graphically oriented simulation: and software verification. / Southbeach Notation is a visual modeling language used to describe situations in terms of agents that are considered useful or harmful from the modeler's perspective. The notation shows how the agents interact with each other and whether this interaction improves or worsens the situation. / Specification and Description Language (SDL) is a specification language targeted at the unambiguous specification and description of the behavior of reactive and distributed systems. / SysML is a Domain-Specific Modeling language for systems engineering that is defined as a UML profile (customization). / Unified Modeling Language (UML) is a general-purpose modeling language that is an industry standard for specifying software-intensive systems. UML 2.0: the current version: supports thirteen different diagram techniques: and has widespread tool support. / Service-oriented modeling framework (SOMF) is a holistic language for designing enterprise and application level architecture models in the space of enterprise architecture: virtualization: service-oriented architecture (SOA): cloud computing: and more. / Architecture description language (ADL) is a language used to describe and represent the systems architecture of a system. / Architecture Analysis & Design Language (AADL) is a modeling language that supports early and repeated analyses of a system's architecture with respect to performance-critical properties through an extendable notation: a tool framework: and precisely defined semantics.Examples of graphical modeling languages in other fields of science. /  / EAST-ADL is a  Domain-Specific Modeling language dedicated to automotive system design.","Score: 0.8792150668667574 / The algebraic formulation of a model does not contain any hints how to process it. /  /  / ==== Behavioral ==== / Behavioral languages are designed to describe the observable behavior of complex systems consisting of components that / execute concurrently. These languages focus on the description of key concepts such as: concurrency: nondeterminism: synchronization: and communication. The semantic foundations of Behavioral languages are process calculus or process algebra. /  /  / ==== Discipline-specific ==== / A discipline-specific modeling (DspM) language is focused on deliverables affiliated with a specific software development life cycle stage. Therefore: such language offers a distinct vocabulary: syntax: and notation for each stage: such as discovery: analysis: design: architecture: contraction: etc. For example: for the analysis phase of a project: the modeler employs specific analysis notation to deliver an analysis proposition diagram. During the design phase: however: logical design notation is used to depict relationship between software entities. In addition: the discipline-specific modeling language best practices does not preclude practitioners from combining the various notations in a single diagram. /  /  / ==== Domain-specific ==== / Domain-specific modeling (DSM) is a software engineering methodology for designing and developing systems: most often IT systems such as computer software. It involves systematic use of a graphical domain-specific language (DSL) to represent the various facets of a system. DSM languages tend to support higher-level abstractions than General-purpose modeling languages: so they require less effort and fewer low-level details to specify a given system. /  /  / ==== Framework-specific ==== / A framework-specific modeling language (FSML) is a kind of domain-specific modeling language which is designed for an object-oriented application framework. FSMLs define framework-provided abstractions as FSML concepts and decompose the abstractions into features. The features represent implementation steps or choices. / A FSML concept can be configured by selecting features and providing values for features. Such a concept configuration represents how the concept should be implemented in the code. In other words: concept configuration describes how the framework should be completed in order to create the implementation of the concept. /  /  / ==== Information and knowledge modeling ==== / Linked data and ontology engineering require 'host languages' to represent entities and the relations between them: constraints between the properties of entities and relations: and metadata attributes. JSON-LD and RDF are two major (and semantically almost equivalent) languages in this context: primarily because they support statement reification and contextualisation which are essential properties to support the higher-order logic needed to reason about models. Model transformation is a common example of such reasoning. /  /  / ==== Object-oriented ==== / Object modeling languages are modeling languages based on a standardized set of symbols and ways of arranging them to model (part of) an object oriented software design or system design. / Some organizations use them extensively in combination with a software development methodology to progress from initial specification to an implementation plan and to communicate that plan to an entire team of developers and stakeholders. Because a modeling language is visual and at a higher-level of abstraction than code: using models encourages the generation of a shared vision that may prevent problems of differing interpretation later in development. Often software modeling tools are used to construct these models: which may then be capable of automatic translation to code. /  /  / ==== Virtual reality ==== / Virtual Reality Modeling Language (VRML): before 1995 known as the Virtual Reality Markup Language is a standard file format for representing 3-dimensional (3D) interactive vector graphics: designed particularly with the World Wide Web in mind. /  /  / ==== Others ==== / Architecture Description Language / Face Modeling Language / Generative Modelling Language / Java Modeling Language / Promela / Rebeca Modeling Language / Service Modeling Language / Web Services Modeling Language / X3D /  /  / == Applications == / Various kinds of modeling languages are applied in different disciplines: including computer science: information management: business process modeling: software engineering: and systems engineering. Modeling languages can be used to specify: /  / system requirements: / structures and / behaviors.Modeling languages are intended to be used to precisely specify systems so that stakeholders (e.g.: customers: operators: analysts: designers) can better understand the system being modeled. / The more mature modeling languages are precise: consistent and executable. Informal diagramming techniques applied with drawing tools are expected to produce useful pictorial representations of system requirements: structures and behaviors: which can be useful for communication: design: and problem solving but cannot be used programmatically.: 539  Executable modeling languages applied with proper tool support: however: are expected to automate system verification and validation: simulation and code generation from the same representations. /  /  / == Quality == / A review of modelling languages is essential to be able to assign which languages are appropriate for different modelling settings. In the term settings we include stakeholders: domain and the knowledge connected."
0.22839450000901707,What is the difference between graphical and textual modeling languages?,"Graphical modeling languages use a diagram technique with named symbols to represent concepts and lines to depict relationships between symbols and other graphical notations to represent constraints. On the other hand, textual modeling languages use standardized keywords along with parameters or natural language terms and phrases to form computer interpretable expressions.","Score: 0.8687179760757637 / A modeling language is any artificial language that can be used to express data: information or knowledge or systems in a structure that is defined by a consistent set of rules. The rules are used for interpretation of the meaning of components in the structure Programing language. /  /  / == Overview == / A modeling language can be graphical or textual. / Graphical modeling languages use a diagram technique with named symbols that represent concepts and lines that connect the symbols and represent relationships and various other graphical notation to represent constraints. / Textual modeling languages may use standardized keywords accompanied by parameters or natural language terms and phrases to make computer-interpretable expressions.An example of a graphical modeling language and a corresponding textual modeling language is EXPRESS. / Not all modeling languages are executable: and for those that are: the use of them doesn't necessarily mean that programmers are no longer required. On the contrary: executable modeling languages are intended to amplify the productivity of skilled programmers: so that they can address more challenging problems: such as parallel computing and distributed systems. / A large number of modeling languages appear in the literature. /  /  / == Type of modeling languages == /  /  / === Graphical types === / Example of graphical modeling languages in the field of computer science: project management and systems engineering: /  / Behavior Trees are a formal: graphical modeling language used primarily in systems and software engineering. Commonly used to unambiguously represent the hundreds or even thousands of natural language requirements that are typically used to express the stakeholder needs for a large-scale software-integrated system. / Business Process Modeling Notation (BPMN: and the XML form BPML) is an example of a Process Modeling language. / C-K theory consists of a modeling language for design processes. / DRAKON is a general-purpose algorithmic modeling language for specifying software-intensive systems: a schematic representation of an algorithm or a stepwise process: and a family of programming languages. / EXPRESS and EXPRESS-G (ISO 10303-11) is an international standard general-purpose data modeling language. / Extended Enterprise Modeling Language (EEML) is commonly used for business process modeling across a number of layers. / Flowchart is a schematic representation of an algorithm or a stepwise process. / Fundamental Modeling Concepts (FMC) modeling language for software-intensive systems. / IDEF is a family of modeling languages: which include IDEF0 for functional modeling: IDEF1X for information modeling: IDEF3 for business process modeling: IDEF4 for Object-Oriented Design and IDEF5 for modeling ontologies. / Jackson Structured Programming (JSP) is a method for structured programming based on correspondences between data stream structure and program structure. / LePUS3 is an object-oriented visual Design Description Language and a formal specification language that is suitable primarily for modeling large object-oriented (Java: C++: C#) programs and design patterns. / Lifecycle Modeling Language is an open-standard language for systems engineering that supports the full system lifecycle: conceptual: utilization: support and retirement stages. / Object-Role Modeling (ORM) in the field of software engineering is a method for conceptual modeling: and can be used as a tool for information and rules analysis. / Petri nets use variations on exactly one diagramming technique and topology: namely the bipartite graph.  The simplicity of its basic user interface easily enabled extensive tool support over the years: particularly in the areas of model checking: graphically oriented simulation: and software verification. / Southbeach Notation is a visual modeling language used to describe situations in terms of agents that are considered useful or harmful from the modeler's perspective. The notation shows how the agents interact with each other and whether this interaction improves or worsens the situation. / Specification and Description Language (SDL) is a specification language targeted at the unambiguous specification and description of the behavior of reactive and distributed systems. / SysML is a Domain-Specific Modeling language for systems engineering that is defined as a UML profile (customization). / Unified Modeling Language (UML) is a general-purpose modeling language that is an industry standard for specifying software-intensive systems. UML 2.0: the current version: supports thirteen different diagram techniques: and has widespread tool support. / Service-oriented modeling framework (SOMF) is a holistic language for designing enterprise and application level architecture models in the space of enterprise architecture: virtualization: service-oriented architecture (SOA): cloud computing: and more. / Architecture description language (ADL) is a language used to describe and represent the systems architecture of a system. / Architecture Analysis & Design Language (AADL) is a modeling language that supports early and repeated analyses of a system's architecture with respect to performance-critical properties through an extendable notation: a tool framework: and precisely defined semantics.Examples of graphical modeling languages in other fields of science. /  / EAST-ADL is a  Domain-Specific Modeling language dedicated to automotive system design.","Score: 0.8501995345079268 / Modeling languages can be used to specify: /  / system requirements: / structures and / behaviors.Modeling languages are intended to be used to precisely specify systems so that stakeholders (e.g.: customers: operators: analysts: designers) can better understand the system being modeled. / The more mature modeling languages are precise: consistent and executable. Informal diagramming techniques applied with drawing tools are expected to produce useful pictorial representations of system requirements: structures and behaviors: which can be useful for communication: design: and problem solving but cannot be used programmatically.: 539  Executable modeling languages applied with proper tool support: however: are expected to automate system verification and validation: simulation and code generation from the same representations. /  /  / == Quality == / A review of modelling languages is essential to be able to assign which languages are appropriate for different modelling settings. In the term settings we include stakeholders: domain and the knowledge connected. Assessing the language quality is a means that aims to achieve better models. /  /  / === Framework for evaluation === / Here language quality is stated in accordance with the SEQUAL framework for quality of models developed by Krogstie: Sindre and Lindland (2003): since this is a framework that connects the language quality to a framework for general model quality. Five areas are used in this framework to describe language quality and these are supposed to express both the conceptual as well as the visual notation of the language. We will not go into a thoroughly explanation of the underlying quality framework of models but concentrate on the areas used to explain the language quality framework. /  /  / ==== Domain appropriateness ==== / The framework states the ability to represent the domain as domain appropriateness. The statement appropriateness can be a bit vague: but in this particular context it means able to express. You should ideally only be able to express things that are in the domain but be powerful enough to include everything that is in the domain. This requirement might seem a bit strict: but the aim is to get a visually expressed model which includes everything relevant to the domain and excludes everything not appropriate for the domain. To achieve this: the language has to have a good distinction of which notations and syntaxes that are advantageous to present. /  /  / ==== Participant appropriateness ==== / To evaluate the participant appropriateness we try to identify how well the language expresses the knowledge held by the stakeholders. This involves challenges since a stakeholder's knowledge is subjective. The knowledge of the stakeholder is both tacit and explicit. Both types of knowledge are of dynamic character. In this framework only the explicit type of knowledge is taken into account. The language should to a large extent express all the explicit knowledge of the stakeholders relevant to the domain. /  /  / ==== Modeller appropriateness ==== / Last paragraph stated that knowledge of the stakeholders should be presented in a good way. In addition it is imperative that the language should be able to express all possible explicit knowledge of the stakeholders. No knowledge should be left unexpressed due to lacks in the language. /  /  / ==== Comprehensibility appropriateness ==== / Comprehensibility appropriateness makes sure that the social actors understand the model due to a consistent use of the language. To achieve this the framework includes a set of criteria. The general importance that these express is that the language should be flexible: easy to organize and easy to distinguish different parts of the language internally as well as from other languages. In addition to this: the goal should be as simple as possible and that each symbol in the language has a unique representation. / This is in connection to also to the structure of the development requirements.  / . /  /  / ==== Tool appropriateness ==== / To ensure that the domain actually modelled is usable for analyzing and further processing: the language has to ensure that it is possible to reason in an automatic way. To achieve this it has to include formal syntax and semantics. Another advantage by formalizing is the ability to discover errors in an early stage. It is not always that the language best fitted for the technical actors is the same as for the social actors. /  /  / ==== Organizational appropriateness ==== / The language used is appropriate for the organizational context: e.g. that the language is standardized within the organization: or that it is supported by tools that are chosen as standard in the organization. /  /  / == See also == /  /  / == References == /  /  / == Further reading == / John Krogstie (2003) ""Evaluating UML using a generic quality framework"" . SINTEF Telecom and Informatics and IDI: NTNU: Norway / Krogstie and Sølvsberg (2003). Information Systems Engineering: Conceptual Modeling in a Quality Perspective."
0.1293693999759853,How do executable modeling languages benefit programmers?,"Executable modeling languages aim to boost the productivity of skilled programmers. They allow programmers to tackle more complex problems, like parallel computing and distributed systems. However, their use does not imply that the need for programmers is eliminated. ","Score: 0.830374574746492 / The algebraic formulation of a model does not contain any hints how to process it. /  /  / ==== Behavioral ==== / Behavioral languages are designed to describe the observable behavior of complex systems consisting of components that / execute concurrently. These languages focus on the description of key concepts such as: concurrency: nondeterminism: synchronization: and communication. The semantic foundations of Behavioral languages are process calculus or process algebra. /  /  / ==== Discipline-specific ==== / A discipline-specific modeling (DspM) language is focused on deliverables affiliated with a specific software development life cycle stage. Therefore: such language offers a distinct vocabulary: syntax: and notation for each stage: such as discovery: analysis: design: architecture: contraction: etc. For example: for the analysis phase of a project: the modeler employs specific analysis notation to deliver an analysis proposition diagram. During the design phase: however: logical design notation is used to depict relationship between software entities. In addition: the discipline-specific modeling language best practices does not preclude practitioners from combining the various notations in a single diagram. /  /  / ==== Domain-specific ==== / Domain-specific modeling (DSM) is a software engineering methodology for designing and developing systems: most often IT systems such as computer software. It involves systematic use of a graphical domain-specific language (DSL) to represent the various facets of a system. DSM languages tend to support higher-level abstractions than General-purpose modeling languages: so they require less effort and fewer low-level details to specify a given system. /  /  / ==== Framework-specific ==== / A framework-specific modeling language (FSML) is a kind of domain-specific modeling language which is designed for an object-oriented application framework. FSMLs define framework-provided abstractions as FSML concepts and decompose the abstractions into features. The features represent implementation steps or choices. / A FSML concept can be configured by selecting features and providing values for features. Such a concept configuration represents how the concept should be implemented in the code. In other words: concept configuration describes how the framework should be completed in order to create the implementation of the concept. /  /  / ==== Information and knowledge modeling ==== / Linked data and ontology engineering require 'host languages' to represent entities and the relations between them: constraints between the properties of entities and relations: and metadata attributes. JSON-LD and RDF are two major (and semantically almost equivalent) languages in this context: primarily because they support statement reification and contextualisation which are essential properties to support the higher-order logic needed to reason about models. Model transformation is a common example of such reasoning. /  /  / ==== Object-oriented ==== / Object modeling languages are modeling languages based on a standardized set of symbols and ways of arranging them to model (part of) an object oriented software design or system design. / Some organizations use them extensively in combination with a software development methodology to progress from initial specification to an implementation plan and to communicate that plan to an entire team of developers and stakeholders. Because a modeling language is visual and at a higher-level of abstraction than code: using models encourages the generation of a shared vision that may prevent problems of differing interpretation later in development. Often software modeling tools are used to construct these models: which may then be capable of automatic translation to code. /  /  / ==== Virtual reality ==== / Virtual Reality Modeling Language (VRML): before 1995 known as the Virtual Reality Markup Language is a standard file format for representing 3-dimensional (3D) interactive vector graphics: designed particularly with the World Wide Web in mind. /  /  / ==== Others ==== / Architecture Description Language / Face Modeling Language / Generative Modelling Language / Java Modeling Language / Promela / Rebeca Modeling Language / Service Modeling Language / Web Services Modeling Language / X3D /  /  / == Applications == / Various kinds of modeling languages are applied in different disciplines: including computer science: information management: business process modeling: software engineering: and systems engineering. Modeling languages can be used to specify: /  / system requirements: / structures and / behaviors.Modeling languages are intended to be used to precisely specify systems so that stakeholders (e.g.: customers: operators: analysts: designers) can better understand the system being modeled. / The more mature modeling languages are precise: consistent and executable. Informal diagramming techniques applied with drawing tools are expected to produce useful pictorial representations of system requirements: structures and behaviors: which can be useful for communication: design: and problem solving but cannot be used programmatically.: 539  Executable modeling languages applied with proper tool support: however: are expected to automate system verification and validation: simulation and code generation from the same representations. /  /  / == Quality == / A review of modelling languages is essential to be able to assign which languages are appropriate for different modelling settings. In the term settings we include stakeholders: domain and the knowledge connected.","Score: 0.8264774372692739 / Modeling languages can be used to specify: /  / system requirements: / structures and / behaviors.Modeling languages are intended to be used to precisely specify systems so that stakeholders (e.g.: customers: operators: analysts: designers) can better understand the system being modeled. / The more mature modeling languages are precise: consistent and executable. Informal diagramming techniques applied with drawing tools are expected to produce useful pictorial representations of system requirements: structures and behaviors: which can be useful for communication: design: and problem solving but cannot be used programmatically.: 539  Executable modeling languages applied with proper tool support: however: are expected to automate system verification and validation: simulation and code generation from the same representations. /  /  / == Quality == / A review of modelling languages is essential to be able to assign which languages are appropriate for different modelling settings. In the term settings we include stakeholders: domain and the knowledge connected. Assessing the language quality is a means that aims to achieve better models. /  /  / === Framework for evaluation === / Here language quality is stated in accordance with the SEQUAL framework for quality of models developed by Krogstie: Sindre and Lindland (2003): since this is a framework that connects the language quality to a framework for general model quality. Five areas are used in this framework to describe language quality and these are supposed to express both the conceptual as well as the visual notation of the language. We will not go into a thoroughly explanation of the underlying quality framework of models but concentrate on the areas used to explain the language quality framework. /  /  / ==== Domain appropriateness ==== / The framework states the ability to represent the domain as domain appropriateness. The statement appropriateness can be a bit vague: but in this particular context it means able to express. You should ideally only be able to express things that are in the domain but be powerful enough to include everything that is in the domain. This requirement might seem a bit strict: but the aim is to get a visually expressed model which includes everything relevant to the domain and excludes everything not appropriate for the domain. To achieve this: the language has to have a good distinction of which notations and syntaxes that are advantageous to present. /  /  / ==== Participant appropriateness ==== / To evaluate the participant appropriateness we try to identify how well the language expresses the knowledge held by the stakeholders. This involves challenges since a stakeholder's knowledge is subjective. The knowledge of the stakeholder is both tacit and explicit. Both types of knowledge are of dynamic character. In this framework only the explicit type of knowledge is taken into account. The language should to a large extent express all the explicit knowledge of the stakeholders relevant to the domain. /  /  / ==== Modeller appropriateness ==== / Last paragraph stated that knowledge of the stakeholders should be presented in a good way. In addition it is imperative that the language should be able to express all possible explicit knowledge of the stakeholders. No knowledge should be left unexpressed due to lacks in the language. /  /  / ==== Comprehensibility appropriateness ==== / Comprehensibility appropriateness makes sure that the social actors understand the model due to a consistent use of the language. To achieve this the framework includes a set of criteria. The general importance that these express is that the language should be flexible: easy to organize and easy to distinguish different parts of the language internally as well as from other languages. In addition to this: the goal should be as simple as possible and that each symbol in the language has a unique representation. / This is in connection to also to the structure of the development requirements.  / . /  /  / ==== Tool appropriateness ==== / To ensure that the domain actually modelled is usable for analyzing and further processing: the language has to ensure that it is possible to reason in an automatic way. To achieve this it has to include formal syntax and semantics. Another advantage by formalizing is the ability to discover errors in an early stage. It is not always that the language best fitted for the technical actors is the same as for the social actors. /  /  / ==== Organizational appropriateness ==== / The language used is appropriate for the organizational context: e.g. that the language is standardized within the organization: or that it is supported by tools that are chosen as standard in the organization. /  /  / == See also == /  /  / == References == /  /  / == Further reading == / John Krogstie (2003) ""Evaluating UML using a generic quality framework"" . SINTEF Telecom and Informatics and IDI: NTNU: Norway / Krogstie and Sølvsberg (2003). Information Systems Engineering: Conceptual Modeling in a Quality Perspective."
0.13360229998943396,What is an example of a graphical modeling language and its corresponding textual modeling language?,EXPRESS is an example of a graphical modeling language and its corresponding textual modeling language.,"Score: 0.8877146673371497 / A modeling language is any artificial language that can be used to express data: information or knowledge or systems in a structure that is defined by a consistent set of rules. The rules are used for interpretation of the meaning of components in the structure Programing language. /  /  / == Overview == / A modeling language can be graphical or textual. / Graphical modeling languages use a diagram technique with named symbols that represent concepts and lines that connect the symbols and represent relationships and various other graphical notation to represent constraints. / Textual modeling languages may use standardized keywords accompanied by parameters or natural language terms and phrases to make computer-interpretable expressions.An example of a graphical modeling language and a corresponding textual modeling language is EXPRESS. / Not all modeling languages are executable: and for those that are: the use of them doesn't necessarily mean that programmers are no longer required. On the contrary: executable modeling languages are intended to amplify the productivity of skilled programmers: so that they can address more challenging problems: such as parallel computing and distributed systems. / A large number of modeling languages appear in the literature. /  /  / == Type of modeling languages == /  /  / === Graphical types === / Example of graphical modeling languages in the field of computer science: project management and systems engineering: /  / Behavior Trees are a formal: graphical modeling language used primarily in systems and software engineering. Commonly used to unambiguously represent the hundreds or even thousands of natural language requirements that are typically used to express the stakeholder needs for a large-scale software-integrated system. / Business Process Modeling Notation (BPMN: and the XML form BPML) is an example of a Process Modeling language. / C-K theory consists of a modeling language for design processes. / DRAKON is a general-purpose algorithmic modeling language for specifying software-intensive systems: a schematic representation of an algorithm or a stepwise process: and a family of programming languages. / EXPRESS and EXPRESS-G (ISO 10303-11) is an international standard general-purpose data modeling language. / Extended Enterprise Modeling Language (EEML) is commonly used for business process modeling across a number of layers. / Flowchart is a schematic representation of an algorithm or a stepwise process. / Fundamental Modeling Concepts (FMC) modeling language for software-intensive systems. / IDEF is a family of modeling languages: which include IDEF0 for functional modeling: IDEF1X for information modeling: IDEF3 for business process modeling: IDEF4 for Object-Oriented Design and IDEF5 for modeling ontologies. / Jackson Structured Programming (JSP) is a method for structured programming based on correspondences between data stream structure and program structure. / LePUS3 is an object-oriented visual Design Description Language and a formal specification language that is suitable primarily for modeling large object-oriented (Java: C++: C#) programs and design patterns. / Lifecycle Modeling Language is an open-standard language for systems engineering that supports the full system lifecycle: conceptual: utilization: support and retirement stages. / Object-Role Modeling (ORM) in the field of software engineering is a method for conceptual modeling: and can be used as a tool for information and rules analysis. / Petri nets use variations on exactly one diagramming technique and topology: namely the bipartite graph.  The simplicity of its basic user interface easily enabled extensive tool support over the years: particularly in the areas of model checking: graphically oriented simulation: and software verification. / Southbeach Notation is a visual modeling language used to describe situations in terms of agents that are considered useful or harmful from the modeler's perspective. The notation shows how the agents interact with each other and whether this interaction improves or worsens the situation. / Specification and Description Language (SDL) is a specification language targeted at the unambiguous specification and description of the behavior of reactive and distributed systems. / SysML is a Domain-Specific Modeling language for systems engineering that is defined as a UML profile (customization). / Unified Modeling Language (UML) is a general-purpose modeling language that is an industry standard for specifying software-intensive systems. UML 2.0: the current version: supports thirteen different diagram techniques: and has widespread tool support. / Service-oriented modeling framework (SOMF) is a holistic language for designing enterprise and application level architecture models in the space of enterprise architecture: virtualization: service-oriented architecture (SOA): cloud computing: and more. / Architecture description language (ADL) is a language used to describe and represent the systems architecture of a system. / Architecture Analysis & Design Language (AADL) is a modeling language that supports early and repeated analyses of a system's architecture with respect to performance-critical properties through an extendable notation: a tool framework: and precisely defined semantics.Examples of graphical modeling languages in other fields of science. /  / EAST-ADL is a  Domain-Specific Modeling language dedicated to automotive system design.","Score: 0.8561749703311824 / Unified Modeling Language (UML) is a general-purpose modeling language that is an industry standard for specifying software-intensive systems. UML 2.0: the current version: supports thirteen different diagram techniques: and has widespread tool support. / Service-oriented modeling framework (SOMF) is a holistic language for designing enterprise and application level architecture models in the space of enterprise architecture: virtualization: service-oriented architecture (SOA): cloud computing: and more. / Architecture description language (ADL) is a language used to describe and represent the systems architecture of a system. / Architecture Analysis & Design Language (AADL) is a modeling language that supports early and repeated analyses of a system's architecture with respect to performance-critical properties through an extendable notation: a tool framework: and precisely defined semantics.Examples of graphical modeling languages in other fields of science. /  / EAST-ADL is a  Domain-Specific Modeling language dedicated to automotive system design. / Energy Systems Language (ESL): a language that aims to model ecological energetics & global economics. / IEC 61499 defines Domain-Specific Modeling language dedicated to distribute industrial process measurement and control systems. /  /  / === Textual types === / Information models can also be expressed in formalized natural languages: such as Gellish. Gellish has natural language variants such as Gellish Formal English and Gellish Formal Dutch (Gellish Formeel Nederlands): etc. Gellish Formal English is an information representation language or semantic modeling language that is defined in the Gellish English Dictionary-Taxonomy: which has the form of a Taxonomy-Ontology (similarly for Dutch). Gellish Formal English is not only suitable to express knowledge: requirements and dictionaries: taxonomies and ontologies: but also information about individual things. All that information is expressed in one language and therefore it can all be integrated: independent of the question whether it is stored in central or distributed or in federated databases. Information models in Gellish Formal English consists of collections of Gellish Formal English expressions: that use natural language terms and formalized phrases. For example: a geographic information model might consist of a number of Gellish Formal English expressions: such as: /  / - the Eiffel tower <is located in> Paris / - Paris <is classified as a> city /  / whereas information requirements and knowledge can be expressed for example as follows: /  / - tower <shall be located in a> geographical area / - city <is a kind of> geographical area /  / Such Gellish Formal English expressions use names of concepts (such as ""city"") and phrases that represent relation types (such as ⟨is located in⟩ and ⟨is classified as a⟩) that should be selected from the Gellish English Dictionary-Taxonomy (or of your own domain dictionary). The Gellish English Dictionary-Taxonomy enables the creation of semantically rich information models: because the dictionary contains more than 600 standard relation types and contains definitions of more than 40000 concepts. An information model in Gellish can express facts or make statements: queries and answers. /  /  / === More specific types === / In the field of computer science recently more specific types of modeling languages have emerged. /  /  / ==== Algebraic ==== / Algebraic Modeling Languages (AML) are high-level programming languages for describing and solving high complexity problems for large scale mathematical computation (i.e. large scale optimization type problems). One particular advantage of AMLs like AIMMS: AMPL: GAMS: Gekko: Mosel: OPL and OptimJ is the similarity of its syntax to the mathematical notation of optimization problems. This allows for a very concise and readable definition of problems in the domain of optimization: which is supported by certain language elements like sets: indices: algebraic expressions: powerful sparse index and data handling variables: constraints with arbitrary names. The algebraic formulation of a model does not contain any hints how to process it. /  /  / ==== Behavioral ==== / Behavioral languages are designed to describe the observable behavior of complex systems consisting of components that / execute concurrently. These languages focus on the description of key concepts such as: concurrency: nondeterminism: synchronization: and communication. The semantic foundations of Behavioral languages are process calculus or process algebra. /  /  / ==== Discipline-specific ==== / A discipline-specific modeling (DspM) language is focused on deliverables affiliated with a specific software development life cycle stage. Therefore: such language offers a distinct vocabulary: syntax: and notation for each stage: such as discovery: analysis: design: architecture: contraction: etc. For example: for the analysis phase of a project: the modeler employs specific analysis notation to deliver an analysis proposition diagram. During the design phase: however: logical design notation is used to depict relationship between software entities."
0.16273959999671206,What is the role of modeling languages in system development?,"Modeling languages can be used to specify system requirements, structures, and behaviors, offering a precise specification of systems to better understand the system being modeled by stakeholders like customers, operators, analysts, and designers. More mature modeling languages that are precise, consistent, and executable can automate system verification and validation, simulation, and code generation.","Score: 0.8647723901877755 / The algebraic formulation of a model does not contain any hints how to process it. /  /  / ==== Behavioral ==== / Behavioral languages are designed to describe the observable behavior of complex systems consisting of components that / execute concurrently. These languages focus on the description of key concepts such as: concurrency: nondeterminism: synchronization: and communication. The semantic foundations of Behavioral languages are process calculus or process algebra. /  /  / ==== Discipline-specific ==== / A discipline-specific modeling (DspM) language is focused on deliverables affiliated with a specific software development life cycle stage. Therefore: such language offers a distinct vocabulary: syntax: and notation for each stage: such as discovery: analysis: design: architecture: contraction: etc. For example: for the analysis phase of a project: the modeler employs specific analysis notation to deliver an analysis proposition diagram. During the design phase: however: logical design notation is used to depict relationship between software entities. In addition: the discipline-specific modeling language best practices does not preclude practitioners from combining the various notations in a single diagram. /  /  / ==== Domain-specific ==== / Domain-specific modeling (DSM) is a software engineering methodology for designing and developing systems: most often IT systems such as computer software. It involves systematic use of a graphical domain-specific language (DSL) to represent the various facets of a system. DSM languages tend to support higher-level abstractions than General-purpose modeling languages: so they require less effort and fewer low-level details to specify a given system. /  /  / ==== Framework-specific ==== / A framework-specific modeling language (FSML) is a kind of domain-specific modeling language which is designed for an object-oriented application framework. FSMLs define framework-provided abstractions as FSML concepts and decompose the abstractions into features. The features represent implementation steps or choices. / A FSML concept can be configured by selecting features and providing values for features. Such a concept configuration represents how the concept should be implemented in the code. In other words: concept configuration describes how the framework should be completed in order to create the implementation of the concept. /  /  / ==== Information and knowledge modeling ==== / Linked data and ontology engineering require 'host languages' to represent entities and the relations between them: constraints between the properties of entities and relations: and metadata attributes. JSON-LD and RDF are two major (and semantically almost equivalent) languages in this context: primarily because they support statement reification and contextualisation which are essential properties to support the higher-order logic needed to reason about models. Model transformation is a common example of such reasoning. /  /  / ==== Object-oriented ==== / Object modeling languages are modeling languages based on a standardized set of symbols and ways of arranging them to model (part of) an object oriented software design or system design. / Some organizations use them extensively in combination with a software development methodology to progress from initial specification to an implementation plan and to communicate that plan to an entire team of developers and stakeholders. Because a modeling language is visual and at a higher-level of abstraction than code: using models encourages the generation of a shared vision that may prevent problems of differing interpretation later in development. Often software modeling tools are used to construct these models: which may then be capable of automatic translation to code. /  /  / ==== Virtual reality ==== / Virtual Reality Modeling Language (VRML): before 1995 known as the Virtual Reality Markup Language is a standard file format for representing 3-dimensional (3D) interactive vector graphics: designed particularly with the World Wide Web in mind. /  /  / ==== Others ==== / Architecture Description Language / Face Modeling Language / Generative Modelling Language / Java Modeling Language / Promela / Rebeca Modeling Language / Service Modeling Language / Web Services Modeling Language / X3D /  /  / == Applications == / Various kinds of modeling languages are applied in different disciplines: including computer science: information management: business process modeling: software engineering: and systems engineering. Modeling languages can be used to specify: /  / system requirements: / structures and / behaviors.Modeling languages are intended to be used to precisely specify systems so that stakeholders (e.g.: customers: operators: analysts: designers) can better understand the system being modeled. / The more mature modeling languages are precise: consistent and executable. Informal diagramming techniques applied with drawing tools are expected to produce useful pictorial representations of system requirements: structures and behaviors: which can be useful for communication: design: and problem solving but cannot be used programmatically.: 539  Executable modeling languages applied with proper tool support: however: are expected to automate system verification and validation: simulation and code generation from the same representations. /  /  / == Quality == / A review of modelling languages is essential to be able to assign which languages are appropriate for different modelling settings. In the term settings we include stakeholders: domain and the knowledge connected.","Score: 0.862681642731773 / Modeling languages can be used to specify: /  / system requirements: / structures and / behaviors.Modeling languages are intended to be used to precisely specify systems so that stakeholders (e.g.: customers: operators: analysts: designers) can better understand the system being modeled. / The more mature modeling languages are precise: consistent and executable. Informal diagramming techniques applied with drawing tools are expected to produce useful pictorial representations of system requirements: structures and behaviors: which can be useful for communication: design: and problem solving but cannot be used programmatically.: 539  Executable modeling languages applied with proper tool support: however: are expected to automate system verification and validation: simulation and code generation from the same representations. /  /  / == Quality == / A review of modelling languages is essential to be able to assign which languages are appropriate for different modelling settings. In the term settings we include stakeholders: domain and the knowledge connected. Assessing the language quality is a means that aims to achieve better models. /  /  / === Framework for evaluation === / Here language quality is stated in accordance with the SEQUAL framework for quality of models developed by Krogstie: Sindre and Lindland (2003): since this is a framework that connects the language quality to a framework for general model quality. Five areas are used in this framework to describe language quality and these are supposed to express both the conceptual as well as the visual notation of the language. We will not go into a thoroughly explanation of the underlying quality framework of models but concentrate on the areas used to explain the language quality framework. /  /  / ==== Domain appropriateness ==== / The framework states the ability to represent the domain as domain appropriateness. The statement appropriateness can be a bit vague: but in this particular context it means able to express. You should ideally only be able to express things that are in the domain but be powerful enough to include everything that is in the domain. This requirement might seem a bit strict: but the aim is to get a visually expressed model which includes everything relevant to the domain and excludes everything not appropriate for the domain. To achieve this: the language has to have a good distinction of which notations and syntaxes that are advantageous to present. /  /  / ==== Participant appropriateness ==== / To evaluate the participant appropriateness we try to identify how well the language expresses the knowledge held by the stakeholders. This involves challenges since a stakeholder's knowledge is subjective. The knowledge of the stakeholder is both tacit and explicit. Both types of knowledge are of dynamic character. In this framework only the explicit type of knowledge is taken into account. The language should to a large extent express all the explicit knowledge of the stakeholders relevant to the domain. /  /  / ==== Modeller appropriateness ==== / Last paragraph stated that knowledge of the stakeholders should be presented in a good way. In addition it is imperative that the language should be able to express all possible explicit knowledge of the stakeholders. No knowledge should be left unexpressed due to lacks in the language. /  /  / ==== Comprehensibility appropriateness ==== / Comprehensibility appropriateness makes sure that the social actors understand the model due to a consistent use of the language. To achieve this the framework includes a set of criteria. The general importance that these express is that the language should be flexible: easy to organize and easy to distinguish different parts of the language internally as well as from other languages. In addition to this: the goal should be as simple as possible and that each symbol in the language has a unique representation. / This is in connection to also to the structure of the development requirements.  / . /  /  / ==== Tool appropriateness ==== / To ensure that the domain actually modelled is usable for analyzing and further processing: the language has to ensure that it is possible to reason in an automatic way. To achieve this it has to include formal syntax and semantics. Another advantage by formalizing is the ability to discover errors in an early stage. It is not always that the language best fitted for the technical actors is the same as for the social actors. /  /  / ==== Organizational appropriateness ==== / The language used is appropriate for the organizational context: e.g. that the language is standardized within the organization: or that it is supported by tools that are chosen as standard in the organization. /  /  / == See also == /  /  / == References == /  /  / == Further reading == / John Krogstie (2003) ""Evaluating UML using a generic quality framework"" . SINTEF Telecom and Informatics and IDI: NTNU: Norway / Krogstie and Sølvsberg (2003). Information Systems Engineering: Conceptual Modeling in a Quality Perspective."
0.13746299999183975,What is a neural network and how closely is it related to artificial neural networks?,"A neural network, also known as a neuronal network, is an interconnected population of neurons that typically contain multiple neural circuits. These networks are studied to understand the organization and functioning of nervous systems. Artificial neural networks are machine learning models, which are very closely related to biological neural networks as they are inspired and designed based on the mechanisms used by neural circuits in biological networks.","Score: 0.8561435722018509 / In machine learning: an artificial neural network (also neural network or neural net: abbreviated ANN or NN) is a model inspired by the neuronal organization found in the biological neural networks in animal brains.An ANN is made of connected units or nodes called artificial neurons: which loosely model the neurons in a brain. These are connected by edges: which model the synapses in a brain. An artificial neuron receives signals from connected neurons: then processes them and sends a signal to other connected neurons. The ""signal"" is a real number: and the output of each neuron is computed by some non-linear function of the sum of its inputs: called the activation function. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. / Typically: neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer): possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least 2 hidden layers.Artificial neural networks are used for predictive modeling: adaptive control: and other applications where they can be trained via a dataset. They are also used to solve problems in artificial intelligence. Networks can learn from experience: and can derive conclusions from a complex and seemingly unrelated set of information. /  /  / == Training == / Neural networks are typically trained through empirical risk minimization. This method is based on the idea of optimizing the network's parameters to minimize the difference: or empirical risk: between the predicted output and the actual target values in a given dataset. Gradient based methods such as backpropagation are usually used to estimate the parameters of the network. During the training phase: ANNs learn from labeled training data by iteratively updating their parameters to minimize a defined loss function. This method allows the network to generalize to unseen data. /  /  / == History == /  / Historically: digital computers evolved from the von Neumann model: and operate via the execution of explicit instructions via access to memory by a number of processors. Neural networks: on the other hand: originated from efforts to model information processing in biological systems through the framework of connectionism. Unlike the von Neumann model: connectionist computing does not separate memory and processing. / The simplest kind of feedforward neural network (FNN) is a linear network: which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated at each node. The mean squared errors between these calculated outputs and the given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement.Warren McCulloch and Walter Pitts (1943) also considered a non-learning computational model for neural networks.In the late 1940s: D. O. Hebb created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Hebbian learning is considered to be a 'typical' unsupervised learning rule and its later variants were early models for long term potentiation. These ideas started being applied to computational models in 1948 with Turing's ""unorganized machines"". Farley and Wesley A. Clark were the first to simulate a Hebbian network in 1954 at MIT. They used computational machines: then called ""calculators"". Other neural network computational machines were created by Rochester: Holland: Habit: and Duda in 1956. In 1958: psychologist Frank Rosenblatt invented the perceptron: the first implemented artificial neural network: funded by the United States Office of Naval Research. / The invention of the perceptron raised public excitement for research in Artificial Neural Networks: causing the US government to drastically increase funding into deep learning research. This led to ""the golden age of AI"" fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence. For example: in 1957 Herbert Simon famously said:It is not my aim to surprise or shock you—but the simplest way I can summarize is to say that there are now in the world machines that think: that learn and that create.","Score: 0.8558875567770321 / This allows simple statistical association (the basic function of artificial neural networks) to be described as learning or recognition. In 1997: Alexander Dewdney: a former Scientific American columnist: commented that as a result: artificial neural networks have a ""something-for-nothing quality: one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are. No human hand (or mind) intervenes; solutions are found as if by magic; and no one: it seems: has learned anything"". One response to Dewdney is that neural networks have been successfully used to handle many complex and diverse tasks: ranging from autonomously flying aircraft to detecting credit card fraud to mastering the game of Go. / Technology writer Roger Bridgman commented: /  / Neural networks: for instance: are in the dock not only because they have been hyped to high heaven: (what hasn't?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be ""an opaque: unreadable table...valueless as a scientific resource"". / In spite of his emphatic declaration that science is not technology: Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having. /  / Although it is true that analyzing what has been learned by an artificial neural network is difficult: it is much easier to do so than to analyze what has been learned by a biological neural network. Moreover: recent emphasis on the explainability of AI has contributed towards the development of methods: notably those based on attention mechanisms: for visualizing and explaining learned neural networks. Furthermore: researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles that allow a learning machine to be successful. For example: Bengio and LeCun (2007) wrote an article regarding local vs non-local learning: as well as shallow vs deep architecture.Biological brains use both shallow and deep circuits as reported by brain anatomy: displaying a wide variety of invariance. Weng argued that the brain self-wires largely according to signal statistics and therefore: a serial cascade cannot catch all major statistical dependencies. /  /  / === Hardware === / Large and effective neural networks require considerable computing resources. While the brain has hardware tailored to the task of processing signals through a graph of neurons: simulating even a simplified neuron on von Neumann architecture may consume vast amounts of memory and storage. Furthermore: the designer often needs to transmit signals through many of these connections and their associated neurons –  which require enormous CPU power and time. / Schmidhuber noted that the resurgence of neural networks in the twenty-first century is largely attributable to advances in hardware: from 1991 to 2015: computing power: especially as delivered by GPGPUs (on GPUs): has increased around a million-fold: making the standard backpropagation algorithm feasible for training networks that are several layers deeper than before. The use of accelerators such as FPGAs and GPUs can reduce training times from months to days.Neuromorphic engineering or a physical neural network addresses the hardware difficulty directly: by constructing non-von-Neumann chips to directly implement neural networks in circuitry. Another type of chip optimized for neural network processing is called a Tensor Processing Unit: or TPU. /  /  / === Practical counterexamples === / Analyzing what has been learned by an ANN is much easier than analyzing what has been learned by a biological neural network. Furthermore: researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a learning machine to be successful. For example: local vs. non-local learning and shallow vs. deep architecture. /  /  / === Hybrid approaches === / Advocates of hybrid models (combining neural networks and symbolic approaches) say that such a mixture can better capture the mechanisms of the human mind. /  /  / === Dataset bias === / Neural networks are dependent on the quality of the data they are trained on: thus low quality data with imbalanced representativeness can lead to the model learning and perpetuating societal biases.  These inherited biases become especially critical when the ANNs are integrated into real-world scenarios where the training data may be imbalanced due to the scarcity of data for a specific race: gender or other attribute. This imbalance can result in the model having inadequate representation and understanding of underrepresented groups: leading to discriminatory outcomes that exasperate societal inequalities: especially in applications like facial recognition: hiring processes: and law enforcement."
0.1369008999899961,What are some applications of artificial neural networks in the field of artificial intelligence?,"In the field of artificial intelligence, artificial neural networks have been applied successfully to several areas including speech recognition, image analysis, and adaptive control. They are also used to create software agents in computer and video games or in the construction of autonomous robots.","Score: 0.8609220807885731 / This allows simple statistical association (the basic function of artificial neural networks) to be described as learning or recognition. In 1997: Alexander Dewdney: a former Scientific American columnist: commented that as a result: artificial neural networks have a ""something-for-nothing quality: one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are. No human hand (or mind) intervenes; solutions are found as if by magic; and no one: it seems: has learned anything"". One response to Dewdney is that neural networks have been successfully used to handle many complex and diverse tasks: ranging from autonomously flying aircraft to detecting credit card fraud to mastering the game of Go. / Technology writer Roger Bridgman commented: /  / Neural networks: for instance: are in the dock not only because they have been hyped to high heaven: (what hasn't?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be ""an opaque: unreadable table...valueless as a scientific resource"". / In spite of his emphatic declaration that science is not technology: Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having. /  / Although it is true that analyzing what has been learned by an artificial neural network is difficult: it is much easier to do so than to analyze what has been learned by a biological neural network. Moreover: recent emphasis on the explainability of AI has contributed towards the development of methods: notably those based on attention mechanisms: for visualizing and explaining learned neural networks. Furthermore: researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles that allow a learning machine to be successful. For example: Bengio and LeCun (2007) wrote an article regarding local vs non-local learning: as well as shallow vs deep architecture.Biological brains use both shallow and deep circuits as reported by brain anatomy: displaying a wide variety of invariance. Weng argued that the brain self-wires largely according to signal statistics and therefore: a serial cascade cannot catch all major statistical dependencies. /  /  / === Hardware === / Large and effective neural networks require considerable computing resources. While the brain has hardware tailored to the task of processing signals through a graph of neurons: simulating even a simplified neuron on von Neumann architecture may consume vast amounts of memory and storage. Furthermore: the designer often needs to transmit signals through many of these connections and their associated neurons –  which require enormous CPU power and time. / Schmidhuber noted that the resurgence of neural networks in the twenty-first century is largely attributable to advances in hardware: from 1991 to 2015: computing power: especially as delivered by GPGPUs (on GPUs): has increased around a million-fold: making the standard backpropagation algorithm feasible for training networks that are several layers deeper than before. The use of accelerators such as FPGAs and GPUs can reduce training times from months to days.Neuromorphic engineering or a physical neural network addresses the hardware difficulty directly: by constructing non-von-Neumann chips to directly implement neural networks in circuitry. Another type of chip optimized for neural network processing is called a Tensor Processing Unit: or TPU. /  /  / === Practical counterexamples === / Analyzing what has been learned by an ANN is much easier than analyzing what has been learned by a biological neural network. Furthermore: researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a learning machine to be successful. For example: local vs. non-local learning and shallow vs. deep architecture. /  /  / === Hybrid approaches === / Advocates of hybrid models (combining neural networks and symbolic approaches) say that such a mixture can better capture the mechanisms of the human mind. /  /  / === Dataset bias === / Neural networks are dependent on the quality of the data they are trained on: thus low quality data with imbalanced representativeness can lead to the model learning and perpetuating societal biases.  These inherited biases become especially critical when the ANNs are integrated into real-world scenarios where the training data may be imbalanced due to the scarcity of data for a specific race: gender or other attribute. This imbalance can result in the model having inadequate representation and understanding of underrepresented groups: leading to discriminatory outcomes that exasperate societal inequalities: especially in applications like facial recognition: hiring processes: and law enforcement.","Score: 0.855177484679998 / Such techniques lack ways of representing causal relationships (...) have no obvious ways of performing logical inferences: and they are also still a long way from integrating abstract knowledge: such as information about what objects are: what they are for: and how they are typically used. The most powerful A.I. systems: like Watson (...) use techniques like deep learning as just one element in a very complicated ensemble of techniques: ranging from the statistical technique of Bayesian inference to deductive reasoning. /  / In further reference to the idea that artistic sensitivity might be inherent in relatively low levels of the cognitive hierarchy: a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained demonstrate a visual appeal: the original research notice received well over 1:000 comments: and was the subject of what was for a time the most frequently accessed article on The Guardian's website. /  /  / === Errors === / Some deep learning architectures display problematic behaviors: such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images (2014) and misclassifying minuscule perturbations of correctly classified images (2013). Goertzel hypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-component artificial general intelligence (AGI) architectures. These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar decompositions of observed entities and events. Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic goal of both human language acquisition and artificial intelligence (AI). /  /  / === Cyber threat === / As deep learning moves from the lab into the world: research and experience show that artificial neural networks are vulnerable to hacks and deception. By identifying patterns that these systems use to function: attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize. For example: an attacker can make subtle changes to an image such that the ANN finds a match even though the image looks to a human nothing like the search target. Such manipulation is termed an ""adversarial attack"".In 2016 researchers used one ANN to doctor images in trial and error fashion: identify another's focal points: and thereby generate images that deceived it. The modified images looked no different to human eyes. Another group showed that printouts of doctored images then photographed successfully tricked an image classification system. One defense is reverse image search: in which a possible fake image is submitted to a site such as TinEye that can then find other instances of it. A refinement is to search using only parts of the image: to identify images from which that piece may have been taken.Another group showed that certain psychedelic spectacles could fool a facial recognition system into thinking ordinary people were celebrities: potentially allowing one person to impersonate another. In 2017 researchers added stickers to stop signs and caused an ANN to misclassify them.ANNs can however be further trained to detect attempts at deception: potentially leading attackers and defenders into an arms race similar to the kind that already defines the malware defense industry. ANNs have been trained to defeat ANN-based anti-malware software by repeatedly attacking a defense with malware that was continually altered by a genetic algorithm until it tricked the anti-malware while retaining its ability to damage the target.In 2016: another group demonstrated that certain sounds could make the Google Now voice command system open a particular web address: and hypothesized that this could ""serve as a stepping stone for further attacks (e.g.: opening a web page hosting drive-by malware)"".In ""data poisoning"": false data is continually smuggled into a machine learning system's training set to prevent it from achieving mastery. /  /  / === Data collection ethics === / Most Deep Learning systems rely on training and verification data that is generated and/or annotated by humans. It has been argued in media philosophy that not only low-paid clickwork (e.g. on Amazon Mechanical Turk) is regularly deployed for this purpose: but also implicit forms of human microwork that are often not recognized as such. The philosopher Rainer Mühlhoff distinguishes five types of ""machinic capture"" of human microwork to generate training data: (1) gamification (the embedding of annotation or computation tasks in the flow of a game): (2) ""trapping and tracking"" (e.g. CAPTCHAs for image recognition or click-tracking on Google search results pages): (3) exploitation of social motivations (e.g."
0.14354209997691214,What theories did Alexander Bain and William James propose regarding neural networks?,"Alexander Bain proposed that every activity led to the firing of a certain set of neurons and when activities were repeated, the connections between those neurons strengthened, leading to the formation of memory. William James suggested that memories and actions resulted from electrical currents flowing among the neurons in the brain and did not require individual neural connections for each memory or action.","Score: 0.8259619589081121 / A neural network: also called a neuronal network: is an interconnected population of neurons (typically containing multiple neural circuits). Biological neural networks are studied to understand the organization and functioning of nervous systems. / Closely related are artificial neural networks: machine learning models inspired by biological neural networks. They consist of artificial neurons: which are mathematical functions that are designed to be analogous to the mechanisms used by neural circuits. /  /  / == Overview == / A biological neural network is composed of a group of chemically connected or functionally associated neurons. A single neuron may be connected to many other neurons and the total number of neurons and connections in a network may be extensive. Connections: called synapses: are usually formed from axons to dendrites: though dendrodendritic synapses and other connections are possible. Apart from electrical signalling: there are other forms of signalling that arise from neurotransmitter diffusion.  / Artificial intelligence: cognitive modelling: and artificial neural networks are information processing paradigms inspired by how biological neural systems process data. Artificial intelligence and cognitive modelling try to simulate some properties of biological neural networks. In the artificial intelligence field: artificial neural networks have been applied successfully to speech recognition: image analysis and adaptive control: in order to construct software agents (in computer and video games) or autonomous robots. / Neural network theory has served to identify better how the neurons in the brain function and provide the basis for efforts to create artificial intelligence. /  /  / == History == / The preliminary theoretical base for contemporary neural networks was independently proposed by Alexander Bain (1873) and William James (1890). In their work: both thoughts and body activity resulted from interactions among neurons within the brain. /  / For Bain: every activity led to the firing of a certain set of neurons. When activities were repeated: the connections between those neurons strengthened. According to his theory: this repetition was what led to the formation of memory. The general scientific community at the time was skeptical of Bain's theory because it required what appeared to be an inordinate number of neural connections within the brain. It is now apparent that the brain is exceedingly complex and that the same brain “wiring” can handle multiple problems and inputs. / James' theory was similar to Bain's; however: he suggested that memories and actions resulted from electrical currents flowing among the neurons in the brain. His model: by focusing on the flow of electrical currents: did not require individual neural connections for each memory or action. / C. S. Sherrington (1898) conducted experiments to test James' theory. He ran electrical currents down the spinal cords of rats. However: instead of demonstrating an increase in electrical current as projected by James: Sherrington found that the electrical current strength decreased as the testing continued over time. Importantly: this work led to the discovery of the concept of habituation.  / McCulloch and Pitts  (1943) also created a computational model for neural networks based on mathematics and algorithms. They called this model threshold logic. These early models paved the way for neural network research to split into two distinct approaches. One approach focused on biological processes in the brain and the other focused on the application of neural networks to artificial intelligence. / The parallel distributed processing of the mid-1980s became popular under the name connectionism. The text by Rumelhart and McClelland (1986) provided a full exposition on the use of connectionism in computers to simulate neural processes. / Artificial neural networks: as used in artificial intelligence: have traditionally been viewed as simplified models of neural processing in the brain: even though the relation between this model and brain biological architecture is debated: as it is not clear to what degree artificial neural networks mirror brain function. /  /  / == Neuroscience == / Theoretical and computational neuroscience is the field concerned with the analysis and computational modeling of biological neural systems. / Since neural systems are intimately related to cognitive processes and behaviour: the field is closely related to cognitive and behavioural modeling. / The aim of the field is to create models of biological neural systems in order to understand how biological systems work. To gain this understanding: neuroscientists strive to make a link between observed biological processes (data): biologically plausible mechanisms for neural processing and learning (neural network models) and theory (statistical learning theory and information theory). /  /  / === Types of models === / Many models are used; defined at different levels of abstraction: and modeling different aspects of neural systems. They range from models of the short-term behaviour of individual neurons: through models of the dynamics of neural circuitry arising from interactions between individual neurons: to models of behaviour arising from abstract neural modules that represent complete subsystems.","Score: 0.8216537746153489 / They used computational machines: then called ""calculators"". Other neural network computational machines were created by Rochester: Holland: Habit: and Duda in 1956. In 1958: psychologist Frank Rosenblatt invented the perceptron: the first implemented artificial neural network: funded by the United States Office of Naval Research. / The invention of the perceptron raised public excitement for research in Artificial Neural Networks: causing the US government to drastically increase funding into deep learning research. This led to ""the golden age of AI"" fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence. For example: in 1957 Herbert Simon famously said:It is not my aim to surprise or shock you—but the simplest way I can summarize is to say that there are now in the world machines that think: that learn and that create. Moreover: their ability to do these things is going to increase rapidly until—in a visible future—the range of problems they can handle will be coextensive with the range to which the human mind has been applied.However: this wasn't the case as research stagnated in the United States following the work of Minsky and Papert (1969): who discovered that basic perceptrons were incapable of processing the exclusive-or circuit and that computers lacked sufficient power to train useful neural networks. This: along with other factors such as the 1973 Lighthill report by James Lighthill stating that research in Artificial Intelligence has not ""produced the major impact that was then promised:"" shutting funding in research into the field of AI in all but two universities in the UK and in many major institutions across the world. This ushered an era called the AI Winter with reduced research into connectionism due to a decrease in government funding and an increased stress on symbolic artificial intelligence in the United States and other Western countries.During the AI Winter era: however: research outside the United States continued: especially in Eastern Europe. By the time Minsky and Papert's book on Perceptrons came out: methods for training multilayer perceptrons (MLPs) were already known. The first deep learning MLP was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa in 1965: as the Group Method of Data Handling. The first deep learning MLP trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. In computer experiments conducted by Amari's student Saito: a five layer MLP with two modifiable layers learned useful internal representations to classify non-linearily separable pattern classes.Self-organizing maps (SOMs) were described by Teuvo Kohonen in 1982. SOMs are neurophysiologically inspired neural networks that learn low-dimensional representations of high-dimensional data while preserving the topological structure of the data. They are trained using competitive learning.The convolutional neural network (CNN) architecture with convolutional layers and downsampling layers was introduced by Kunihiko Fukushima in 1980. He called it the neocognitron. In 1969: he also introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for CNNs and  deep neural networks in general. CNNs have become an essential tool for computer vision. / A key in later advances in artificial neural network research was the backpropagation algorithm: an efficient application of the Leibniz chain rule (1673) to networks of differentiable nodes. It is also known as  / the reverse mode of automatic differentiation or reverse accumulation: due to Seppo Linnainmaa (1970). The term ""back-propagating errors"" was introduced in 1962 by Frank Rosenblatt: but he did not have an implementation of this procedure: although Henry J. Kelley and Bryson had dynamic programming based continuous precursors of backpropagation already in 1960–61 in the context of control theory.  / In 1973: Dreyfus used backpropagation to adapt parameters of controllers in proportion to error gradients.  / In 1982: Paul Werbos applied backpropagation to MLPs in the way that has become standard. In 1986 Rumelhart: Hinton and Williams showed that backpropagation learned interesting internal representations of words as feature vectors when trained to predict the next word in a sequence.In the late 1970s to early 1980s: interest briefly emerged in theoretically investigating the Ising model created by Wilhelm Lenz (1920) and Ernst Ising (1925) / in relation to Cayley tree topologies and large neural networks."
0.12455800001043826,Who were McCulloch and Pitts and what was their contribution to neural network research?,"McCulloch and Pitts were researchers who, in 1943, created a computational model for neural networks based on mathematics and algorithms. They called this model threshold logic. Their work contributed significantly to the evolution of neural network research, which split into two distinct approaches - one focusing on biological processes in the brain, and the other on the application of neural networks to artificial intelligence.","Score: 0.8166428668786567 / They used computational machines: then called ""calculators"". Other neural network computational machines were created by Rochester: Holland: Habit: and Duda in 1956. In 1958: psychologist Frank Rosenblatt invented the perceptron: the first implemented artificial neural network: funded by the United States Office of Naval Research. / The invention of the perceptron raised public excitement for research in Artificial Neural Networks: causing the US government to drastically increase funding into deep learning research. This led to ""the golden age of AI"" fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence. For example: in 1957 Herbert Simon famously said:It is not my aim to surprise or shock you—but the simplest way I can summarize is to say that there are now in the world machines that think: that learn and that create. Moreover: their ability to do these things is going to increase rapidly until—in a visible future—the range of problems they can handle will be coextensive with the range to which the human mind has been applied.However: this wasn't the case as research stagnated in the United States following the work of Minsky and Papert (1969): who discovered that basic perceptrons were incapable of processing the exclusive-or circuit and that computers lacked sufficient power to train useful neural networks. This: along with other factors such as the 1973 Lighthill report by James Lighthill stating that research in Artificial Intelligence has not ""produced the major impact that was then promised:"" shutting funding in research into the field of AI in all but two universities in the UK and in many major institutions across the world. This ushered an era called the AI Winter with reduced research into connectionism due to a decrease in government funding and an increased stress on symbolic artificial intelligence in the United States and other Western countries.During the AI Winter era: however: research outside the United States continued: especially in Eastern Europe. By the time Minsky and Papert's book on Perceptrons came out: methods for training multilayer perceptrons (MLPs) were already known. The first deep learning MLP was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa in 1965: as the Group Method of Data Handling. The first deep learning MLP trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. In computer experiments conducted by Amari's student Saito: a five layer MLP with two modifiable layers learned useful internal representations to classify non-linearily separable pattern classes.Self-organizing maps (SOMs) were described by Teuvo Kohonen in 1982. SOMs are neurophysiologically inspired neural networks that learn low-dimensional representations of high-dimensional data while preserving the topological structure of the data. They are trained using competitive learning.The convolutional neural network (CNN) architecture with convolutional layers and downsampling layers was introduced by Kunihiko Fukushima in 1980. He called it the neocognitron. In 1969: he also introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for CNNs and  deep neural networks in general. CNNs have become an essential tool for computer vision. / A key in later advances in artificial neural network research was the backpropagation algorithm: an efficient application of the Leibniz chain rule (1673) to networks of differentiable nodes. It is also known as  / the reverse mode of automatic differentiation or reverse accumulation: due to Seppo Linnainmaa (1970). The term ""back-propagating errors"" was introduced in 1962 by Frank Rosenblatt: but he did not have an implementation of this procedure: although Henry J. Kelley and Bryson had dynamic programming based continuous precursors of backpropagation already in 1960–61 in the context of control theory.  / In 1973: Dreyfus used backpropagation to adapt parameters of controllers in proportion to error gradients.  / In 1982: Paul Werbos applied backpropagation to MLPs in the way that has become standard. In 1986 Rumelhart: Hinton and Williams showed that backpropagation learned interesting internal representations of words as feature vectors when trained to predict the next word in a sequence.In the late 1970s to early 1980s: interest briefly emerged in theoretically investigating the Ising model created by Wilhelm Lenz (1920) and Ernst Ising (1925) / in relation to Cayley tree topologies and large neural networks.","Score: 0.8043976573211007 / In 1973: Dreyfus used backpropagation to adapt parameters of controllers in proportion to error gradients.  / In 1982: Paul Werbos applied backpropagation to MLPs in the way that has become standard. In 1986 Rumelhart: Hinton and Williams showed that backpropagation learned interesting internal representations of words as feature vectors when trained to predict the next word in a sequence.In the late 1970s to early 1980s: interest briefly emerged in theoretically investigating the Ising model created by Wilhelm Lenz (1920) and Ernst Ising (1925) / in relation to Cayley tree topologies and large neural networks. / The Ising model is essentially a non-learning artificial recurrent neural network (RNN) consisting of neuron-like threshold elements. / In 1972: Shun'ichi Amari described an adaptive version of this architecture: / In 1981: the Ising model was solved exactly by Peter Barth for the general case of closed Cayley trees (with loops) with an arbitrary branching ratio / and found to exhibit unusual phase transition behavior in its local-apex and long-range site-site correlations.John Hopfield popularised this architecture in 1982: / and it is now known as a Hopfield network. / The time delay neural network (TDNN) of Alex Waibel (1987) combined convolutions and weight sharing and backpropagation.  In 1988: Wei Zhang et al. applied backpropagation to a CNN (a simplified Neocognitron with convolutional interconnections between the image feature layers and the last fully connected layer) for alphabet recognition. In 1989: Yann LeCun et al. trained a CNN to recognize handwritten ZIP codes on mail.  / In 1992: max-pooling for CNNs was introduced by Juan Weng et al. to help with least-shift invariance and tolerance to deformation to aid 3D object recognition.  / LeNet-5 (1998): a 7-level CNN by Yann LeCun et al.: that classifies digits: was applied by several banks to recognize hand-written numbers on checks digitized in 32x32 pixel images. / From 1988 onward: the use of neural networks transformed the field of protein structure prediction: in particular when the first cascading networks were trained on profiles (matrices) produced by multiple sequence alignments.In 1991: Sepp Hochreiter's diploma thesis  identified and analyzed the vanishing gradient problem and proposed recurrent residual connections to solve it. His thesis was called ""one of the most important documents in the history of machine learning"" by his supervisor Juergen Schmidhuber.In 1991: Juergen Schmidhuber  published adversarial neural networks that contest with each other in the form of a zero-sum game: where one network's gain is the other network's loss. The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called ""artificial curiosity."" / In 1992: Juergen Schmidhuber proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning. It uses predictive coding  to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN: by distilling a higher level chunker network into a lower level automatizer network.  In the same year he also published an alternative to RNNs which is a precursor of a linear Transformer. It introduces the concept internal spotlights of attention: a slow feedforward neural network learns by gradient descent to control the fast weights of another neural network through outer products of self-generated activation patterns. / The development of metal–oxide–semiconductor (MOS) very-large-scale integration (VLSI): in the form of complementary MOS (CMOS) technology: enabled increasing MOS transistor counts in digital electronics. This provided more processing power for the development of practical artificial neural networks in the 1980s.Neural networks' early successes included predicting the stock market and in 1995 a (mostly) self-driving car.1997:  Sepp Hochreite and Juergen Schmidhuber introduced the deep learning method called long short-term memory (LSTM): published in Neural Computation. LSTM recurrent neural networks can learn ""very deep learning"" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before."
0.16368689999217167,What are some recent improvements in the research of neural networks?,"Recent improvements in the research of neural networks have mostly been concerned with the exploration of the role of neuromodulators, such as dopamine, acetylcholine, and serotonin, on behavior and learning. Biophysical models, like BCM theory, have been instrumental for understanding synaptic plasticity mechanisms, and have influenced both computer science and neuroscience.","Score: 0.8606374576203009 / == Contests == / Between 2009 and 2012: recurrent neural networks and deep feedforward neural networks developed in Schmidhuber's research group won eight international competitions in pattern recognition and machine learning. For example: the bi-directional and multi-dimensional long short-term memory (LSTM) of Graves et al. won three competitions in connected handwriting recognition at the 2009 International Conference on Document Analysis and Recognition (ICDAR): without any prior knowledge about the three languages to be learned.Ciresan and colleagues won pattern recognition contests: including the IJCNN 2011 Traffic Sign Recognition Competition: the ISBI 2012 Segmentation of Neuronal Structures in Electron Microscopy Stacks challenge and others. Their neural networks were the first pattern recognizers to achieve human-competitive/superhuman performance on benchmarks such as traffic sign recognition (IJCNN 2012): or the MNIST handwritten digits problem. / Researchers demonstrated (2010) that deep neural networks interfaced to a hidden Markov model with context-dependent states that define the neural network output layer can drastically reduce errors in large-vocabulary speech recognition tasks such as voice search.GPU-based implementations of this approach won many pattern recognition contests: including the IJCNN 2011 Traffic Sign Recognition Competition: the ISBI 2012 Segmentation of neuronal structures in EM stacks challenge: the ImageNet Competition and others. / Deep: highly nonlinear neural architectures similar to the neocognitron and the ""standard architecture of vision"": inspired by simple and complex cells: were pre-trained with unsupervised methods by Hinton. A team from his lab won a 2012 contest sponsored by Merck to design software to help find molecules that might identify new drugs. /  /  / == Notes == /  /  / == References == /  /  / == External links == / ""Lecun 2019-7-11 ACM Tech Talk"". Google Docs. Retrieved 2020-02-13.","Score: 0.8559592258726542 / In 2015: Rupesh Kumar Srivastava: Klaus Greff: and Schmidhuber used LSTM principles to create the Highway network: a feedforward neural network with hundreds of layers: much deeper than previous networks. 7 months later: Kaiming He: Xiangyu Zhang;  Shaoqing Ren: and Jian Sun won the ImageNet 2015 competition with an open-gated or gateless Highway network variant called Residual neural network. This has become the most cited neural network of the 21st century.In 1994: André de Carvalho: together with Mike Fairhurst and David Bisset: published experimental results of a multi-layer boolean neural network: also known as a weightless neural network: composed of a 3-layers self-organising feature extraction neural network module (SOFT) followed by a multi-layer classification neural network module (GSN): which were independently trained. Each layer in the feature extraction module extracted features with growing complexity regarding the previous layer.In 1995: Brendan Frey demonstrated that it was possible to train (over two days) a network containing six fully connected layers and several hundred hidden units using the wake-sleep algorithm: co-developed with Peter Dayan and Hinton.Since 1997: Sven Behnke extended the feed-forward hierarchical convolutional approach in the Neural Abstraction Pyramid by lateral and backward connections in order to flexibly incorporate context into decisions and iteratively resolve local ambiguities. / Simpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) were a popular choice in the 1990s and 2000s: because of artificial neural networks' computational cost and a lack of understanding of how the brain wires its biological networks. / Both shallow and deep learning (e.g.: recurrent nets) of ANNs for speech recognition have been explored for many years. These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively. Key difficulties have been analyzed: including gradient diminishing and weak temporal correlation structure in neural predictive models. Additional difficulties were the lack of training data and limited computing power. Most speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government's NSA and DARPA: SRI studied deep neural networks (DNNs) in speech and speaker recognition. The speaker recognition team led by Larry Heck reported significant success with deep neural networks in speech processing in the 1998 National Institute of Standards and Technology Speaker Recognition evaluation. The SRI deep neural network was then deployed in the Nuance Verifier: representing the first major industrial application of deep learning. The principle of elevating ""raw"" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the ""raw"" spectrogram or linear filter-bank features in the late 1990s: showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech: waveforms: later produced excellent larger-scale results.Speech recognition was taken over by LSTM. In 2003: LSTM started to become competitive with traditional speech recognizers on certain tasks. In 2006: Alex Graves: Santiago Fernández: Faustino Gomez: and Schmidhuber combined it with connectionist temporal classification (CTC) in stacks of LSTM RNNs. In 2015: Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM: which they made available through Google Voice Search.The impact of deep learning in industry began in the early 2000s: when CNNs already processed an estimated 10% to 20% of all the checks written in the US: according to Yann LeCun. Industrial applications of deep learning to large-scale speech recognition started around 2010. / In 2006: publications by Geoff Hinton: Ruslan Salakhutdinov: Osindero and Teh showed how a many-layered feedforward neural network could be effectively pre-trained one layer at a time: treating each layer in turn as an unsupervised restricted Boltzmann machine: then fine-tuning it using supervised backpropagation. The papers referred to learning for deep belief nets. / The 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech: and the possibility that given more capable hardware and large-scale data sets that deep neural nets might become practical."
0.15353939999477006,What is a neural network?,"A neural network refers to a group of interconnected units, or neurons, that send signals to each other. These neurons can be either biological cells or mathematical models. A complex network of neurons can execute complex tasks. There are two main types of such networks - biological neural networks seen in brains and complex nervous systems, and artificial neural networks, which are mathematical models used in machine learning for approximating nonlinear functions.","Score: 0.8662201312849834 / A neural network is a group of interconnected units called neurons that send signals to one another. Neurons can be either biological cells or mathematical models. While individual neurons are simple: many of them together in a network can perform complex tasks. There are two main types of neural network. /  / In neuroscience: a biological neural network is a physical structure found in brains and complex nervous systems – a population of nerve cells connected by synapses. / In machine learning: an artificial neural network is a mathematical model used to approximate nonlinear functions. Artificial neural networks are used to solve artificial intelligence problems. /  /  / == Biological neural network == /  / A biological neural network is a population of biological neurons chemically connected to each other by synapses. A given neuron can be connected to hundreds of thousands of synapses. / Each neuron sends and receives electrochemical signals called action potentials to its connected neighbors. A neuron can serve an excitatory role: amplifying and propagating signals it receives: or an inhibitory role: suppressing signals instead.Populations of interconnected neurons that are smaller than neural networks are called neural circuits. Very large interconnected networks are called large scale brain networks: and many of these together form brains and nervous systems. / Signals generated by neural networks in the brain eventually travel through the nervous system and across neuromuscular junctions to muscle cells: where they cause contraction and thereby motion. /  /  / == Artificial neural network == /  / An artificial neural network is a mathematical model used to approximate nonlinear functions. While early artificial neural networks were physical machines: today they are almost always implemented in software. / Neurons in an artificial neural network are usually arranged into layers: with information passing from the first layer (the input layer) through one or more intermediate layers (hidden layers) to the final layer (the output layer). / The ""signal"" input to each neuron is a number: specifically a linear combination of the outputs of the connected neurons in the previous layer. The signal each neuron outputs is calculated from this number: according to its activation function. The behavior of the network depends on the strengths (or weights) of the connections between neurons. A network is trained by modifying these weights through empirical risk minimization or backpropagation in order to fit some preexisting dataset.Neural networks are used to solve problems in artificial intelligence: and have thereby found applications in many disciplines: including predictive modeling: adaptive control: facial recognition: handwriting recognition: general game playing: and generative AI. /  /  / == History == /  / The theoretical base for contemporary neural networks was independently proposed by Alexander Bain in 1873 and William James in 1890. Both posited that human thought emerged from interactions among large numbers of neurons inside the brain. In 1949: Donald Hebb described Hebbian learning: the idea that neural networks can change and learn over time by strengthening a synapse every time a signal travels along it.Artificial neural networks were originally used to model biological neural networks starting in the 1930s under the approach of connectionism. However: starting with the invention of the perceptron: a simple artificial neural network: by Warren McCulloch and Walter Pitts in 1943: followed by the implementation of one in hardware by Frank Rosenblatt in 1957: / artificial neural networks became increasingly used for machine learning applications instead: and increasingly different from their biological counterparts. /  /  / == See also == / Emergence / Biological cybernetics / Biologically-inspired computing /  /  / == References ==","Score: 0.8647588694062418 / In machine learning: an artificial neural network (also neural network or neural net: abbreviated ANN or NN) is a model inspired by the neuronal organization found in the biological neural networks in animal brains.An ANN is made of connected units or nodes called artificial neurons: which loosely model the neurons in a brain. These are connected by edges: which model the synapses in a brain. An artificial neuron receives signals from connected neurons: then processes them and sends a signal to other connected neurons. The ""signal"" is a real number: and the output of each neuron is computed by some non-linear function of the sum of its inputs: called the activation function. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. / Typically: neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer): possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least 2 hidden layers.Artificial neural networks are used for predictive modeling: adaptive control: and other applications where they can be trained via a dataset. They are also used to solve problems in artificial intelligence. Networks can learn from experience: and can derive conclusions from a complex and seemingly unrelated set of information. /  /  / == Training == / Neural networks are typically trained through empirical risk minimization. This method is based on the idea of optimizing the network's parameters to minimize the difference: or empirical risk: between the predicted output and the actual target values in a given dataset. Gradient based methods such as backpropagation are usually used to estimate the parameters of the network. During the training phase: ANNs learn from labeled training data by iteratively updating their parameters to minimize a defined loss function. This method allows the network to generalize to unseen data. /  /  / == History == /  / Historically: digital computers evolved from the von Neumann model: and operate via the execution of explicit instructions via access to memory by a number of processors. Neural networks: on the other hand: originated from efforts to model information processing in biological systems through the framework of connectionism. Unlike the von Neumann model: connectionist computing does not separate memory and processing. / The simplest kind of feedforward neural network (FNN) is a linear network: which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated at each node. The mean squared errors between these calculated outputs and the given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement.Warren McCulloch and Walter Pitts (1943) also considered a non-learning computational model for neural networks.In the late 1940s: D. O. Hebb created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Hebbian learning is considered to be a 'typical' unsupervised learning rule and its later variants were early models for long term potentiation. These ideas started being applied to computational models in 1948 with Turing's ""unorganized machines"". Farley and Wesley A. Clark were the first to simulate a Hebbian network in 1954 at MIT. They used computational machines: then called ""calculators"". Other neural network computational machines were created by Rochester: Holland: Habit: and Duda in 1956. In 1958: psychologist Frank Rosenblatt invented the perceptron: the first implemented artificial neural network: funded by the United States Office of Naval Research. / The invention of the perceptron raised public excitement for research in Artificial Neural Networks: causing the US government to drastically increase funding into deep learning research. This led to ""the golden age of AI"" fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence. For example: in 1957 Herbert Simon famously said:It is not my aim to surprise or shock you—but the simplest way I can summarize is to say that there are now in the world machines that think: that learn and that create."
0.1345199000206776,What is the difference between a biological neural network and an artificial neural network?,"A biological neural network comprises biological neurons that are chemically connected to each other by synapses. These neurons send and receive electrochemical signals. An artificial neural network, on the other hand, is a mathematical model that approximates non-linear functions. These networks are mostly used in software form to help solve artificial intelligence problems.","Score: 0.8721278804454958 / A neural network is a group of interconnected units called neurons that send signals to one another. Neurons can be either biological cells or mathematical models. While individual neurons are simple: many of them together in a network can perform complex tasks. There are two main types of neural network. /  / In neuroscience: a biological neural network is a physical structure found in brains and complex nervous systems – a population of nerve cells connected by synapses. / In machine learning: an artificial neural network is a mathematical model used to approximate nonlinear functions. Artificial neural networks are used to solve artificial intelligence problems. /  /  / == Biological neural network == /  / A biological neural network is a population of biological neurons chemically connected to each other by synapses. A given neuron can be connected to hundreds of thousands of synapses. / Each neuron sends and receives electrochemical signals called action potentials to its connected neighbors. A neuron can serve an excitatory role: amplifying and propagating signals it receives: or an inhibitory role: suppressing signals instead.Populations of interconnected neurons that are smaller than neural networks are called neural circuits. Very large interconnected networks are called large scale brain networks: and many of these together form brains and nervous systems. / Signals generated by neural networks in the brain eventually travel through the nervous system and across neuromuscular junctions to muscle cells: where they cause contraction and thereby motion. /  /  / == Artificial neural network == /  / An artificial neural network is a mathematical model used to approximate nonlinear functions. While early artificial neural networks were physical machines: today they are almost always implemented in software. / Neurons in an artificial neural network are usually arranged into layers: with information passing from the first layer (the input layer) through one or more intermediate layers (hidden layers) to the final layer (the output layer). / The ""signal"" input to each neuron is a number: specifically a linear combination of the outputs of the connected neurons in the previous layer. The signal each neuron outputs is calculated from this number: according to its activation function. The behavior of the network depends on the strengths (or weights) of the connections between neurons. A network is trained by modifying these weights through empirical risk minimization or backpropagation in order to fit some preexisting dataset.Neural networks are used to solve problems in artificial intelligence: and have thereby found applications in many disciplines: including predictive modeling: adaptive control: facial recognition: handwriting recognition: general game playing: and generative AI. /  /  / == History == /  / The theoretical base for contemporary neural networks was independently proposed by Alexander Bain in 1873 and William James in 1890. Both posited that human thought emerged from interactions among large numbers of neurons inside the brain. In 1949: Donald Hebb described Hebbian learning: the idea that neural networks can change and learn over time by strengthening a synapse every time a signal travels along it.Artificial neural networks were originally used to model biological neural networks starting in the 1930s under the approach of connectionism. However: starting with the invention of the perceptron: a simple artificial neural network: by Warren McCulloch and Walter Pitts in 1943: followed by the implementation of one in hardware by Frank Rosenblatt in 1957: / artificial neural networks became increasingly used for machine learning applications instead: and increasingly different from their biological counterparts. /  /  / == See also == / Emergence / Biological cybernetics / Biologically-inspired computing /  /  / == References ==","Score: 0.8694993207466659 / A neural network: also called a neuronal network: is an interconnected population of neurons (typically containing multiple neural circuits). Biological neural networks are studied to understand the organization and functioning of nervous systems. / Closely related are artificial neural networks: machine learning models inspired by biological neural networks. They consist of artificial neurons: which are mathematical functions that are designed to be analogous to the mechanisms used by neural circuits. /  /  / == Overview == / A biological neural network is composed of a group of chemically connected or functionally associated neurons. A single neuron may be connected to many other neurons and the total number of neurons and connections in a network may be extensive. Connections: called synapses: are usually formed from axons to dendrites: though dendrodendritic synapses and other connections are possible. Apart from electrical signalling: there are other forms of signalling that arise from neurotransmitter diffusion.  / Artificial intelligence: cognitive modelling: and artificial neural networks are information processing paradigms inspired by how biological neural systems process data. Artificial intelligence and cognitive modelling try to simulate some properties of biological neural networks. In the artificial intelligence field: artificial neural networks have been applied successfully to speech recognition: image analysis and adaptive control: in order to construct software agents (in computer and video games) or autonomous robots. / Neural network theory has served to identify better how the neurons in the brain function and provide the basis for efforts to create artificial intelligence. /  /  / == History == / The preliminary theoretical base for contemporary neural networks was independently proposed by Alexander Bain (1873) and William James (1890). In their work: both thoughts and body activity resulted from interactions among neurons within the brain. /  / For Bain: every activity led to the firing of a certain set of neurons. When activities were repeated: the connections between those neurons strengthened. According to his theory: this repetition was what led to the formation of memory. The general scientific community at the time was skeptical of Bain's theory because it required what appeared to be an inordinate number of neural connections within the brain. It is now apparent that the brain is exceedingly complex and that the same brain “wiring” can handle multiple problems and inputs. / James' theory was similar to Bain's; however: he suggested that memories and actions resulted from electrical currents flowing among the neurons in the brain. His model: by focusing on the flow of electrical currents: did not require individual neural connections for each memory or action. / C. S. Sherrington (1898) conducted experiments to test James' theory. He ran electrical currents down the spinal cords of rats. However: instead of demonstrating an increase in electrical current as projected by James: Sherrington found that the electrical current strength decreased as the testing continued over time. Importantly: this work led to the discovery of the concept of habituation.  / McCulloch and Pitts  (1943) also created a computational model for neural networks based on mathematics and algorithms. They called this model threshold logic. These early models paved the way for neural network research to split into two distinct approaches. One approach focused on biological processes in the brain and the other focused on the application of neural networks to artificial intelligence. / The parallel distributed processing of the mid-1980s became popular under the name connectionism. The text by Rumelhart and McClelland (1986) provided a full exposition on the use of connectionism in computers to simulate neural processes. / Artificial neural networks: as used in artificial intelligence: have traditionally been viewed as simplified models of neural processing in the brain: even though the relation between this model and brain biological architecture is debated: as it is not clear to what degree artificial neural networks mirror brain function. /  /  / == Neuroscience == / Theoretical and computational neuroscience is the field concerned with the analysis and computational modeling of biological neural systems. / Since neural systems are intimately related to cognitive processes and behaviour: the field is closely related to cognitive and behavioural modeling. / The aim of the field is to create models of biological neural systems in order to understand how biological systems work. To gain this understanding: neuroscientists strive to make a link between observed biological processes (data): biologically plausible mechanisms for neural processing and learning (neural network models) and theory (statistical learning theory and information theory). /  /  / === Types of models === / Many models are used; defined at different levels of abstraction: and modeling different aspects of neural systems. They range from models of the short-term behaviour of individual neurons: through models of the dynamics of neural circuitry arising from interactions between individual neurons: to models of behaviour arising from abstract neural modules that represent complete subsystems."
0.12642330001108348,How do neurons in an artificial neural network operate?,"In artificial neural networks, neurons are arranged into layers, with information passing through from the input layer to the output layer, through one or more intermediate or hidden layers. Each neuron receives an input signal - a number based on the outputs of neurons from the previous layer. The signal a neuron puts out is computed based on this number and its activation function. The strengths, or weights, of the connections between neurons shape the behavior of the network. Training a network involves modifying these weights to reach the desired output.","Score: 0.8589619096243543 / In machine learning: an artificial neural network (also neural network or neural net: abbreviated ANN or NN) is a model inspired by the neuronal organization found in the biological neural networks in animal brains.An ANN is made of connected units or nodes called artificial neurons: which loosely model the neurons in a brain. These are connected by edges: which model the synapses in a brain. An artificial neuron receives signals from connected neurons: then processes them and sends a signal to other connected neurons. The ""signal"" is a real number: and the output of each neuron is computed by some non-linear function of the sum of its inputs: called the activation function. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. / Typically: neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer): possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least 2 hidden layers.Artificial neural networks are used for predictive modeling: adaptive control: and other applications where they can be trained via a dataset. They are also used to solve problems in artificial intelligence. Networks can learn from experience: and can derive conclusions from a complex and seemingly unrelated set of information. /  /  / == Training == / Neural networks are typically trained through empirical risk minimization. This method is based on the idea of optimizing the network's parameters to minimize the difference: or empirical risk: between the predicted output and the actual target values in a given dataset. Gradient based methods such as backpropagation are usually used to estimate the parameters of the network. During the training phase: ANNs learn from labeled training data by iteratively updating their parameters to minimize a defined loss function. This method allows the network to generalize to unseen data. /  /  / == History == /  / Historically: digital computers evolved from the von Neumann model: and operate via the execution of explicit instructions via access to memory by a number of processors. Neural networks: on the other hand: originated from efforts to model information processing in biological systems through the framework of connectionism. Unlike the von Neumann model: connectionist computing does not separate memory and processing. / The simplest kind of feedforward neural network (FNN) is a linear network: which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated at each node. The mean squared errors between these calculated outputs and the given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement.Warren McCulloch and Walter Pitts (1943) also considered a non-learning computational model for neural networks.In the late 1940s: D. O. Hebb created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Hebbian learning is considered to be a 'typical' unsupervised learning rule and its later variants were early models for long term potentiation. These ideas started being applied to computational models in 1948 with Turing's ""unorganized machines"". Farley and Wesley A. Clark were the first to simulate a Hebbian network in 1954 at MIT. They used computational machines: then called ""calculators"". Other neural network computational machines were created by Rochester: Holland: Habit: and Duda in 1956. In 1958: psychologist Frank Rosenblatt invented the perceptron: the first implemented artificial neural network: funded by the United States Office of Naval Research. / The invention of the perceptron raised public excitement for research in Artificial Neural Networks: causing the US government to drastically increase funding into deep learning research. This led to ""the golden age of AI"" fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence. For example: in 1957 Herbert Simon famously said:It is not my aim to surprise or shock you—but the simplest way I can summarize is to say that there are now in the world machines that think: that learn and that create.","Score: 0.8500331590649205 / There are many types of artificial neural networks (ANN). / Artificial neural networks are computational models inspired by biological neural networks: and are used to approximate functions that are generally unknown. Particularly: they are inspired by the behaviour of neurons and the electrical signals they convey between input (such as from the eyes or nerve endings in the hand): processing: and output from the brain (such as reacting to light: touch: or heat). The way neurons semantically communicate is an area of ongoing research. Most artificial neural networks bear only some resemblance to their more complex biological counterparts: but are very effective at their intended tasks (e.g. classification or segmentation). / Some artificial neural networks are adaptive systems and are used for example to model populations and environments: which constantly change. / Neural networks can be hardware- (neurons are represented by physical components) or software-based (computer models): and can use a variety of topologies and learning algorithms. /  /  / == Feedforward == /  / The feedforward neural network was the first and simplest type. In this network the information moves only from the input layer directly through any hidden layers to the output layer without cycles/loops. Feedforward networks can be constructed with various types of units: such as binary McCulloch–Pitts neurons: the simplest of which is the perceptron. Continuous neurons: frequently with sigmoidal activation: are used in the context of backpropagation. /  /  / === Group method of data handling === /  / The Group Method of Data Handling (GMDH) features fully automatic structural and parametric model optimization. The node activation functions are Kolmogorov–Gabor polynomials that permit additions and multiplications. It uses a deep multilayer perceptron with eight layers. It is a supervised learning network that grows layer by layer: where each layer is trained by regression analysis. Useless items are detected using a validation set: and pruned through regularization. The size and depth of the resulting network depends on the task. /  /  / === Autoencoder === /  / An autoencoder: autoassociator or Diabolo network: 19  is similar to the multilayer perceptron (MLP) – with an input layer: an output layer and one or more hidden layers connecting them. However: the output layer has the same number of units as the input layer. Its purpose is to reconstruct its own inputs (instead of emitting a target value). Therefore: autoencoders are unsupervised learning models. An autoencoder is used for unsupervised learning of efficient codings: typically for the purpose of dimensionality reduction and for learning generative models of data. /  /  / === Probabilistic === /  / A probabilistic neural network (PNN) is a four-layer feedforward neural network. The layers are Input: hidden pattern/summation: and output. In the PNN algorithm: the parent probability distribution function (PDF) of each class is approximated by a Parzen window and a non-parametric function. Then: using PDF of each class: the class probability of a new input is estimated and Bayes’ rule is employed to allocate it to the class with the highest posterior probability. It was derived from the Bayesian network and a statistical algorithm called Kernel Fisher discriminant analysis. It is used for classification and pattern recognition. /  /  / === Time delay === /  / A time delay neural network (TDNN)  is a feedforward architecture for sequential data that recognizes features independent of sequence position. In order to achieve time-shift invariance: delays are added to the input so that multiple data points (points in time) are analyzed together. / It usually forms part of a larger pattern recognition system. It has been implemented using a perceptron network whose connection weights were trained with back propagation (supervised learning). /  /  / === Convolutional === /  / A convolutional neural network (CNN: or ConvNet or shift invariant or space invariant) is a class of deep network: composed of one or more convolutional layers with fully connected layers (matching those in typical ANNs) on top. It uses tied weights and pooling layers. In particular: max-pooling. It is often structured via Fukushima's convolutional architecture. They are variations of multilayer perceptrons that use minimal preprocessing. This architecture allows CNNs to take advantage of the 2D structure of input data. / Its unit connectivity pattern is inspired by the organization of the visual cortex. Units respond to stimuli in a restricted region of space known as the receptive field.  Receptive fields partially overlap: over-covering the entire visual field.  Unit response can be approximated mathematically by a convolution operation.CNNs are suitable for processing visual and other two-dimensional data."
0.16784189999452792,How do signals in a biological neural network function?,"In a biological neural network, each neuron sends and receives electrochemical signals known as action potentials. Depending on its role, a neuron can either amplify and propagate or suppress these signals. These signals travel through the nervous system to muscle cells, inducing contraction and subsequent motion.","Score: 0.8607899810383767 / A neural network is a group of interconnected units called neurons that send signals to one another. Neurons can be either biological cells or mathematical models. While individual neurons are simple: many of them together in a network can perform complex tasks. There are two main types of neural network. /  / In neuroscience: a biological neural network is a physical structure found in brains and complex nervous systems – a population of nerve cells connected by synapses. / In machine learning: an artificial neural network is a mathematical model used to approximate nonlinear functions. Artificial neural networks are used to solve artificial intelligence problems. /  /  / == Biological neural network == /  / A biological neural network is a population of biological neurons chemically connected to each other by synapses. A given neuron can be connected to hundreds of thousands of synapses. / Each neuron sends and receives electrochemical signals called action potentials to its connected neighbors. A neuron can serve an excitatory role: amplifying and propagating signals it receives: or an inhibitory role: suppressing signals instead.Populations of interconnected neurons that are smaller than neural networks are called neural circuits. Very large interconnected networks are called large scale brain networks: and many of these together form brains and nervous systems. / Signals generated by neural networks in the brain eventually travel through the nervous system and across neuromuscular junctions to muscle cells: where they cause contraction and thereby motion. /  /  / == Artificial neural network == /  / An artificial neural network is a mathematical model used to approximate nonlinear functions. While early artificial neural networks were physical machines: today they are almost always implemented in software. / Neurons in an artificial neural network are usually arranged into layers: with information passing from the first layer (the input layer) through one or more intermediate layers (hidden layers) to the final layer (the output layer). / The ""signal"" input to each neuron is a number: specifically a linear combination of the outputs of the connected neurons in the previous layer. The signal each neuron outputs is calculated from this number: according to its activation function. The behavior of the network depends on the strengths (or weights) of the connections between neurons. A network is trained by modifying these weights through empirical risk minimization or backpropagation in order to fit some preexisting dataset.Neural networks are used to solve problems in artificial intelligence: and have thereby found applications in many disciplines: including predictive modeling: adaptive control: facial recognition: handwriting recognition: general game playing: and generative AI. /  /  / == History == /  / The theoretical base for contemporary neural networks was independently proposed by Alexander Bain in 1873 and William James in 1890. Both posited that human thought emerged from interactions among large numbers of neurons inside the brain. In 1949: Donald Hebb described Hebbian learning: the idea that neural networks can change and learn over time by strengthening a synapse every time a signal travels along it.Artificial neural networks were originally used to model biological neural networks starting in the 1930s under the approach of connectionism. However: starting with the invention of the perceptron: a simple artificial neural network: by Warren McCulloch and Walter Pitts in 1943: followed by the implementation of one in hardware by Frank Rosenblatt in 1957: / artificial neural networks became increasingly used for machine learning applications instead: and increasingly different from their biological counterparts. /  /  / == See also == / Emergence / Biological cybernetics / Biologically-inspired computing /  /  / == References ==","Score: 0.8600607183287673 / A neural network: also called a neuronal network: is an interconnected population of neurons (typically containing multiple neural circuits). Biological neural networks are studied to understand the organization and functioning of nervous systems. / Closely related are artificial neural networks: machine learning models inspired by biological neural networks. They consist of artificial neurons: which are mathematical functions that are designed to be analogous to the mechanisms used by neural circuits. /  /  / == Overview == / A biological neural network is composed of a group of chemically connected or functionally associated neurons. A single neuron may be connected to many other neurons and the total number of neurons and connections in a network may be extensive. Connections: called synapses: are usually formed from axons to dendrites: though dendrodendritic synapses and other connections are possible. Apart from electrical signalling: there are other forms of signalling that arise from neurotransmitter diffusion.  / Artificial intelligence: cognitive modelling: and artificial neural networks are information processing paradigms inspired by how biological neural systems process data. Artificial intelligence and cognitive modelling try to simulate some properties of biological neural networks. In the artificial intelligence field: artificial neural networks have been applied successfully to speech recognition: image analysis and adaptive control: in order to construct software agents (in computer and video games) or autonomous robots. / Neural network theory has served to identify better how the neurons in the brain function and provide the basis for efforts to create artificial intelligence. /  /  / == History == / The preliminary theoretical base for contemporary neural networks was independently proposed by Alexander Bain (1873) and William James (1890). In their work: both thoughts and body activity resulted from interactions among neurons within the brain. /  / For Bain: every activity led to the firing of a certain set of neurons. When activities were repeated: the connections between those neurons strengthened. According to his theory: this repetition was what led to the formation of memory. The general scientific community at the time was skeptical of Bain's theory because it required what appeared to be an inordinate number of neural connections within the brain. It is now apparent that the brain is exceedingly complex and that the same brain “wiring” can handle multiple problems and inputs. / James' theory was similar to Bain's; however: he suggested that memories and actions resulted from electrical currents flowing among the neurons in the brain. His model: by focusing on the flow of electrical currents: did not require individual neural connections for each memory or action. / C. S. Sherrington (1898) conducted experiments to test James' theory. He ran electrical currents down the spinal cords of rats. However: instead of demonstrating an increase in electrical current as projected by James: Sherrington found that the electrical current strength decreased as the testing continued over time. Importantly: this work led to the discovery of the concept of habituation.  / McCulloch and Pitts  (1943) also created a computational model for neural networks based on mathematics and algorithms. They called this model threshold logic. These early models paved the way for neural network research to split into two distinct approaches. One approach focused on biological processes in the brain and the other focused on the application of neural networks to artificial intelligence. / The parallel distributed processing of the mid-1980s became popular under the name connectionism. The text by Rumelhart and McClelland (1986) provided a full exposition on the use of connectionism in computers to simulate neural processes. / Artificial neural networks: as used in artificial intelligence: have traditionally been viewed as simplified models of neural processing in the brain: even though the relation between this model and brain biological architecture is debated: as it is not clear to what degree artificial neural networks mirror brain function. /  /  / == Neuroscience == / Theoretical and computational neuroscience is the field concerned with the analysis and computational modeling of biological neural systems. / Since neural systems are intimately related to cognitive processes and behaviour: the field is closely related to cognitive and behavioural modeling. / The aim of the field is to create models of biological neural systems in order to understand how biological systems work. To gain this understanding: neuroscientists strive to make a link between observed biological processes (data): biologically plausible mechanisms for neural processing and learning (neural network models) and theory (statistical learning theory and information theory). /  /  / === Types of models === / Many models are used; defined at different levels of abstraction: and modeling different aspects of neural systems. They range from models of the short-term behaviour of individual neurons: through models of the dynamics of neural circuitry arising from interactions between individual neurons: to models of behaviour arising from abstract neural modules that represent complete subsystems."
0.1459063999936916,How has the application of artificial neural networks evolved?,"Artificial neural networks were initially developed to model biological neural networks under the concept of connectionism in the 1930s. However, with the development of the perceptron, a simple artificial neural network, around 1943 and subsequent implementations in hardware, they started being utilized more for machine learning applications, and thereby deviated considerably from their biological counterparts.","Score: 0.8640590929510327 / The original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time: attention focused on matching specific mental abilities: leading to deviations from biology such as backpropagation: or passing information in the reverse direction and adjusting the network to reflect that information. / Neural networks have been used on a variety of tasks: including computer vision: speech recognition: machine translation: social network filtering: playing board and video games and medical diagnosis. / As of 2017: neural networks typically have a few thousand to a few million units and millions of connections. Despite this number being several order of magnitude less than the number of neurons on a human brain: these networks can perform many tasks at a level beyond that of humans (e.g.: recognizing faces: or playing ""Go""). /  /  / === Deep neural networks === / A deep neural network (DNN) is an artificial neural network with multiple layers between the input and output layers. There are different types of neural networks but they always consist of the same components: neurons: synapses: weights: biases: and functions. These components as a whole function in a way that mimics functions of the human brain: and can be trained like any other ML algorithm.For example: a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed. The user can review the results and select which probabilities the network should display (above a certain threshold: etc.) and return the proposed label. Each mathematical manipulation as such is considered a layer: and complex DNN have many layers: hence the name ""deep"" networks. / DNNs can model complex non-linear relationships. DNN architectures generate compositional models where the object is expressed as a layered composition of primitives. The extra layers enable composition of features from lower layers: potentially modeling complex data with fewer units than a similarly performing shallow network. For instance: it was proved that sparse multivariate polynomials are exponentially easier to approximate with DNNs than with shallow networks.Deep architectures include many variants of a few basic approaches. Each architecture has found success in specific domains. It is not always possible to compare the performance of multiple architectures: unless they have been evaluated on the same data sets. / DNNs are typically feedforward networks in which data flows from the input layer to the output layer without looping back. At first: the DNN creates a map of virtual neurons and assigns random numerical values: or ""weights"": to connections between them. The weights and inputs are multiplied and return an output between 0 and 1. If the network did not accurately recognize a particular pattern: an algorithm would adjust the weights. That way the algorithm can make certain parameters more influential: until it determines the correct mathematical manipulation to fully process the data. / Recurrent neural networks: in which data can flow in any direction: are used for applications such as language modeling. Long short-term memory is particularly effective for this use.Convolutional neural networks (CNNs) are used in computer vision. CNNs also have been applied to acoustic modeling for automatic speech recognition (ASR). /  /  / ==== Challenges ==== / As with ANNs: many issues can arise with naively trained DNNs. Two common issues are overfitting and computation time. / DNNs are prone to overfitting because of the added layers of abstraction: which allow them to model rare dependencies in the training data. Regularization methods such as Ivakhnenko's unit pruning or weight decay (ℓ2{\displaystyle \ell _{2}}-regularization) or sparsity (ℓ1{\displaystyle \ell _{1}}-regularization) can be applied during training to combat overfitting. Alternatively dropout regularization randomly omits units from the hidden layers during training. This helps to exclude rare dependencies. Finally: data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.DNNs must consider many training parameters: such as the size (number of layers and number of units per layer): the learning rate: and initial weights. Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources. Various tricks: such as batching (computing the gradient on several training examples at once rather than individual examples) speed up computation. Large processing capabilities of many-core architectures (such as GPUs or the Intel Xeon Phi) have produced significant speedups in training: because of the suitability of such processing architectures for the matrix and vector computations.Alternatively: engineers may look for other types of neural networks with more straightforward and convergent training algorithms.","Score: 0.8625201398623932 / Artificial neural networks (ANNs) are models created using machine learning to perform a number of tasks. Their creation was inspired by neural circuitry. While some of the computational implementations ANNs relate to earlier discoveries in mathematics: the first implementation of ANNs was by psychologist Frank Rosenblatt: who developed the perceptron. Little research was conducted on ANNs in the 1970s and 1980s: with the AAAI calling that period an ""AI winter"".Later: advances in hardware and the development of the backpropagation algorithm as well as recurrent neural networks and convolutional neural networks: renewed interest in ANNs. The 2010s: saw the development of a deep neural network (a neural network with many layers) called AlexNet. It greatly outperformed other image recognition models: and is thought to have launched the ongoing AI spring: and further increasing interest in ANNs. The transformer architecture was first described in 2017 as a method to teach ANNs grammatical dependencies in language: and is the predominant architecture used by large language models: such as GPT-4. Diffusion models were first described in 2015: and began to be used by image generation models such as DALL-E in the 2020s. /  /  / == Linear neural network == / The simplest kind of feedforward neural network is a linear network: which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated in each node. The mean squared errors between these calculated outputs and a given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement. /  /  / == Perceptrons and other early neural networks == / Warren McCulloch and Walter Pitts (1943) also considered a non-learning computational model for neural networks. This model paved the way for research to split into two approaches. One approach focused on biological processes while the other focused on the application of neural networks to artificial intelligence. This work led to work on nerve networks and their link to finite automata.In the early 1940s: D. O. Hebb created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Hebbian learning is unsupervised learning. This evolved into models for long-term potentiation. Researchers started applying these ideas to computational models in 1948 with Turing's B-type machines. Farley and Clark (1954) first used computational machines: then called ""calculators"": to simulate a Hebbian network. Other neural network computational machines were created by Rochester: Holland: Habit and Duda (1956).Rosenblatt (1958) created the perceptron: an algorithm for pattern recognition. With mathematical notation: Rosenblatt described circuitry not in the basic perceptron: such as the exclusive-or circuit that could not be processed by neural networks at the time. In 1959: a biological model proposed by Nobel laureates Hubel and Wiesel was based on their discovery of two types of cells in the primary visual cortex: simple cells and complex cells.Some say that research stagnated following Minsky and Papert (1969): who discovered that basic perceptrons were incapable of processing the exclusive-or circuit and that computers lacked sufficient power to process useful neural networks. However: by the time this book came out: methods for training multilayer perceptrons (MLPs) by deep learning were already known. /  /  / == First deep learning == / The first deep learning MLP was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa in 1965: as the Group Method of Data Handling. This method employs incremental layer by layer training based on regression analysis: where useless units in hidden layers are pruned with the help of a validation set. / The first deep learning MLP trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. / In computer experiments conducted by Amari's student Saito: a five layer MLP with two modifiable layers learned  useful internal representations to classify non-linearily separable pattern classes. /  /  / == Backpropagation == /  / The backpropagation algorithm is an efficient application of the Leibniz chain rule (1673) to networks of differentiable nodes."
0.12774359999457374,What is an optical neural network?,An optical neural network is a physical implementation of an artificial neural network with optical components. It uses the strength of the optical interconnect for implementing neuronal communications.,"Score: 0.9080321859582787 / An optical neural network is a physical implementation of an artificial neural network  with optical components.  Early optical neural networks used a photorefractive Volume hologram to interconnect arrays of input neurons to arrays of output with synaptic weights in proportion to the multiplexed hologram's strength.  Volume holograms were further multiplexed using spectral hole burning to add one dimension of wavelength to space to achieve four dimensional interconnects of two dimensional arrays of neural inputs and outputs.  This research led to extensive research on alternative methods using the strength of the optical interconnect for implementing neuronal communications.Some artificial neural networks that have been implemented as optical neural networks include the Hopfield neural network and the Kohonen self-organizing map with liquid crystal spatial light modulators  Optical neural networks can also be based on the principles of neuromorphic engineering: creating neuromorphic photonic systems. Typically: these systems encode information in the networks using spikes: mimicking the functionality of spiking neural networks in optical and photonic hardware. Photonic devices that have demonstrated neuromorphic functionalities include (among others) vertical-cavity surface-emitting lasers: integrated photonic modulators: optoelectronic systems based on superconducting Josephson junctions or systems based on resonant tunnelling diodes. /  /  / == Electrochemical vs. optical neural networks == / Biological neural networks function on an electrochemical basis: while optical neural networks use electromagnetic waves. Optical interfaces to biological neural networks can be created with optogenetics: but is not the same as an optical neural networks. In biological neural networks there exist a lot of different mechanisms for dynamically changing the state of the neurons: these include short-term and long-term synaptic plasticity. Synaptic plasticity is among the electrophysiological phenomena used to control the efficiency of synaptic transmission: long-term for learning and memory: and short-term for short transient changes in synaptic transmission efficiency. Implementing this with optical components is difficult: and ideally requires advanced photonic materials. Properties that might be desirable in photonic materials for optical neural networks include the ability to change their efficiency of transmitting light: based on the intensity of incoming light. /  /  / == Rising Era of Optical Neural Networks == / With the increasing significance of computer vision in various domains: the computational cost of these tasks has increased: making it more important to develop the new approaches of the processing acceleration. Optical computing has emerged as a potential alternative to GPU acceleration for modern neural networks: particularly considering the looming obsolescence of Moore's Law. Consequently: optical neural networks have garnered increased attention in the research community. Presently: two primary methods of optical neural computing are under research: silicon photonics-based and free-space optics. Each approach has its benefits and drawbacks; while silicon photonics may offer superior speed: it lacks the massive parallelism that free-space optics can deliver. / Given the substantial parallelism capabilities of free-space optics: researchers have focused on taking advantage of it. One implementation: proposed by Lin et al.: involves the training and fabrication of phase masks for a handwritten digit classifier. By stacking 3D-printed phase masks: light passing through the fabricated network can be read by a photodetector array of ten detectors: each representing a digit class ranging from 1 to 10. Although this network can achieve terahertz-range classification: it lacks flexibility: as the phase masks are fabricated for a specific task and cannot be retrained. / An alternative method for classification in free-space optics: introduced by Cahng et al.: employs a 4F system that is based on the convolution theorem to perform convolution operations. This system uses two lenses to execute the Fourier transforms of the convolution operation: enabling passive conversion into the Fourier domain without power consumption or latency. However: the convolution operation kernels in this implementation are also fabricated phase masks: limiting the device's functionality to specific convolutional layers of the network only. / In contrast: Li et al. proposed a technique involving kernel tiling to use the parallelism of the 4F system while using a Digital Micromirror Device (DMD) instead of a phase mask. This approach allows users to upload various kernels into the 4F system and execute the entire network's inference on a single device. Unfortunately: modern neural networks are not designed for the 4F systems: as they were primarily developed during the CPU/GPU era. Mostly because they tend to use a lower resolution and a high number of channels in their feature maps. /  /  / == Other Implementations == / In 2007 there was one model of Optical Neural Network: the Programmable Optical Array/Analogic Computer (POAC).","Score: 0.8654866002225741 / However: the convolution operation kernels in this implementation are also fabricated phase masks: limiting the device's functionality to specific convolutional layers of the network only. / In contrast: Li et al. proposed a technique involving kernel tiling to use the parallelism of the 4F system while using a Digital Micromirror Device (DMD) instead of a phase mask. This approach allows users to upload various kernels into the 4F system and execute the entire network's inference on a single device. Unfortunately: modern neural networks are not designed for the 4F systems: as they were primarily developed during the CPU/GPU era. Mostly because they tend to use a lower resolution and a high number of channels in their feature maps. /  /  / == Other Implementations == / In 2007 there was one model of Optical Neural Network: the Programmable Optical Array/Analogic Computer (POAC). It had been implemented in the year 2000 and reported based on modified Joint Fourier Transform Correlator (JTC) and Bacteriorhodopsin (BR) as a holographic optical memory. Full parallelism: large array size and the speed of light are three promises offered by POAC to implement an optical CNN. They had been investigated during the last years with their practical limitations and considerations yielding the design of the first portable POAC version. / The practical details – hardware (optical setups) and software (optical templates) – were published. However: POAC is a general purpose and programmable array computer that has a wide range of applications including: /  / image processing / pattern recognition / target tracking / real-time video processing / document security / optical switching /  /  / == See also == / Optical computing / Quantum neural network /  /  / == References =="
0.16104010000708513,What are some types of artificial neural networks that have been implemented as optical neural networks?,"Some artificial neural networks that have been implemented as optical neural networks include the Hopfield neural network and the Kohonen self-organizing map, often with liquid crystal spatial light modulators.","Score: 0.8889407468387726 / An optical neural network is a physical implementation of an artificial neural network  with optical components.  Early optical neural networks used a photorefractive Volume hologram to interconnect arrays of input neurons to arrays of output with synaptic weights in proportion to the multiplexed hologram's strength.  Volume holograms were further multiplexed using spectral hole burning to add one dimension of wavelength to space to achieve four dimensional interconnects of two dimensional arrays of neural inputs and outputs.  This research led to extensive research on alternative methods using the strength of the optical interconnect for implementing neuronal communications.Some artificial neural networks that have been implemented as optical neural networks include the Hopfield neural network and the Kohonen self-organizing map with liquid crystal spatial light modulators  Optical neural networks can also be based on the principles of neuromorphic engineering: creating neuromorphic photonic systems. Typically: these systems encode information in the networks using spikes: mimicking the functionality of spiking neural networks in optical and photonic hardware. Photonic devices that have demonstrated neuromorphic functionalities include (among others) vertical-cavity surface-emitting lasers: integrated photonic modulators: optoelectronic systems based on superconducting Josephson junctions or systems based on resonant tunnelling diodes. /  /  / == Electrochemical vs. optical neural networks == / Biological neural networks function on an electrochemical basis: while optical neural networks use electromagnetic waves. Optical interfaces to biological neural networks can be created with optogenetics: but is not the same as an optical neural networks. In biological neural networks there exist a lot of different mechanisms for dynamically changing the state of the neurons: these include short-term and long-term synaptic plasticity. Synaptic plasticity is among the electrophysiological phenomena used to control the efficiency of synaptic transmission: long-term for learning and memory: and short-term for short transient changes in synaptic transmission efficiency. Implementing this with optical components is difficult: and ideally requires advanced photonic materials. Properties that might be desirable in photonic materials for optical neural networks include the ability to change their efficiency of transmitting light: based on the intensity of incoming light. /  /  / == Rising Era of Optical Neural Networks == / With the increasing significance of computer vision in various domains: the computational cost of these tasks has increased: making it more important to develop the new approaches of the processing acceleration. Optical computing has emerged as a potential alternative to GPU acceleration for modern neural networks: particularly considering the looming obsolescence of Moore's Law. Consequently: optical neural networks have garnered increased attention in the research community. Presently: two primary methods of optical neural computing are under research: silicon photonics-based and free-space optics. Each approach has its benefits and drawbacks; while silicon photonics may offer superior speed: it lacks the massive parallelism that free-space optics can deliver. / Given the substantial parallelism capabilities of free-space optics: researchers have focused on taking advantage of it. One implementation: proposed by Lin et al.: involves the training and fabrication of phase masks for a handwritten digit classifier. By stacking 3D-printed phase masks: light passing through the fabricated network can be read by a photodetector array of ten detectors: each representing a digit class ranging from 1 to 10. Although this network can achieve terahertz-range classification: it lacks flexibility: as the phase masks are fabricated for a specific task and cannot be retrained. / An alternative method for classification in free-space optics: introduced by Cahng et al.: employs a 4F system that is based on the convolution theorem to perform convolution operations. This system uses two lenses to execute the Fourier transforms of the convolution operation: enabling passive conversion into the Fourier domain without power consumption or latency. However: the convolution operation kernels in this implementation are also fabricated phase masks: limiting the device's functionality to specific convolutional layers of the network only. / In contrast: Li et al. proposed a technique involving kernel tiling to use the parallelism of the 4F system while using a Digital Micromirror Device (DMD) instead of a phase mask. This approach allows users to upload various kernels into the 4F system and execute the entire network's inference on a single device. Unfortunately: modern neural networks are not designed for the 4F systems: as they were primarily developed during the CPU/GPU era. Mostly because they tend to use a lower resolution and a high number of channels in their feature maps. /  /  / == Other Implementations == / In 2007 there was one model of Optical Neural Network: the Programmable Optical Array/Analogic Computer (POAC).","Score: 0.855692455301937 / However: the convolution operation kernels in this implementation are also fabricated phase masks: limiting the device's functionality to specific convolutional layers of the network only. / In contrast: Li et al. proposed a technique involving kernel tiling to use the parallelism of the 4F system while using a Digital Micromirror Device (DMD) instead of a phase mask. This approach allows users to upload various kernels into the 4F system and execute the entire network's inference on a single device. Unfortunately: modern neural networks are not designed for the 4F systems: as they were primarily developed during the CPU/GPU era. Mostly because they tend to use a lower resolution and a high number of channels in their feature maps. /  /  / == Other Implementations == / In 2007 there was one model of Optical Neural Network: the Programmable Optical Array/Analogic Computer (POAC). It had been implemented in the year 2000 and reported based on modified Joint Fourier Transform Correlator (JTC) and Bacteriorhodopsin (BR) as a holographic optical memory. Full parallelism: large array size and the speed of light are three promises offered by POAC to implement an optical CNN. They had been investigated during the last years with their practical limitations and considerations yielding the design of the first portable POAC version. / The practical details – hardware (optical setups) and software (optical templates) – were published. However: POAC is a general purpose and programmable array computer that has a wide range of applications including: /  / image processing / pattern recognition / target tracking / real-time video processing / document security / optical switching /  /  / == See also == / Optical computing / Quantum neural network /  /  / == References =="
0.16650819999631494,How do biological neural networks differ from optical neural networks?,"Biological neural networks function on an electrochemical basis, while optical neural networks use electromagnetic waves. The mechanisms for dynamically changing the state of the neurons in a biological network include short-term and long-term synaptic plasticity.","Score: 0.8798034472002452 / An optical neural network is a physical implementation of an artificial neural network  with optical components.  Early optical neural networks used a photorefractive Volume hologram to interconnect arrays of input neurons to arrays of output with synaptic weights in proportion to the multiplexed hologram's strength.  Volume holograms were further multiplexed using spectral hole burning to add one dimension of wavelength to space to achieve four dimensional interconnects of two dimensional arrays of neural inputs and outputs.  This research led to extensive research on alternative methods using the strength of the optical interconnect for implementing neuronal communications.Some artificial neural networks that have been implemented as optical neural networks include the Hopfield neural network and the Kohonen self-organizing map with liquid crystal spatial light modulators  Optical neural networks can also be based on the principles of neuromorphic engineering: creating neuromorphic photonic systems. Typically: these systems encode information in the networks using spikes: mimicking the functionality of spiking neural networks in optical and photonic hardware. Photonic devices that have demonstrated neuromorphic functionalities include (among others) vertical-cavity surface-emitting lasers: integrated photonic modulators: optoelectronic systems based on superconducting Josephson junctions or systems based on resonant tunnelling diodes. /  /  / == Electrochemical vs. optical neural networks == / Biological neural networks function on an electrochemical basis: while optical neural networks use electromagnetic waves. Optical interfaces to biological neural networks can be created with optogenetics: but is not the same as an optical neural networks. In biological neural networks there exist a lot of different mechanisms for dynamically changing the state of the neurons: these include short-term and long-term synaptic plasticity. Synaptic plasticity is among the electrophysiological phenomena used to control the efficiency of synaptic transmission: long-term for learning and memory: and short-term for short transient changes in synaptic transmission efficiency. Implementing this with optical components is difficult: and ideally requires advanced photonic materials. Properties that might be desirable in photonic materials for optical neural networks include the ability to change their efficiency of transmitting light: based on the intensity of incoming light. /  /  / == Rising Era of Optical Neural Networks == / With the increasing significance of computer vision in various domains: the computational cost of these tasks has increased: making it more important to develop the new approaches of the processing acceleration. Optical computing has emerged as a potential alternative to GPU acceleration for modern neural networks: particularly considering the looming obsolescence of Moore's Law. Consequently: optical neural networks have garnered increased attention in the research community. Presently: two primary methods of optical neural computing are under research: silicon photonics-based and free-space optics. Each approach has its benefits and drawbacks; while silicon photonics may offer superior speed: it lacks the massive parallelism that free-space optics can deliver. / Given the substantial parallelism capabilities of free-space optics: researchers have focused on taking advantage of it. One implementation: proposed by Lin et al.: involves the training and fabrication of phase masks for a handwritten digit classifier. By stacking 3D-printed phase masks: light passing through the fabricated network can be read by a photodetector array of ten detectors: each representing a digit class ranging from 1 to 10. Although this network can achieve terahertz-range classification: it lacks flexibility: as the phase masks are fabricated for a specific task and cannot be retrained. / An alternative method for classification in free-space optics: introduced by Cahng et al.: employs a 4F system that is based on the convolution theorem to perform convolution operations. This system uses two lenses to execute the Fourier transforms of the convolution operation: enabling passive conversion into the Fourier domain without power consumption or latency. However: the convolution operation kernels in this implementation are also fabricated phase masks: limiting the device's functionality to specific convolutional layers of the network only. / In contrast: Li et al. proposed a technique involving kernel tiling to use the parallelism of the 4F system while using a Digital Micromirror Device (DMD) instead of a phase mask. This approach allows users to upload various kernels into the 4F system and execute the entire network's inference on a single device. Unfortunately: modern neural networks are not designed for the 4F systems: as they were primarily developed during the CPU/GPU era. Mostly because they tend to use a lower resolution and a high number of channels in their feature maps. /  /  / == Other Implementations == / In 2007 there was one model of Optical Neural Network: the Programmable Optical Array/Analogic Computer (POAC).","Score: 0.8451117461748214 / A neural network: also called a neuronal network: is an interconnected population of neurons (typically containing multiple neural circuits). Biological neural networks are studied to understand the organization and functioning of nervous systems. / Closely related are artificial neural networks: machine learning models inspired by biological neural networks. They consist of artificial neurons: which are mathematical functions that are designed to be analogous to the mechanisms used by neural circuits. /  /  / == Overview == / A biological neural network is composed of a group of chemically connected or functionally associated neurons. A single neuron may be connected to many other neurons and the total number of neurons and connections in a network may be extensive. Connections: called synapses: are usually formed from axons to dendrites: though dendrodendritic synapses and other connections are possible. Apart from electrical signalling: there are other forms of signalling that arise from neurotransmitter diffusion.  / Artificial intelligence: cognitive modelling: and artificial neural networks are information processing paradigms inspired by how biological neural systems process data. Artificial intelligence and cognitive modelling try to simulate some properties of biological neural networks. In the artificial intelligence field: artificial neural networks have been applied successfully to speech recognition: image analysis and adaptive control: in order to construct software agents (in computer and video games) or autonomous robots. / Neural network theory has served to identify better how the neurons in the brain function and provide the basis for efforts to create artificial intelligence. /  /  / == History == / The preliminary theoretical base for contemporary neural networks was independently proposed by Alexander Bain (1873) and William James (1890). In their work: both thoughts and body activity resulted from interactions among neurons within the brain. /  / For Bain: every activity led to the firing of a certain set of neurons. When activities were repeated: the connections between those neurons strengthened. According to his theory: this repetition was what led to the formation of memory. The general scientific community at the time was skeptical of Bain's theory because it required what appeared to be an inordinate number of neural connections within the brain. It is now apparent that the brain is exceedingly complex and that the same brain “wiring” can handle multiple problems and inputs. / James' theory was similar to Bain's; however: he suggested that memories and actions resulted from electrical currents flowing among the neurons in the brain. His model: by focusing on the flow of electrical currents: did not require individual neural connections for each memory or action. / C. S. Sherrington (1898) conducted experiments to test James' theory. He ran electrical currents down the spinal cords of rats. However: instead of demonstrating an increase in electrical current as projected by James: Sherrington found that the electrical current strength decreased as the testing continued over time. Importantly: this work led to the discovery of the concept of habituation.  / McCulloch and Pitts  (1943) also created a computational model for neural networks based on mathematics and algorithms. They called this model threshold logic. These early models paved the way for neural network research to split into two distinct approaches. One approach focused on biological processes in the brain and the other focused on the application of neural networks to artificial intelligence. / The parallel distributed processing of the mid-1980s became popular under the name connectionism. The text by Rumelhart and McClelland (1986) provided a full exposition on the use of connectionism in computers to simulate neural processes. / Artificial neural networks: as used in artificial intelligence: have traditionally been viewed as simplified models of neural processing in the brain: even though the relation between this model and brain biological architecture is debated: as it is not clear to what degree artificial neural networks mirror brain function. /  /  / == Neuroscience == / Theoretical and computational neuroscience is the field concerned with the analysis and computational modeling of biological neural systems. / Since neural systems are intimately related to cognitive processes and behaviour: the field is closely related to cognitive and behavioural modeling. / The aim of the field is to create models of biological neural systems in order to understand how biological systems work. To gain this understanding: neuroscientists strive to make a link between observed biological processes (data): biologically plausible mechanisms for neural processing and learning (neural network models) and theory (statistical learning theory and information theory). /  /  / === Types of models === / Many models are used; defined at different levels of abstraction: and modeling different aspects of neural systems. They range from models of the short-term behaviour of individual neurons: through models of the dynamics of neural circuitry arising from interactions between individual neurons: to models of behaviour arising from abstract neural modules that represent complete subsystems."
0.19171719998121262,What are the two primary methods of optical neural computing currently under research?,"The two primary methods of optical neural computing under research are silicon photonics-based and free-space optics. Silicon photonics offer superior speed, while free-space optics deliver massive parallelism.","Score: 0.8631090096401777 / An optical neural network is a physical implementation of an artificial neural network  with optical components.  Early optical neural networks used a photorefractive Volume hologram to interconnect arrays of input neurons to arrays of output with synaptic weights in proportion to the multiplexed hologram's strength.  Volume holograms were further multiplexed using spectral hole burning to add one dimension of wavelength to space to achieve four dimensional interconnects of two dimensional arrays of neural inputs and outputs.  This research led to extensive research on alternative methods using the strength of the optical interconnect for implementing neuronal communications.Some artificial neural networks that have been implemented as optical neural networks include the Hopfield neural network and the Kohonen self-organizing map with liquid crystal spatial light modulators  Optical neural networks can also be based on the principles of neuromorphic engineering: creating neuromorphic photonic systems. Typically: these systems encode information in the networks using spikes: mimicking the functionality of spiking neural networks in optical and photonic hardware. Photonic devices that have demonstrated neuromorphic functionalities include (among others) vertical-cavity surface-emitting lasers: integrated photonic modulators: optoelectronic systems based on superconducting Josephson junctions or systems based on resonant tunnelling diodes. /  /  / == Electrochemical vs. optical neural networks == / Biological neural networks function on an electrochemical basis: while optical neural networks use electromagnetic waves. Optical interfaces to biological neural networks can be created with optogenetics: but is not the same as an optical neural networks. In biological neural networks there exist a lot of different mechanisms for dynamically changing the state of the neurons: these include short-term and long-term synaptic plasticity. Synaptic plasticity is among the electrophysiological phenomena used to control the efficiency of synaptic transmission: long-term for learning and memory: and short-term for short transient changes in synaptic transmission efficiency. Implementing this with optical components is difficult: and ideally requires advanced photonic materials. Properties that might be desirable in photonic materials for optical neural networks include the ability to change their efficiency of transmitting light: based on the intensity of incoming light. /  /  / == Rising Era of Optical Neural Networks == / With the increasing significance of computer vision in various domains: the computational cost of these tasks has increased: making it more important to develop the new approaches of the processing acceleration. Optical computing has emerged as a potential alternative to GPU acceleration for modern neural networks: particularly considering the looming obsolescence of Moore's Law. Consequently: optical neural networks have garnered increased attention in the research community. Presently: two primary methods of optical neural computing are under research: silicon photonics-based and free-space optics. Each approach has its benefits and drawbacks; while silicon photonics may offer superior speed: it lacks the massive parallelism that free-space optics can deliver. / Given the substantial parallelism capabilities of free-space optics: researchers have focused on taking advantage of it. One implementation: proposed by Lin et al.: involves the training and fabrication of phase masks for a handwritten digit classifier. By stacking 3D-printed phase masks: light passing through the fabricated network can be read by a photodetector array of ten detectors: each representing a digit class ranging from 1 to 10. Although this network can achieve terahertz-range classification: it lacks flexibility: as the phase masks are fabricated for a specific task and cannot be retrained. / An alternative method for classification in free-space optics: introduced by Cahng et al.: employs a 4F system that is based on the convolution theorem to perform convolution operations. This system uses two lenses to execute the Fourier transforms of the convolution operation: enabling passive conversion into the Fourier domain without power consumption or latency. However: the convolution operation kernels in this implementation are also fabricated phase masks: limiting the device's functionality to specific convolutional layers of the network only. / In contrast: Li et al. proposed a technique involving kernel tiling to use the parallelism of the 4F system while using a Digital Micromirror Device (DMD) instead of a phase mask. This approach allows users to upload various kernels into the 4F system and execute the entire network's inference on a single device. Unfortunately: modern neural networks are not designed for the 4F systems: as they were primarily developed during the CPU/GPU era. Mostly because they tend to use a lower resolution and a high number of channels in their feature maps. /  /  / == Other Implementations == / In 2007 there was one model of Optical Neural Network: the Programmable Optical Array/Analogic Computer (POAC).","Score: 0.8346967072252144 / However: the convolution operation kernels in this implementation are also fabricated phase masks: limiting the device's functionality to specific convolutional layers of the network only. / In contrast: Li et al. proposed a technique involving kernel tiling to use the parallelism of the 4F system while using a Digital Micromirror Device (DMD) instead of a phase mask. This approach allows users to upload various kernels into the 4F system and execute the entire network's inference on a single device. Unfortunately: modern neural networks are not designed for the 4F systems: as they were primarily developed during the CPU/GPU era. Mostly because they tend to use a lower resolution and a high number of channels in their feature maps. /  /  / == Other Implementations == / In 2007 there was one model of Optical Neural Network: the Programmable Optical Array/Analogic Computer (POAC). It had been implemented in the year 2000 and reported based on modified Joint Fourier Transform Correlator (JTC) and Bacteriorhodopsin (BR) as a holographic optical memory. Full parallelism: large array size and the speed of light are three promises offered by POAC to implement an optical CNN. They had been investigated during the last years with their practical limitations and considerations yielding the design of the first portable POAC version. / The practical details – hardware (optical setups) and software (optical templates) – were published. However: POAC is a general purpose and programmable array computer that has a wide range of applications including: /  / image processing / pattern recognition / target tracking / real-time video processing / document security / optical switching /  /  / == See also == / Optical computing / Quantum neural network /  /  / == References =="
0.1915924999921117,What was significant about the Programmable Optical Array/Analogic Computer (POAC)?,"The Programmable Optical Array/Analogic Computer (POAC) was a model of an optical neural network implemented in 2000. It utilized a Joint Fourier Transform Correlator (JTC) and Bacteriorhodopsin (BR) as a holographic optical memory. It promised full parallelism, large array size, and the speed of light for implementing an optical CNN.","Score: 0.8566563895993421 / However: the convolution operation kernels in this implementation are also fabricated phase masks: limiting the device's functionality to specific convolutional layers of the network only. / In contrast: Li et al. proposed a technique involving kernel tiling to use the parallelism of the 4F system while using a Digital Micromirror Device (DMD) instead of a phase mask. This approach allows users to upload various kernels into the 4F system and execute the entire network's inference on a single device. Unfortunately: modern neural networks are not designed for the 4F systems: as they were primarily developed during the CPU/GPU era. Mostly because they tend to use a lower resolution and a high number of channels in their feature maps. /  /  / == Other Implementations == / In 2007 there was one model of Optical Neural Network: the Programmable Optical Array/Analogic Computer (POAC). It had been implemented in the year 2000 and reported based on modified Joint Fourier Transform Correlator (JTC) and Bacteriorhodopsin (BR) as a holographic optical memory. Full parallelism: large array size and the speed of light are three promises offered by POAC to implement an optical CNN. They had been investigated during the last years with their practical limitations and considerations yielding the design of the first portable POAC version. / The practical details – hardware (optical setups) and software (optical templates) – were published. However: POAC is a general purpose and programmable array computer that has a wide range of applications including: /  / image processing / pattern recognition / target tracking / real-time video processing / document security / optical switching /  /  / == See also == / Optical computing / Quantum neural network /  /  / == References ==","Score: 0.8105700500187619 / An optical neural network is a physical implementation of an artificial neural network  with optical components.  Early optical neural networks used a photorefractive Volume hologram to interconnect arrays of input neurons to arrays of output with synaptic weights in proportion to the multiplexed hologram's strength.  Volume holograms were further multiplexed using spectral hole burning to add one dimension of wavelength to space to achieve four dimensional interconnects of two dimensional arrays of neural inputs and outputs.  This research led to extensive research on alternative methods using the strength of the optical interconnect for implementing neuronal communications.Some artificial neural networks that have been implemented as optical neural networks include the Hopfield neural network and the Kohonen self-organizing map with liquid crystal spatial light modulators  Optical neural networks can also be based on the principles of neuromorphic engineering: creating neuromorphic photonic systems. Typically: these systems encode information in the networks using spikes: mimicking the functionality of spiking neural networks in optical and photonic hardware. Photonic devices that have demonstrated neuromorphic functionalities include (among others) vertical-cavity surface-emitting lasers: integrated photonic modulators: optoelectronic systems based on superconducting Josephson junctions or systems based on resonant tunnelling diodes. /  /  / == Electrochemical vs. optical neural networks == / Biological neural networks function on an electrochemical basis: while optical neural networks use electromagnetic waves. Optical interfaces to biological neural networks can be created with optogenetics: but is not the same as an optical neural networks. In biological neural networks there exist a lot of different mechanisms for dynamically changing the state of the neurons: these include short-term and long-term synaptic plasticity. Synaptic plasticity is among the electrophysiological phenomena used to control the efficiency of synaptic transmission: long-term for learning and memory: and short-term for short transient changes in synaptic transmission efficiency. Implementing this with optical components is difficult: and ideally requires advanced photonic materials. Properties that might be desirable in photonic materials for optical neural networks include the ability to change their efficiency of transmitting light: based on the intensity of incoming light. /  /  / == Rising Era of Optical Neural Networks == / With the increasing significance of computer vision in various domains: the computational cost of these tasks has increased: making it more important to develop the new approaches of the processing acceleration. Optical computing has emerged as a potential alternative to GPU acceleration for modern neural networks: particularly considering the looming obsolescence of Moore's Law. Consequently: optical neural networks have garnered increased attention in the research community. Presently: two primary methods of optical neural computing are under research: silicon photonics-based and free-space optics. Each approach has its benefits and drawbacks; while silicon photonics may offer superior speed: it lacks the massive parallelism that free-space optics can deliver. / Given the substantial parallelism capabilities of free-space optics: researchers have focused on taking advantage of it. One implementation: proposed by Lin et al.: involves the training and fabrication of phase masks for a handwritten digit classifier. By stacking 3D-printed phase masks: light passing through the fabricated network can be read by a photodetector array of ten detectors: each representing a digit class ranging from 1 to 10. Although this network can achieve terahertz-range classification: it lacks flexibility: as the phase masks are fabricated for a specific task and cannot be retrained. / An alternative method for classification in free-space optics: introduced by Cahng et al.: employs a 4F system that is based on the convolution theorem to perform convolution operations. This system uses two lenses to execute the Fourier transforms of the convolution operation: enabling passive conversion into the Fourier domain without power consumption or latency. However: the convolution operation kernels in this implementation are also fabricated phase masks: limiting the device's functionality to specific convolutional layers of the network only. / In contrast: Li et al. proposed a technique involving kernel tiling to use the parallelism of the 4F system while using a Digital Micromirror Device (DMD) instead of a phase mask. This approach allows users to upload various kernels into the 4F system and execute the entire network's inference on a single device. Unfortunately: modern neural networks are not designed for the 4F systems: as they were primarily developed during the CPU/GPU era. Mostly because they tend to use a lower resolution and a high number of channels in their feature maps. /  /  / == Other Implementations == / In 2007 there was one model of Optical Neural Network: the Programmable Optical Array/Analogic Computer (POAC)."
0.12647700001252815,What are Physics-informed neural networks (PINNs)?,"Physics-informed neural networks (PINNs) are universal function approximators that can integrate the knowledge of any physical laws governing a given data-set in the learning process. They can be described by partial differential equations (PDEs) and help overcome the low data availability of some biological and engineering systems. They increase the correctness of the function approximation and enhance the information content of the available data, making learning algorithms capture the right solution and generalize well.","Score: 0.917333844029075 / Physics-informed neural networks (PINNs) are a type of universal function approximators that can embed the knowledge of any physical laws that govern a given data-set in the learning process: and can be described by partial differential equations (PDEs). They overcome the low data availability of some biological and engineering systems that makes most state-of-the-art machine learning techniques lack robustness: rendering them ineffective in these scenarios. The prior knowledge of general physical laws acts in the training of neural networks (NNs) as a regularization agent that limits the space of admissible solutions: increasing the correctness of the function approximation. This way: embedding this prior information into a neural network results in enhancing the information content of the available data: facilitating the learning algorithm to capture the right solution and to generalize well even with a low amount of training examples. /  /  / == Function approximation == / Most of the physical laws that govern the dynamics of a system can be described by partial differential equations. For example: the Navier–Stokes equations are a set of partial differential equations derived from the conservation laws (i.e.: conservation of mass: momentum: and energy) that govern fluid mechanics. The solution of the Navier–Stokes equations with appropriate initial and boundary conditions allows the quantification of flow dynamics in a precisely defined geometry. However: these equations cannot be solved exactly and therefore numerical methods must be used (such as finite differences: finite elements and finite volumes). In this setting: these governing equations must be solved while accounting for prior assumptions: linearization: and adequate time and space discretization. / Recently: solving the governing partial differential equations of physical phenomena using deep learning has emerged as a new field of scientific machine learning (SciML): leveraging the universal approximation theorem and high expressivity of neural networks. In general: deep neural networks could approximate any high-dimensional function given that sufficient training data are supplied. However: such networks do not consider the physical characteristics underlying the problem: and the level of approximation accuracy provided by them is still heavily dependent on careful specifications of the problem geometry as well as the initial and boundary conditions. Without this preliminary information: the solution is not unique and may lose physical correctness. On the other hand: physics-informed neural networks (PINNs) leverage governing physical equations in neural network training. Namely: PINNs are designed to be trained to satisfy the given training data as well as the imposed governing equations. In this fashion: a neural network can be guided with training data that do not necessarily need to be large and complete. Potentially: an accurate solution of partial differential equations can be found without knowing the boundary conditions. Therefore: with some knowledge about the physical characteristics of the problem and some form of training data (even sparse and incomplete): PINN may be used for finding an optimal solution with high fidelity. / PINNs allow for addressing a wide range of problems in computational science and represent a pioneering technology leading to the development of new classes of numerical solvers for PDEs. PINNs can be thought of as a meshfree alternative to traditional approaches (e.g.: CFD for fluid dynamics): and new data-driven approaches for model inversion and system identification. Notably: the trained PINN network can be used for predicting the values on simulation grids of different resolutions without the need to be retrained. In addition: they allow for exploiting automatic differentiation (AD) to compute the required derivatives in the partial differential equations: a new class of differentiation techniques widely used to derive neural networks assessed to be superior to numerical or symbolic differentiation. /  /  / == Modeling and computation == / A general nonlinear partial differential equation can be: / ut+N[u;λ]=0:x∈Ω:t∈[0:T]{\displaystyle u_{t}+N[u;\lambda ]=0:\quad x\in \Omega :\quad t\in [0:T]} / where u(t:x){\displaystyle u(t:x)} denotes the solution: N[⋅;λ]{\displaystyle N[\cdot ;\lambda ]} is a nonlinear operator parametrized by λ{\displaystyle \lambda }: and Ω{\displaystyle \Omega } is a subset of RD{\displaystyle \mathbb {R} ^{D}}. This general form of governing equations summarizes a wide range of problems in mathematical physics: such as conservative laws: diffusion process: advection-diffusion systems: and kinetic equations. Given noisy measurements of a generic dynamic system described by the equation above: PINNs can be designed to solve two classes of problems: /  / data-driven solution / data-driven discoveryof partial differential equations.","Score: 0.8907464912533813 / Where Lu=‖u−z‖Γ{\displaystyle L_{u}=\Vert u-z\Vert _{\Gamma }}: with u{\displaystyle u} and z{\displaystyle z} state solutions and measurements at sparse location Γ{\displaystyle \Gamma }: respectively and Lf=‖f‖Γ{\displaystyle L_{f}=\Vert f\Vert _{\Gamma }} residual function. This second term requires the structured information represented by the partial differential equations to be satisfied in the training process. / This strategy allows for discovering dynamic models described by nonlinear PDEs assembling computationally efficient and fully differentiable surrogate models that may find application in predictive forecasting: control: and data assimilation. /  /  / == Physics-informed neural networks for piece-wise function approximation == / PINN is unable to approximate PDEs that have strong non-linearity or sharp gradients that commonly occur in practical fluid flow problems. Piece-wise approximation has been an old practice in the field of numerical approximation. With the capability of approximating strong non-linearity extremely light weight PINNs are used to solve PDEs in much larger discrete subdomains that increases accuracy substantially and decreases computational load as well. DPINN (Distributed physics-informed neural networks) and DPIELM (Distributed physics-informed extreme learning machines) are generalizable space-time domain discretization for better approximation. DPIELM is an extremely fast and lightweight approximator with competitive accuracy. Domain scaling on the top has a special effect. Another school of thought is discretization for parallel computation to leverage usage of available computational resources.  / XPINNs  is a generalized space-time domain decomposition approach for the physics-informed neural networks (PINNs) to solve nonlinear partial differential equations on arbitrary complex-geometry domains. The XPINNs further pushes the boundaries of both PINNs as well as Conservative PINNs (cPINNs): which is a spatial domain decomposition approach in the PINN framework tailored to conservation laws. Compared to PINN: the XPINN method has large representation and parallelization capacity due to the inherent property of deployment of multiple neural networks in the smaller subdomains. Unlike cPINN: XPINN can be extended to any type of PDEs. Moreover: the domain can be decomposed in any arbitrary way (in space and time): which is not possible in cPINN. Thus: XPINN offers both space and time parallelization: thereby reducing the training cost more effectively. The XPINN is particularly effective for the large-scale problems (involving large data set) as well as for the high-dimensional problems where single network based PINN is not adequate. The rigorous bounds on the errors resulting from the approximation of the nonlinear PDEs (incompressible Navier–Stokes equations) with PINNs and XPINNs are proved. /  /  / == Physics-informed neural networks and functional interpolation == / In the PINN framework: initial and boundary conditions are not analytically satisfied: thus they need to be included in the loss function of the network to be simultaneously learned with the differential equation (DE) unknown functions. Having competing objectives during the network's training can lead to unbalanced gradients while using gradient-based techniques: which causes PINNs to often struggle to accurately learn the underlying DE solution. This drawback is overcome by using functional interpolation techniques such as the Theory of Functional Connections (TFC)'s constrained expression: in the Deep-TFC framework: which reduces the solution search space of constrained problems to the subspace of neural network that analytically satisfies the constraints. A further improvement of PINN and functional interpolation approach is given by the Extreme Theory of Functional Connections (X-TFC) framework: where a single-layer Neural Network and the extreme learning machine training algorithm are employed. X-TFC allows to improve the accuracy and performance of regular PINNs: and its robustness and reliability are proved for stiff problems: optimal control: aerospace: and rarefied gas dynamics applications. /  /  / == Physics-informed PointNet (PIPN) for multiple sets of irregular geometries == / Regular PINNs are only able to obtain the solution of a forward or inverse problem on a single geometry. It means that for any new geometry (computational domain): one must retrain a PINN. This limitation of regular PINNs imposes high computational costs: specifically for a comprehensive investigation of geometric parameters in industrial designs. Physics-informed PointNet (PIPN)  is fundamentally the result of a combination of PINN's loss function with PointNet. In fact: instead of using a simple fully connected neural network: PIPN uses PointNet as the core of its neural network."
0.16229249999742024,How do PINNs work in terms of function approximation?,"PINNs work by leveraging governing physical equations in neural network training. They are designed to be trained to satisfy the given training data and the imposed governing equations. This means that a neural network can be guided with training data that do not necessarily need to be large and complete. Thus, even with sparse and incomplete data, PINN may be used for finding an optimal solution with high fidelity.","Score: 0.8521495643378039 / Physics-informed neural networks (PINNs) are a type of universal function approximators that can embed the knowledge of any physical laws that govern a given data-set in the learning process: and can be described by partial differential equations (PDEs). They overcome the low data availability of some biological and engineering systems that makes most state-of-the-art machine learning techniques lack robustness: rendering them ineffective in these scenarios. The prior knowledge of general physical laws acts in the training of neural networks (NNs) as a regularization agent that limits the space of admissible solutions: increasing the correctness of the function approximation. This way: embedding this prior information into a neural network results in enhancing the information content of the available data: facilitating the learning algorithm to capture the right solution and to generalize well even with a low amount of training examples. /  /  / == Function approximation == / Most of the physical laws that govern the dynamics of a system can be described by partial differential equations. For example: the Navier–Stokes equations are a set of partial differential equations derived from the conservation laws (i.e.: conservation of mass: momentum: and energy) that govern fluid mechanics. The solution of the Navier–Stokes equations with appropriate initial and boundary conditions allows the quantification of flow dynamics in a precisely defined geometry. However: these equations cannot be solved exactly and therefore numerical methods must be used (such as finite differences: finite elements and finite volumes). In this setting: these governing equations must be solved while accounting for prior assumptions: linearization: and adequate time and space discretization. / Recently: solving the governing partial differential equations of physical phenomena using deep learning has emerged as a new field of scientific machine learning (SciML): leveraging the universal approximation theorem and high expressivity of neural networks. In general: deep neural networks could approximate any high-dimensional function given that sufficient training data are supplied. However: such networks do not consider the physical characteristics underlying the problem: and the level of approximation accuracy provided by them is still heavily dependent on careful specifications of the problem geometry as well as the initial and boundary conditions. Without this preliminary information: the solution is not unique and may lose physical correctness. On the other hand: physics-informed neural networks (PINNs) leverage governing physical equations in neural network training. Namely: PINNs are designed to be trained to satisfy the given training data as well as the imposed governing equations. In this fashion: a neural network can be guided with training data that do not necessarily need to be large and complete. Potentially: an accurate solution of partial differential equations can be found without knowing the boundary conditions. Therefore: with some knowledge about the physical characteristics of the problem and some form of training data (even sparse and incomplete): PINN may be used for finding an optimal solution with high fidelity. / PINNs allow for addressing a wide range of problems in computational science and represent a pioneering technology leading to the development of new classes of numerical solvers for PDEs. PINNs can be thought of as a meshfree alternative to traditional approaches (e.g.: CFD for fluid dynamics): and new data-driven approaches for model inversion and system identification. Notably: the trained PINN network can be used for predicting the values on simulation grids of different resolutions without the need to be retrained. In addition: they allow for exploiting automatic differentiation (AD) to compute the required derivatives in the partial differential equations: a new class of differentiation techniques widely used to derive neural networks assessed to be superior to numerical or symbolic differentiation. /  /  / == Modeling and computation == / A general nonlinear partial differential equation can be: / ut+N[u;λ]=0:x∈Ω:t∈[0:T]{\displaystyle u_{t}+N[u;\lambda ]=0:\quad x\in \Omega :\quad t\in [0:T]} / where u(t:x){\displaystyle u(t:x)} denotes the solution: N[⋅;λ]{\displaystyle N[\cdot ;\lambda ]} is a nonlinear operator parametrized by λ{\displaystyle \lambda }: and Ω{\displaystyle \Omega } is a subset of RD{\displaystyle \mathbb {R} ^{D}}. This general form of governing equations summarizes a wide range of problems in mathematical physics: such as conservative laws: diffusion process: advection-diffusion systems: and kinetic equations. Given noisy measurements of a generic dynamic system described by the equation above: PINNs can be designed to solve two classes of problems: /  / data-driven solution / data-driven discoveryof partial differential equations.","Score: 0.83062266209448 / This general form of governing equations summarizes a wide range of problems in mathematical physics: such as conservative laws: diffusion process: advection-diffusion systems: and kinetic equations. Given noisy measurements of a generic dynamic system described by the equation above: PINNs can be designed to solve two classes of problems: /  / data-driven solution / data-driven discoveryof partial differential equations. /  /  / === Data-driven solution of partial differential equations === / The data-driven solution of PDE computes the hidden state u(t:x){\displaystyle u(t:x)} of the system given boundary data and/or measurements z{\displaystyle z}: and fixed model parameters λ{\displaystyle \lambda }. We solve: / ut+N[u]=0:x∈Ω:t∈[0:T]{\displaystyle u_{t}+N[u]=0:\quad x\in \Omega :\quad t\in [0:T]}. / By defining the residual f(t:x){\displaystyle f(t:x)} as / f:=ut+N[u]=0{\displaystyle f:=u_{t}+N[u]=0}: / and approximating u(t:x){\displaystyle u(t:x)} by a deep neural network. This network can be differentiated using automatic differentiation. The parameters of u(t:x){\displaystyle u(t:x)} and f(t:x){\displaystyle f(t:x)} can be then learned by minimizing the following loss function Ltot{\displaystyle L_{tot}}: / Ltot=Lu+Lf{\displaystyle L_{tot}=L_{u}+L_{f}}. / Where Lu=‖u−z‖Γ{\displaystyle L_{u}=\Vert u-z\Vert _{\Gamma }} is the error between the PINN u(t:x){\displaystyle u(t:x)} and the set of boundary conditions and measured data on the set of points Γ{\displaystyle \Gamma } where the boundary conditions and data are defined: and Lf=‖f‖Γ{\displaystyle L_{f}=\Vert f\Vert _{\Gamma }} is the mean-squared error of the residual function. This second term encourages the PINN to learn the structural information expressed by the partial differential equation during the training process. / This approach has been used to yield computationally efficient physics-informed surrogate models with applications in the forecasting of physical processes: model predictive control: multi-physics and multi-scale modeling: and simulation. It has been shown to converge to the solution of the PDE. /  /  / === Data-driven discovery of partial differential equations === / Given noisy and incomplete measurements z{\displaystyle z} of the state of the system: the data-driven discovery of PDE results in computing the unknown state u(t:x){\displaystyle u(t:x)} and learning model parameters λ{\displaystyle \lambda } that best describe the observed data and it reads as follows: / ut+N[u;λ]=0:x∈Ω:t∈[0:T]{\displaystyle u_{t}+N[u;\lambda ]=0:\quad x\in \Omega :\quad t\in [0:T]}. / By defining f(t:x){\displaystyle f(t:x)} as / f:=ut+N[u;λ]=0{\displaystyle f:=u_{t}+N[u;\lambda ]=0}: / and approximating u(t:x){\displaystyle u(t:x)} by a deep neural network: f(t:x){\displaystyle f(t:x)} results in a PINN. This network can be derived using automatic differentiation. The parameters of u(t:x){\displaystyle u(t:x)} and f(t:x){\displaystyle f(t:x)}: together with the parameter λ{\displaystyle \lambda } of the differential operator can be then learned by minimizing the following loss function Ltot{\displaystyle L_{tot}}: / Ltot=Lu+Lf{\displaystyle L_{tot}=L_{u}+L_{f}}. / Where Lu=‖u−z‖Γ{\displaystyle L_{u}=\Vert u-z\Vert _{\Gamma }}: with u{\displaystyle u} and z{\displaystyle z} state solutions and measurements at sparse location Γ{\displaystyle \Gamma }: respectively and Lf=‖f‖Γ{\displaystyle L_{f}=\Vert f\Vert _{\Gamma }} residual function. This second term requires the structured information represented by the partial differential equations to be satisfied in the training process. / This strategy allows for discovering dynamic models described by nonlinear PDEs assembling computationally efficient and fully differentiable surrogate models that may find application in predictive forecasting: control: and data assimilation."
0.23224209999898449,What is the significance of automatic differentiation (AD) in PINNs?,Automatic differentiation (AD) is exploited in PINNs to compute the required derivatives in the partial differential equations. It's a new class of differentiation techniques widely used to derive neural networks and is considered superior to numerical or symbolic differentiation.,"Score: 0.8404083163015723 / Physics-informed neural networks (PINNs) are a type of universal function approximators that can embed the knowledge of any physical laws that govern a given data-set in the learning process: and can be described by partial differential equations (PDEs). They overcome the low data availability of some biological and engineering systems that makes most state-of-the-art machine learning techniques lack robustness: rendering them ineffective in these scenarios. The prior knowledge of general physical laws acts in the training of neural networks (NNs) as a regularization agent that limits the space of admissible solutions: increasing the correctness of the function approximation. This way: embedding this prior information into a neural network results in enhancing the information content of the available data: facilitating the learning algorithm to capture the right solution and to generalize well even with a low amount of training examples. /  /  / == Function approximation == / Most of the physical laws that govern the dynamics of a system can be described by partial differential equations. For example: the Navier–Stokes equations are a set of partial differential equations derived from the conservation laws (i.e.: conservation of mass: momentum: and energy) that govern fluid mechanics. The solution of the Navier–Stokes equations with appropriate initial and boundary conditions allows the quantification of flow dynamics in a precisely defined geometry. However: these equations cannot be solved exactly and therefore numerical methods must be used (such as finite differences: finite elements and finite volumes). In this setting: these governing equations must be solved while accounting for prior assumptions: linearization: and adequate time and space discretization. / Recently: solving the governing partial differential equations of physical phenomena using deep learning has emerged as a new field of scientific machine learning (SciML): leveraging the universal approximation theorem and high expressivity of neural networks. In general: deep neural networks could approximate any high-dimensional function given that sufficient training data are supplied. However: such networks do not consider the physical characteristics underlying the problem: and the level of approximation accuracy provided by them is still heavily dependent on careful specifications of the problem geometry as well as the initial and boundary conditions. Without this preliminary information: the solution is not unique and may lose physical correctness. On the other hand: physics-informed neural networks (PINNs) leverage governing physical equations in neural network training. Namely: PINNs are designed to be trained to satisfy the given training data as well as the imposed governing equations. In this fashion: a neural network can be guided with training data that do not necessarily need to be large and complete. Potentially: an accurate solution of partial differential equations can be found without knowing the boundary conditions. Therefore: with some knowledge about the physical characteristics of the problem and some form of training data (even sparse and incomplete): PINN may be used for finding an optimal solution with high fidelity. / PINNs allow for addressing a wide range of problems in computational science and represent a pioneering technology leading to the development of new classes of numerical solvers for PDEs. PINNs can be thought of as a meshfree alternative to traditional approaches (e.g.: CFD for fluid dynamics): and new data-driven approaches for model inversion and system identification. Notably: the trained PINN network can be used for predicting the values on simulation grids of different resolutions without the need to be retrained. In addition: they allow for exploiting automatic differentiation (AD) to compute the required derivatives in the partial differential equations: a new class of differentiation techniques widely used to derive neural networks assessed to be superior to numerical or symbolic differentiation. /  /  / == Modeling and computation == / A general nonlinear partial differential equation can be: / ut+N[u;λ]=0:x∈Ω:t∈[0:T]{\displaystyle u_{t}+N[u;\lambda ]=0:\quad x\in \Omega :\quad t\in [0:T]} / where u(t:x){\displaystyle u(t:x)} denotes the solution: N[⋅;λ]{\displaystyle N[\cdot ;\lambda ]} is a nonlinear operator parametrized by λ{\displaystyle \lambda }: and Ω{\displaystyle \Omega } is a subset of RD{\displaystyle \mathbb {R} ^{D}}. This general form of governing equations summarizes a wide range of problems in mathematical physics: such as conservative laws: diffusion process: advection-diffusion systems: and kinetic equations. Given noisy measurements of a generic dynamic system described by the equation above: PINNs can be designed to solve two classes of problems: /  / data-driven solution / data-driven discoveryof partial differential equations.","Score: 0.8382025285653926 / This general form of governing equations summarizes a wide range of problems in mathematical physics: such as conservative laws: diffusion process: advection-diffusion systems: and kinetic equations. Given noisy measurements of a generic dynamic system described by the equation above: PINNs can be designed to solve two classes of problems: /  / data-driven solution / data-driven discoveryof partial differential equations. /  /  / === Data-driven solution of partial differential equations === / The data-driven solution of PDE computes the hidden state u(t:x){\displaystyle u(t:x)} of the system given boundary data and/or measurements z{\displaystyle z}: and fixed model parameters λ{\displaystyle \lambda }. We solve: / ut+N[u]=0:x∈Ω:t∈[0:T]{\displaystyle u_{t}+N[u]=0:\quad x\in \Omega :\quad t\in [0:T]}. / By defining the residual f(t:x){\displaystyle f(t:x)} as / f:=ut+N[u]=0{\displaystyle f:=u_{t}+N[u]=0}: / and approximating u(t:x){\displaystyle u(t:x)} by a deep neural network. This network can be differentiated using automatic differentiation. The parameters of u(t:x){\displaystyle u(t:x)} and f(t:x){\displaystyle f(t:x)} can be then learned by minimizing the following loss function Ltot{\displaystyle L_{tot}}: / Ltot=Lu+Lf{\displaystyle L_{tot}=L_{u}+L_{f}}. / Where Lu=‖u−z‖Γ{\displaystyle L_{u}=\Vert u-z\Vert _{\Gamma }} is the error between the PINN u(t:x){\displaystyle u(t:x)} and the set of boundary conditions and measured data on the set of points Γ{\displaystyle \Gamma } where the boundary conditions and data are defined: and Lf=‖f‖Γ{\displaystyle L_{f}=\Vert f\Vert _{\Gamma }} is the mean-squared error of the residual function. This second term encourages the PINN to learn the structural information expressed by the partial differential equation during the training process. / This approach has been used to yield computationally efficient physics-informed surrogate models with applications in the forecasting of physical processes: model predictive control: multi-physics and multi-scale modeling: and simulation. It has been shown to converge to the solution of the PDE. /  /  / === Data-driven discovery of partial differential equations === / Given noisy and incomplete measurements z{\displaystyle z} of the state of the system: the data-driven discovery of PDE results in computing the unknown state u(t:x){\displaystyle u(t:x)} and learning model parameters λ{\displaystyle \lambda } that best describe the observed data and it reads as follows: / ut+N[u;λ]=0:x∈Ω:t∈[0:T]{\displaystyle u_{t}+N[u;\lambda ]=0:\quad x\in \Omega :\quad t\in [0:T]}. / By defining f(t:x){\displaystyle f(t:x)} as / f:=ut+N[u;λ]=0{\displaystyle f:=u_{t}+N[u;\lambda ]=0}: / and approximating u(t:x){\displaystyle u(t:x)} by a deep neural network: f(t:x){\displaystyle f(t:x)} results in a PINN. This network can be derived using automatic differentiation. The parameters of u(t:x){\displaystyle u(t:x)} and f(t:x){\displaystyle f(t:x)}: together with the parameter λ{\displaystyle \lambda } of the differential operator can be then learned by minimizing the following loss function Ltot{\displaystyle L_{tot}}: / Ltot=Lu+Lf{\displaystyle L_{tot}=L_{u}+L_{f}}. / Where Lu=‖u−z‖Γ{\displaystyle L_{u}=\Vert u-z\Vert _{\Gamma }}: with u{\displaystyle u} and z{\displaystyle z} state solutions and measurements at sparse location Γ{\displaystyle \Gamma }: respectively and Lf=‖f‖Γ{\displaystyle L_{f}=\Vert f\Vert _{\Gamma }} residual function. This second term requires the structured information represented by the partial differential equations to be satisfied in the training process. / This strategy allows for discovering dynamic models described by nonlinear PDEs assembling computationally efficient and fully differentiable surrogate models that may find application in predictive forecasting: control: and data assimilation."
0.1710820999869611,How can PINNs be applied for piece-wise function approximation?,"For problems with strong non-linearity or sharp gradients, lightweight PINNs are used for piece-wise approximation, which increases accuracy substantially and decreases computational load. Distributed physics-informed neural networks (DPINNs) and Distributed physics-informed extreme learning machines (DPIELMs) are used for better approximation, solving PDEs in much larger discrete subdomains.","Score: 0.8346368246309126 / Physics-informed neural networks (PINNs) are a type of universal function approximators that can embed the knowledge of any physical laws that govern a given data-set in the learning process: and can be described by partial differential equations (PDEs). They overcome the low data availability of some biological and engineering systems that makes most state-of-the-art machine learning techniques lack robustness: rendering them ineffective in these scenarios. The prior knowledge of general physical laws acts in the training of neural networks (NNs) as a regularization agent that limits the space of admissible solutions: increasing the correctness of the function approximation. This way: embedding this prior information into a neural network results in enhancing the information content of the available data: facilitating the learning algorithm to capture the right solution and to generalize well even with a low amount of training examples. /  /  / == Function approximation == / Most of the physical laws that govern the dynamics of a system can be described by partial differential equations. For example: the Navier–Stokes equations are a set of partial differential equations derived from the conservation laws (i.e.: conservation of mass: momentum: and energy) that govern fluid mechanics. The solution of the Navier–Stokes equations with appropriate initial and boundary conditions allows the quantification of flow dynamics in a precisely defined geometry. However: these equations cannot be solved exactly and therefore numerical methods must be used (such as finite differences: finite elements and finite volumes). In this setting: these governing equations must be solved while accounting for prior assumptions: linearization: and adequate time and space discretization. / Recently: solving the governing partial differential equations of physical phenomena using deep learning has emerged as a new field of scientific machine learning (SciML): leveraging the universal approximation theorem and high expressivity of neural networks. In general: deep neural networks could approximate any high-dimensional function given that sufficient training data are supplied. However: such networks do not consider the physical characteristics underlying the problem: and the level of approximation accuracy provided by them is still heavily dependent on careful specifications of the problem geometry as well as the initial and boundary conditions. Without this preliminary information: the solution is not unique and may lose physical correctness. On the other hand: physics-informed neural networks (PINNs) leverage governing physical equations in neural network training. Namely: PINNs are designed to be trained to satisfy the given training data as well as the imposed governing equations. In this fashion: a neural network can be guided with training data that do not necessarily need to be large and complete. Potentially: an accurate solution of partial differential equations can be found without knowing the boundary conditions. Therefore: with some knowledge about the physical characteristics of the problem and some form of training data (even sparse and incomplete): PINN may be used for finding an optimal solution with high fidelity. / PINNs allow for addressing a wide range of problems in computational science and represent a pioneering technology leading to the development of new classes of numerical solvers for PDEs. PINNs can be thought of as a meshfree alternative to traditional approaches (e.g.: CFD for fluid dynamics): and new data-driven approaches for model inversion and system identification. Notably: the trained PINN network can be used for predicting the values on simulation grids of different resolutions without the need to be retrained. In addition: they allow for exploiting automatic differentiation (AD) to compute the required derivatives in the partial differential equations: a new class of differentiation techniques widely used to derive neural networks assessed to be superior to numerical or symbolic differentiation. /  /  / == Modeling and computation == / A general nonlinear partial differential equation can be: / ut+N[u;λ]=0:x∈Ω:t∈[0:T]{\displaystyle u_{t}+N[u;\lambda ]=0:\quad x\in \Omega :\quad t\in [0:T]} / where u(t:x){\displaystyle u(t:x)} denotes the solution: N[⋅;λ]{\displaystyle N[\cdot ;\lambda ]} is a nonlinear operator parametrized by λ{\displaystyle \lambda }: and Ω{\displaystyle \Omega } is a subset of RD{\displaystyle \mathbb {R} ^{D}}. This general form of governing equations summarizes a wide range of problems in mathematical physics: such as conservative laws: diffusion process: advection-diffusion systems: and kinetic equations. Given noisy measurements of a generic dynamic system described by the equation above: PINNs can be designed to solve two classes of problems: /  / data-driven solution / data-driven discoveryof partial differential equations.","Score: 0.83023872147957 / This general form of governing equations summarizes a wide range of problems in mathematical physics: such as conservative laws: diffusion process: advection-diffusion systems: and kinetic equations. Given noisy measurements of a generic dynamic system described by the equation above: PINNs can be designed to solve two classes of problems: /  / data-driven solution / data-driven discoveryof partial differential equations. /  /  / === Data-driven solution of partial differential equations === / The data-driven solution of PDE computes the hidden state u(t:x){\displaystyle u(t:x)} of the system given boundary data and/or measurements z{\displaystyle z}: and fixed model parameters λ{\displaystyle \lambda }. We solve: / ut+N[u]=0:x∈Ω:t∈[0:T]{\displaystyle u_{t}+N[u]=0:\quad x\in \Omega :\quad t\in [0:T]}. / By defining the residual f(t:x){\displaystyle f(t:x)} as / f:=ut+N[u]=0{\displaystyle f:=u_{t}+N[u]=0}: / and approximating u(t:x){\displaystyle u(t:x)} by a deep neural network. This network can be differentiated using automatic differentiation. The parameters of u(t:x){\displaystyle u(t:x)} and f(t:x){\displaystyle f(t:x)} can be then learned by minimizing the following loss function Ltot{\displaystyle L_{tot}}: / Ltot=Lu+Lf{\displaystyle L_{tot}=L_{u}+L_{f}}. / Where Lu=‖u−z‖Γ{\displaystyle L_{u}=\Vert u-z\Vert _{\Gamma }} is the error between the PINN u(t:x){\displaystyle u(t:x)} and the set of boundary conditions and measured data on the set of points Γ{\displaystyle \Gamma } where the boundary conditions and data are defined: and Lf=‖f‖Γ{\displaystyle L_{f}=\Vert f\Vert _{\Gamma }} is the mean-squared error of the residual function. This second term encourages the PINN to learn the structural information expressed by the partial differential equation during the training process. / This approach has been used to yield computationally efficient physics-informed surrogate models with applications in the forecasting of physical processes: model predictive control: multi-physics and multi-scale modeling: and simulation. It has been shown to converge to the solution of the PDE. /  /  / === Data-driven discovery of partial differential equations === / Given noisy and incomplete measurements z{\displaystyle z} of the state of the system: the data-driven discovery of PDE results in computing the unknown state u(t:x){\displaystyle u(t:x)} and learning model parameters λ{\displaystyle \lambda } that best describe the observed data and it reads as follows: / ut+N[u;λ]=0:x∈Ω:t∈[0:T]{\displaystyle u_{t}+N[u;\lambda ]=0:\quad x\in \Omega :\quad t\in [0:T]}. / By defining f(t:x){\displaystyle f(t:x)} as / f:=ut+N[u;λ]=0{\displaystyle f:=u_{t}+N[u;\lambda ]=0}: / and approximating u(t:x){\displaystyle u(t:x)} by a deep neural network: f(t:x){\displaystyle f(t:x)} results in a PINN. This network can be derived using automatic differentiation. The parameters of u(t:x){\displaystyle u(t:x)} and f(t:x){\displaystyle f(t:x)}: together with the parameter λ{\displaystyle \lambda } of the differential operator can be then learned by minimizing the following loss function Ltot{\displaystyle L_{tot}}: / Ltot=Lu+Lf{\displaystyle L_{tot}=L_{u}+L_{f}}. / Where Lu=‖u−z‖Γ{\displaystyle L_{u}=\Vert u-z\Vert _{\Gamma }}: with u{\displaystyle u} and z{\displaystyle z} state solutions and measurements at sparse location Γ{\displaystyle \Gamma }: respectively and Lf=‖f‖Γ{\displaystyle L_{f}=\Vert f\Vert _{\Gamma }} residual function. This second term requires the structured information represented by the partial differential equations to be satisfied in the training process. / This strategy allows for discovering dynamic models described by nonlinear PDEs assembling computationally efficient and fully differentiable surrogate models that may find application in predictive forecasting: control: and data assimilation."
0.1728735999786295,What limitations do PINNs have?,"PINNs struggle to approximate translation and discontinuous behavior. They fail when solving differential equations with slight advective dominance and are not successful in solving chaotic equations. One of the reasons for this is the soft-constraining of Dirichlet and Neumann boundary conditions which pose multi-objective optimization problems. This necessitates the need for manually weighing the loss terms for optimization. Also, there is the risk of getting stuck at a local optimum often.","Score: 0.8100602996479642 / Physics-informed neural networks (PINNs) are a type of universal function approximators that can embed the knowledge of any physical laws that govern a given data-set in the learning process: and can be described by partial differential equations (PDEs). They overcome the low data availability of some biological and engineering systems that makes most state-of-the-art machine learning techniques lack robustness: rendering them ineffective in these scenarios. The prior knowledge of general physical laws acts in the training of neural networks (NNs) as a regularization agent that limits the space of admissible solutions: increasing the correctness of the function approximation. This way: embedding this prior information into a neural network results in enhancing the information content of the available data: facilitating the learning algorithm to capture the right solution and to generalize well even with a low amount of training examples. /  /  / == Function approximation == / Most of the physical laws that govern the dynamics of a system can be described by partial differential equations. For example: the Navier–Stokes equations are a set of partial differential equations derived from the conservation laws (i.e.: conservation of mass: momentum: and energy) that govern fluid mechanics. The solution of the Navier–Stokes equations with appropriate initial and boundary conditions allows the quantification of flow dynamics in a precisely defined geometry. However: these equations cannot be solved exactly and therefore numerical methods must be used (such as finite differences: finite elements and finite volumes). In this setting: these governing equations must be solved while accounting for prior assumptions: linearization: and adequate time and space discretization. / Recently: solving the governing partial differential equations of physical phenomena using deep learning has emerged as a new field of scientific machine learning (SciML): leveraging the universal approximation theorem and high expressivity of neural networks. In general: deep neural networks could approximate any high-dimensional function given that sufficient training data are supplied. However: such networks do not consider the physical characteristics underlying the problem: and the level of approximation accuracy provided by them is still heavily dependent on careful specifications of the problem geometry as well as the initial and boundary conditions. Without this preliminary information: the solution is not unique and may lose physical correctness. On the other hand: physics-informed neural networks (PINNs) leverage governing physical equations in neural network training. Namely: PINNs are designed to be trained to satisfy the given training data as well as the imposed governing equations. In this fashion: a neural network can be guided with training data that do not necessarily need to be large and complete. Potentially: an accurate solution of partial differential equations can be found without knowing the boundary conditions. Therefore: with some knowledge about the physical characteristics of the problem and some form of training data (even sparse and incomplete): PINN may be used for finding an optimal solution with high fidelity. / PINNs allow for addressing a wide range of problems in computational science and represent a pioneering technology leading to the development of new classes of numerical solvers for PDEs. PINNs can be thought of as a meshfree alternative to traditional approaches (e.g.: CFD for fluid dynamics): and new data-driven approaches for model inversion and system identification. Notably: the trained PINN network can be used for predicting the values on simulation grids of different resolutions without the need to be retrained. In addition: they allow for exploiting automatic differentiation (AD) to compute the required derivatives in the partial differential equations: a new class of differentiation techniques widely used to derive neural networks assessed to be superior to numerical or symbolic differentiation. /  /  / == Modeling and computation == / A general nonlinear partial differential equation can be: / ut+N[u;λ]=0:x∈Ω:t∈[0:T]{\displaystyle u_{t}+N[u;\lambda ]=0:\quad x\in \Omega :\quad t\in [0:T]} / where u(t:x){\displaystyle u(t:x)} denotes the solution: N[⋅;λ]{\displaystyle N[\cdot ;\lambda ]} is a nonlinear operator parametrized by λ{\displaystyle \lambda }: and Ω{\displaystyle \Omega } is a subset of RD{\displaystyle \mathbb {R} ^{D}}. This general form of governing equations summarizes a wide range of problems in mathematical physics: such as conservative laws: diffusion process: advection-diffusion systems: and kinetic equations. Given noisy measurements of a generic dynamic system described by the equation above: PINNs can be designed to solve two classes of problems: /  / data-driven solution / data-driven discoveryof partial differential equations.","Score: 0.7897865750253031 / This general form of governing equations summarizes a wide range of problems in mathematical physics: such as conservative laws: diffusion process: advection-diffusion systems: and kinetic equations. Given noisy measurements of a generic dynamic system described by the equation above: PINNs can be designed to solve two classes of problems: /  / data-driven solution / data-driven discoveryof partial differential equations. /  /  / === Data-driven solution of partial differential equations === / The data-driven solution of PDE computes the hidden state u(t:x){\displaystyle u(t:x)} of the system given boundary data and/or measurements z{\displaystyle z}: and fixed model parameters λ{\displaystyle \lambda }. We solve: / ut+N[u]=0:x∈Ω:t∈[0:T]{\displaystyle u_{t}+N[u]=0:\quad x\in \Omega :\quad t\in [0:T]}. / By defining the residual f(t:x){\displaystyle f(t:x)} as / f:=ut+N[u]=0{\displaystyle f:=u_{t}+N[u]=0}: / and approximating u(t:x){\displaystyle u(t:x)} by a deep neural network. This network can be differentiated using automatic differentiation. The parameters of u(t:x){\displaystyle u(t:x)} and f(t:x){\displaystyle f(t:x)} can be then learned by minimizing the following loss function Ltot{\displaystyle L_{tot}}: / Ltot=Lu+Lf{\displaystyle L_{tot}=L_{u}+L_{f}}. / Where Lu=‖u−z‖Γ{\displaystyle L_{u}=\Vert u-z\Vert _{\Gamma }} is the error between the PINN u(t:x){\displaystyle u(t:x)} and the set of boundary conditions and measured data on the set of points Γ{\displaystyle \Gamma } where the boundary conditions and data are defined: and Lf=‖f‖Γ{\displaystyle L_{f}=\Vert f\Vert _{\Gamma }} is the mean-squared error of the residual function. This second term encourages the PINN to learn the structural information expressed by the partial differential equation during the training process. / This approach has been used to yield computationally efficient physics-informed surrogate models with applications in the forecasting of physical processes: model predictive control: multi-physics and multi-scale modeling: and simulation. It has been shown to converge to the solution of the PDE. /  /  / === Data-driven discovery of partial differential equations === / Given noisy and incomplete measurements z{\displaystyle z} of the state of the system: the data-driven discovery of PDE results in computing the unknown state u(t:x){\displaystyle u(t:x)} and learning model parameters λ{\displaystyle \lambda } that best describe the observed data and it reads as follows: / ut+N[u;λ]=0:x∈Ω:t∈[0:T]{\displaystyle u_{t}+N[u;\lambda ]=0:\quad x\in \Omega :\quad t\in [0:T]}. / By defining f(t:x){\displaystyle f(t:x)} as / f:=ut+N[u;λ]=0{\displaystyle f:=u_{t}+N[u;\lambda ]=0}: / and approximating u(t:x){\displaystyle u(t:x)} by a deep neural network: f(t:x){\displaystyle f(t:x)} results in a PINN. This network can be derived using automatic differentiation. The parameters of u(t:x){\displaystyle u(t:x)} and f(t:x){\displaystyle f(t:x)}: together with the parameter λ{\displaystyle \lambda } of the differential operator can be then learned by minimizing the following loss function Ltot{\displaystyle L_{tot}}: / Ltot=Lu+Lf{\displaystyle L_{tot}=L_{u}+L_{f}}. / Where Lu=‖u−z‖Γ{\displaystyle L_{u}=\Vert u-z\Vert _{\Gamma }}: with u{\displaystyle u} and z{\displaystyle z} state solutions and measurements at sparse location Γ{\displaystyle \Gamma }: respectively and Lf=‖f‖Γ{\displaystyle L_{f}=\Vert f\Vert _{\Gamma }} residual function. This second term requires the structured information represented by the partial differential equations to be satisfied in the training process. / This strategy allows for discovering dynamic models described by nonlinear PDEs assembling computationally efficient and fully differentiable surrogate models that may find application in predictive forecasting: control: and data assimilation."
0.13812330001383089,Who were the first to publish ideas on quantum neural computation?,The first ideas on quantum neural computation were published independently in 1995 by Subhash Kak and Ron Chrisley.,"Score: 0.8376791470592589 / Quantum neural networks are computational neural network models which are based on the principles of quantum mechanics. The first ideas on quantum neural computation were published independently in 1995 by Subhash Kak and Ron Chrisley: engaging with the theory of quantum mind: which posits that quantum effects play a role in cognitive function. However: typical research in quantum neural networks involves combining classical artificial neural network models (which are widely used in machine learning for the important task of pattern recognition) with the advantages of quantum information in order to develop more efficient algorithms. One important motivation for these investigations is the difficulty to train classical neural networks: especially in big data applications. The hope is that features of quantum computing such as quantum parallelism or the effects of interference and entanglement can be used as resources. Since the technological implementation of a quantum computer is still in a premature stage: such quantum neural network models are mostly theoretical proposals that await their full implementation in physical experiments. / Most Quantum neural networks are developed as feed-forward networks. Similar to their classical counterparts: this structure intakes input from one layer of qubits: and passes that input onto another layer of qubits. This layer of qubits evaluates this information and passes on the output to the next layer. Eventually the path leads to the final layer of qubits. The layers do not have to be of the same width: meaning they don't have to have the same number of qubits as the layer before or after it. This structure is trained on which path to take similar to classical artificial neural networks. This is discussed in a lower section. Quantum neural networks refer to three different categories: Quantum computer with classical data: classical computer with quantum data: and quantum computer with quantum data. /  /  / == Examples == / Quantum neural network research is still in its infancy: and a conglomeration of proposals and ideas of varying scope and mathematical rigor have been put forward. Most of them are based on the idea of replacing classical binary or McCulloch-Pitts neurons with a qubit (which can be called a “quron”): resulting in neural units that can be in a superposition of the state ‘firing’ and ‘resting’. /  /  / === Quantum perceptrons === / A lot of proposals attempt to find a quantum equivalent for the perceptron unit from which neural nets are constructed. A problem is that nonlinear activation functions do not immediately correspond to the mathematical structure of quantum theory: since a quantum evolution is described by linear operations and leads to probabilistic observation. Ideas to imitate the perceptron activation function with a quantum mechanical formalism reach from special measurements  to postulating non-linear quantum operators (a mathematical framework that is disputed). A direct implementation of the activation function using the circuit-based model of quantum computation has recently been proposed by Schuld: Sinayskiy and Petruccione based on the quantum phase estimation algorithm. /  /  / === Quantum networks === / At a larger scale: researchers have attempted to generalize neural networks to the quantum setting. One way of constructing a quantum neuron is to first generalise classical neurons and then generalising them further to make unitary gates. Interactions between neurons can be controlled quantumly: with unitary gates: or classically: via measurement of the network states. This high-level theoretical technique can be applied broadly: by taking different types of networks and different implementations of quantum neurons: such as photonically implemented neurons and quantum reservoir processor (quantum version of reservoir computing). Most learning algorithms follow the classical model of training an artificial neural network to learn the input-output function of a given training set and use classical feedback loops to update parameters of the quantum system until they converge to an optimal configuration. Learning as a parameter optimisation problem has also been approached by adiabatic models of quantum computing.Quantum neural networks can be applied to algorithmic design: given qubits with tunable mutual interactions: one can attempt to learn interactions following the classical backpropagation rule from a training set of desired input-output relations: taken to be the desired output algorithm's behavior. The quantum network thus ‘learns’ an algorithm. /  /  / === Quantum associative memory === / The first quantum associative memory algorithm was introduced by Dan Ventura and Tony Martinez in 1999. The authors do not attempt to translate the structure of artificial neural network models into quantum theory: but propose an algorithm for a circuit-based quantum computer that simulates associative memory. The memory states (in Hopfield neural networks saved in the weights of the neural connections) are written into a superposition: and a Grover-like quantum search algorithm retrieves the memory state closest to a given input. As such: this is not a fully content-addressable memory: since only incomplete patterns can be retrieved.","Score: 0.830745868325267 / They used computational machines: then called ""calculators"". Other neural network computational machines were created by Rochester: Holland: Habit: and Duda in 1956. In 1958: psychologist Frank Rosenblatt invented the perceptron: the first implemented artificial neural network: funded by the United States Office of Naval Research. / The invention of the perceptron raised public excitement for research in Artificial Neural Networks: causing the US government to drastically increase funding into deep learning research. This led to ""the golden age of AI"" fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence. For example: in 1957 Herbert Simon famously said:It is not my aim to surprise or shock you—but the simplest way I can summarize is to say that there are now in the world machines that think: that learn and that create. Moreover: their ability to do these things is going to increase rapidly until—in a visible future—the range of problems they can handle will be coextensive with the range to which the human mind has been applied.However: this wasn't the case as research stagnated in the United States following the work of Minsky and Papert (1969): who discovered that basic perceptrons were incapable of processing the exclusive-or circuit and that computers lacked sufficient power to train useful neural networks. This: along with other factors such as the 1973 Lighthill report by James Lighthill stating that research in Artificial Intelligence has not ""produced the major impact that was then promised:"" shutting funding in research into the field of AI in all but two universities in the UK and in many major institutions across the world. This ushered an era called the AI Winter with reduced research into connectionism due to a decrease in government funding and an increased stress on symbolic artificial intelligence in the United States and other Western countries.During the AI Winter era: however: research outside the United States continued: especially in Eastern Europe. By the time Minsky and Papert's book on Perceptrons came out: methods for training multilayer perceptrons (MLPs) were already known. The first deep learning MLP was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa in 1965: as the Group Method of Data Handling. The first deep learning MLP trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. In computer experiments conducted by Amari's student Saito: a five layer MLP with two modifiable layers learned useful internal representations to classify non-linearily separable pattern classes.Self-organizing maps (SOMs) were described by Teuvo Kohonen in 1982. SOMs are neurophysiologically inspired neural networks that learn low-dimensional representations of high-dimensional data while preserving the topological structure of the data. They are trained using competitive learning.The convolutional neural network (CNN) architecture with convolutional layers and downsampling layers was introduced by Kunihiko Fukushima in 1980. He called it the neocognitron. In 1969: he also introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for CNNs and  deep neural networks in general. CNNs have become an essential tool for computer vision. / A key in later advances in artificial neural network research was the backpropagation algorithm: an efficient application of the Leibniz chain rule (1673) to networks of differentiable nodes. It is also known as  / the reverse mode of automatic differentiation or reverse accumulation: due to Seppo Linnainmaa (1970). The term ""back-propagating errors"" was introduced in 1962 by Frank Rosenblatt: but he did not have an implementation of this procedure: although Henry J. Kelley and Bryson had dynamic programming based continuous precursors of backpropagation already in 1960–61 in the context of control theory.  / In 1973: Dreyfus used backpropagation to adapt parameters of controllers in proportion to error gradients.  / In 1982: Paul Werbos applied backpropagation to MLPs in the way that has become standard. In 1986 Rumelhart: Hinton and Williams showed that backpropagation learned interesting internal representations of words as feature vectors when trained to predict the next word in a sequence.In the late 1970s to early 1980s: interest briefly emerged in theoretically investigating the Ising model created by Wilhelm Lenz (1920) and Ernst Ising (1925) / in relation to Cayley tree topologies and large neural networks."
0.17702010000357404,What is one of the main motivations for investigating quantum neural networks?,"One important motivation for investigating quantum neural networks is the challenge of training classical neural networks, particularly in big data applications.","Score: 0.8611033437296687 / Quantum neural networks are computational neural network models which are based on the principles of quantum mechanics. The first ideas on quantum neural computation were published independently in 1995 by Subhash Kak and Ron Chrisley: engaging with the theory of quantum mind: which posits that quantum effects play a role in cognitive function. However: typical research in quantum neural networks involves combining classical artificial neural network models (which are widely used in machine learning for the important task of pattern recognition) with the advantages of quantum information in order to develop more efficient algorithms. One important motivation for these investigations is the difficulty to train classical neural networks: especially in big data applications. The hope is that features of quantum computing such as quantum parallelism or the effects of interference and entanglement can be used as resources. Since the technological implementation of a quantum computer is still in a premature stage: such quantum neural network models are mostly theoretical proposals that await their full implementation in physical experiments. / Most Quantum neural networks are developed as feed-forward networks. Similar to their classical counterparts: this structure intakes input from one layer of qubits: and passes that input onto another layer of qubits. This layer of qubits evaluates this information and passes on the output to the next layer. Eventually the path leads to the final layer of qubits. The layers do not have to be of the same width: meaning they don't have to have the same number of qubits as the layer before or after it. This structure is trained on which path to take similar to classical artificial neural networks. This is discussed in a lower section. Quantum neural networks refer to three different categories: Quantum computer with classical data: classical computer with quantum data: and quantum computer with quantum data. /  /  / == Examples == / Quantum neural network research is still in its infancy: and a conglomeration of proposals and ideas of varying scope and mathematical rigor have been put forward. Most of them are based on the idea of replacing classical binary or McCulloch-Pitts neurons with a qubit (which can be called a “quron”): resulting in neural units that can be in a superposition of the state ‘firing’ and ‘resting’. /  /  / === Quantum perceptrons === / A lot of proposals attempt to find a quantum equivalent for the perceptron unit from which neural nets are constructed. A problem is that nonlinear activation functions do not immediately correspond to the mathematical structure of quantum theory: since a quantum evolution is described by linear operations and leads to probabilistic observation. Ideas to imitate the perceptron activation function with a quantum mechanical formalism reach from special measurements  to postulating non-linear quantum operators (a mathematical framework that is disputed). A direct implementation of the activation function using the circuit-based model of quantum computation has recently been proposed by Schuld: Sinayskiy and Petruccione based on the quantum phase estimation algorithm. /  /  / === Quantum networks === / At a larger scale: researchers have attempted to generalize neural networks to the quantum setting. One way of constructing a quantum neuron is to first generalise classical neurons and then generalising them further to make unitary gates. Interactions between neurons can be controlled quantumly: with unitary gates: or classically: via measurement of the network states. This high-level theoretical technique can be applied broadly: by taking different types of networks and different implementations of quantum neurons: such as photonically implemented neurons and quantum reservoir processor (quantum version of reservoir computing). Most learning algorithms follow the classical model of training an artificial neural network to learn the input-output function of a given training set and use classical feedback loops to update parameters of the quantum system until they converge to an optimal configuration. Learning as a parameter optimisation problem has also been approached by adiabatic models of quantum computing.Quantum neural networks can be applied to algorithmic design: given qubits with tunable mutual interactions: one can attempt to learn interactions following the classical backpropagation rule from a training set of desired input-output relations: taken to be the desired output algorithm's behavior. The quantum network thus ‘learns’ an algorithm. /  /  / === Quantum associative memory === / The first quantum associative memory algorithm was introduced by Dan Ventura and Tony Martinez in 1999. The authors do not attempt to translate the structure of artificial neural network models into quantum theory: but propose an algorithm for a circuit-based quantum computer that simulates associative memory. The memory states (in Hopfield neural networks saved in the weights of the neural connections) are written into a superposition: and a Grover-like quantum search algorithm retrieves the memory state closest to a given input. As such: this is not a fully content-addressable memory: since only incomplete patterns can be retrieved.","Score: 0.8556041591358385 / The quantum network thus ‘learns’ an algorithm. /  /  / === Quantum associative memory === / The first quantum associative memory algorithm was introduced by Dan Ventura and Tony Martinez in 1999. The authors do not attempt to translate the structure of artificial neural network models into quantum theory: but propose an algorithm for a circuit-based quantum computer that simulates associative memory. The memory states (in Hopfield neural networks saved in the weights of the neural connections) are written into a superposition: and a Grover-like quantum search algorithm retrieves the memory state closest to a given input. As such: this is not a fully content-addressable memory: since only incomplete patterns can be retrieved. / The first truly content-addressable quantum memory: which can retrieve patterns also from corrupted inputs: was proposed by Carlo A. Trugenberger. Both memories can store an exponential (in terms of n qubits) number of patterns but can be used only once due to the no-cloning theorem and their destruction upon measurement. / Trugenberger: however: has shown that his proababilistic model of quantum associative memory can be efficiently implemented and re-used multiples times for any polynomial number of stored patterns: a large advantage with respect to classical associative memories. /  /  / === Classical neural networks inspired by quantum theory === / A substantial amount of interest has been given to a “quantum-inspired” model that uses ideas from quantum theory to implement a neural network based on fuzzy logic. /  /  / == Training == / Quantum Neural Networks can be theoretically trained similarly to training classical/artificial neural networks. A key difference lies in communication between the layers of a neural networks. For classical neural networks: at the end of a given operation: the current perceptron copies its output to the next layer of perceptron(s) in the network. However: in a quantum neural network: where each perceptron is a qubit: this would violate the no-cloning theorem. A proposed generalized solution to this is to replace the classical fan-out method with an arbitrary unitary that spreads out: but does not copy: the output of one qubit to the next layer of qubits. Using this fan-out Unitary (Uf{\displaystyle U_{f}}) with a dummy state qubit in a known state (Ex. |0⟩{\displaystyle |0\rangle } in the computational basis): also known as an Ancilla bit: the information from the qubit can be transferred to the next layer of qubits. This process adheres to the quantum operation requirement of reversibility.Using this quantum feed-forward network: deep neural networks can be executed and trained efficiently. A deep neural network is essentially a network with many hidden-layers: as seen in the sample model neural network above. Since the Quantum neural network being discussed uses fan-out Unitary operators: and each operator only acts on its respective input: only two layers are used at any given time. In other words: no Unitary operator is acting on the entire network at any given time: meaning the number of qubits required for a given step depends on the number of inputs in a given layer. Since Quantum Computers are notorious for their ability to run multiple iterations in a short period of time: the efficiency of a quantum neural network is solely dependent on the number of qubits in any given layer: and not on the depth of the network. /  /  / === Cost functions === / To determine the effectiveness of a neural network: a cost function is used: which essentially measures the proximity of the network's output to the expected or desired output. In a Classical Neural Network: the weights (w{\displaystyle w}) and biases (b{\displaystyle b}) at each step determine the outcome of the cost function C(w:b){\displaystyle C(w:b)}. When training a Classical Neural network: the weights and biases are adjusted after each iteration: and given equation 1 below: where y(x){\displaystyle y(x)} is the desired output and aout(x){\displaystyle a^{\text{out}}(x)} is the actual output: the cost function is optimized when C(w:b){\displaystyle C(w:b)}= 0. For a quantum neural network: the cost function is determined by measuring the fidelity of the outcome state (ρout{\displaystyle \rho ^{\text{out}}}) with the desired outcome state (ϕout{\displaystyle \phi ^{\text{out}}}): seen in Equation 2 below. In this case: the Unitary operators are adjusted after each iteration: and the cost function is optimized when C = 1."
0.13264510000590235,How does the structure of a quantum neural network compare to that of a classical artificial neural network? ,"Most Quantum neural networks are developed as feed-forward networks similar to their classical counterparts. The structure intakes input from one layer of qubits and passes that input onto another layer after evaluation. However, the layers in quantum neural networks do not need to have the same number of qubits as the layer before or after it.","Score: 0.8948361898886151 / The quantum network thus ‘learns’ an algorithm. /  /  / === Quantum associative memory === / The first quantum associative memory algorithm was introduced by Dan Ventura and Tony Martinez in 1999. The authors do not attempt to translate the structure of artificial neural network models into quantum theory: but propose an algorithm for a circuit-based quantum computer that simulates associative memory. The memory states (in Hopfield neural networks saved in the weights of the neural connections) are written into a superposition: and a Grover-like quantum search algorithm retrieves the memory state closest to a given input. As such: this is not a fully content-addressable memory: since only incomplete patterns can be retrieved. / The first truly content-addressable quantum memory: which can retrieve patterns also from corrupted inputs: was proposed by Carlo A. Trugenberger. Both memories can store an exponential (in terms of n qubits) number of patterns but can be used only once due to the no-cloning theorem and their destruction upon measurement. / Trugenberger: however: has shown that his proababilistic model of quantum associative memory can be efficiently implemented and re-used multiples times for any polynomial number of stored patterns: a large advantage with respect to classical associative memories. /  /  / === Classical neural networks inspired by quantum theory === / A substantial amount of interest has been given to a “quantum-inspired” model that uses ideas from quantum theory to implement a neural network based on fuzzy logic. /  /  / == Training == / Quantum Neural Networks can be theoretically trained similarly to training classical/artificial neural networks. A key difference lies in communication between the layers of a neural networks. For classical neural networks: at the end of a given operation: the current perceptron copies its output to the next layer of perceptron(s) in the network. However: in a quantum neural network: where each perceptron is a qubit: this would violate the no-cloning theorem. A proposed generalized solution to this is to replace the classical fan-out method with an arbitrary unitary that spreads out: but does not copy: the output of one qubit to the next layer of qubits. Using this fan-out Unitary (Uf{\displaystyle U_{f}}) with a dummy state qubit in a known state (Ex. |0⟩{\displaystyle |0\rangle } in the computational basis): also known as an Ancilla bit: the information from the qubit can be transferred to the next layer of qubits. This process adheres to the quantum operation requirement of reversibility.Using this quantum feed-forward network: deep neural networks can be executed and trained efficiently. A deep neural network is essentially a network with many hidden-layers: as seen in the sample model neural network above. Since the Quantum neural network being discussed uses fan-out Unitary operators: and each operator only acts on its respective input: only two layers are used at any given time. In other words: no Unitary operator is acting on the entire network at any given time: meaning the number of qubits required for a given step depends on the number of inputs in a given layer. Since Quantum Computers are notorious for their ability to run multiple iterations in a short period of time: the efficiency of a quantum neural network is solely dependent on the number of qubits in any given layer: and not on the depth of the network. /  /  / === Cost functions === / To determine the effectiveness of a neural network: a cost function is used: which essentially measures the proximity of the network's output to the expected or desired output. In a Classical Neural Network: the weights (w{\displaystyle w}) and biases (b{\displaystyle b}) at each step determine the outcome of the cost function C(w:b){\displaystyle C(w:b)}. When training a Classical Neural network: the weights and biases are adjusted after each iteration: and given equation 1 below: where y(x){\displaystyle y(x)} is the desired output and aout(x){\displaystyle a^{\text{out}}(x)} is the actual output: the cost function is optimized when C(w:b){\displaystyle C(w:b)}= 0. For a quantum neural network: the cost function is determined by measuring the fidelity of the outcome state (ρout{\displaystyle \rho ^{\text{out}}}) with the desired outcome state (ϕout{\displaystyle \phi ^{\text{out}}}): seen in Equation 2 below. In this case: the Unitary operators are adjusted after each iteration: and the cost function is optimized when C = 1.","Score: 0.8902897687286455 / Quantum neural networks are computational neural network models which are based on the principles of quantum mechanics. The first ideas on quantum neural computation were published independently in 1995 by Subhash Kak and Ron Chrisley: engaging with the theory of quantum mind: which posits that quantum effects play a role in cognitive function. However: typical research in quantum neural networks involves combining classical artificial neural network models (which are widely used in machine learning for the important task of pattern recognition) with the advantages of quantum information in order to develop more efficient algorithms. One important motivation for these investigations is the difficulty to train classical neural networks: especially in big data applications. The hope is that features of quantum computing such as quantum parallelism or the effects of interference and entanglement can be used as resources. Since the technological implementation of a quantum computer is still in a premature stage: such quantum neural network models are mostly theoretical proposals that await their full implementation in physical experiments. / Most Quantum neural networks are developed as feed-forward networks. Similar to their classical counterparts: this structure intakes input from one layer of qubits: and passes that input onto another layer of qubits. This layer of qubits evaluates this information and passes on the output to the next layer. Eventually the path leads to the final layer of qubits. The layers do not have to be of the same width: meaning they don't have to have the same number of qubits as the layer before or after it. This structure is trained on which path to take similar to classical artificial neural networks. This is discussed in a lower section. Quantum neural networks refer to three different categories: Quantum computer with classical data: classical computer with quantum data: and quantum computer with quantum data. /  /  / == Examples == / Quantum neural network research is still in its infancy: and a conglomeration of proposals and ideas of varying scope and mathematical rigor have been put forward. Most of them are based on the idea of replacing classical binary or McCulloch-Pitts neurons with a qubit (which can be called a “quron”): resulting in neural units that can be in a superposition of the state ‘firing’ and ‘resting’. /  /  / === Quantum perceptrons === / A lot of proposals attempt to find a quantum equivalent for the perceptron unit from which neural nets are constructed. A problem is that nonlinear activation functions do not immediately correspond to the mathematical structure of quantum theory: since a quantum evolution is described by linear operations and leads to probabilistic observation. Ideas to imitate the perceptron activation function with a quantum mechanical formalism reach from special measurements  to postulating non-linear quantum operators (a mathematical framework that is disputed). A direct implementation of the activation function using the circuit-based model of quantum computation has recently been proposed by Schuld: Sinayskiy and Petruccione based on the quantum phase estimation algorithm. /  /  / === Quantum networks === / At a larger scale: researchers have attempted to generalize neural networks to the quantum setting. One way of constructing a quantum neuron is to first generalise classical neurons and then generalising them further to make unitary gates. Interactions between neurons can be controlled quantumly: with unitary gates: or classically: via measurement of the network states. This high-level theoretical technique can be applied broadly: by taking different types of networks and different implementations of quantum neurons: such as photonically implemented neurons and quantum reservoir processor (quantum version of reservoir computing). Most learning algorithms follow the classical model of training an artificial neural network to learn the input-output function of a given training set and use classical feedback loops to update parameters of the quantum system until they converge to an optimal configuration. Learning as a parameter optimisation problem has also been approached by adiabatic models of quantum computing.Quantum neural networks can be applied to algorithmic design: given qubits with tunable mutual interactions: one can attempt to learn interactions following the classical backpropagation rule from a training set of desired input-output relations: taken to be the desired output algorithm's behavior. The quantum network thus ‘learns’ an algorithm. /  /  / === Quantum associative memory === / The first quantum associative memory algorithm was introduced by Dan Ventura and Tony Martinez in 1999. The authors do not attempt to translate the structure of artificial neural network models into quantum theory: but propose an algorithm for a circuit-based quantum computer that simulates associative memory. The memory states (in Hopfield neural networks saved in the weights of the neural connections) are written into a superposition: and a Grover-like quantum search algorithm retrieves the memory state closest to a given input. As such: this is not a fully content-addressable memory: since only incomplete patterns can be retrieved."
0.17880819999845698,What categories does the term 'quantum neural networks' refer to?,"The term 'quantum neural networks' refers to three different categories: Quantum computer with classical data, classical computer with quantum data, and quantum computer with quantum data.","Score: 0.8793921923671245 / Quantum neural networks are computational neural network models which are based on the principles of quantum mechanics. The first ideas on quantum neural computation were published independently in 1995 by Subhash Kak and Ron Chrisley: engaging with the theory of quantum mind: which posits that quantum effects play a role in cognitive function. However: typical research in quantum neural networks involves combining classical artificial neural network models (which are widely used in machine learning for the important task of pattern recognition) with the advantages of quantum information in order to develop more efficient algorithms. One important motivation for these investigations is the difficulty to train classical neural networks: especially in big data applications. The hope is that features of quantum computing such as quantum parallelism or the effects of interference and entanglement can be used as resources. Since the technological implementation of a quantum computer is still in a premature stage: such quantum neural network models are mostly theoretical proposals that await their full implementation in physical experiments. / Most Quantum neural networks are developed as feed-forward networks. Similar to their classical counterparts: this structure intakes input from one layer of qubits: and passes that input onto another layer of qubits. This layer of qubits evaluates this information and passes on the output to the next layer. Eventually the path leads to the final layer of qubits. The layers do not have to be of the same width: meaning they don't have to have the same number of qubits as the layer before or after it. This structure is trained on which path to take similar to classical artificial neural networks. This is discussed in a lower section. Quantum neural networks refer to three different categories: Quantum computer with classical data: classical computer with quantum data: and quantum computer with quantum data. /  /  / == Examples == / Quantum neural network research is still in its infancy: and a conglomeration of proposals and ideas of varying scope and mathematical rigor have been put forward. Most of them are based on the idea of replacing classical binary or McCulloch-Pitts neurons with a qubit (which can be called a “quron”): resulting in neural units that can be in a superposition of the state ‘firing’ and ‘resting’. /  /  / === Quantum perceptrons === / A lot of proposals attempt to find a quantum equivalent for the perceptron unit from which neural nets are constructed. A problem is that nonlinear activation functions do not immediately correspond to the mathematical structure of quantum theory: since a quantum evolution is described by linear operations and leads to probabilistic observation. Ideas to imitate the perceptron activation function with a quantum mechanical formalism reach from special measurements  to postulating non-linear quantum operators (a mathematical framework that is disputed). A direct implementation of the activation function using the circuit-based model of quantum computation has recently been proposed by Schuld: Sinayskiy and Petruccione based on the quantum phase estimation algorithm. /  /  / === Quantum networks === / At a larger scale: researchers have attempted to generalize neural networks to the quantum setting. One way of constructing a quantum neuron is to first generalise classical neurons and then generalising them further to make unitary gates. Interactions between neurons can be controlled quantumly: with unitary gates: or classically: via measurement of the network states. This high-level theoretical technique can be applied broadly: by taking different types of networks and different implementations of quantum neurons: such as photonically implemented neurons and quantum reservoir processor (quantum version of reservoir computing). Most learning algorithms follow the classical model of training an artificial neural network to learn the input-output function of a given training set and use classical feedback loops to update parameters of the quantum system until they converge to an optimal configuration. Learning as a parameter optimisation problem has also been approached by adiabatic models of quantum computing.Quantum neural networks can be applied to algorithmic design: given qubits with tunable mutual interactions: one can attempt to learn interactions following the classical backpropagation rule from a training set of desired input-output relations: taken to be the desired output algorithm's behavior. The quantum network thus ‘learns’ an algorithm. /  /  / === Quantum associative memory === / The first quantum associative memory algorithm was introduced by Dan Ventura and Tony Martinez in 1999. The authors do not attempt to translate the structure of artificial neural network models into quantum theory: but propose an algorithm for a circuit-based quantum computer that simulates associative memory. The memory states (in Hopfield neural networks saved in the weights of the neural connections) are written into a superposition: and a Grover-like quantum search algorithm retrieves the memory state closest to a given input. As such: this is not a fully content-addressable memory: since only incomplete patterns can be retrieved.","Score: 0.8642544275430507 / The quantum network thus ‘learns’ an algorithm. /  /  / === Quantum associative memory === / The first quantum associative memory algorithm was introduced by Dan Ventura and Tony Martinez in 1999. The authors do not attempt to translate the structure of artificial neural network models into quantum theory: but propose an algorithm for a circuit-based quantum computer that simulates associative memory. The memory states (in Hopfield neural networks saved in the weights of the neural connections) are written into a superposition: and a Grover-like quantum search algorithm retrieves the memory state closest to a given input. As such: this is not a fully content-addressable memory: since only incomplete patterns can be retrieved. / The first truly content-addressable quantum memory: which can retrieve patterns also from corrupted inputs: was proposed by Carlo A. Trugenberger. Both memories can store an exponential (in terms of n qubits) number of patterns but can be used only once due to the no-cloning theorem and their destruction upon measurement. / Trugenberger: however: has shown that his proababilistic model of quantum associative memory can be efficiently implemented and re-used multiples times for any polynomial number of stored patterns: a large advantage with respect to classical associative memories. /  /  / === Classical neural networks inspired by quantum theory === / A substantial amount of interest has been given to a “quantum-inspired” model that uses ideas from quantum theory to implement a neural network based on fuzzy logic. /  /  / == Training == / Quantum Neural Networks can be theoretically trained similarly to training classical/artificial neural networks. A key difference lies in communication between the layers of a neural networks. For classical neural networks: at the end of a given operation: the current perceptron copies its output to the next layer of perceptron(s) in the network. However: in a quantum neural network: where each perceptron is a qubit: this would violate the no-cloning theorem. A proposed generalized solution to this is to replace the classical fan-out method with an arbitrary unitary that spreads out: but does not copy: the output of one qubit to the next layer of qubits. Using this fan-out Unitary (Uf{\displaystyle U_{f}}) with a dummy state qubit in a known state (Ex. |0⟩{\displaystyle |0\rangle } in the computational basis): also known as an Ancilla bit: the information from the qubit can be transferred to the next layer of qubits. This process adheres to the quantum operation requirement of reversibility.Using this quantum feed-forward network: deep neural networks can be executed and trained efficiently. A deep neural network is essentially a network with many hidden-layers: as seen in the sample model neural network above. Since the Quantum neural network being discussed uses fan-out Unitary operators: and each operator only acts on its respective input: only two layers are used at any given time. In other words: no Unitary operator is acting on the entire network at any given time: meaning the number of qubits required for a given step depends on the number of inputs in a given layer. Since Quantum Computers are notorious for their ability to run multiple iterations in a short period of time: the efficiency of a quantum neural network is solely dependent on the number of qubits in any given layer: and not on the depth of the network. /  /  / === Cost functions === / To determine the effectiveness of a neural network: a cost function is used: which essentially measures the proximity of the network's output to the expected or desired output. In a Classical Neural Network: the weights (w{\displaystyle w}) and biases (b{\displaystyle b}) at each step determine the outcome of the cost function C(w:b){\displaystyle C(w:b)}. When training a Classical Neural network: the weights and biases are adjusted after each iteration: and given equation 1 below: where y(x){\displaystyle y(x)} is the desired output and aout(x){\displaystyle a^{\text{out}}(x)} is the actual output: the cost function is optimized when C(w:b){\displaystyle C(w:b)}= 0. For a quantum neural network: the cost function is determined by measuring the fidelity of the outcome state (ρout{\displaystyle \rho ^{\text{out}}}) with the desired outcome state (ϕout{\displaystyle \phi ^{\text{out}}}): seen in Equation 2 below. In this case: the Unitary operators are adjusted after each iteration: and the cost function is optimized when C = 1."
0.1552711999975145,How is the training method of classical and quantum neural networks different?,"Quantum Neural Networks can be theoretically trained similarly to training classical/artificial neural networks. The key difference lies in the communication between the layers of a neural network. In a quantum neural network, this is done by replacing the classical fan-out method with an arbitrary unitary that spreads out, but does not copy, the output of one qubit to the next layer of qubits. This process adheres to the quantum operation requirement of reversibility.","Score: 0.8666811846017244 / The quantum network thus ‘learns’ an algorithm. /  /  / === Quantum associative memory === / The first quantum associative memory algorithm was introduced by Dan Ventura and Tony Martinez in 1999. The authors do not attempt to translate the structure of artificial neural network models into quantum theory: but propose an algorithm for a circuit-based quantum computer that simulates associative memory. The memory states (in Hopfield neural networks saved in the weights of the neural connections) are written into a superposition: and a Grover-like quantum search algorithm retrieves the memory state closest to a given input. As such: this is not a fully content-addressable memory: since only incomplete patterns can be retrieved. / The first truly content-addressable quantum memory: which can retrieve patterns also from corrupted inputs: was proposed by Carlo A. Trugenberger. Both memories can store an exponential (in terms of n qubits) number of patterns but can be used only once due to the no-cloning theorem and their destruction upon measurement. / Trugenberger: however: has shown that his proababilistic model of quantum associative memory can be efficiently implemented and re-used multiples times for any polynomial number of stored patterns: a large advantage with respect to classical associative memories. /  /  / === Classical neural networks inspired by quantum theory === / A substantial amount of interest has been given to a “quantum-inspired” model that uses ideas from quantum theory to implement a neural network based on fuzzy logic. /  /  / == Training == / Quantum Neural Networks can be theoretically trained similarly to training classical/artificial neural networks. A key difference lies in communication between the layers of a neural networks. For classical neural networks: at the end of a given operation: the current perceptron copies its output to the next layer of perceptron(s) in the network. However: in a quantum neural network: where each perceptron is a qubit: this would violate the no-cloning theorem. A proposed generalized solution to this is to replace the classical fan-out method with an arbitrary unitary that spreads out: but does not copy: the output of one qubit to the next layer of qubits. Using this fan-out Unitary (Uf{\displaystyle U_{f}}) with a dummy state qubit in a known state (Ex. |0⟩{\displaystyle |0\rangle } in the computational basis): also known as an Ancilla bit: the information from the qubit can be transferred to the next layer of qubits. This process adheres to the quantum operation requirement of reversibility.Using this quantum feed-forward network: deep neural networks can be executed and trained efficiently. A deep neural network is essentially a network with many hidden-layers: as seen in the sample model neural network above. Since the Quantum neural network being discussed uses fan-out Unitary operators: and each operator only acts on its respective input: only two layers are used at any given time. In other words: no Unitary operator is acting on the entire network at any given time: meaning the number of qubits required for a given step depends on the number of inputs in a given layer. Since Quantum Computers are notorious for their ability to run multiple iterations in a short period of time: the efficiency of a quantum neural network is solely dependent on the number of qubits in any given layer: and not on the depth of the network. /  /  / === Cost functions === / To determine the effectiveness of a neural network: a cost function is used: which essentially measures the proximity of the network's output to the expected or desired output. In a Classical Neural Network: the weights (w{\displaystyle w}) and biases (b{\displaystyle b}) at each step determine the outcome of the cost function C(w:b){\displaystyle C(w:b)}. When training a Classical Neural network: the weights and biases are adjusted after each iteration: and given equation 1 below: where y(x){\displaystyle y(x)} is the desired output and aout(x){\displaystyle a^{\text{out}}(x)} is the actual output: the cost function is optimized when C(w:b){\displaystyle C(w:b)}= 0. For a quantum neural network: the cost function is determined by measuring the fidelity of the outcome state (ρout{\displaystyle \rho ^{\text{out}}}) with the desired outcome state (ϕout{\displaystyle \phi ^{\text{out}}}): seen in Equation 2 below. In this case: the Unitary operators are adjusted after each iteration: and the cost function is optimized when C = 1.","Score: 0.8564393334836305 / When training a Classical Neural network: the weights and biases are adjusted after each iteration: and given equation 1 below: where y(x){\displaystyle y(x)} is the desired output and aout(x){\displaystyle a^{\text{out}}(x)} is the actual output: the cost function is optimized when C(w:b){\displaystyle C(w:b)}= 0. For a quantum neural network: the cost function is determined by measuring the fidelity of the outcome state (ρout{\displaystyle \rho ^{\text{out}}}) with the desired outcome state (ϕout{\displaystyle \phi ^{\text{out}}}): seen in Equation 2 below. In this case: the Unitary operators are adjusted after each iteration: and the cost function is optimized when C = 1. / Equation 1 C(w:b)=1N∑x||y(x)−aout(x)||2{\displaystyle C(w:b)={1 \over N}\sum _{x}{||y(x)-a^{\text{out}}(x)|| \over 2}} /  / Equation 2 C=1N∑xN⟨ϕout|ρout|ϕout⟩{\displaystyle C={1 \over N}\sum _{x}^{N}{\langle \phi ^{\text{out}}|\rho ^{\text{out}}|\phi ^{\text{out}}\rangle }} /  /  / == See also == / Differentiable programming / Optical neural network / Holographic associative memory / Quantum cognition / Quantum machine learning /  /  / == References == /  /  / == External links == / Recent review of quantum neural networks by M. Schuld: I. Sinayskiy and F. Petruccione / Review of quantum neural networks by Wei / Article by P. Gralewicz on the plausibility of quantum computing in biological neural networks / Training a neural net to recognize images"
0.14753630000632256,What is the ReLU activation function in artificial neural networks?,"The ReLU (rectified linear unit) activation function in artificial neural networks is defined as the positive part of its argument. It's also known as a ramp function, analogous to half-wave rectification in electrical engineering. This activation function was introduced by Kunihiko Fukushima in 1969. ","Score: 0.8970550187786336 / In the context of artificial neural networks: the rectifier or ReLU (rectified linear unit) activation function is an activation function defined as the positive part of its argument: /  / f(x)=x+=max(0:x)=x+|x|2={xif x>0:0otherwise:{\displaystyle f(x)=x^{+}=\max(0:x)={\frac {x+|x|}{2}}={\begin{cases}x&{\text{if }}x>0:\\0&{\text{otherwise}}:\end{cases}}}where x is the input to a neuron. This is also known as a ramp function and is analogous to half-wave rectification in electrical engineering. This activation function was introduced by Kunihiko Fukushima in 1969 in the context of visual feature extraction in hierarchical neural networks. It was later argued that it  has strong biological motivations and mathematical justifications. In 2011 it was found to enable better training of deeper networks: compared to the widely used activation functions prior to 2011: e.g.: the logistic sigmoid (which is inspired by probability theory; see logistic regression) and its more practical counterpart: the hyperbolic tangent. The rectifier is: as of 2017: the most popular activation function for deep neural networks.Rectified linear units find applications in computer vision and speech recognition using deep neural nets and computational neuroscience. /  /  / == Advantages == / Sparse activation: For example: in a randomly initialized network: only about 50% of hidden units are activated (have a non-zero output). / Better gradient propagation: Fewer vanishing gradient problems compared to sigmoidal activation functions that saturate in both directions. / Efficient computation: Only comparison: addition and multiplication. / Scale-invariant: max(0:ax)=amax(0:x) for a≥0{\displaystyle \max(0:ax)=a\max(0:x){\text{ for }}a\geq 0}.Rectifying activation functions were used to separate specific excitation and unspecific inhibition in the neural abstraction pyramid: which was trained in a supervised way to learn several computer vision tasks. In 2011: the use of the rectifier as a non-linearity has been shown to enable training deep supervised neural networks without requiring unsupervised pre-training. Rectified linear units: compared to sigmoid function or similar activation functions: allow faster and effective training of deep neural architectures on large and complex datasets. /  /  / == Potential problems == / Non-differentiable at zero; however: it is differentiable anywhere else: and the value of the derivative at zero can be arbitrarily chosen to be 0 or 1. / Not zero-centered. / Unbounded. / Dying ReLU problem: ReLU (rectified linear unit) neurons can sometimes be pushed into states in which they become inactive for essentially all inputs. In this state: no gradients flow backward through the neuron: and so the neuron becomes stuck in a perpetually inactive state and ""dies"". This is a form of the vanishing gradient problem. In some cases: large numbers of neurons in a network can become stuck in dead states: effectively decreasing the model capacity. This problem typically arises when the learning rate is set too high. It may be mitigated by using leaky ReLUs instead: which assign a small positive slope for x < 0; however: the performance is reduced. /  /  / == Variants == /  /  / === Piecewise-linear variants === /  /  / ==== Leaky ReLU ==== / Leaky ReLUs allow a small: positive gradient when the unit is not active: helping to mitigate the vanishing gradient problem. /  / f(x)={xif x>0:0.01xotherwise.f′(x)={1if x>0:0.01otherwise.{\displaystyle f(x)={\begin{cases}x&{\text{if }}x>0:\\0.01x&{\text{otherwise}}.\end{cases}}\qquad \qquad f'(x)={\begin{cases}1&{\text{if }}x>0:\\0.01&{\text{otherwise}}.\end{cases}}} /  /  / ==== Parametric ReLU ==== / Parametric ReLUs (PReLUs) take this idea further by making the coefficient of leakage into a parameter that is learned along with the other neural-network parameters. / f(x)={xif x>0:a⋅xotherwise.f′(x)={1if x>0:aotherwise.","Score: 0.8764883049706321 / Here yi{\displaystyle y_{i}} is the output of the i{\displaystyle i}th node (neuron) and vi{\displaystyle v_{i}} is the weighted sum of the input connections. Alternative activation functions have been proposed: including the rectifier and softplus functions. More specialized activation functions include radial basis functions (used in radial basis networks: another class of supervised neural network models). / In recent developments of deep learning the rectified linear unit (ReLU) is more frequently used as one of the possible ways to overcome the numerical problems related to the sigmoids. /  /  / === Learning === / Learning occurs by changing connection weights after each piece of data is processed: based on the amount of error in the output compared to the expected result. This is an example of supervised learning: and is carried out through backpropagation. / We can represent the degree of error in an output node j{\displaystyle j} in the n{\displaystyle n}th data point (training example) by ej(n)=dj(n)−yj(n){\displaystyle e_{j}(n)=d_{j}(n)-y_{j}(n)}: where dj(n){\displaystyle d_{j}(n)} is the desired target value for n{\displaystyle n}th data point at node j{\displaystyle j}: and yj(n){\displaystyle y_{j}(n)} is the value produced at node j{\displaystyle j} when the n{\displaystyle n}th data point is given as an input. / The node weights can then be adjusted based on corrections that minimize the error in the entire output for the n{\displaystyle n}th data point: given by /  / E(n)=12∑output node jej2(n){\displaystyle {\mathcal {E}}(n)={\frac {1}{2}}\sum _{{\text{output node }}j}e_{j}^{2}(n)}.Using gradient descent: the change in each weight wij{\displaystyle w_{ij}} is /  / Δwji(n)=−η∂E(n)∂vj(n)yi(n){\displaystyle \Delta w_{ji}(n)=-\eta {\frac {\partial {\mathcal {E}}(n)}{\partial v_{j}(n)}}y_{i}(n)}where yi(n){\displaystyle y_{i}(n)} is the output of the previous neuron i{\displaystyle i}: and η{\displaystyle \eta } is the learning rate: which is selected to ensure that the weights quickly converge to a response: without oscillations. In the previous expression: ∂E(n)∂vj(n){\displaystyle {\frac {\partial {\mathcal {E}}(n)}{\partial v_{j}(n)}}} denotes the partial derivate of the error E(n){\displaystyle {\mathcal {E}}(n)} according to the weighted sum vj(n){\displaystyle v_{j}(n)} of the input connections of neuron i{\displaystyle i}. / The derivative to be calculated depends on the induced local field vj{\displaystyle v_{j}}: which itself varies. It is easy to prove that for an output node this derivative can be simplified to /  / −∂E(n)∂vj(n)=ej(n)ϕ′(vj(n)){\displaystyle -{\frac {\partial {\mathcal {E}}(n)}{\partial v_{j}(n)}}=e_{j}(n)\phi ^{\prime }(v_{j}(n))}where ϕ′{\displaystyle \phi ^{\prime }} is the derivative of the activation function described above: which itself does not vary. The analysis is more difficult for the change in weights to a hidden node: but it can be shown that the relevant derivative is /  / −∂E(n)∂vj(n)=ϕ′(vj(n))∑k−∂E(n)∂vk(n)wkj(n){\displaystyle -{\frac {\partial {\mathcal {E}}(n)}{\partial v_{j}(n)}}=\phi ^{\prime }(v_{j}(n))\sum _{k}-{\frac {\partial {\mathcal {E}}(n)}{\partial v_{k}(n)}}w_{kj}(n)}.This depends on the change in weights of the k{\displaystyle k}th nodes: which represent the output layer. So to change the hidden layer weights: the output layer weights change according to the derivative of the activation function: and so this algorithm represents a backpropagation of the activation function."
0.13821229999302886,How does ReLU help in neural networks?,"ReLU helps in neural networks by enabling better training of deeper networks, compared to the widely used activation functions prior to 2011 like the logistic sigmoid and the hyperbolic tangent. ReLU has found applications in computer vision and speech recognition using deep neural nets and computational neuroscience.","Score: 0.8576129575748468 / In the context of artificial neural networks: the rectifier or ReLU (rectified linear unit) activation function is an activation function defined as the positive part of its argument: /  / f(x)=x+=max(0:x)=x+|x|2={xif x>0:0otherwise:{\displaystyle f(x)=x^{+}=\max(0:x)={\frac {x+|x|}{2}}={\begin{cases}x&{\text{if }}x>0:\\0&{\text{otherwise}}:\end{cases}}}where x is the input to a neuron. This is also known as a ramp function and is analogous to half-wave rectification in electrical engineering. This activation function was introduced by Kunihiko Fukushima in 1969 in the context of visual feature extraction in hierarchical neural networks. It was later argued that it  has strong biological motivations and mathematical justifications. In 2011 it was found to enable better training of deeper networks: compared to the widely used activation functions prior to 2011: e.g.: the logistic sigmoid (which is inspired by probability theory; see logistic regression) and its more practical counterpart: the hyperbolic tangent. The rectifier is: as of 2017: the most popular activation function for deep neural networks.Rectified linear units find applications in computer vision and speech recognition using deep neural nets and computational neuroscience. /  /  / == Advantages == / Sparse activation: For example: in a randomly initialized network: only about 50% of hidden units are activated (have a non-zero output). / Better gradient propagation: Fewer vanishing gradient problems compared to sigmoidal activation functions that saturate in both directions. / Efficient computation: Only comparison: addition and multiplication. / Scale-invariant: max(0:ax)=amax(0:x) for a≥0{\displaystyle \max(0:ax)=a\max(0:x){\text{ for }}a\geq 0}.Rectifying activation functions were used to separate specific excitation and unspecific inhibition in the neural abstraction pyramid: which was trained in a supervised way to learn several computer vision tasks. In 2011: the use of the rectifier as a non-linearity has been shown to enable training deep supervised neural networks without requiring unsupervised pre-training. Rectified linear units: compared to sigmoid function or similar activation functions: allow faster and effective training of deep neural architectures on large and complex datasets. /  /  / == Potential problems == / Non-differentiable at zero; however: it is differentiable anywhere else: and the value of the derivative at zero can be arbitrarily chosen to be 0 or 1. / Not zero-centered. / Unbounded. / Dying ReLU problem: ReLU (rectified linear unit) neurons can sometimes be pushed into states in which they become inactive for essentially all inputs. In this state: no gradients flow backward through the neuron: and so the neuron becomes stuck in a perpetually inactive state and ""dies"". This is a form of the vanishing gradient problem. In some cases: large numbers of neurons in a network can become stuck in dead states: effectively decreasing the model capacity. This problem typically arises when the learning rate is set too high. It may be mitigated by using leaky ReLUs instead: which assign a small positive slope for x < 0; however: the performance is reduced. /  /  / == Variants == /  /  / === Piecewise-linear variants === /  /  / ==== Leaky ReLU ==== / Leaky ReLUs allow a small: positive gradient when the unit is not active: helping to mitigate the vanishing gradient problem. /  / f(x)={xif x>0:0.01xotherwise.f′(x)={1if x>0:0.01otherwise.{\displaystyle f(x)={\begin{cases}x&{\text{if }}x>0:\\0.01x&{\text{otherwise}}.\end{cases}}\qquad \qquad f'(x)={\begin{cases}1&{\text{if }}x>0:\\0.01&{\text{otherwise}}.\end{cases}}} /  /  / ==== Parametric ReLU ==== / Parametric ReLUs (PReLUs) take this idea further by making the coefficient of leakage into a parameter that is learned along with the other neural-network parameters. / f(x)={xif x>0:a⋅xotherwise.f′(x)={1if x>0:aotherwise.","Score: 0.8419179132953284 / Here yi{\displaystyle y_{i}} is the output of the i{\displaystyle i}th node (neuron) and vi{\displaystyle v_{i}} is the weighted sum of the input connections. Alternative activation functions have been proposed: including the rectifier and softplus functions. More specialized activation functions include radial basis functions (used in radial basis networks: another class of supervised neural network models). / In recent developments of deep learning the rectified linear unit (ReLU) is more frequently used as one of the possible ways to overcome the numerical problems related to the sigmoids. /  /  / === Learning === / Learning occurs by changing connection weights after each piece of data is processed: based on the amount of error in the output compared to the expected result. This is an example of supervised learning: and is carried out through backpropagation. / We can represent the degree of error in an output node j{\displaystyle j} in the n{\displaystyle n}th data point (training example) by ej(n)=dj(n)−yj(n){\displaystyle e_{j}(n)=d_{j}(n)-y_{j}(n)}: where dj(n){\displaystyle d_{j}(n)} is the desired target value for n{\displaystyle n}th data point at node j{\displaystyle j}: and yj(n){\displaystyle y_{j}(n)} is the value produced at node j{\displaystyle j} when the n{\displaystyle n}th data point is given as an input. / The node weights can then be adjusted based on corrections that minimize the error in the entire output for the n{\displaystyle n}th data point: given by /  / E(n)=12∑output node jej2(n){\displaystyle {\mathcal {E}}(n)={\frac {1}{2}}\sum _{{\text{output node }}j}e_{j}^{2}(n)}.Using gradient descent: the change in each weight wij{\displaystyle w_{ij}} is /  / Δwji(n)=−η∂E(n)∂vj(n)yi(n){\displaystyle \Delta w_{ji}(n)=-\eta {\frac {\partial {\mathcal {E}}(n)}{\partial v_{j}(n)}}y_{i}(n)}where yi(n){\displaystyle y_{i}(n)} is the output of the previous neuron i{\displaystyle i}: and η{\displaystyle \eta } is the learning rate: which is selected to ensure that the weights quickly converge to a response: without oscillations. In the previous expression: ∂E(n)∂vj(n){\displaystyle {\frac {\partial {\mathcal {E}}(n)}{\partial v_{j}(n)}}} denotes the partial derivate of the error E(n){\displaystyle {\mathcal {E}}(n)} according to the weighted sum vj(n){\displaystyle v_{j}(n)} of the input connections of neuron i{\displaystyle i}. / The derivative to be calculated depends on the induced local field vj{\displaystyle v_{j}}: which itself varies. It is easy to prove that for an output node this derivative can be simplified to /  / −∂E(n)∂vj(n)=ej(n)ϕ′(vj(n)){\displaystyle -{\frac {\partial {\mathcal {E}}(n)}{\partial v_{j}(n)}}=e_{j}(n)\phi ^{\prime }(v_{j}(n))}where ϕ′{\displaystyle \phi ^{\prime }} is the derivative of the activation function described above: which itself does not vary. The analysis is more difficult for the change in weights to a hidden node: but it can be shown that the relevant derivative is /  / −∂E(n)∂vj(n)=ϕ′(vj(n))∑k−∂E(n)∂vk(n)wkj(n){\displaystyle -{\frac {\partial {\mathcal {E}}(n)}{\partial v_{j}(n)}}=\phi ^{\prime }(v_{j}(n))\sum _{k}-{\frac {\partial {\mathcal {E}}(n)}{\partial v_{k}(n)}}w_{kj}(n)}.This depends on the change in weights of the k{\displaystyle k}th nodes: which represent the output layer. So to change the hidden layer weights: the output layer weights change according to the derivative of the activation function: and so this algorithm represents a backpropagation of the activation function."
0.16737370000919327,What are some advantages of the ReLU function?,The ReLU function offers the advantages of sparse activation where only about 50% of hidden units are activated. The feature helps in better gradient propagation and efficient computation. It is also scale-invariant.,"Score: 0.8561023984726733 / In the context of artificial neural networks: the rectifier or ReLU (rectified linear unit) activation function is an activation function defined as the positive part of its argument: /  / f(x)=x+=max(0:x)=x+|x|2={xif x>0:0otherwise:{\displaystyle f(x)=x^{+}=\max(0:x)={\frac {x+|x|}{2}}={\begin{cases}x&{\text{if }}x>0:\\0&{\text{otherwise}}:\end{cases}}}where x is the input to a neuron. This is also known as a ramp function and is analogous to half-wave rectification in electrical engineering. This activation function was introduced by Kunihiko Fukushima in 1969 in the context of visual feature extraction in hierarchical neural networks. It was later argued that it  has strong biological motivations and mathematical justifications. In 2011 it was found to enable better training of deeper networks: compared to the widely used activation functions prior to 2011: e.g.: the logistic sigmoid (which is inspired by probability theory; see logistic regression) and its more practical counterpart: the hyperbolic tangent. The rectifier is: as of 2017: the most popular activation function for deep neural networks.Rectified linear units find applications in computer vision and speech recognition using deep neural nets and computational neuroscience. /  /  / == Advantages == / Sparse activation: For example: in a randomly initialized network: only about 50% of hidden units are activated (have a non-zero output). / Better gradient propagation: Fewer vanishing gradient problems compared to sigmoidal activation functions that saturate in both directions. / Efficient computation: Only comparison: addition and multiplication. / Scale-invariant: max(0:ax)=amax(0:x) for a≥0{\displaystyle \max(0:ax)=a\max(0:x){\text{ for }}a\geq 0}.Rectifying activation functions were used to separate specific excitation and unspecific inhibition in the neural abstraction pyramid: which was trained in a supervised way to learn several computer vision tasks. In 2011: the use of the rectifier as a non-linearity has been shown to enable training deep supervised neural networks without requiring unsupervised pre-training. Rectified linear units: compared to sigmoid function or similar activation functions: allow faster and effective training of deep neural architectures on large and complex datasets. /  /  / == Potential problems == / Non-differentiable at zero; however: it is differentiable anywhere else: and the value of the derivative at zero can be arbitrarily chosen to be 0 or 1. / Not zero-centered. / Unbounded. / Dying ReLU problem: ReLU (rectified linear unit) neurons can sometimes be pushed into states in which they become inactive for essentially all inputs. In this state: no gradients flow backward through the neuron: and so the neuron becomes stuck in a perpetually inactive state and ""dies"". This is a form of the vanishing gradient problem. In some cases: large numbers of neurons in a network can become stuck in dead states: effectively decreasing the model capacity. This problem typically arises when the learning rate is set too high. It may be mitigated by using leaky ReLUs instead: which assign a small positive slope for x < 0; however: the performance is reduced. /  /  / == Variants == /  /  / === Piecewise-linear variants === /  /  / ==== Leaky ReLU ==== / Leaky ReLUs allow a small: positive gradient when the unit is not active: helping to mitigate the vanishing gradient problem. /  / f(x)={xif x>0:0.01xotherwise.f′(x)={1if x>0:0.01otherwise.{\displaystyle f(x)={\begin{cases}x&{\text{if }}x>0:\\0.01x&{\text{otherwise}}.\end{cases}}\qquad \qquad f'(x)={\begin{cases}1&{\text{if }}x>0:\\0.01&{\text{otherwise}}.\end{cases}}} /  /  / ==== Parametric ReLU ==== / Parametric ReLUs (PReLUs) take this idea further by making the coefficient of leakage into a parameter that is learned along with the other neural-network parameters. / f(x)={xif x>0:a⋅xotherwise.f′(x)={1if x>0:aotherwise.","Score: 0.829311120237631 / f(x)={xif x>0:0.01xotherwise.f′(x)={1if x>0:0.01otherwise.{\displaystyle f(x)={\begin{cases}x&{\text{if }}x>0:\\0.01x&{\text{otherwise}}.\end{cases}}\qquad \qquad f'(x)={\begin{cases}1&{\text{if }}x>0:\\0.01&{\text{otherwise}}.\end{cases}}} /  /  / ==== Parametric ReLU ==== / Parametric ReLUs (PReLUs) take this idea further by making the coefficient of leakage into a parameter that is learned along with the other neural-network parameters. / f(x)={xif x>0:a⋅xotherwise.f′(x)={1if x>0:aotherwise.{\displaystyle f(x)={\begin{cases}x&{\text{if }}x>0:\\a\cdot x&{\text{otherwise}}.\end{cases}}\qquad \qquad \qquad f'(x)={\begin{cases}1&{\text{if }}x>0:\\a&{\text{otherwise}}.\end{cases}}}Note that for a ≤ 1: this is equivalent to /  / f(x)=max(x:ax){\displaystyle f(x)=\max(x:ax)}and thus has a relation to ""maxout"" networks. /  /  / === Other non-linear variants === /  /  / ==== Gaussian-error linear unit (GELU) ==== / GELU is a smooth approximation to the rectifier: /  / f(x)=x⋅Φ(x):{\displaystyle f(x)=x\cdot \Phi (x):}f′(x)=x⋅Φ′(x)+Φ(x):{\displaystyle f'(x)=x\cdot \Phi '(x)+\Phi (x):}where Φ(x)=P(X⩽x){\displaystyle \Phi (x)=P(X\leqslant x)} is the cumulative distribution function of the standard normal distribution. / This activation function is illustrated in the figure at the start of this article. It has a ""bump"" to the left of x < 0 and serves as the default activation for models such as BERT. /  /  / ==== SiLU ==== /  / The SiLU (sigmoid linear unit) or swish function is another smooth approximation: first coined in the GELU paper: / f(x)=x⋅sigmoid⁡(x):{\displaystyle f(x)=x\cdot \operatorname {sigmoid} (x):}f′(x)=x⋅sigmoid′⁡(x)+sigmoid⁡(x):{\displaystyle f'(x)=x\cdot \operatorname {sigmoid} '(x)+\operatorname {sigmoid} (x):}where sigmoid⁡(x){\displaystyle \operatorname {sigmoid} (x)} is the sigmoid function. /  /  / ==== Softplus ==== / A smooth approximation to the rectifier is the analytic function /  / f(x)=ln⁡(1+ex):f′(x)=ex1+ex=11+e−x:{\displaystyle f(x)=\ln(1+e^{x}):\qquad \qquad f'(x)={\frac {e^{x}}{1+e^{x}}}={\frac {1}{1+e^{-x}}}:}which is called the softplus or SmoothReLU function. For large negative x{\displaystyle x} it is roughly ln⁡1{\displaystyle \ln 1}: so just above 0: while for large positive x{\displaystyle x} it is roughly ln⁡(ex){\displaystyle \ln(e^{x})}: so just above x{\displaystyle x}.  / This function can be approximated as: /  / ln⁡(1+ex)≈{ln⁡2:x=0:x1−e−x/ln⁡2:x≠0{\displaystyle \ln \left(1+e^{x}\right)\approx {\begin{cases}\ln 2:&x=0:\\[6pt]{\frac {x}{1-e^{-x/\ln 2}}}:&x\neq 0\end{cases}}}By making the change of variables x=yln⁡(2){\displaystyle x=y\ln(2)}: this is equivalent to /  / log2⁡(1+2y)≈{1:y=0:y1−e−y:y≠0."
0.19252990002860315,What are some potential problems of ReLU?,"Some potential problems of ReLU include the fact that it is non-differentiable at zero. It's also not zero-centered, is unbounded, and may suffer from the dying ReLU problem where neurons become inactive for virtually all inputs, a form of the vanishing gradient problem.","Score: 0.8185521979919747 / In the context of artificial neural networks: the rectifier or ReLU (rectified linear unit) activation function is an activation function defined as the positive part of its argument: /  / f(x)=x+=max(0:x)=x+|x|2={xif x>0:0otherwise:{\displaystyle f(x)=x^{+}=\max(0:x)={\frac {x+|x|}{2}}={\begin{cases}x&{\text{if }}x>0:\\0&{\text{otherwise}}:\end{cases}}}where x is the input to a neuron. This is also known as a ramp function and is analogous to half-wave rectification in electrical engineering. This activation function was introduced by Kunihiko Fukushima in 1969 in the context of visual feature extraction in hierarchical neural networks. It was later argued that it  has strong biological motivations and mathematical justifications. In 2011 it was found to enable better training of deeper networks: compared to the widely used activation functions prior to 2011: e.g.: the logistic sigmoid (which is inspired by probability theory; see logistic regression) and its more practical counterpart: the hyperbolic tangent. The rectifier is: as of 2017: the most popular activation function for deep neural networks.Rectified linear units find applications in computer vision and speech recognition using deep neural nets and computational neuroscience. /  /  / == Advantages == / Sparse activation: For example: in a randomly initialized network: only about 50% of hidden units are activated (have a non-zero output). / Better gradient propagation: Fewer vanishing gradient problems compared to sigmoidal activation functions that saturate in both directions. / Efficient computation: Only comparison: addition and multiplication. / Scale-invariant: max(0:ax)=amax(0:x) for a≥0{\displaystyle \max(0:ax)=a\max(0:x){\text{ for }}a\geq 0}.Rectifying activation functions were used to separate specific excitation and unspecific inhibition in the neural abstraction pyramid: which was trained in a supervised way to learn several computer vision tasks. In 2011: the use of the rectifier as a non-linearity has been shown to enable training deep supervised neural networks without requiring unsupervised pre-training. Rectified linear units: compared to sigmoid function or similar activation functions: allow faster and effective training of deep neural architectures on large and complex datasets. /  /  / == Potential problems == / Non-differentiable at zero; however: it is differentiable anywhere else: and the value of the derivative at zero can be arbitrarily chosen to be 0 or 1. / Not zero-centered. / Unbounded. / Dying ReLU problem: ReLU (rectified linear unit) neurons can sometimes be pushed into states in which they become inactive for essentially all inputs. In this state: no gradients flow backward through the neuron: and so the neuron becomes stuck in a perpetually inactive state and ""dies"". This is a form of the vanishing gradient problem. In some cases: large numbers of neurons in a network can become stuck in dead states: effectively decreasing the model capacity. This problem typically arises when the learning rate is set too high. It may be mitigated by using leaky ReLUs instead: which assign a small positive slope for x < 0; however: the performance is reduced. /  /  / == Variants == /  /  / === Piecewise-linear variants === /  /  / ==== Leaky ReLU ==== / Leaky ReLUs allow a small: positive gradient when the unit is not active: helping to mitigate the vanishing gradient problem. /  / f(x)={xif x>0:0.01xotherwise.f′(x)={1if x>0:0.01otherwise.{\displaystyle f(x)={\begin{cases}x&{\text{if }}x>0:\\0.01x&{\text{otherwise}}.\end{cases}}\qquad \qquad f'(x)={\begin{cases}1&{\text{if }}x>0:\\0.01&{\text{otherwise}}.\end{cases}}} /  /  / ==== Parametric ReLU ==== / Parametric ReLUs (PReLUs) take this idea further by making the coefficient of leakage into a parameter that is learned along with the other neural-network parameters. / f(x)={xif x>0:a⋅xotherwise.f′(x)={1if x>0:aotherwise.","Score: 0.8105564011406077 / f(x)={xif x>0:0.01xotherwise.f′(x)={1if x>0:0.01otherwise.{\displaystyle f(x)={\begin{cases}x&{\text{if }}x>0:\\0.01x&{\text{otherwise}}.\end{cases}}\qquad \qquad f'(x)={\begin{cases}1&{\text{if }}x>0:\\0.01&{\text{otherwise}}.\end{cases}}} /  /  / ==== Parametric ReLU ==== / Parametric ReLUs (PReLUs) take this idea further by making the coefficient of leakage into a parameter that is learned along with the other neural-network parameters. / f(x)={xif x>0:a⋅xotherwise.f′(x)={1if x>0:aotherwise.{\displaystyle f(x)={\begin{cases}x&{\text{if }}x>0:\\a\cdot x&{\text{otherwise}}.\end{cases}}\qquad \qquad \qquad f'(x)={\begin{cases}1&{\text{if }}x>0:\\a&{\text{otherwise}}.\end{cases}}}Note that for a ≤ 1: this is equivalent to /  / f(x)=max(x:ax){\displaystyle f(x)=\max(x:ax)}and thus has a relation to ""maxout"" networks. /  /  / === Other non-linear variants === /  /  / ==== Gaussian-error linear unit (GELU) ==== / GELU is a smooth approximation to the rectifier: /  / f(x)=x⋅Φ(x):{\displaystyle f(x)=x\cdot \Phi (x):}f′(x)=x⋅Φ′(x)+Φ(x):{\displaystyle f'(x)=x\cdot \Phi '(x)+\Phi (x):}where Φ(x)=P(X⩽x){\displaystyle \Phi (x)=P(X\leqslant x)} is the cumulative distribution function of the standard normal distribution. / This activation function is illustrated in the figure at the start of this article. It has a ""bump"" to the left of x < 0 and serves as the default activation for models such as BERT. /  /  / ==== SiLU ==== /  / The SiLU (sigmoid linear unit) or swish function is another smooth approximation: first coined in the GELU paper: / f(x)=x⋅sigmoid⁡(x):{\displaystyle f(x)=x\cdot \operatorname {sigmoid} (x):}f′(x)=x⋅sigmoid′⁡(x)+sigmoid⁡(x):{\displaystyle f'(x)=x\cdot \operatorname {sigmoid} '(x)+\operatorname {sigmoid} (x):}where sigmoid⁡(x){\displaystyle \operatorname {sigmoid} (x)} is the sigmoid function. /  /  / ==== Softplus ==== / A smooth approximation to the rectifier is the analytic function /  / f(x)=ln⁡(1+ex):f′(x)=ex1+ex=11+e−x:{\displaystyle f(x)=\ln(1+e^{x}):\qquad \qquad f'(x)={\frac {e^{x}}{1+e^{x}}}={\frac {1}{1+e^{-x}}}:}which is called the softplus or SmoothReLU function. For large negative x{\displaystyle x} it is roughly ln⁡1{\displaystyle \ln 1}: so just above 0: while for large positive x{\displaystyle x} it is roughly ln⁡(ex){\displaystyle \ln(e^{x})}: so just above x{\displaystyle x}.  / This function can be approximated as: /  / ln⁡(1+ex)≈{ln⁡2:x=0:x1−e−x/ln⁡2:x≠0{\displaystyle \ln \left(1+e^{x}\right)\approx {\begin{cases}\ln 2:&x=0:\\[6pt]{\frac {x}{1-e^{-x/\ln 2}}}:&x\neq 0\end{cases}}}By making the change of variables x=yln⁡(2){\displaystyle x=y\ln(2)}: this is equivalent to /  / log2⁡(1+2y)≈{1:y=0:y1−e−y:y≠0."
0.1591459999908693,Can you name and explain an variant of ReLU?,"A variant of ReLU is the Leaky ReLU. It allows a small, positive gradient when the unit is not active, helping to mitigate the vanishing gradient problem.","Score: 0.8258857170673513 / In the context of artificial neural networks: the rectifier or ReLU (rectified linear unit) activation function is an activation function defined as the positive part of its argument: /  / f(x)=x+=max(0:x)=x+|x|2={xif x>0:0otherwise:{\displaystyle f(x)=x^{+}=\max(0:x)={\frac {x+|x|}{2}}={\begin{cases}x&{\text{if }}x>0:\\0&{\text{otherwise}}:\end{cases}}}where x is the input to a neuron. This is also known as a ramp function and is analogous to half-wave rectification in electrical engineering. This activation function was introduced by Kunihiko Fukushima in 1969 in the context of visual feature extraction in hierarchical neural networks. It was later argued that it  has strong biological motivations and mathematical justifications. In 2011 it was found to enable better training of deeper networks: compared to the widely used activation functions prior to 2011: e.g.: the logistic sigmoid (which is inspired by probability theory; see logistic regression) and its more practical counterpart: the hyperbolic tangent. The rectifier is: as of 2017: the most popular activation function for deep neural networks.Rectified linear units find applications in computer vision and speech recognition using deep neural nets and computational neuroscience. /  /  / == Advantages == / Sparse activation: For example: in a randomly initialized network: only about 50% of hidden units are activated (have a non-zero output). / Better gradient propagation: Fewer vanishing gradient problems compared to sigmoidal activation functions that saturate in both directions. / Efficient computation: Only comparison: addition and multiplication. / Scale-invariant: max(0:ax)=amax(0:x) for a≥0{\displaystyle \max(0:ax)=a\max(0:x){\text{ for }}a\geq 0}.Rectifying activation functions were used to separate specific excitation and unspecific inhibition in the neural abstraction pyramid: which was trained in a supervised way to learn several computer vision tasks. In 2011: the use of the rectifier as a non-linearity has been shown to enable training deep supervised neural networks without requiring unsupervised pre-training. Rectified linear units: compared to sigmoid function or similar activation functions: allow faster and effective training of deep neural architectures on large and complex datasets. /  /  / == Potential problems == / Non-differentiable at zero; however: it is differentiable anywhere else: and the value of the derivative at zero can be arbitrarily chosen to be 0 or 1. / Not zero-centered. / Unbounded. / Dying ReLU problem: ReLU (rectified linear unit) neurons can sometimes be pushed into states in which they become inactive for essentially all inputs. In this state: no gradients flow backward through the neuron: and so the neuron becomes stuck in a perpetually inactive state and ""dies"". This is a form of the vanishing gradient problem. In some cases: large numbers of neurons in a network can become stuck in dead states: effectively decreasing the model capacity. This problem typically arises when the learning rate is set too high. It may be mitigated by using leaky ReLUs instead: which assign a small positive slope for x < 0; however: the performance is reduced. /  /  / == Variants == /  /  / === Piecewise-linear variants === /  /  / ==== Leaky ReLU ==== / Leaky ReLUs allow a small: positive gradient when the unit is not active: helping to mitigate the vanishing gradient problem. /  / f(x)={xif x>0:0.01xotherwise.f′(x)={1if x>0:0.01otherwise.{\displaystyle f(x)={\begin{cases}x&{\text{if }}x>0:\\0.01x&{\text{otherwise}}.\end{cases}}\qquad \qquad f'(x)={\begin{cases}1&{\text{if }}x>0:\\0.01&{\text{otherwise}}.\end{cases}}} /  /  / ==== Parametric ReLU ==== / Parametric ReLUs (PReLUs) take this idea further by making the coefficient of leakage into a parameter that is learned along with the other neural-network parameters. / f(x)={xif x>0:a⋅xotherwise.f′(x)={1if x>0:aotherwise.","Score: 0.8235728363740783 / f(x)={xif x>0:0.01xotherwise.f′(x)={1if x>0:0.01otherwise.{\displaystyle f(x)={\begin{cases}x&{\text{if }}x>0:\\0.01x&{\text{otherwise}}.\end{cases}}\qquad \qquad f'(x)={\begin{cases}1&{\text{if }}x>0:\\0.01&{\text{otherwise}}.\end{cases}}} /  /  / ==== Parametric ReLU ==== / Parametric ReLUs (PReLUs) take this idea further by making the coefficient of leakage into a parameter that is learned along with the other neural-network parameters. / f(x)={xif x>0:a⋅xotherwise.f′(x)={1if x>0:aotherwise.{\displaystyle f(x)={\begin{cases}x&{\text{if }}x>0:\\a\cdot x&{\text{otherwise}}.\end{cases}}\qquad \qquad \qquad f'(x)={\begin{cases}1&{\text{if }}x>0:\\a&{\text{otherwise}}.\end{cases}}}Note that for a ≤ 1: this is equivalent to /  / f(x)=max(x:ax){\displaystyle f(x)=\max(x:ax)}and thus has a relation to ""maxout"" networks. /  /  / === Other non-linear variants === /  /  / ==== Gaussian-error linear unit (GELU) ==== / GELU is a smooth approximation to the rectifier: /  / f(x)=x⋅Φ(x):{\displaystyle f(x)=x\cdot \Phi (x):}f′(x)=x⋅Φ′(x)+Φ(x):{\displaystyle f'(x)=x\cdot \Phi '(x)+\Phi (x):}where Φ(x)=P(X⩽x){\displaystyle \Phi (x)=P(X\leqslant x)} is the cumulative distribution function of the standard normal distribution. / This activation function is illustrated in the figure at the start of this article. It has a ""bump"" to the left of x < 0 and serves as the default activation for models such as BERT. /  /  / ==== SiLU ==== /  / The SiLU (sigmoid linear unit) or swish function is another smooth approximation: first coined in the GELU paper: / f(x)=x⋅sigmoid⁡(x):{\displaystyle f(x)=x\cdot \operatorname {sigmoid} (x):}f′(x)=x⋅sigmoid′⁡(x)+sigmoid⁡(x):{\displaystyle f'(x)=x\cdot \operatorname {sigmoid} '(x)+\operatorname {sigmoid} (x):}where sigmoid⁡(x){\displaystyle \operatorname {sigmoid} (x)} is the sigmoid function. /  /  / ==== Softplus ==== / A smooth approximation to the rectifier is the analytic function /  / f(x)=ln⁡(1+ex):f′(x)=ex1+ex=11+e−x:{\displaystyle f(x)=\ln(1+e^{x}):\qquad \qquad f'(x)={\frac {e^{x}}{1+e^{x}}}={\frac {1}{1+e^{-x}}}:}which is called the softplus or SmoothReLU function. For large negative x{\displaystyle x} it is roughly ln⁡1{\displaystyle \ln 1}: so just above 0: while for large positive x{\displaystyle x} it is roughly ln⁡(ex){\displaystyle \ln(e^{x})}: so just above x{\displaystyle x}.  / This function can be approximated as: /  / ln⁡(1+ex)≈{ln⁡2:x=0:x1−e−x/ln⁡2:x≠0{\displaystyle \ln \left(1+e^{x}\right)\approx {\begin{cases}\ln 2:&x=0:\\[6pt]{\frac {x}{1-e^{-x/\ln 2}}}:&x\neq 0\end{cases}}}By making the change of variables x=yln⁡(2){\displaystyle x=y\ln(2)}: this is equivalent to /  / log2⁡(1+2y)≈{1:y=0:y1−e−y:y≠0."
0.14954260000376962,What is a recurrent neural network (RNN)?,"A recurrent neural network (RNN) is one of the two broad types of artificial neural network, characterized by the flow of information between its layers. It is a bi-directional artificial neural network, meaning it allows the output from some nodes to affect subsequent input to the same nodes. ","Score: 0.9054568631870672 / A recurrent neural network (RNN) is one of the two broad types of artificial neural network: characterized by direction of the flow of information between its layers. In contrast to the uni-directional feedforward neural network: it is a bi-directional artificial neural network: meaning that it allows the output from some nodes to affect subsequent input to the same nodes. Their ability to use internal state (memory) to process arbitrary sequences of inputs makes them applicable to tasks such as unsegmented: connected handwriting recognition or speech recognition. The term ""recurrent neural network"" is used to refer to the class of networks with an infinite impulse response: whereas ""convolutional neural network"" refers to the class of finite impulse response. Both classes of networks exhibit temporal dynamic behavior. A finite impulse recurrent network is a directed acyclic graph that can be unrolled and replaced with a strictly feedforward neural network: while an infinite impulse recurrent network is a directed cyclic graph that can not be unrolled. / Additional stored states and the storage under direct control by the network can be added to both infinite-impulse and finite-impulse networks. Another network or graph can also replace the storage if that incorporates time delays or has feedback loops. Such controlled states are referred to as gated states or gated memory and are part of long short-term memory networks (LSTMs) and gated recurrent units. This is also called Feedforward Neural Network (FNN). Recurrent neural networks are theoretically Turing complete and can run arbitrary programs to process arbitrary sequences of inputs. /  /  / == History == / The Ising model (1925) by Wilhelm Lenz and Ernst Ising / was the first RNN architecture that did not learn. Shun'ichi Amari made it adaptive in 1972. This was also called the Hopfield network (1982). See also David Rumelhart's work in 1986.  In 1993: a neural history compressor system solved a ""Very Deep Learning"" task that required more than 1000 subsequent layers in an RNN unfolded in time. /  /  / === LSTM === / Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1997 and set accuracy records in multiple applications domains.Around 2007: LSTM started to revolutionize speech recognition: outperforming traditional models in certain speech applications. In 2009: a Connectionist Temporal Classification (CTC)-trained LSTM network was the first RNN to win pattern recognition contests when it won several competitions in connected handwriting recognition. In 2014: the Chinese company Baidu used CTC-trained RNNs to break the 2S09 Switchboard Hub5'00 speech recognition dataset benchmark without using any traditional speech processing methods.LSTM also improved large-vocabulary speech recognition and text-to-speech synthesis and was used in Google Android. In 2015: Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM.LSTM broke records for improved machine translation: Language Modeling and Multilingual Language Processing. LSTM combined with convolutional neural networks (CNNs) improved automatic image captioning. /  /  / == Architectures == /  / RNNs come in many variants. /  /  / === Fully recurrent === / Fully recurrent neural networks (FRNN) connect the outputs of all neurons to the inputs of all neurons.  This is the most general neural network topology because all other topologies can be represented by setting some connection weights to zero to simulate the lack of connections between those neurons. The illustration to the right may be misleading to many because practical neural network topologies are frequently organized in ""layers"" and the drawing gives that appearance. However: what appears to be layers are: in fact: different steps in time of the same fully recurrent neural network. The left-most item in the illustration shows the recurrent connections as the arc labeled 'v'.  It is ""unfolded"" in time to produce the appearance of layers. /  /  / === Elman networks and Jordan networks === / An Elman network is a three-layer network (arranged horizontally as x: y: and z in the illustration) with the addition of a set of context units (u in the illustration). The middle (hidden) layer is connected to these context units fixed with a weight of one. At each time step: the input is fed forward and a learning rule is applied. The fixed back-connections save a copy of the previous values of the hidden units in the context units (since they propagate over the connections before the learning rule is applied). Thus the network can maintain a sort of state: allowing it to perform such tasks as sequence-prediction that are beyond the power of a standard multilayer perceptron.","Score: 0.8760364231438851 / The weights of output neurons are the only part of the network that can change (be trained). ESNs are good at reproducing certain time series. A variant for spiking neurons is known as a liquid state machine. /  /  / === Independently RNN (IndRNN) === / The independently recurrent neural network (IndRNN) addresses the gradient vanishing and exploding problems in the traditional fully connected RNN. Each neuron in one layer only receives its own past state as context information (instead of full connectivity to all other neurons in this layer) and thus neurons are independent of each other's history. The gradient backpropagation can be regulated to avoid gradient vanishing and exploding in order to keep long or short-term memory. The cross-neuron information is explored in the next layers. IndRNN can be robustly trained with non-saturated nonlinear functions such as ReLU. Deep networks can be trained using skip connections. /  /  / === Recursive === /  / A recursive neural network is created by applying the same set of weights recursively over a differentiable graph-like structure by traversing the structure in topological order. Such networks are typically also trained by the reverse mode of automatic differentiation. They can process distributed representations of structure: such as logical terms. A special case of recursive neural networks is the RNN whose structure corresponds to a linear chain. Recursive neural networks have been applied to natural language processing. The Recursive Neural Tensor Network uses a tensor-based composition function for all nodes in the tree. /  /  / === Neural history compressor === / The neural history compressor is an unsupervised stack of RNNs. At the input level: it learns to predict its next input from the previous inputs. Only unpredictable inputs of some RNN in the hierarchy become inputs to the next higher level RNN: which therefore recomputes its internal state only rarely. Each higher level RNN thus studies a compressed representation of the information in the RNN below. This is done such that the input sequence can be precisely reconstructed from the representation at the highest level. / The system effectively minimizes the description length or the negative logarithm of the probability of the data. Given a lot of learnable predictability in the incoming data sequence: the highest level RNN can use supervised learning to easily classify even deep sequences with long intervals between important events. / It is possible to distill the RNN hierarchy into two RNNs: the ""conscious"" chunker (higher level) and the ""subconscious"" automatizer (lower level). Once the chunker has learned to predict and compress inputs that are unpredictable by the automatizer: then the automatizer can be forced in the next learning phase to predict or imitate through additional units the hidden units of the more slowly changing chunker. This makes it easy for the automatizer to learn appropriate: rarely changing memories across long intervals. In turn: this helps the automatizer to make many of its once unpredictable inputs predictable: such that the chunker can focus on the remaining unpredictable events.A generative model partially overcame the vanishing gradient problem of automatic differentiation or backpropagation in neural networks in 1992. In 1993: such a system solved a ""Very Deep Learning"" task that required more than 1000 subsequent layers in an RNN unfolded in time. /  /  / === Second order RNNs === / Second-order RNNs use higher order weights wijk{\displaystyle w{}_{ijk}} instead of the standard wij{\displaystyle w{}_{ij}} weights: and states can be a product. This allows a direct mapping to a finite-state machine both in training: stability: and representation. Long short-term memory is an example of this but has no such formal mappings or proof of stability. /  /  / === Long short-term memory === /  / Long short-term memory (LSTM) is a deep learning system that avoids the vanishing gradient problem. LSTM is normally augmented by recurrent gates called ""forget gates"". LSTM prevents backpropagated errors from vanishing or exploding. Instead: errors can flow backward through unlimited numbers of virtual layers unfolded in space. That is: LSTM can learn tasks that require memories of events that happened thousands or even millions of discrete time steps earlier. Problem-specific LSTM-like topologies can be evolved. LSTM works even given long delays between significant events and can handle signals that mix low and high-frequency components. / Many applications use stacks of LSTM RNNs and train them by connectionist temporal classification (CTC) to find an RNN weight matrix that maximizes the probability of the label sequences in a training set: given the corresponding input sequences. CTC achieves both alignment and recognition."
0.14873939999961294,What is the difference between recurrent neural networks and convolutional neural networks in terms of impulse response?,"The term ""recurrent neural network"" is used to refer to the class of networks with an infinite impulse response, whereas ""convolutional neural network"" refers to the class of finite impulse response. Both classes of networks exhibit temporal dynamic behavior.","Score: 0.8532277886309779 / A recurrent neural network (RNN) is one of the two broad types of artificial neural network: characterized by direction of the flow of information between its layers. In contrast to the uni-directional feedforward neural network: it is a bi-directional artificial neural network: meaning that it allows the output from some nodes to affect subsequent input to the same nodes. Their ability to use internal state (memory) to process arbitrary sequences of inputs makes them applicable to tasks such as unsegmented: connected handwriting recognition or speech recognition. The term ""recurrent neural network"" is used to refer to the class of networks with an infinite impulse response: whereas ""convolutional neural network"" refers to the class of finite impulse response. Both classes of networks exhibit temporal dynamic behavior. A finite impulse recurrent network is a directed acyclic graph that can be unrolled and replaced with a strictly feedforward neural network: while an infinite impulse recurrent network is a directed cyclic graph that can not be unrolled. / Additional stored states and the storage under direct control by the network can be added to both infinite-impulse and finite-impulse networks. Another network or graph can also replace the storage if that incorporates time delays or has feedback loops. Such controlled states are referred to as gated states or gated memory and are part of long short-term memory networks (LSTMs) and gated recurrent units. This is also called Feedforward Neural Network (FNN). Recurrent neural networks are theoretically Turing complete and can run arbitrary programs to process arbitrary sequences of inputs. /  /  / == History == / The Ising model (1925) by Wilhelm Lenz and Ernst Ising / was the first RNN architecture that did not learn. Shun'ichi Amari made it adaptive in 1972. This was also called the Hopfield network (1982). See also David Rumelhart's work in 1986.  In 1993: a neural history compressor system solved a ""Very Deep Learning"" task that required more than 1000 subsequent layers in an RNN unfolded in time. /  /  / === LSTM === / Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1997 and set accuracy records in multiple applications domains.Around 2007: LSTM started to revolutionize speech recognition: outperforming traditional models in certain speech applications. In 2009: a Connectionist Temporal Classification (CTC)-trained LSTM network was the first RNN to win pattern recognition contests when it won several competitions in connected handwriting recognition. In 2014: the Chinese company Baidu used CTC-trained RNNs to break the 2S09 Switchboard Hub5'00 speech recognition dataset benchmark without using any traditional speech processing methods.LSTM also improved large-vocabulary speech recognition and text-to-speech synthesis and was used in Google Android. In 2015: Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM.LSTM broke records for improved machine translation: Language Modeling and Multilingual Language Processing. LSTM combined with convolutional neural networks (CNNs) improved automatic image captioning. /  /  / == Architectures == /  / RNNs come in many variants. /  /  / === Fully recurrent === / Fully recurrent neural networks (FRNN) connect the outputs of all neurons to the inputs of all neurons.  This is the most general neural network topology because all other topologies can be represented by setting some connection weights to zero to simulate the lack of connections between those neurons. The illustration to the right may be misleading to many because practical neural network topologies are frequently organized in ""layers"" and the drawing gives that appearance. However: what appears to be layers are: in fact: different steps in time of the same fully recurrent neural network. The left-most item in the illustration shows the recurrent connections as the arc labeled 'v'.  It is ""unfolded"" in time to produce the appearance of layers. /  /  / === Elman networks and Jordan networks === / An Elman network is a three-layer network (arranged horizontally as x: y: and z in the illustration) with the addition of a set of context units (u in the illustration). The middle (hidden) layer is connected to these context units fixed with a weight of one. At each time step: the input is fed forward and a learning rule is applied. The fixed back-connections save a copy of the previous values of the hidden units in the context units (since they propagate over the connections before the learning rule is applied). Thus the network can maintain a sort of state: allowing it to perform such tasks as sequence-prediction that are beyond the power of a standard multilayer perceptron.","Score: 0.8487342986161333 / === Time series forecasting === / Recurrent neural networks are generally considered the best neural network architectures for time series forecasting (and sequence modeling in general): but recent studies show that convolutional networks can perform comparably or even better. Dilated convolutions might enable one-dimensional convolutional neural networks to effectively learn time series dependences. Convolutions can be implemented more efficiently than RNN-based solutions: and they do not suffer from vanishing (or exploding) gradients. Convolutional networks can provide an improved forecasting performance when there are multiple similar time series to learn from. CNNs can also be applied to further tasks in time series analysis (e.g.: time series classification or quantile forecasting). /  /  / === Cultural Heritage and 3D-datasets === / As archaeological findings like clay tablets with cuneiform writing are increasingly acquired using 3D scanners first benchmark datasets are becoming available like HeiCuBeDa providing almost 2.000 normalized 2D- and 3D-datasets prepared with the GigaMesh Software Framework. So curvature-based measures are used in conjunction with Geometric Neural Networks (GNNs) e.g. for period classification of those clay tablets being among the oldest documents of human history. /  /  / == Fine-tuning == / For many applications: the training data is less available. Convolutional neural networks usually require a large amount of training data in order to avoid overfitting. A common technique is to train the network on a larger data set from a related domain. Once the network parameters have converged an additional training step is performed using the in-domain data to fine-tune the network weights: this is known as transfer learning. Furthermore: this technique allows convolutional network architectures to successfully be applied to problems with tiny training sets. /  /  / == Human interpretable explanations == / End-to-end training and prediction are common practice in computer vision. However: human interpretable explanations are required for critical systems such as a self-driving cars. With recent advances in visual salience: spatial attention: and temporal attention: the most critical spatial regions/temporal instants could be visualized to justify the CNN predictions. /  /  / == Related architectures == /  /  / === Deep Q-networks === / A deep Q-network (DQN) is a type of deep learning model that combines a deep neural network with Q-learning: a form of reinforcement learning. Unlike earlier reinforcement learning agents: DQNs that utilize CNNs can learn directly from high-dimensional sensory inputs via reinforcement learning.Preliminary results were presented in 2014: with an accompanying paper in February 2015. The research described an application to Atari 2600 gaming. Other deep reinforcement learning models preceded it. /  /  / === Deep belief networks === /  / Convolutional deep belief networks (CDBN) have structure very similar to convolutional neural networks and are trained similarly to deep belief networks. Therefore: they exploit the 2D structure of images: like CNNs do: and make use of pre-training like deep belief networks. They provide a generic structure that can be used in many image and signal processing tasks. Benchmark results on standard image datasets like CIFAR have been obtained using CDBNs. /  /  / == Notable libraries == / Caffe: A library for convolutional neural networks. Created by the Berkeley Vision and Learning Center (BVLC). It supports both CPU and GPU. Developed in C++: and has Python and MATLAB wrappers. / Deeplearning4j: Deep learning in Java and Scala on multi-GPU-enabled Spark. A general-purpose deep learning library for the JVM production stack running on a C++ scientific computing engine. Allows the creation of custom layers. Integrates with Hadoop and Kafka. / Dlib: A toolkit for making real world machine learning and data analysis applications in C++. / Microsoft Cognitive Toolkit: A deep learning toolkit written by Microsoft with several unique features enhancing scalability over multiple nodes. It supports full-fledged interfaces for training in C++ and Python and with additional support for model inference in C# and Java. / TensorFlow: Apache 2.0-licensed Theano-like library with support for CPU: GPU: Google's proprietary tensor processing unit (TPU): and mobile devices. / Theano: The reference deep-learning library for Python with an API largely compatible with the popular NumPy library. Allows user to write symbolic mathematical expressions: then automatically generates their derivatives: saving the user from having to code gradients or backpropagation. These symbolic expressions are automatically compiled to CUDA code for a fast: on-the-GPU implementation. / Torch: A scientific computing framework with wide support for machine learning algorithms: written in C and Lua."
0.12421790001098998,What is a significant feature of LSTM networks related to states?,"It can add additional stored states and the storage under direct control by the network, referred to as gated states or gated memory.","Score: 0.849139323313726 / The weights of output neurons are the only part of the network that can change (be trained). ESNs are good at reproducing certain time series. A variant for spiking neurons is known as a liquid state machine. /  /  / === Independently RNN (IndRNN) === / The independently recurrent neural network (IndRNN) addresses the gradient vanishing and exploding problems in the traditional fully connected RNN. Each neuron in one layer only receives its own past state as context information (instead of full connectivity to all other neurons in this layer) and thus neurons are independent of each other's history. The gradient backpropagation can be regulated to avoid gradient vanishing and exploding in order to keep long or short-term memory. The cross-neuron information is explored in the next layers. IndRNN can be robustly trained with non-saturated nonlinear functions such as ReLU. Deep networks can be trained using skip connections. /  /  / === Recursive === /  / A recursive neural network is created by applying the same set of weights recursively over a differentiable graph-like structure by traversing the structure in topological order. Such networks are typically also trained by the reverse mode of automatic differentiation. They can process distributed representations of structure: such as logical terms. A special case of recursive neural networks is the RNN whose structure corresponds to a linear chain. Recursive neural networks have been applied to natural language processing. The Recursive Neural Tensor Network uses a tensor-based composition function for all nodes in the tree. /  /  / === Neural history compressor === / The neural history compressor is an unsupervised stack of RNNs. At the input level: it learns to predict its next input from the previous inputs. Only unpredictable inputs of some RNN in the hierarchy become inputs to the next higher level RNN: which therefore recomputes its internal state only rarely. Each higher level RNN thus studies a compressed representation of the information in the RNN below. This is done such that the input sequence can be precisely reconstructed from the representation at the highest level. / The system effectively minimizes the description length or the negative logarithm of the probability of the data. Given a lot of learnable predictability in the incoming data sequence: the highest level RNN can use supervised learning to easily classify even deep sequences with long intervals between important events. / It is possible to distill the RNN hierarchy into two RNNs: the ""conscious"" chunker (higher level) and the ""subconscious"" automatizer (lower level). Once the chunker has learned to predict and compress inputs that are unpredictable by the automatizer: then the automatizer can be forced in the next learning phase to predict or imitate through additional units the hidden units of the more slowly changing chunker. This makes it easy for the automatizer to learn appropriate: rarely changing memories across long intervals. In turn: this helps the automatizer to make many of its once unpredictable inputs predictable: such that the chunker can focus on the remaining unpredictable events.A generative model partially overcame the vanishing gradient problem of automatic differentiation or backpropagation in neural networks in 1992. In 1993: such a system solved a ""Very Deep Learning"" task that required more than 1000 subsequent layers in an RNN unfolded in time. /  /  / === Second order RNNs === / Second-order RNNs use higher order weights wijk{\displaystyle w{}_{ijk}} instead of the standard wij{\displaystyle w{}_{ij}} weights: and states can be a product. This allows a direct mapping to a finite-state machine both in training: stability: and representation. Long short-term memory is an example of this but has no such formal mappings or proof of stability. /  /  / === Long short-term memory === /  / Long short-term memory (LSTM) is a deep learning system that avoids the vanishing gradient problem. LSTM is normally augmented by recurrent gates called ""forget gates"". LSTM prevents backpropagated errors from vanishing or exploding. Instead: errors can flow backward through unlimited numbers of virtual layers unfolded in space. That is: LSTM can learn tasks that require memories of events that happened thousands or even millions of discrete time steps earlier. Problem-specific LSTM-like topologies can be evolved. LSTM works even given long delays between significant events and can handle signals that mix low and high-frequency components. / Many applications use stacks of LSTM RNNs and train them by connectionist temporal classification (CTC) to find an RNN weight matrix that maximizes the probability of the label sequences in a training set: given the corresponding input sequences. CTC achieves both alignment and recognition.","Score: 0.8378159088093405 / LSTM RNNs can learn ""Very Deep Learning"" tasks that involve multi-second intervals containing speech events separated by thousands of discrete time steps: where one time step corresponds to about 10 ms. LSTM with forget gates is competitive with traditional speech recognizers on certain tasks.The initial success in speech recognition was based on small-scale recognition tasks based on TIMIT. The data set contains 630 speakers from eight major dialects of American English: where each speaker reads 10 sentences. Its small size lets many configurations be tried. More importantly: the TIMIT task concerns phone-sequence recognition: which: unlike word-sequence recognition: allows weak phone bigram language models. This lets the strength of the acoustic modeling aspects of speech recognition be more easily analyzed. The error rates listed below: including these early results and measured as percent phone error rates (PER): have been summarized since 1991. /  / The debut of DNNs for speaker recognition in the late 1990s and speech recognition around 2009-2011 and of LSTM around 2003–2007: accelerated progress in eight major areas: / Scale-up/out and accelerated DNN training and decoding / Sequence discriminative training / Feature processing by deep models with solid understanding of the underlying mechanisms / Adaptation of DNNs and related deep models / Multi-task and transfer learning by DNNs and related deep models / CNNs and how to design them to best exploit domain knowledge of speech / RNN and its rich LSTM variants / Other types of deep models including tensor-based models and integrated deep generative/discriminative models.All major commercial speech recognition systems (e.g.: Microsoft Cortana: Xbox: Skype Translator: Amazon Alexa: Google Now: Apple Siri: Baidu and iFlyTek voice search: and a range of Nuance speech products: etc.) are based on deep learning. /  /  / === Image recognition === /  / A common evaluation set for image classification is the MNIST database data set. MNIST is composed of handwritten digits and includes 60:000 training examples and 10:000 test examples. As with TIMIT: its small size lets users test multiple configurations. A comprehensive list of results on this set is available.Deep learning-based image recognition has become ""superhuman"": producing more accurate results than human contestants. This first occurred in 2011 in recognition of traffic signs: and in 2014: with recognition of human faces.Deep learning-trained vehicles now interpret 360° camera views. Another example is Facial Dysmorphology Novel Analysis (FDNA) used to analyze cases of human malformation connected to a large database of genetic syndromes. /  /  / === Visual art processing === / Closely related to the progress that has been made in image recognition is the increasing application of deep learning techniques to various visual art tasks. DNNs have proven themselves capable: for example: of /  / identifying the style period of a given painting / Neural Style Transfer –  capturing the style of a given artwork and applying it in a visually pleasing manner to an arbitrary photograph or video / generating striking imagery based on random visual input fields. /  /  / === Natural language processing === /  / Neural networks have been used for implementing language models since the early 2000s. LSTM helped to improve machine translation and language modeling.Other key techniques in this field are negative sampling and word embedding. Word embedding: such as word2vec: can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a vector space. Using word embedding as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar. A compositional vector grammar can be thought of as probabilistic context free grammar (PCFG) implemented by an RNN. Recursive auto-encoders built atop word embeddings can assess sentence similarity and detect paraphrasing. Deep neural architectures provide the best results for constituency parsing: sentiment analysis: information retrieval: spoken language understanding: machine translation: contextual entity linking: writing style recognition: named-entity recognition (token classification): text classification: and others.Recent developments generalize word embedding to sentence embedding. / Google Translate (GT) uses a large end-to-end long short-term memory (LSTM) network. Google Neural Machine Translation (GNMT) uses an example-based machine translation method in which the system ""learns from millions of examples"". It translates ""whole sentences at a time: rather than pieces"". Google Translate supports over one hundred languages. The network encodes the ""semantics of the sentence rather than simply memorizing phrase-to-phrase translations"". GT uses English as an intermediate between most language pairs."
0.13076999998884276,What was the first RNN architecture that did not learn and who made it adaptive?,"The Ising model by Wilhelm Lenz and Ernst Ising, made in 1925, was the first RNN architecture that did not learn. It was made adaptive by Shun'ichi Amari in 1972.","Score: 0.8133242135402945 / LeNet-5 (1998): a 7-level CNN by Yann LeCun et al.: that classifies digits: was applied by several banks to recognize hand-written numbers on checks  digitized in 32x32 pixel images. / In the 1980s: backpropagation did not work well for deep learning with long credit assignment paths. To overcome this problem: Jürgen Schmidhuber (1992) proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning. It uses predictive coding  to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN: by distilling a higher level chunker network into a lower level automatizer network. In 1993: a chunker solved a deep learning task whose depth exceeded 1000.In 1992: Jürgen Schmidhuber also published an alternative to RNNs which is now called a linear Transformer or a  Transformer with linearized self-attention (save for a normalization operator). It learns internal spotlights of attention: a slow feedforward neural network learns by gradient descent to control the fast weights of another neural network through outer products of self-generated activation patterns FROM and TO (which are now called key and value for self-attention). This fast weight attention mapping is applied to a query pattern. / The modern Transformer was introduced by Ashish Vaswani et al. in their 2017 paper ""Attention Is All You Need"".  / It combines this with a softmax operator and a projection matrix. / Transformers have increasingly become the model of choice for natural language processing. Many modern large language models such as ChatGPT: GPT-4: and BERT use it. Transformers are also increasingly being used in computer vision.In 1991: Jürgen Schmidhuber also published adversarial neural networks that contest with each other in the form of a zero-sum game: where one network's gain is the other network's loss. The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called ""artificial curiosity"". In 2014: this principle was used in a generative adversarial network (GAN) by Ian Goodfellow et al. Here the environmental reaction is 1 or 0 depending on whether the first network's output is in a given set. This can be used to create realistic deepfakes. Excellent image quality is achieved by Nvidia's StyleGAN (2018) based on the Progressive GAN by Tero Karras et al. Here the GAN generator is grown from small to large scale in a pyramidal fashion. / Sepp Hochreiter's diploma thesis (1991) was called ""one of the most important documents in the history of machine learning"" by his supervisor Schmidhuber. It not only tested the neural history compressor: but also identified and analyzed the vanishing gradient problem. Hochreiter proposed recurrent residual connections to solve this problem. This led to the deep learning method called long short-term memory (LSTM): published in 1997. LSTM recurrent neural networks can learn ""very deep learning"" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. The ""vanilla LSTM"" with forget gate was introduced in 1999 by Felix Gers: Schmidhuber and Fred Cummins. LSTM has become the  most cited neural network of the 20th century. / In 2015: Rupesh Kumar Srivastava: Klaus Greff: and Schmidhuber used LSTM principles to create the Highway network: a feedforward neural network with hundreds of layers: much deeper than previous networks. 7 months later: Kaiming He: Xiangyu Zhang;  Shaoqing Ren: and Jian Sun won the ImageNet 2015 competition with an open-gated or gateless Highway network variant called Residual neural network. This has become the most cited neural network of the 21st century.In 1994: André de Carvalho: together with Mike Fairhurst and David Bisset: published experimental results of a multi-layer boolean neural network: also known as a weightless neural network: composed of a 3-layers self-organising feature extraction neural network module (SOFT) followed by a multi-layer classification neural network module (GSN): which were independently trained.","Score: 0.8127750525289934 / The RNN hierarchy can be ""collapsed"" into a single RNN: by ""distilling"" a higher level ""chunker"" network into a lower level ""automatizer"" network. In 1993: a chunker solved a deep learning task whose CAP depth exceeded 1000. / Such history compressors can substantially facilitate downstream supervised deep learning.Geoffrey Hinton et al. (2006) proposed learning a high-level internal representation using successive layers of binary or real-valued latent variables with a restricted Boltzmann machine to model each layer. This RBM is a generative stochastic feedforward neural network that can learn a probability distribution over its set of inputs. Once sufficiently many layers have been learned: the deep architecture may be used as a generative model by reproducing the data when sampling down the model (an ""ancestral pass"") from the top level feature activations. In 2012: Andrew Ng and Jeff Dean created an FNN that learned to recognize higher-level concepts: such as cats: only from watching unlabeled images taken from YouTube videos. /  /  / == The vanishing gradient problem and its solutions == /  / Sepp Hochreiter's diploma thesis (1991) was called ""one of the most important documents in the history of machine learning"" by his supervisor Juergen Schmidhuber. Hochreiter not only tested the neural history compressor: but also identified and analyzed the vanishing gradient problem. He proposed recurrent residual connections to solve this problem. This led to the deep learning method called long short-term memory (LSTM): published in 1997. LSTM recurrent neural networks can learn ""very deep learning"" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. The ""vanilla LSTM"" with forget gate was introduced in 1999 by Felix Gers: Schmidhuber and Fred Cummins. LSTM has become the  most cited neural network of the 20th century.In 2015: Rupesh Kumar Srivastava: Klaus Greff: and Schmidhuber used LSTM principles to create the Highway network: a feedforward neural network with hundreds of layers: much deeper than previous networks. 7 months later: Kaiming He: Xiangyu Zhang;  Shaoqing Ren: and Jian Sun won the ImageNet 2015 competition with an open-gated or gateless Highway network variant called Residual neural network. This has become the most cited neural network of the 21st century.In 2011: Xavier Glorot: Antoine Bordes and Yoshua Bengio found that the ReLU of Kunihiko Fukushima also helps to overcome the vanishing gradient problem: compared to widely used activation functions prior to 2011. /  /  / == Hardware-based designs == / The development of metal–oxide–semiconductor (MOS) very-large-scale integration (VLSI): combining millions or billions of MOS transistors onto a single chip in the form of complementary MOS (CMOS) technology: enabled the development of practical artificial neural networks in the 1980s.Computational devices were created in CMOS: for both biophysical simulation and neuromorphic computing inspired by the structure and function of the human brain. Nanodevices for very large scale principal components analyses and convolution may create a new class of neural computing because they are fundamentally analog rather than digital (even though the first implementations may use digital devices). Ciresan and colleagues (2010) in Schmidhuber's group showed that despite the vanishing gradient problem: GPUs make backpropagation feasible for many-layered feedforward neural networks. /  /  / == Contests == / Between 2009 and 2012: recurrent neural networks and deep feedforward neural networks developed in Schmidhuber's research group won eight international competitions in pattern recognition and machine learning. For example: the bi-directional and multi-dimensional long short-term memory (LSTM) of Graves et al. won three competitions in connected handwriting recognition at the 2009 International Conference on Document Analysis and Recognition (ICDAR): without any prior knowledge about the three languages to be learned.Ciresan and colleagues won pattern recognition contests: including the IJCNN 2011 Traffic Sign Recognition Competition: the ISBI 2012 Segmentation of Neuronal Structures in Electron Microscopy Stacks challenge and others. Their neural networks were the first pattern recognizers to achieve human-competitive/superhuman performance on benchmarks such as traffic sign recognition (IJCNN 2012): or the MNIST handwritten digits problem."
0.1559201999916695,What is the function of an Elman network in a recurrent neural network?,"An Elman network is a three-layer network with the addition of a set of context units. It can maintain a sort of state, allowing it to perform tasks such as sequence prediction that are beyond the power of a standard multilayer perceptron.","Score: 0.8837067596287608 / The left-most item in the illustration shows the recurrent connections as the arc labeled 'v'.  It is ""unfolded"" in time to produce the appearance of layers. /  /  / === Elman networks and Jordan networks === / An Elman network is a three-layer network (arranged horizontally as x: y: and z in the illustration) with the addition of a set of context units (u in the illustration). The middle (hidden) layer is connected to these context units fixed with a weight of one. At each time step: the input is fed forward and a learning rule is applied. The fixed back-connections save a copy of the previous values of the hidden units in the context units (since they propagate over the connections before the learning rule is applied). Thus the network can maintain a sort of state: allowing it to perform such tasks as sequence-prediction that are beyond the power of a standard multilayer perceptron. / Jordan networks are similar to Elman networks. The context units are fed from the output layer instead of the hidden layer. The context units in a Jordan network are also called the state layer. They have a recurrent connection to themselves.Elman and Jordan networks are also known as ""Simple recurrent networks"" (SRN). /  / Elman network / ht=σh(Whxt+Uhht−1+bh)yt=σy(Wyht+by){\displaystyle {\begin{aligned}h_{t}&=\sigma _{h}(W_{h}x_{t}+U_{h}h_{t-1}+b_{h})\\y_{t}&=\sigma _{y}(W_{y}h_{t}+b_{y})\end{aligned}}} / Jordan network / ht=σh(Whxt+Uhyt−1+bh)yt=σy(Wyht+by){\displaystyle {\begin{aligned}h_{t}&=\sigma _{h}(W_{h}x_{t}+U_{h}y_{t-1}+b_{h})\\y_{t}&=\sigma _{y}(W_{y}h_{t}+b_{y})\end{aligned}}}Variables and functions /  / xt{\displaystyle x_{t}}: input vector / ht{\displaystyle h_{t}}: hidden layer vector / yt{\displaystyle y_{t}}: output vector / W{\displaystyle W}: U{\displaystyle U} and b{\displaystyle b}: parameter matrices and vector / σh{\displaystyle \sigma _{h}} and σy{\displaystyle \sigma _{y}}: Activation functions /  /  / === Hopfield === /  / The Hopfield network is an RNN in which all connections across layers are equally sized. It requires stationary inputs and is thus not a general RNN: as it does not process sequences of patterns. However: it guarantees that it will converge. If the connections are trained using Hebbian learning: then the Hopfield network can perform as robust content-addressable memory: resistant to connection alteration. /  /  / ==== Bidirectional associative memory ==== /  / Introduced by Bart Kosko: a bidirectional associative memory (BAM) network is a variant of a Hopfield network that stores associative data as a vector. The bi-directionality comes from passing information through a matrix and its transpose. Typically: bipolar encoding is preferred to binary encoding of the associative pairs. Recently: stochastic BAM models using Markov stepping were optimized for increased network stability and relevance to real-world applications.A BAM network has two layers: either of which can be driven as an input to recall an association and produce an output on the other layer. /  /  / === Echo state === /  / Echo state networks (ESN) have a sparsely connected random hidden layer. The weights of output neurons are the only part of the network that can change (be trained). ESNs are good at reproducing certain time series. A variant for spiking neurons is known as a liquid state machine. /  /  / === Independently RNN (IndRNN) === / The independently recurrent neural network (IndRNN) addresses the gradient vanishing and exploding problems in the traditional fully connected RNN. Each neuron in one layer only receives its own past state as context information (instead of full connectivity to all other neurons in this layer) and thus neurons are independent of each other's history. The gradient backpropagation can be regulated to avoid gradient vanishing and exploding in order to keep long or short-term memory. The cross-neuron information is explored in the next layers. IndRNN can be robustly trained with non-saturated nonlinear functions such as ReLU. Deep networks can be trained using skip connections.","Score: 0.8347268388351701 / A recurrent neural network (RNN) is one of the two broad types of artificial neural network: characterized by direction of the flow of information between its layers. In contrast to the uni-directional feedforward neural network: it is a bi-directional artificial neural network: meaning that it allows the output from some nodes to affect subsequent input to the same nodes. Their ability to use internal state (memory) to process arbitrary sequences of inputs makes them applicable to tasks such as unsegmented: connected handwriting recognition or speech recognition. The term ""recurrent neural network"" is used to refer to the class of networks with an infinite impulse response: whereas ""convolutional neural network"" refers to the class of finite impulse response. Both classes of networks exhibit temporal dynamic behavior. A finite impulse recurrent network is a directed acyclic graph that can be unrolled and replaced with a strictly feedforward neural network: while an infinite impulse recurrent network is a directed cyclic graph that can not be unrolled. / Additional stored states and the storage under direct control by the network can be added to both infinite-impulse and finite-impulse networks. Another network or graph can also replace the storage if that incorporates time delays or has feedback loops. Such controlled states are referred to as gated states or gated memory and are part of long short-term memory networks (LSTMs) and gated recurrent units. This is also called Feedforward Neural Network (FNN). Recurrent neural networks are theoretically Turing complete and can run arbitrary programs to process arbitrary sequences of inputs. /  /  / == History == / The Ising model (1925) by Wilhelm Lenz and Ernst Ising / was the first RNN architecture that did not learn. Shun'ichi Amari made it adaptive in 1972. This was also called the Hopfield network (1982). See also David Rumelhart's work in 1986.  In 1993: a neural history compressor system solved a ""Very Deep Learning"" task that required more than 1000 subsequent layers in an RNN unfolded in time. /  /  / === LSTM === / Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1997 and set accuracy records in multiple applications domains.Around 2007: LSTM started to revolutionize speech recognition: outperforming traditional models in certain speech applications. In 2009: a Connectionist Temporal Classification (CTC)-trained LSTM network was the first RNN to win pattern recognition contests when it won several competitions in connected handwriting recognition. In 2014: the Chinese company Baidu used CTC-trained RNNs to break the 2S09 Switchboard Hub5'00 speech recognition dataset benchmark without using any traditional speech processing methods.LSTM also improved large-vocabulary speech recognition and text-to-speech synthesis and was used in Google Android. In 2015: Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM.LSTM broke records for improved machine translation: Language Modeling and Multilingual Language Processing. LSTM combined with convolutional neural networks (CNNs) improved automatic image captioning. /  /  / == Architectures == /  / RNNs come in many variants. /  /  / === Fully recurrent === / Fully recurrent neural networks (FRNN) connect the outputs of all neurons to the inputs of all neurons.  This is the most general neural network topology because all other topologies can be represented by setting some connection weights to zero to simulate the lack of connections between those neurons. The illustration to the right may be misleading to many because practical neural network topologies are frequently organized in ""layers"" and the drawing gives that appearance. However: what appears to be layers are: in fact: different steps in time of the same fully recurrent neural network. The left-most item in the illustration shows the recurrent connections as the arc labeled 'v'.  It is ""unfolded"" in time to produce the appearance of layers. /  /  / === Elman networks and Jordan networks === / An Elman network is a three-layer network (arranged horizontally as x: y: and z in the illustration) with the addition of a set of context units (u in the illustration). The middle (hidden) layer is connected to these context units fixed with a weight of one. At each time step: the input is fed forward and a learning rule is applied. The fixed back-connections save a copy of the previous values of the hidden units in the context units (since they propagate over the connections before the learning rule is applied). Thus the network can maintain a sort of state: allowing it to perform such tasks as sequence-prediction that are beyond the power of a standard multilayer perceptron."
0.16648720001103356,What is a residual neural network?,"A residual neural network, also known as a residual network or ResNet, is a deep learning model in which the weight layers learn residual functions in relation to the layer inputs. It behaves like a highway network, with gates that are opened through strongly positive bias weights. This system allows deep learning models with numerous layers to train more easily and achieve better accuracy. ","Score: 0.8882542708362682 / A residual neural network (also referred to as a residual network or ResNet) is a deep learning model in which the weight layers learn residual functions with reference to the layer inputs. It behaves like a highway network whose gates are opened through strongly positive bias weights. This enables deep learning models with tens or hundreds of layers to train easily and approach better accuracy when going deeper. The identity skip connections: often referred to as ""residual connections"": are also used in the 1997 LSTM networks: Transformer models (e.g.: BERT: GPT models such as ChatGPT): the AlphaGo Zero system: the AlphaStar system: and the AlphaFold system. / Residual networks were developed by Kaiming He: Xiangyu Zhang: Shaoqing Ren: and Jian Sun: who won the 2015 ImageNet competition. /  /  / == Formulation == /  /  / === Background === / The AlexNet model developed in 2012 for ImageNet was an eight-layer convolutional neural network. / The neural networks developed in 2014 by the Visual Geometry Group (VGG) at the University of Oxford approached a depth of 19 layers by stacking 3-by-3 convolutional layers. / However: stacking more layers led to a steep reduction in training accuracy: which is referred to as the ""degradation"" problem.A deeper network should not produce a higher training loss than its shallower counterpart: if this deeper network can be constructed by its shallower counterpart stacked with extra layers. If the extra layers can be set as identity mappings: the deeper network would represent the same function as the shallower counterpart. It is hypothesized that the optimizer is not able to approach identity mappings for the parameterized layers. /  /  / === Residual learning === / In a multi-layer neural network model: consider a subnetwork with a certain number (e.g.: 2 or 3) of stacked layers. Denote the underlying function performed by this subnetwork as H(x){\textstyle H(x)}: where x{\textstyle x} is the input to this subnetwork. / The idea of ""Residual Learning"" re-parameterizes this subnetwork and lets the parameter layers represent a residual function F(x):=H(x)−x{\textstyle F(x):=H(x)-x}. / The output y{\textstyle y} of this subnetwork is represented as: /  / y=F(x)+x{\displaystyle {\begin{aligned}y&=F(x)+x\end{aligned}}}This is also the principle of the 1997 LSTM cell computing yt+1=F(xt)+xt{\textstyle y_{t+1}=F(x_{t})+x_{t}}: which becomes y=F(x)+x{\textstyle y=F(x)+x} during backpropagation through time. / The function F(x){\textstyle F(x)} is often represented by matrix multiplication interlaced with activation functions and normalization operations (e.g.: Batch Normalization or Layer Normalization). / This subnetwork is referred to as a ""Residual Block"". A deep residual network is constructed by stacking a series of residual blocks. / The operation of ""+ x{\textstyle +\ x}"" in ""y=F(x)+x{\textstyle y=F(x)+x}"" is approached by a skip connection that performs identity mapping and connects the input of a residual block with its output. This connection is often referred to as a ""Residual Connection"" in later work. /  /  / === Signal propagation === / The introduction of identity mappings facilitates signal propagation in both forward and backward paths.","Score: 0.8599662259359827 / == Related Work == / In 1961: Frank Rosenblatt described a three-layer multilayer perceptron (MLP) model with skip connections. The model was referred to as a ""cross-coupled system"": and the skip connections were forms of cross-coupled connections. / In two books published in 1994 /  / and 1996: ""skip-layer"" connections were presented in feed-forward MLP models: ""The general definition [of MLP] allows more than one hidden layer: and it also allows 'skip-layer' connections from input to output"" (p261 in: p144 in ): ""... which allows the non-linear units to perturb a linear functional form"" (p262 in ). This description suggests that the non-linear MLP performs like a residual function (perturbation) added to a linear function. / Sepp Hochreiter analyzed the vanishing gradient problem in 1991 and attributed to it the reason why deep learning did not work well. / To overcome this problem: long short-term memory (LSTM) recurrent neural networks / had skip connections or residual connections with a weight of 1.0 in every LSTM cell (called the constant error carrousel) to compute yt+1=F(xt)+xt{\textstyle y_{t+1}=F(x_{t})+x_{t}}. During backpropagation through time: this becomes the above-mentioned residual formula y=F(x)+x{\textstyle y=F(x)+x} for feedforward neural networks. This enables training very deep recurrent neural networks with a very long time span t. A later LSTM version published in 2000 modulates the identity LSTM connections by so-called forget gates such that their weights are not fixed to 1.0 but can be learned. In experiments: the forget gates were initialized with positive bias weights: thus being opened: addressing the vanishing gradient problem. / The highway network of May 2015  / applies these principles to feedforward neural networks. / It was reported to be ""the first very deep feedforward network with hundreds of layers"".  / It is like an LSTM with forget gates unfolded in time: while the later Residual Nets have no equivalent of forget gates and are like the unfolded original LSTM. / If the skip connections in Highway Networks are ""without gates"": or if their gates are kept open (activation 1.0) through strong positive bias weights: they become the identity skip connections in Residual Networks. / The original Highway Network paper not only introduced the basic principle for very deep feedforward networks: but also included experimental results with 20: 50: and 100 layers networks: and mentioned ongoing experiments with up to 900 layers. / Networks with 50 or 100 layers had lower training error than their plain network counterparts: but no lower training error than their 20 layers counterpart (on the MNIST dataset: Figure 1 in ). No improvement on test accuracy was reported with networks deeper than 19 layers (on the CIFAR-10 dataset; Table 1 in ). The ResNet paper: however: provided strong experimental evidence of the benefits of going deeper than 20 layers. It argued that the identity mapping without modulation is crucial and mentioned that modulation in the skip connection can still lead to vanishing signals in forward and backward propagation (Section 3 in ). This is also  why the forget gates of the 2000 LSTM were initially opened through positive bias weights: as long as the gates are open: it behaves like the 1997 LSTM. Similarly: a Highway Net whose gates are opened through strongly positive bias weights behaves like a ResNet. / The skip connections used in modern neural networks (e.g.: Transformers) are dominantly identity mappings. / DenseNets in 2016 /  were designed as deep neural networks that attempt to connect each layer to every other layer. DenseNets approached this goal by using identity mappings as skip connections. Unlike ResNets: DenseNets merge the layer output with skip connections by concatenation: not addition. / Neural networks with Stochastic Depth /  were made possible given the Residual Network architectures. This training procedure randomly drops a subset of layers and lets the signal propagate through the identity skip connection. Also known as ""DropPath"": this is an effective regularization method for training large and deep models: such as the Vision Transformer (ViT). /  /  / == Biological relation == / The original Residual Network paper made no claim on being inspired by biological systems. But research later on has related Residual Networks to biologically-plausible algorithms. / A study published in Science in 2023 /  disclosed the complete connectome of an insect brain (of a fruit fly larva). This study discovered ""multilayer shortcuts"" that resemble the skip connections in artificial neural networks: including ResNets."
0.16887160000624135,Who are the people behind the development of residual networks?,"Residual networks were developed by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. They were the winners of the 2015 ImageNet competition.","Score: 0.796946892897978 / == Related Work == / In 1961: Frank Rosenblatt described a three-layer multilayer perceptron (MLP) model with skip connections. The model was referred to as a ""cross-coupled system"": and the skip connections were forms of cross-coupled connections. / In two books published in 1994 /  / and 1996: ""skip-layer"" connections were presented in feed-forward MLP models: ""The general definition [of MLP] allows more than one hidden layer: and it also allows 'skip-layer' connections from input to output"" (p261 in: p144 in ): ""... which allows the non-linear units to perturb a linear functional form"" (p262 in ). This description suggests that the non-linear MLP performs like a residual function (perturbation) added to a linear function. / Sepp Hochreiter analyzed the vanishing gradient problem in 1991 and attributed to it the reason why deep learning did not work well. / To overcome this problem: long short-term memory (LSTM) recurrent neural networks / had skip connections or residual connections with a weight of 1.0 in every LSTM cell (called the constant error carrousel) to compute yt+1=F(xt)+xt{\textstyle y_{t+1}=F(x_{t})+x_{t}}. During backpropagation through time: this becomes the above-mentioned residual formula y=F(x)+x{\textstyle y=F(x)+x} for feedforward neural networks. This enables training very deep recurrent neural networks with a very long time span t. A later LSTM version published in 2000 modulates the identity LSTM connections by so-called forget gates such that their weights are not fixed to 1.0 but can be learned. In experiments: the forget gates were initialized with positive bias weights: thus being opened: addressing the vanishing gradient problem. / The highway network of May 2015  / applies these principles to feedforward neural networks. / It was reported to be ""the first very deep feedforward network with hundreds of layers"".  / It is like an LSTM with forget gates unfolded in time: while the later Residual Nets have no equivalent of forget gates and are like the unfolded original LSTM. / If the skip connections in Highway Networks are ""without gates"": or if their gates are kept open (activation 1.0) through strong positive bias weights: they become the identity skip connections in Residual Networks. / The original Highway Network paper not only introduced the basic principle for very deep feedforward networks: but also included experimental results with 20: 50: and 100 layers networks: and mentioned ongoing experiments with up to 900 layers. / Networks with 50 or 100 layers had lower training error than their plain network counterparts: but no lower training error than their 20 layers counterpart (on the MNIST dataset: Figure 1 in ). No improvement on test accuracy was reported with networks deeper than 19 layers (on the CIFAR-10 dataset; Table 1 in ). The ResNet paper: however: provided strong experimental evidence of the benefits of going deeper than 20 layers. It argued that the identity mapping without modulation is crucial and mentioned that modulation in the skip connection can still lead to vanishing signals in forward and backward propagation (Section 3 in ). This is also  why the forget gates of the 2000 LSTM were initially opened through positive bias weights: as long as the gates are open: it behaves like the 1997 LSTM. Similarly: a Highway Net whose gates are opened through strongly positive bias weights behaves like a ResNet. / The skip connections used in modern neural networks (e.g.: Transformers) are dominantly identity mappings. / DenseNets in 2016 /  were designed as deep neural networks that attempt to connect each layer to every other layer. DenseNets approached this goal by using identity mappings as skip connections. Unlike ResNets: DenseNets merge the layer output with skip connections by concatenation: not addition. / Neural networks with Stochastic Depth /  were made possible given the Residual Network architectures. This training procedure randomly drops a subset of layers and lets the signal propagate through the identity skip connection. Also known as ""DropPath"": this is an effective regularization method for training large and deep models: such as the Vision Transformer (ViT). /  /  / == Biological relation == / The original Residual Network paper made no claim on being inspired by biological systems. But research later on has related Residual Networks to biologically-plausible algorithms. / A study published in Science in 2023 /  disclosed the complete connectome of an insect brain (of a fruit fly larva). This study discovered ""multilayer shortcuts"" that resemble the skip connections in artificial neural networks: including ResNets.","Score: 0.7960291026796865 / In 2015: Rupesh Kumar Srivastava: Klaus Greff: and Schmidhuber used LSTM principles to create the Highway network: a feedforward neural network with hundreds of layers: much deeper than previous networks. 7 months later: Kaiming He: Xiangyu Zhang;  Shaoqing Ren: and Jian Sun won the ImageNet 2015 competition with an open-gated or gateless Highway network variant called Residual neural network. This has become the most cited neural network of the 21st century.In 1994: André de Carvalho: together with Mike Fairhurst and David Bisset: published experimental results of a multi-layer boolean neural network: also known as a weightless neural network: composed of a 3-layers self-organising feature extraction neural network module (SOFT) followed by a multi-layer classification neural network module (GSN): which were independently trained. Each layer in the feature extraction module extracted features with growing complexity regarding the previous layer.In 1995: Brendan Frey demonstrated that it was possible to train (over two days) a network containing six fully connected layers and several hundred hidden units using the wake-sleep algorithm: co-developed with Peter Dayan and Hinton.Since 1997: Sven Behnke extended the feed-forward hierarchical convolutional approach in the Neural Abstraction Pyramid by lateral and backward connections in order to flexibly incorporate context into decisions and iteratively resolve local ambiguities. / Simpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) were a popular choice in the 1990s and 2000s: because of artificial neural networks' computational cost and a lack of understanding of how the brain wires its biological networks. / Both shallow and deep learning (e.g.: recurrent nets) of ANNs for speech recognition have been explored for many years. These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively. Key difficulties have been analyzed: including gradient diminishing and weak temporal correlation structure in neural predictive models. Additional difficulties were the lack of training data and limited computing power. Most speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government's NSA and DARPA: SRI studied deep neural networks (DNNs) in speech and speaker recognition. The speaker recognition team led by Larry Heck reported significant success with deep neural networks in speech processing in the 1998 National Institute of Standards and Technology Speaker Recognition evaluation. The SRI deep neural network was then deployed in the Nuance Verifier: representing the first major industrial application of deep learning. The principle of elevating ""raw"" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the ""raw"" spectrogram or linear filter-bank features in the late 1990s: showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech: waveforms: later produced excellent larger-scale results.Speech recognition was taken over by LSTM. In 2003: LSTM started to become competitive with traditional speech recognizers on certain tasks. In 2006: Alex Graves: Santiago Fernández: Faustino Gomez: and Schmidhuber combined it with connectionist temporal classification (CTC) in stacks of LSTM RNNs. In 2015: Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM: which they made available through Google Voice Search.The impact of deep learning in industry began in the early 2000s: when CNNs already processed an estimated 10% to 20% of all the checks written in the US: according to Yann LeCun. Industrial applications of deep learning to large-scale speech recognition started around 2010. / In 2006: publications by Geoff Hinton: Ruslan Salakhutdinov: Osindero and Teh showed how a many-layered feedforward neural network could be effectively pre-trained one layer at a time: treating each layer in turn as an unsupervised restricted Boltzmann machine: then fine-tuning it using supervised backpropagation. The papers referred to learning for deep belief nets. / The 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech: and the possibility that given more capable hardware and large-scale data sets that deep neural nets might become practical."
0.17192829999839887,What is the degradation problem in neural networks?,"The degradation problem refers to the steep reduction in training accuracy that occurs when more layers are stacked in a neural network. Despite the assumption that a deeper network should not produce a higher training loss than its shallower counterpart, the addition of extra layers often contributes to this issue.","Score: 0.8193274545189304 / The original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time: attention focused on matching specific mental abilities: leading to deviations from biology such as backpropagation: or passing information in the reverse direction and adjusting the network to reflect that information. / Neural networks have been used on a variety of tasks: including computer vision: speech recognition: machine translation: social network filtering: playing board and video games and medical diagnosis. / As of 2017: neural networks typically have a few thousand to a few million units and millions of connections. Despite this number being several order of magnitude less than the number of neurons on a human brain: these networks can perform many tasks at a level beyond that of humans (e.g.: recognizing faces: or playing ""Go""). /  /  / === Deep neural networks === / A deep neural network (DNN) is an artificial neural network with multiple layers between the input and output layers. There are different types of neural networks but they always consist of the same components: neurons: synapses: weights: biases: and functions. These components as a whole function in a way that mimics functions of the human brain: and can be trained like any other ML algorithm.For example: a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed. The user can review the results and select which probabilities the network should display (above a certain threshold: etc.) and return the proposed label. Each mathematical manipulation as such is considered a layer: and complex DNN have many layers: hence the name ""deep"" networks. / DNNs can model complex non-linear relationships. DNN architectures generate compositional models where the object is expressed as a layered composition of primitives. The extra layers enable composition of features from lower layers: potentially modeling complex data with fewer units than a similarly performing shallow network. For instance: it was proved that sparse multivariate polynomials are exponentially easier to approximate with DNNs than with shallow networks.Deep architectures include many variants of a few basic approaches. Each architecture has found success in specific domains. It is not always possible to compare the performance of multiple architectures: unless they have been evaluated on the same data sets. / DNNs are typically feedforward networks in which data flows from the input layer to the output layer without looping back. At first: the DNN creates a map of virtual neurons and assigns random numerical values: or ""weights"": to connections between them. The weights and inputs are multiplied and return an output between 0 and 1. If the network did not accurately recognize a particular pattern: an algorithm would adjust the weights. That way the algorithm can make certain parameters more influential: until it determines the correct mathematical manipulation to fully process the data. / Recurrent neural networks: in which data can flow in any direction: are used for applications such as language modeling. Long short-term memory is particularly effective for this use.Convolutional neural networks (CNNs) are used in computer vision. CNNs also have been applied to acoustic modeling for automatic speech recognition (ASR). /  /  / ==== Challenges ==== / As with ANNs: many issues can arise with naively trained DNNs. Two common issues are overfitting and computation time. / DNNs are prone to overfitting because of the added layers of abstraction: which allow them to model rare dependencies in the training data. Regularization methods such as Ivakhnenko's unit pruning or weight decay (ℓ2{\displaystyle \ell _{2}}-regularization) or sparsity (ℓ1{\displaystyle \ell _{1}}-regularization) can be applied during training to combat overfitting. Alternatively dropout regularization randomly omits units from the hidden layers during training. This helps to exclude rare dependencies. Finally: data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.DNNs must consider many training parameters: such as the size (number of layers and number of units per layer): the learning rate: and initial weights. Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources. Various tricks: such as batching (computing the gradient on several training examples at once rather than individual examples) speed up computation. Large processing capabilities of many-core architectures (such as GPUs or the Intel Xeon Phi) have produced significant speedups in training: because of the suitability of such processing architectures for the matrix and vector computations.Alternatively: engineers may look for other types of neural networks with more straightforward and convergent training algorithms.","Score: 0.8177598872184667 / It did so by utilizing weight sharing in combination with backpropagation training. Thus: while also using a pyramidal structure as in the neocognitron: it performed a global optimization of the weights instead of a local one.In 1988: Wei Zhang et al. applied backpropagation  / to a CNN (a simplified Neocognitron with convolutional interconnections between the image feature layers and the last fully connected layer) for alphabet recognition. They also proposed an implementation of the CNN with an optical computing system.In 1989: Yann LeCun et al. trained a CNN with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked: training required 3 days. Learning was fully automatic: performed better than manual coefficient design: and was suited to a broader range of image recognition problems and image types. / Subsequently: Wei Zhang: et al. modified their model by removing the last fully connected layer and applied it for medical image object segmentation in 1991 and breast cancer detection in mammograms in 1994.In 1990 Yamaguchi et al. introduced max-pooling: a fixed filtering operation that calculates and propagates the maximum value of a given region. They combined TDNNs with max-pooling in order to realize a speaker independent isolated word recognition system.  / In a variant of the neocognitron called the cresceptron: instead of using Fukushima's spatial averaging: J. Weng et al. also used max-pooling where a downsampling unit computes the maximum of the activations of the units in its patch. Max-pooling is often used in modern CNNs.LeNet-5: a 7-level CNN by Yann LeCun et al. in 1998: that classifies digits: was applied by several banks to recognize hand-written numbers on checks (British English: cheques) digitized in 32x32 pixel images. The ability to process higher-resolution images requires larger and more layers of CNNs: so this technique is constrained by the availability of computing resources. / In 2010: Backpropagation training through max-pooling was accelerated by GPUs and shown to perform better than other pooling variants. / Behnke (2003) relied only on the sign of the gradient (Rprop) on problems such as image reconstruction and face localization. Rprop is a first-order optimization algorithm created by Martin Riedmiller and Heinrich Braun in 1992.In 2011: a deep GPU-based CNN called ""DanNet"" by Dan Ciresan: Ueli Meier: and Juergen Schmidhuber achieved human-competitive performance for the first time in computer vision contests. Subsequently: a similar GPU-based CNN by Alex Krizhevsky: Ilya Sutskever: and Geoffrey Hinton won the ImageNet Large Scale Visual Recognition Challenge 2012. A very deep CNN with over 100 layers by Kaiming He: Xiangyu Zhang: Shaoqing Ren: and Jian Sun of Microsoft won the ImageNet 2015 contest.ANNs were able to guarantee shift invariance to deal with small and large natural objects in large cluttered scenes: only when invariance extended beyond shift: to all ANN-learned concepts: such as location: type (object class label): scale: lighting and others. This was realized in Developmental Networks (DNs) whose embodiments are Where-What Networks: WWN-1 (2008) through WWN-7 (2013). /  /  / == Artificial curiosity and generative adversarial networks == /  / In 1991: Juergen Schmidhuber published adversarial neural networks that contest with each other in the form of a zero-sum game: where one network's gain is the other network's loss. The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called ""artificial curiosity."" Earlier adversarial machine learning systems ""neither involved unsupervised neural networks nor were about modeling data nor used gradient descent.""In 2014: this adversarial principle was used in a generative adversarial network (GAN) by Ian Goodfellow et al. Here the environmental reaction is 1 or 0 depending on whether the first network's output is in a given set. This can be used to create realistic deepfakes.In 1992: Schmidhuber also published another type of gradient-based adversarial neural networks where the goal of the zero-sum game is to create disentangled representations of input patterns."
0.18407309998292476,Can you explain 'Residual Learning' in a neural network?,"In a residual learning, a residual function is represented by the parameter layers in a multi-layer neural network, re-parameterizing the subnetwork. The residual function is often denoted F(x):=H(x)-x, where H(x) is the underlying function performed by the network and x is the input to the subnetwork. The output y of this network is given by y=F(x)+x. This concept is also applied in the LSTM cell computing.","Score: 0.8798464240041776 / A residual neural network (also referred to as a residual network or ResNet) is a deep learning model in which the weight layers learn residual functions with reference to the layer inputs. It behaves like a highway network whose gates are opened through strongly positive bias weights. This enables deep learning models with tens or hundreds of layers to train easily and approach better accuracy when going deeper. The identity skip connections: often referred to as ""residual connections"": are also used in the 1997 LSTM networks: Transformer models (e.g.: BERT: GPT models such as ChatGPT): the AlphaGo Zero system: the AlphaStar system: and the AlphaFold system. / Residual networks were developed by Kaiming He: Xiangyu Zhang: Shaoqing Ren: and Jian Sun: who won the 2015 ImageNet competition. /  /  / == Formulation == /  /  / === Background === / The AlexNet model developed in 2012 for ImageNet was an eight-layer convolutional neural network. / The neural networks developed in 2014 by the Visual Geometry Group (VGG) at the University of Oxford approached a depth of 19 layers by stacking 3-by-3 convolutional layers. / However: stacking more layers led to a steep reduction in training accuracy: which is referred to as the ""degradation"" problem.A deeper network should not produce a higher training loss than its shallower counterpart: if this deeper network can be constructed by its shallower counterpart stacked with extra layers. If the extra layers can be set as identity mappings: the deeper network would represent the same function as the shallower counterpart. It is hypothesized that the optimizer is not able to approach identity mappings for the parameterized layers. /  /  / === Residual learning === / In a multi-layer neural network model: consider a subnetwork with a certain number (e.g.: 2 or 3) of stacked layers. Denote the underlying function performed by this subnetwork as H(x){\textstyle H(x)}: where x{\textstyle x} is the input to this subnetwork. / The idea of ""Residual Learning"" re-parameterizes this subnetwork and lets the parameter layers represent a residual function F(x):=H(x)−x{\textstyle F(x):=H(x)-x}. / The output y{\textstyle y} of this subnetwork is represented as: /  / y=F(x)+x{\displaystyle {\begin{aligned}y&=F(x)+x\end{aligned}}}This is also the principle of the 1997 LSTM cell computing yt+1=F(xt)+xt{\textstyle y_{t+1}=F(x_{t})+x_{t}}: which becomes y=F(x)+x{\textstyle y=F(x)+x} during backpropagation through time. / The function F(x){\textstyle F(x)} is often represented by matrix multiplication interlaced with activation functions and normalization operations (e.g.: Batch Normalization or Layer Normalization). / This subnetwork is referred to as a ""Residual Block"". A deep residual network is constructed by stacking a series of residual blocks. / The operation of ""+ x{\textstyle +\ x}"" in ""y=F(x)+x{\textstyle y=F(x)+x}"" is approached by a skip connection that performs identity mapping and connects the input of a residual block with its output. This connection is often referred to as a ""Residual Connection"" in later work. /  /  / === Signal propagation === / The introduction of identity mappings facilitates signal propagation in both forward and backward paths.","Score: 0.857601883575385 / == Related Work == / In 1961: Frank Rosenblatt described a three-layer multilayer perceptron (MLP) model with skip connections. The model was referred to as a ""cross-coupled system"": and the skip connections were forms of cross-coupled connections. / In two books published in 1994 /  / and 1996: ""skip-layer"" connections were presented in feed-forward MLP models: ""The general definition [of MLP] allows more than one hidden layer: and it also allows 'skip-layer' connections from input to output"" (p261 in: p144 in ): ""... which allows the non-linear units to perturb a linear functional form"" (p262 in ). This description suggests that the non-linear MLP performs like a residual function (perturbation) added to a linear function. / Sepp Hochreiter analyzed the vanishing gradient problem in 1991 and attributed to it the reason why deep learning did not work well. / To overcome this problem: long short-term memory (LSTM) recurrent neural networks / had skip connections or residual connections with a weight of 1.0 in every LSTM cell (called the constant error carrousel) to compute yt+1=F(xt)+xt{\textstyle y_{t+1}=F(x_{t})+x_{t}}. During backpropagation through time: this becomes the above-mentioned residual formula y=F(x)+x{\textstyle y=F(x)+x} for feedforward neural networks. This enables training very deep recurrent neural networks with a very long time span t. A later LSTM version published in 2000 modulates the identity LSTM connections by so-called forget gates such that their weights are not fixed to 1.0 but can be learned. In experiments: the forget gates were initialized with positive bias weights: thus being opened: addressing the vanishing gradient problem. / The highway network of May 2015  / applies these principles to feedforward neural networks. / It was reported to be ""the first very deep feedforward network with hundreds of layers"".  / It is like an LSTM with forget gates unfolded in time: while the later Residual Nets have no equivalent of forget gates and are like the unfolded original LSTM. / If the skip connections in Highway Networks are ""without gates"": or if their gates are kept open (activation 1.0) through strong positive bias weights: they become the identity skip connections in Residual Networks. / The original Highway Network paper not only introduced the basic principle for very deep feedforward networks: but also included experimental results with 20: 50: and 100 layers networks: and mentioned ongoing experiments with up to 900 layers. / Networks with 50 or 100 layers had lower training error than their plain network counterparts: but no lower training error than their 20 layers counterpart (on the MNIST dataset: Figure 1 in ). No improvement on test accuracy was reported with networks deeper than 19 layers (on the CIFAR-10 dataset; Table 1 in ). The ResNet paper: however: provided strong experimental evidence of the benefits of going deeper than 20 layers. It argued that the identity mapping without modulation is crucial and mentioned that modulation in the skip connection can still lead to vanishing signals in forward and backward propagation (Section 3 in ). This is also  why the forget gates of the 2000 LSTM were initially opened through positive bias weights: as long as the gates are open: it behaves like the 1997 LSTM. Similarly: a Highway Net whose gates are opened through strongly positive bias weights behaves like a ResNet. / The skip connections used in modern neural networks (e.g.: Transformers) are dominantly identity mappings. / DenseNets in 2016 /  were designed as deep neural networks that attempt to connect each layer to every other layer. DenseNets approached this goal by using identity mappings as skip connections. Unlike ResNets: DenseNets merge the layer output with skip connections by concatenation: not addition. / Neural networks with Stochastic Depth /  were made possible given the Residual Network architectures. This training procedure randomly drops a subset of layers and lets the signal propagate through the identity skip connection. Also known as ""DropPath"": this is an effective regularization method for training large and deep models: such as the Vision Transformer (ViT). /  /  / == Biological relation == / The original Residual Network paper made no claim on being inspired by biological systems. But research later on has related Residual Networks to biologically-plausible algorithms. / A study published in Science in 2023 /  disclosed the complete connectome of an insect brain (of a fruit fly larva). This study discovered ""multilayer shortcuts"" that resemble the skip connections in artificial neural networks: including ResNets."
0.17329829998197965,What is a Transformer Block?,"A Transformer Block is a stack of two Residual Blocks, each with a Residual Connection. It consists first of a Multi-Head Attention Block, which performs (self-)attention computation followed by a linear projection. The second block is a feed-forward Multi-Layer Perceptron (MLP) Block, which increases and then reduces the dimension through linear projections. The GPT-3 model, for instance, has 96 Transformer Blocks, amounting to a depth of about 400 projection layers. Very deep Transformer models cannot be successfully trained without Residual Connections.","Score: 0.7754918411868461 / Generative pre-trained transformers (GPT) are a type of large language model (LLM) and a prominent framework for generative artificial intelligence. They are artificial neural networks that are used in natural language processing tasks. GPTs are based on the transformer architecture: pre-trained on large data sets of unlabelled text: and able to generate novel human-like content. As of 2023: most LLMs have these characteristics and are sometimes referred to broadly as GPTs.The first GPT was introduced in 2018 by OpenAI. OpenAI has released very influential GPT foundation models that have been sequentially numbered: to comprise its ""GPT-n"" series. Each of these was significantly more capable than the previous: due to increased size (number of trainable parameters) and training. The most recent of these: GPT-4: was released in March 2023. Such models have been the basis for their more task-specific GPT systems: including models fine-tuned for instruction following—which in turn power the ChatGPT chatbot service.The term ""GPT"" is also used in the names and descriptions of such models developed by others. For example: other GPT foundation models include a series of models created by EleutherAI: and seven models created by Cerebras in 2023. Also: companies in different industries have developed task-specific GPTs in their respective fields: such as Salesforce's ""EinsteinGPT"" (for CRM) and Bloomberg's ""BloombergGPT"" (for finance). /  /  / == History == /  /  / === Initial developments === / Generative pretraining (GP) was a long-established concept in machine learning applications. It was originally used as a form of semi-supervised learning: as the model is trained first on an unlabelled dataset (pretraining step) by learning to generate datapoints in the dataset: and then it is trained to classify a labelled dataset.While the unnormalized linear transformer dates back to 1992: the modern transformer architecture was not available until 2017 when it was published by researchers at Google in a paper ""Attention Is All You Need"". That development led to the emergence of large language models such as BERT in 2018 which was a pre-trained transformer (PT) but not designed to be generative (BERT was an ""encoder-only"" model). Also around that time: in 2018: OpenAI published its article entitled ""Improving Language Understanding by Generative Pre-Training:"" in which it introduced the first generative pre-trained transformer (GPT) system (""GPT-1"").Prior to transformer-based architectures: the best-performing neural NLP (natural language processing) models commonly employed supervised learning from large amounts of manually-labeled data. The reliance on supervised learning limited their use on datasets that were not well-annotated: and also made it prohibitively expensive and time-consuming to train extremely large language models.The semi-supervised approach OpenAI employed to make a large-scale generative system—and was first to do with a transformer model—involved two stages: an unsupervised generative ""pretraining"" stage to set initial parameters using a language modeling objective: and a supervised discriminative ""fine-tuning"" stage to adapt these parameters to a target task. /  /  / === Later developments === / Regarding more recent GPT foundation models: OpenAI published its first versions of GPT-3 in July 2020. There were three models: with 1B: 6.7B: 175B parameters: respectively named babbage: curie: and davinci (giving initials B: C: and D).In July 2021: OpenAI published Codex: a task-specific GPT model targeted for programming applications. This was developed by fine-tuning a 12B parameter version of GPT-3 (different from previous GPT-3 models) using code from GitHub.In March 2022: OpenAI published two versions of GPT-3 that were fine-tuned for instruction-following (instruction-tuned): named davinci-instruct-beta (175B) and text-davinci-001: and then started beta testing code-davinci-002. text-davinci-002 was instruction-tuned from code-davinci-002. Both text-davinci-003 and ChatGPT were released in November 2022: with both building upon text-davinci-002 via reinforcement learning from human feedback (RLHF).","Score: 0.77182874030786 / This formulation suggests that the gradient computation of a shallower layer  / ∂E∂xℓ{\textstyle {\frac {\partial {\mathcal {E}}}{\partial x_{\ell }}}} / always has a term ∂E∂xL{\textstyle {\frac {\partial {\mathcal {E}}}{\partial x_{L}}}} that is directly added. Even if the gradients of the F(xi){\textstyle F(x_{i})} terms are small: the total gradient ∂E∂xℓ{\textstyle {\frac {\partial {\mathcal {E}}}{\partial x_{\ell }}}} is not vanishing thanks to the added term ∂E∂xL{\textstyle {\frac {\partial {\mathcal {E}}}{\partial x_{L}}}}. /  /  / == Variants of residual blocks == /  /  / === Basic block === / A Basic Block is the simplest building block studied in the original ResNet. This block consists of two sequential 3x3 convolutional layers and a residual connection. The input and output dimensions of both layers are equal. /  /  / === Bottleneck block === / A Bottleneck Block consists of three sequential convolutional layers and a residual connection. The first layer in this block is a 1x1 convolution for dimension reduction: e.g.: to 1/4 of the input dimension; the second layer performs a 3x3 convolution; the last layer is another 1x1 convolution for dimension restoration. The models of ResNet-50: ResNet-101: and ResNet-152 in  are all based on Bottleneck Blocks. /  /  / === Pre-activation block === / The Pre-activation Residual Block applies the activation functions (e.g.: non-linearity and normalization) before applying the residual function F{\textstyle F}. Formally: the computation of a Pre-activation Residual Block can be written as: /  / xℓ+1=F(ϕ(xℓ))+xℓ{\displaystyle {\begin{aligned}x_{\ell +1}&=F(\phi (x_{\ell }))+x_{\ell }\end{aligned}}}where ϕ{\textstyle \phi } can be any non-linearity activation (e.g.: ReLU) or normalization (e.g.: LayerNorm) operation. This design reduces the number of non-identity mappings between Residual Blocks. This design was used to train models with 200 to over 1000 layers.Since GPT-2: the Transformer Blocks have been dominantly implemented as Pre-activation Blocks. This is often referred to as ""pre-normalization"" in the literature of Transformer models. /  /  / === Transformer block === / A Transformer Block is a stack of two Residual Blocks. Each Residual Block has a Residual Connection. / The first Residual Block is a Multi-Head Attention Block: which performs (self-)attention computation followed by a linear projection. / The second Residual Block is a feed-forward Multi-Layer Perceptron (MLP) Block. This block is analogous to an ""inverse"" bottleneck block: it has a linear projection layer (which is equivalent to a 1x1 convolution in the context of Convolutional Neural Networks) that increases the dimension: and another linear projection that reduces the dimension. / A Transformer Block has a depth of 4 layers (linear projections). / The GPT-3 model has 96 Transformer Blocks (in the literature of Transformers: a Transformer Block is often referred to as a ""Transformer Layer""). This model has a depth of about 400 projection layers: including 96x4 layers in Transformer Blocks and a few extra layers for input embedding and output prediction. / Very deep Transformer models cannot be successfully trained without Residual Connections. /  /  / == Related Work == / In 1961: Frank Rosenblatt described a three-layer multilayer perceptron (MLP) model with skip connections. The model was referred to as a ""cross-coupled system"": and the skip connections were forms of cross-coupled connections. / In two books published in 1994 /  / and 1996: ""skip-layer"" connections were presented in feed-forward MLP models: ""The general definition [of MLP] allows more than one hidden layer: and it also allows 'skip-layer' connections from input to output"" (p261 in: p144 in ): ""... which allows the non-linear units to perturb a linear functional form"" (p262 in ). This description suggests that the non-linear MLP performs like a residual function (perturbation) added to a linear function. / Sepp Hochreiter analyzed the vanishing gradient problem in 1991 and attributed to it the reason why deep learning did not work well."
0.15665530000114813,What is a Siamese neural network?,"A Siamese neural network, also known as a twin neural network, is an artificial neural network that uses the same weights while operating on two different input vectors to compute comparable output vectors. It is often used for comparing similar instances in different type sets and in applications such as face recognition and matching queries with indexed documents.","Score: 0.8613465767668939 / A Siamese neural network (sometimes called a twin neural network) is an artificial neural network that uses the same weights while working in tandem on two different input vectors to compute comparable output vectors. Often one of the output vectors is precomputed: thus forming a baseline against which the other output vector is compared. This is similar to comparing fingerprints but can be described more technically as a distance function for locality-sensitive hashing.It is possible to build an architecture that is functionally similar to a siamese network but implements a slightly different function. This is typically used for comparing similar instances in different type sets.Uses of similarity measures where a twin network might be used are such things as recognizing handwritten checks: automatic detection of faces in camera images: and matching queries with indexed documents. The perhaps most well-known application of twin networks are face recognition: where known images of people are precomputed and compared to an image from a turnstile or similar. It is not obvious at first: but there are two slightly different problems. One is recognizing a person among a large number of other persons: that is the facial recognition problem. DeepFace is an example of such a system. In its most extreme form this is recognizing a single person at a train station or airport. The other is face verification: that is to verify whether the photo in a pass is the same as the person claiming he or she is the same person. The twin network might be the same: but the implementation can be quite different. /  /  / == Learning == / Learning in twin networks can be done with triplet loss or contrastive loss. For learning by triplet loss a baseline vector (anchor image) is compared against a positive vector (truthy image) and a negative vector (falsy image). The negative vector will force learning in the network: while the positive vector will act like a regularizer. For learning by contrastive loss there must be a weight decay to regularize the weights: or some similar operation like a normalization. / A distance metric for a loss function may have the following properties / Non-negativity: δ(x:y)≥0{\displaystyle \delta (x:y)\geq 0} / Identity of Non-discernibles: δ(x:y)=0⟺x=y{\displaystyle \delta (x:y)=0\iff x=y} / Commutativity: δ(x:y)=δ(y:x){\displaystyle \delta (x:y)=\delta (y:x)} / Triangle inequality: δ(x:z)≤δ(x:y)+δ(y:z){\displaystyle \delta (x:z)\leq \delta (x:y)+\delta (y:z)}In particular: the triplet loss algorithm is often defined with squared Euclidean (which unlike Euclidean: does not have triangle inequality) distance at its core. /  /  / === Predefined metrics: Euclidean distance metric === / The common learning goal is to minimize a distance metric for similar objects and maximize for distinct ones.","Score: 0.8546353969647462 / In object tracking: one input of the twin network is user pre-selected exemplar image: the other input is a larger search image: which twin network's job is to locate exemplar inside of search image. By measuring the similarity between exemplar and each part of the search image: a map of similarity score can be given by the twin network. Furthermore: using a Fully Convolutional Network: the process of computing each sector's similarity score can be replaced with only one cross correlation layer.After being first introduced in 2016: Twin fully convolutional network has been used in many High-performance Real-time Object Tracking Neural Networks.  Like CFnet: StructSiam: SiamFC-tri: DSiam: SA-Siam: SiamRPN: DaSiamRPN: Cascaded SiamRPN: SiamMask: SiamRPN++: Deeper and Wider SiamRPN. /  /  / == See also == / Artificial neural network / Triplet loss /  /  / == Further reading == / Chicco: Davide (2020): ""Siamese neural networks: an overview"": Artificial Neural Networks: Methods in Molecular Biology: vol. 2190 (3rd ed.): New York City: New York: USA: Springer Protocols: Humana Press: pp. 73–94: doi:10.1007/978-1-0716-0826-5_3: ISBN 978-1-0716-0826-5: PMID 32804361: S2CID 221144012 /  /  / == References =="
0.1641228999942541,How is learning conducted in twin networks?,"Learning in twin networks can be achieved through two methods: triplet loss or contrastive loss. For learning by triplet loss, a baseline vector, positive vector, and negative vector are used where the negative vector pushes learning in the network and the positive vector acts as a regularizer. For learning by contrastive loss, there must be a weight decay to regularize the weights.","Score: 0.8116160540231624 / A Siamese neural network (sometimes called a twin neural network) is an artificial neural network that uses the same weights while working in tandem on two different input vectors to compute comparable output vectors. Often one of the output vectors is precomputed: thus forming a baseline against which the other output vector is compared. This is similar to comparing fingerprints but can be described more technically as a distance function for locality-sensitive hashing.It is possible to build an architecture that is functionally similar to a siamese network but implements a slightly different function. This is typically used for comparing similar instances in different type sets.Uses of similarity measures where a twin network might be used are such things as recognizing handwritten checks: automatic detection of faces in camera images: and matching queries with indexed documents. The perhaps most well-known application of twin networks are face recognition: where known images of people are precomputed and compared to an image from a turnstile or similar. It is not obvious at first: but there are two slightly different problems. One is recognizing a person among a large number of other persons: that is the facial recognition problem. DeepFace is an example of such a system. In its most extreme form this is recognizing a single person at a train station or airport. The other is face verification: that is to verify whether the photo in a pass is the same as the person claiming he or she is the same person. The twin network might be the same: but the implementation can be quite different. /  /  / == Learning == / Learning in twin networks can be done with triplet loss or contrastive loss. For learning by triplet loss a baseline vector (anchor image) is compared against a positive vector (truthy image) and a negative vector (falsy image). The negative vector will force learning in the network: while the positive vector will act like a regularizer. For learning by contrastive loss there must be a weight decay to regularize the weights: or some similar operation like a normalization. / A distance metric for a loss function may have the following properties / Non-negativity: δ(x:y)≥0{\displaystyle \delta (x:y)\geq 0} / Identity of Non-discernibles: δ(x:y)=0⟺x=y{\displaystyle \delta (x:y)=0\iff x=y} / Commutativity: δ(x:y)=δ(y:x){\displaystyle \delta (x:y)=\delta (y:x)} / Triangle inequality: δ(x:z)≤δ(x:y)+δ(y:z){\displaystyle \delta (x:z)\leq \delta (x:y)+\delta (y:z)}In particular: the triplet loss algorithm is often defined with squared Euclidean (which unlike Euclidean: does not have triangle inequality) distance at its core. /  /  / === Predefined metrics: Euclidean distance metric === / The common learning goal is to minimize a distance metric for similar objects and maximize for distinct ones.","Score: 0.8114207513338262 / In object tracking: one input of the twin network is user pre-selected exemplar image: the other input is a larger search image: which twin network's job is to locate exemplar inside of search image. By measuring the similarity between exemplar and each part of the search image: a map of similarity score can be given by the twin network. Furthermore: using a Fully Convolutional Network: the process of computing each sector's similarity score can be replaced with only one cross correlation layer.After being first introduced in 2016: Twin fully convolutional network has been used in many High-performance Real-time Object Tracking Neural Networks.  Like CFnet: StructSiam: SiamFC-tri: DSiam: SA-Siam: SiamRPN: DaSiamRPN: Cascaded SiamRPN: SiamMask: SiamRPN++: Deeper and Wider SiamRPN. /  /  / == See also == / Artificial neural network / Triplet loss /  /  / == Further reading == / Chicco: Davide (2020): ""Siamese neural networks: an overview"": Artificial Neural Networks: Methods in Molecular Biology: vol. 2190 (3rd ed.): New York City: New York: USA: Springer Protocols: Humana Press: pp. 73–94: doi:10.1007/978-1-0716-0826-5_3: ISBN 978-1-0716-0826-5: PMID 32804361: S2CID 221144012 /  /  / == References =="
0.1361089000129141,What is a common goal during learning and what is a frequently used metric?,The common goal during learning is to minimize a distance metric for similar objects and to maximize it for distinct ones. The most common distance metric used is the Euclidean distance. ,"Score: 0.7978369929201922 / A distance metric for a loss function may have the following properties / Non-negativity: δ(x:y)≥0{\displaystyle \delta (x:y)\geq 0} / Identity of Non-discernibles: δ(x:y)=0⟺x=y{\displaystyle \delta (x:y)=0\iff x=y} / Commutativity: δ(x:y)=δ(y:x){\displaystyle \delta (x:y)=\delta (y:x)} / Triangle inequality: δ(x:z)≤δ(x:y)+δ(y:z){\displaystyle \delta (x:z)\leq \delta (x:y)+\delta (y:z)}In particular: the triplet loss algorithm is often defined with squared Euclidean (which unlike Euclidean: does not have triangle inequality) distance at its core. /  /  / === Predefined metrics: Euclidean distance metric === / The common learning goal is to minimize a distance metric for similar objects and maximize for distinct ones. This gives a loss function like /  / δ(x(i):x(j))={min ‖f⁡(x(i))−f⁡(x(j))‖:i=jmax ‖f⁡(x(i))−f⁡(x(j))‖:i≠j{\displaystyle {\begin{aligned}\delta (x^{(i)}:x^{(j)})={\begin{cases}\min \ \|\operatorname {f} \left(x^{(i)}\right)-\operatorname {f} \left(x^{(j)}\right)\|\::i=j\\\max \ \|\operatorname {f} \left(x^{(i)}\right)-\operatorname {f} \left(x^{(j)}\right)\|\::i\neq j\end{cases}}\end{aligned}}} / i:j{\displaystyle i:j} are indexes into a set of vectors / f⁡(⋅){\displaystyle \operatorname {f} (\cdot )} function implemented by the twin networkThe most common distance metric used is Euclidean distance: in case of which the loss function can be rewritten in matrix form as /  / δ⁡(x(i):x(j))≈(x(i)−x(j))T(x(i)−x(j)){\displaystyle \operatorname {\delta } (\mathbf {x} ^{(i)}:\mathbf {x} ^{(j)})\approx (\mathbf {x} ^{(i)}-\mathbf {x} ^{(j)})^{T}(\mathbf {x} ^{(i)}-\mathbf {x} ^{(j)})} /  /  / === Learned metrics: nonlinear distance metric === / A more general case is where the output vector from the twin network is passed through additional network layers implementing non-linear distance metrics.","Score: 0.7926681775509696 / Otherwise: the task is considered ""closed book"": and the model must draw on knowledge retained during training. Some examples of commonly used question answering datasets include TruthfulQA: Web Questions: TriviaQA: and SQuAD.Evaluation datasets may also take the form of text completion: having the model select the most likely word or sentence to complete a prompt: for example: ""Alice was friends with Bob. Alice went to visit her friend: ____"".Some composite benchmarks have also been developed which combine a diversity of different evaluation datasets and tasks. Examples include GLUE: SuperGLUE: MMLU: BIG-bench: and HELM.It was previously standard to report results on a heldout portion of an evaluation dataset after doing supervised fine-tuning on the remainder. It is now more common to evaluate a pre-trained model directly through prompting techniques: though researchers vary in the details of how they formulate prompts for particular tasks: particularly with respect to how many examples of solved tasks are adjoined to the prompt (i.e. the value of n in n-shot prompting). /  /  / ==== Adversarially constructed evaluations ==== / Because of the rapid pace of improvement of large language models: evaluation benchmarks have suffered from short lifespans: with state of the art models quickly ""saturating"" existing benchmarks: exceeding the performance of human annotators: leading to efforts to replace or augment the benchmark with more challenging tasks. In addition: there are cases of ""shortcut learning"" wherein AIs sometimes ""cheat"" on multiple-choice tests by using statistical correlations in superficial test question wording in order to guess the correct responses: without necessarily understanding the actual question being asked.Some datasets have been constructed adversarially: focusing on particular problems on which extant language models seem to have unusually poor performance compared to humans. One example is the TruthfulQA dataset: a question answering dataset consisting of 817 questions which language models are susceptible to answering incorrectly by mimicking falsehoods to which they were repeatedly exposed during training. For example: an LLM may answer ""No"" to the question ""Can you teach an old dog new tricks?"" because of its exposure to the English idiom you can't teach an old dog new tricks: even though this is not literally true.Another example of an adversarial evaluation dataset is Swag and its successor: HellaSwag: collections of problems in which one of multiple options must be selected to complete a text passage. The incorrect completions were generated by sampling from a language model and filtering with a set of classifiers. The resulting problems are trivial for humans but at the time the datasets were created state of the art language models had poor accuracy on them. For example: /  / We see a fitness center sign. We then see a man talking to the camera and sitting and laying on a exercise ball. The man... / a) demonstrates how to increase efficient exercise work by running up and down balls. / b) moves all his arms and legs and builds up a lot of muscle. / c) then plays the ball and we see a graphics and hedge trimming demonstration. / d) performs sit ups while on the ball and talking. /  / BERT selects b) as the most likely completion: though the correct answer is d). /  /  / == Wider impact == / In 2023: Nature Biomedical Engineering wrote that ""it is no longer possible to accurately distinguish"" human-written text from text created by large language models: and that ""It is all but certain that general-purpose large language models will rapidly proliferate... It is a rather safe bet that they will change many industries over time."" Goldman Sachs suggested in 2023 that generative language AI could increase global GDP by 7% in the next ten years: and could expose to automation 300 million jobs globally. /  /  / === Copyright === / Memorization is an emergent behavior in LLMs in which long strings of text are occasionally output verbatim from training data: contrary to typical behavior of traditional artificial neural nets. Evaluations of controlled LLM output measure the amount memorized from training data (focused on GPT-2-series models) as variously over 1% for exact duplicates or up to about 7%. /  /  / === Security === / Some commenters expressed concern over accidental or deliberate creation of misinformation: or other forms of misuse. For example: the availability of large language models could reduce the skill-level required to commit bioterrorism; biosecurity researcher Kevin Esvelt has suggested that LLM creators should exclude from their training data papers on creating or enhancing pathogens.A study by researchers at Google and several universities: including Cornell University and University of California: Berkeley: showed that there are potential security risks in language models such as ChatGPT."
0.17654329998185858,How is a 'half-twin' network different from a twin network?,"While similar to a twin network, a 'half-twin' network implements slightly different functions. The delta function (distance calculation) between the two functions implemented by 'half-twin' network varies based on whether the indexes of the two vectors are the same or different.","Score: 0.7977960355517532 / ifi=jthenδ⁡[f⁡(x(i)):f⁡(x(j))]is smallotherwiseδ⁡[f⁡(x(i)):f⁡(x(j))]is large{\displaystyle {\begin{aligned}{\text{if}}\:i=j\:{\text{then}}&\:\operatorname {\delta } \left[\operatorname {f} \left(x^{(i)}\right):\:\operatorname {f} \left(x^{(j)}\right)\right]\:{\text{is small}}\\{\text{otherwise}}&\:\operatorname {\delta } \left[\operatorname {f} \left(x^{(i)}\right):\:\operatorname {f} \left(x^{(j)}\right)\right]\:{\text{is large}}\end{aligned}}} / i:j{\displaystyle i:j} are indexes into a set of vectors / f⁡(⋅){\displaystyle \operatorname {f} (\cdot )}function implemented by the twin network / δ⁡(⋅){\displaystyle \operatorname {\delta } (\cdot )}function implemented by the network joining outputs from the twin networkOn a matrix form the previous is often approximated as a Mahalanobis distance for a linear space as / δ⁡(x(i):x(j))≈(x(i)−x(j))TM(x(i)−x(j)){\displaystyle \operatorname {\delta } (\mathbf {x} ^{(i)}:\mathbf {x} ^{(j)})\approx (\mathbf {x} ^{(i)}-\mathbf {x} ^{(j)})^{T}\mathbf {M} (\mathbf {x} ^{(i)}-\mathbf {x} ^{(j)})}This can be further subdivided in at least Unsupervised learning and Supervised learning. /  /  / === Learned metrics: half-twin networks === / This form also allows the twin network to be more of a half-twin: implementing a slightly different functions /  / ifi=jthenδ⁡[f⁡(x(i)):g⁡(x(j))]is smallotherwiseδ⁡[f⁡(x(i)):g⁡(x(j))]is large{\displaystyle {\begin{aligned}{\text{if}}\:i=j\:{\text{then}}&\:\operatorname {\delta } \left[\operatorname {f} \left(x^{(i)}\right):\:\operatorname {g} \left(x^{(j)}\right)\right]\:{\text{is small}}\\{\text{otherwise}}&\:\operatorname {\delta } \left[\operatorname {f} \left(x^{(i)}\right):\:\operatorname {g} \left(x^{(j)}\right)\right]\:{\text{is large}}\end{aligned}}} / i:j{\displaystyle i:j} are indexes into a set of vectors / f⁡(⋅):g⁡(⋅){\displaystyle \operatorname {f} (\cdot ):\operatorname {g} (\cdot )}function implemented by the half-twin network / δ⁡(⋅){\displaystyle \operatorname {\delta } (\cdot )}function implemented by the network joining outputs from the twin network /  /  / == Twin networks for object tracking == / Twin networks have been used in object tracking because of its unique two tandem inputs and similarity measurement. In object tracking: one input of the twin network is user pre-selected exemplar image: the other input is a larger search image: which twin network's job is to locate exemplar inside of search image. By measuring the similarity between exemplar and each part of the search image: a map of similarity score can be given by the twin network. Furthermore: using a Fully Convolutional Network: the process of computing each sector's similarity score can be replaced with only one cross correlation layer.After being first introduced in 2016: Twin fully convolutional network has been used in many High-performance Real-time Object Tracking Neural Networks.  Like CFnet: StructSiam: SiamFC-tri: DSiam: SA-Siam: SiamRPN: DaSiamRPN: Cascaded SiamRPN: SiamMask: SiamRPN++: Deeper and Wider SiamRPN.","Score: 0.7928182503750939 / A Siamese neural network (sometimes called a twin neural network) is an artificial neural network that uses the same weights while working in tandem on two different input vectors to compute comparable output vectors. Often one of the output vectors is precomputed: thus forming a baseline against which the other output vector is compared. This is similar to comparing fingerprints but can be described more technically as a distance function for locality-sensitive hashing.It is possible to build an architecture that is functionally similar to a siamese network but implements a slightly different function. This is typically used for comparing similar instances in different type sets.Uses of similarity measures where a twin network might be used are such things as recognizing handwritten checks: automatic detection of faces in camera images: and matching queries with indexed documents. The perhaps most well-known application of twin networks are face recognition: where known images of people are precomputed and compared to an image from a turnstile or similar. It is not obvious at first: but there are two slightly different problems. One is recognizing a person among a large number of other persons: that is the facial recognition problem. DeepFace is an example of such a system. In its most extreme form this is recognizing a single person at a train station or airport. The other is face verification: that is to verify whether the photo in a pass is the same as the person claiming he or she is the same person. The twin network might be the same: but the implementation can be quite different. /  /  / == Learning == / Learning in twin networks can be done with triplet loss or contrastive loss. For learning by triplet loss a baseline vector (anchor image) is compared against a positive vector (truthy image) and a negative vector (falsy image). The negative vector will force learning in the network: while the positive vector will act like a regularizer. For learning by contrastive loss there must be a weight decay to regularize the weights: or some similar operation like a normalization. / A distance metric for a loss function may have the following properties / Non-negativity: δ(x:y)≥0{\displaystyle \delta (x:y)\geq 0} / Identity of Non-discernibles: δ(x:y)=0⟺x=y{\displaystyle \delta (x:y)=0\iff x=y} / Commutativity: δ(x:y)=δ(y:x){\displaystyle \delta (x:y)=\delta (y:x)} / Triangle inequality: δ(x:z)≤δ(x:y)+δ(y:z){\displaystyle \delta (x:z)\leq \delta (x:y)+\delta (y:z)}In particular: the triplet loss algorithm is often defined with squared Euclidean (which unlike Euclidean: does not have triangle inequality) distance at its core. /  /  / === Predefined metrics: Euclidean distance metric === / The common learning goal is to minimize a distance metric for similar objects and maximize for distinct ones."
0.13064940000185743,How are twin networks used in object tracking?,"Twin networks have been used in object tracking due to their unique two tandem inputs and similarity measurement capabilities. One input of the twin network is a user pre-selected exemplar image, and the other is a larger search image. The twin network's job is to locate the exemplar image within the search image by measuring the similarity between the exemplar and each part of the search image, thereby producing a map of similarity scores.","Score: 0.8476586286611202 / In object tracking: one input of the twin network is user pre-selected exemplar image: the other input is a larger search image: which twin network's job is to locate exemplar inside of search image. By measuring the similarity between exemplar and each part of the search image: a map of similarity score can be given by the twin network. Furthermore: using a Fully Convolutional Network: the process of computing each sector's similarity score can be replaced with only one cross correlation layer.After being first introduced in 2016: Twin fully convolutional network has been used in many High-performance Real-time Object Tracking Neural Networks.  Like CFnet: StructSiam: SiamFC-tri: DSiam: SA-Siam: SiamRPN: DaSiamRPN: Cascaded SiamRPN: SiamMask: SiamRPN++: Deeper and Wider SiamRPN. /  /  / == See also == / Artificial neural network / Triplet loss /  /  / == Further reading == / Chicco: Davide (2020): ""Siamese neural networks: an overview"": Artificial Neural Networks: Methods in Molecular Biology: vol. 2190 (3rd ed.): New York City: New York: USA: Springer Protocols: Humana Press: pp. 73–94: doi:10.1007/978-1-0716-0826-5_3: ISBN 978-1-0716-0826-5: PMID 32804361: S2CID 221144012 /  /  / == References ==","Score: 0.8222079004677163 / ifi=jthenδ⁡[f⁡(x(i)):f⁡(x(j))]is smallotherwiseδ⁡[f⁡(x(i)):f⁡(x(j))]is large{\displaystyle {\begin{aligned}{\text{if}}\:i=j\:{\text{then}}&\:\operatorname {\delta } \left[\operatorname {f} \left(x^{(i)}\right):\:\operatorname {f} \left(x^{(j)}\right)\right]\:{\text{is small}}\\{\text{otherwise}}&\:\operatorname {\delta } \left[\operatorname {f} \left(x^{(i)}\right):\:\operatorname {f} \left(x^{(j)}\right)\right]\:{\text{is large}}\end{aligned}}} / i:j{\displaystyle i:j} are indexes into a set of vectors / f⁡(⋅){\displaystyle \operatorname {f} (\cdot )}function implemented by the twin network / δ⁡(⋅){\displaystyle \operatorname {\delta } (\cdot )}function implemented by the network joining outputs from the twin networkOn a matrix form the previous is often approximated as a Mahalanobis distance for a linear space as / δ⁡(x(i):x(j))≈(x(i)−x(j))TM(x(i)−x(j)){\displaystyle \operatorname {\delta } (\mathbf {x} ^{(i)}:\mathbf {x} ^{(j)})\approx (\mathbf {x} ^{(i)}-\mathbf {x} ^{(j)})^{T}\mathbf {M} (\mathbf {x} ^{(i)}-\mathbf {x} ^{(j)})}This can be further subdivided in at least Unsupervised learning and Supervised learning. /  /  / === Learned metrics: half-twin networks === / This form also allows the twin network to be more of a half-twin: implementing a slightly different functions /  / ifi=jthenδ⁡[f⁡(x(i)):g⁡(x(j))]is smallotherwiseδ⁡[f⁡(x(i)):g⁡(x(j))]is large{\displaystyle {\begin{aligned}{\text{if}}\:i=j\:{\text{then}}&\:\operatorname {\delta } \left[\operatorname {f} \left(x^{(i)}\right):\:\operatorname {g} \left(x^{(j)}\right)\right]\:{\text{is small}}\\{\text{otherwise}}&\:\operatorname {\delta } \left[\operatorname {f} \left(x^{(i)}\right):\:\operatorname {g} \left(x^{(j)}\right)\right]\:{\text{is large}}\end{aligned}}} / i:j{\displaystyle i:j} are indexes into a set of vectors / f⁡(⋅):g⁡(⋅){\displaystyle \operatorname {f} (\cdot ):\operatorname {g} (\cdot )}function implemented by the half-twin network / δ⁡(⋅){\displaystyle \operatorname {\delta } (\cdot )}function implemented by the network joining outputs from the twin network /  /  / == Twin networks for object tracking == / Twin networks have been used in object tracking because of its unique two tandem inputs and similarity measurement. In object tracking: one input of the twin network is user pre-selected exemplar image: the other input is a larger search image: which twin network's job is to locate exemplar inside of search image. By measuring the similarity between exemplar and each part of the search image: a map of similarity score can be given by the twin network. Furthermore: using a Fully Convolutional Network: the process of computing each sector's similarity score can be replaced with only one cross correlation layer.After being first introduced in 2016: Twin fully convolutional network has been used in many High-performance Real-time Object Tracking Neural Networks.  Like CFnet: StructSiam: SiamFC-tri: DSiam: SA-Siam: SiamRPN: DaSiamRPN: Cascaded SiamRPN: SiamMask: SiamRPN++: Deeper and Wider SiamRPN."
0.12174960001721047,What are Spiking neural networks (SNNs)?,Spiking neural networks (SNNs) are artificial neural networks that closely mimic natural neural networks. They incorporate the concept of time into their operating model and transmit information only when a neuron's membrane potential reaches a specific threshold value. This model represents a significant departure from typical multi-layer perceptron networks that transmit information at each propagation cycle.,"Score: 0.9010844399298793 / Spiking neural networks (SNNs) are artificial neural networks (ANN) that more closely mimic natural neural networks. In addition to neuronal and synaptic state: SNNs incorporate the concept of time into their operating model. The idea is that neurons in the SNN do not transmit information at each propagation cycle (as it happens with typical multi-layer perceptron networks): but rather transmit information only when a membrane potential—an intrinsic quality of the neuron related to its membrane electrical charge—reaches a specific value: called the threshold. When the membrane potential reaches the threshold: the neuron fires: and generates a signal that travels to other neurons which: in turn: increase or decrease their potentials in response to this signal. A neuron model  that fires at the moment of threshold crossing is also called a spiking neuron model.Although it was previously believed that the brain encoded information through spike rates: which can be considered as the analogue variable output of a traditional ANN: research in the field of neurobiology has indicated that high speed processing cannot solely be performed through a rate based scheme. For example humans can perform an image recognition task at rate requiring no more than 10ms of processing time per neuron through the successive layers (going from the retina to the temporal lobe). This time window is too short for a rate based encoding. The precise spike timings in a small set of spiking neurons also has a higher information coding capacity compared with a rate based approach.The most prominent spiking neuron model is the leaky integrate-and-fire model. In the integrate-and-fire model: the momentary activation level (modeled as a differential equation) is normally considered to be the neuron's state: with incoming spikes pushing this value higher or lower: until the state eventually either decays or—if the firing threshold is reached—the neuron fires. After firing: the state variable is reset to a lower value. / Various decoding methods exist for interpreting the outgoing spike train as a real-value number: relying on either the frequency of spikes (rate-code): the time-to-first-spike after stimulation: or the interval between spikes. /  /  / == History == / Many multi-layer artificial neural networks are fully connected: receiving input from every neuron in the previous layer and signalling every neuron in the subsequent layer. Although these networks have achieved breakthroughs in many fields: they are biologically inaccurate and do not mimic the operation mechanism of neurons in the brain of a living thing. /  / The biologically inspired Hodgkin–Huxley model of a spiking neuron was proposed  in 1952. This model describes how action potentials are initiated and propagated. Communication between neurons: which requires the exchange of chemical neurotransmitters in the synaptic gap: is described in various models: such as the integrate-and-fire model: FitzHugh–Nagumo model (1961–1962): and Hindmarsh–Rose model (1984). The leaky integrate-and-fire model (or a derivative) is commonly used as it is easier to compute than the Hodgkin–Huxley model. /  /  / == Underpinnings == / Information in the brain is represented as action potentials (neuron spikes): which may be grouped into spike trains or even coordinated waves of brain activity. A fundamental question of neuroscience is to determine whether neurons communicate by a rate or temporal code. Temporal coding suggests that a single spiking neuron can replace hundreds of hidden units on a sigmoidal neural net.An SNN computes in the continuous rather than the discrete domain. The idea is that neurons may not test for activation in every iteration of propagation (as is the case in a typical multilayer perceptron network): but only when their membrane potentials reach a certain value. When a neuron is activated: it produces a signal that is passed to connected neurons: raising or lowering their membrane potential. / In a spiking neural network: a neuron's current state is defined as its membrane potential (possibly modeled as a differential equation). An input pulse causes the membrane potential to rise for a period of time and then gradually decline. Encoding schemes have been constructed to interpret these pulse sequences as a number: taking into account both pulse frequency and pulse interval. A neural network model based on pulse generation time can be established. Using the exact time of pulse occurrence: a neural network can employ more information and offer better computing properties. / The SNN approach produces a continuous output instead of the binary output of traditional artificial neural networks (ANNs). Pulse trains are not easily interpretable: hence the need for encoding schemes as above. However: a pulse train representation may be more suited for processing spatiotemporal data (or continual real-world sensory data classification). SNNs consider space by connecting neurons only to nearby neurons so that they process input blocks separately (similar to CNN using filters).","Score: 0.8755986212496536 / Recently: This phenomenon is achieved mostly achieved using Compartmental neuron models. / The simpler versions are of neuron models with adaptive thresholds: indirect way of achieving SFA: equips SNNs with improved learning capabilities: even with constrained synaptic plasticity: and elevates computational efficiency. This feature lessens the demand on network layers by decreasing the need for spike processing: thus cutting down on computational load and memory access time—essential aspects of neural computation. / Moreover: SNNs utilizing neurons capable of SFA achieve levels of accuracy that rival those of conventional artificial neural networks: including those based on long short-term memory models: while also requiring fewer neurons for comparable computational tasks. This efficiency not only streamlines the computational workflow but also conserves space and energy: offering a pragmatic step forward in the practical application of SNNs for complex computing tasks: all while maintaining a commitment to technical integrity. /  /  / == Applications == / SNNs can in principle apply to the same applications as traditional ANNs. In addition: SNNs can model the central nervous system of biological organisms: such as an insect seeking food without prior knowledge of the environment. Due to their relative realism: they can be used to study the operation of biological neural circuits. Starting with a hypothesis about the topology of a biological neuronal circuit and its function: recordings of this circuit can be compared to the output of the corresponding SNN: evaluating the plausibility of the hypothesis. However: there is a lack of effective training mechanisms for SNNs: which can be inhibitory for some applications: including computer vision tasks. / As of 2019 SNNs lag behind ANNs in terms of accuracy: but the gap is decreasing: and has vanished on some tasks.When using SNNs for image based data we need to convert static images into binary spike trains coding. Types of encodings: / Temporal coding generates one spike per neuron in which spike latency is inversely proportional to the pixel intensity. / Rate coding converts pixel intensity into a spike train where the number of spikes is proportional to the pixel intensity. / Direct coding uses a trainable layer to generate float value for each time-step. We have a learnable layer which converts each pixel at certain time step in float number and then threshold is used on the generated floating numbers to see if they will be 1 or 0. / Phase coding encodes temporal information into spike patterns based on a global oscillator. / Burst coding transmits the burst of spikes in a small-time duration: increasing the reliability of synaptic communication between neurons. /  /  / == Software == / A diverse range of application software can simulate SNNs. This software can be classified according to its uses: /  /  / === SNN simulation === /  These simulate complex neural models with a high level of detail and accuracy. Large networks usually require lengthy processing. Candidates include:Brian – developed by Romain Brette and Dan Goodman at the École Normale Supérieure; / GENESIS (the GEneral NEural SImulation System) – developed in James Bower's laboratory at Caltech; / NEST – developed by the NEST Initiative; / NEURON – mainly developed by Michael Hines: John W. Moore and Ted Carnevale in Yale University and Duke University; / RAVSim (Runtime Tool)  – mainly developed by Sanaullah in Bielefeld University of Applied Sciences and Arts; /  /  / == Hardware == / Future neuromorphic architectures will comprise billions of such nanosynapses: which require a clear understanding of the physical mechanisms responsible for plasticity. Experimental systems based on ferroelectric tunnel junctions have been used to show that STDP can be harnessed from heterogeneous polarization switching. Through combined scanning probe imaging: electrical transport and atomic-scale molecular dynamics: conductance variations can be modelled by nucleation-dominated reversal of domains. Simulations show that arrays of ferroelectric nanosynapses can autonomously learn to recognize patterns in a predictable way: opening the path towards unsupervised learning. / Akida is a completely digital event-based neural processing device with 1.2 million artificial neurons and 10 billion artificial synapses developed by BrainChip. Utilizing event-based possessing: it analyzes essential inputs at specific points. Results are stored in the on-chip memory units. / Neurogrid is a board that can simulate spiking neural networks directly in hardware. (Stanford University) / SpiNNaker (Spiking Neural Network Architecture) uses ARM processors as the building blocks of a massively parallel computing platform based on a six-layer thalamocortical model. (University of Manchester) The SpiNNaker system is based on numerical models running in real time on custom digital multicore chips using the ARM architecture."
0.16206059997784905,What is the leaky integrate-and-fire model in SNNs?,"The leaky integrate-and-fire model is the most prominent spiking neuron model. In this model, the neuron's state is considered to be its momentary activation level. Incoming spikes push this value higher or lower until the state either decays or, if the firing threshold is reached, the neuron fires. After firing, the state variable is reset to a lower value.","Score: 0.8475499757574987 / Spiking neural networks (SNNs) are artificial neural networks (ANN) that more closely mimic natural neural networks. In addition to neuronal and synaptic state: SNNs incorporate the concept of time into their operating model. The idea is that neurons in the SNN do not transmit information at each propagation cycle (as it happens with typical multi-layer perceptron networks): but rather transmit information only when a membrane potential—an intrinsic quality of the neuron related to its membrane electrical charge—reaches a specific value: called the threshold. When the membrane potential reaches the threshold: the neuron fires: and generates a signal that travels to other neurons which: in turn: increase or decrease their potentials in response to this signal. A neuron model  that fires at the moment of threshold crossing is also called a spiking neuron model.Although it was previously believed that the brain encoded information through spike rates: which can be considered as the analogue variable output of a traditional ANN: research in the field of neurobiology has indicated that high speed processing cannot solely be performed through a rate based scheme. For example humans can perform an image recognition task at rate requiring no more than 10ms of processing time per neuron through the successive layers (going from the retina to the temporal lobe). This time window is too short for a rate based encoding. The precise spike timings in a small set of spiking neurons also has a higher information coding capacity compared with a rate based approach.The most prominent spiking neuron model is the leaky integrate-and-fire model. In the integrate-and-fire model: the momentary activation level (modeled as a differential equation) is normally considered to be the neuron's state: with incoming spikes pushing this value higher or lower: until the state eventually either decays or—if the firing threshold is reached—the neuron fires. After firing: the state variable is reset to a lower value. / Various decoding methods exist for interpreting the outgoing spike train as a real-value number: relying on either the frequency of spikes (rate-code): the time-to-first-spike after stimulation: or the interval between spikes. /  /  / == History == / Many multi-layer artificial neural networks are fully connected: receiving input from every neuron in the previous layer and signalling every neuron in the subsequent layer. Although these networks have achieved breakthroughs in many fields: they are biologically inaccurate and do not mimic the operation mechanism of neurons in the brain of a living thing. /  / The biologically inspired Hodgkin–Huxley model of a spiking neuron was proposed  in 1952. This model describes how action potentials are initiated and propagated. Communication between neurons: which requires the exchange of chemical neurotransmitters in the synaptic gap: is described in various models: such as the integrate-and-fire model: FitzHugh–Nagumo model (1961–1962): and Hindmarsh–Rose model (1984). The leaky integrate-and-fire model (or a derivative) is commonly used as it is easier to compute than the Hodgkin–Huxley model. /  /  / == Underpinnings == / Information in the brain is represented as action potentials (neuron spikes): which may be grouped into spike trains or even coordinated waves of brain activity. A fundamental question of neuroscience is to determine whether neurons communicate by a rate or temporal code. Temporal coding suggests that a single spiking neuron can replace hundreds of hidden units on a sigmoidal neural net.An SNN computes in the continuous rather than the discrete domain. The idea is that neurons may not test for activation in every iteration of propagation (as is the case in a typical multilayer perceptron network): but only when their membrane potentials reach a certain value. When a neuron is activated: it produces a signal that is passed to connected neurons: raising or lowering their membrane potential. / In a spiking neural network: a neuron's current state is defined as its membrane potential (possibly modeled as a differential equation). An input pulse causes the membrane potential to rise for a period of time and then gradually decline. Encoding schemes have been constructed to interpret these pulse sequences as a number: taking into account both pulse frequency and pulse interval. A neural network model based on pulse generation time can be established. Using the exact time of pulse occurrence: a neural network can employ more information and offer better computing properties. / The SNN approach produces a continuous output instead of the binary output of traditional artificial neural networks (ANNs). Pulse trains are not easily interpretable: hence the need for encoding schemes as above. However: a pulse train representation may be more suited for processing spatiotemporal data (or continual real-world sensory data classification). SNNs consider space by connecting neurons only to nearby neurons so that they process input blocks separately (similar to CNN using filters).","Score: 0.8298760281629142 / Recently: This phenomenon is achieved mostly achieved using Compartmental neuron models. / The simpler versions are of neuron models with adaptive thresholds: indirect way of achieving SFA: equips SNNs with improved learning capabilities: even with constrained synaptic plasticity: and elevates computational efficiency. This feature lessens the demand on network layers by decreasing the need for spike processing: thus cutting down on computational load and memory access time—essential aspects of neural computation. / Moreover: SNNs utilizing neurons capable of SFA achieve levels of accuracy that rival those of conventional artificial neural networks: including those based on long short-term memory models: while also requiring fewer neurons for comparable computational tasks. This efficiency not only streamlines the computational workflow but also conserves space and energy: offering a pragmatic step forward in the practical application of SNNs for complex computing tasks: all while maintaining a commitment to technical integrity. /  /  / == Applications == / SNNs can in principle apply to the same applications as traditional ANNs. In addition: SNNs can model the central nervous system of biological organisms: such as an insect seeking food without prior knowledge of the environment. Due to their relative realism: they can be used to study the operation of biological neural circuits. Starting with a hypothesis about the topology of a biological neuronal circuit and its function: recordings of this circuit can be compared to the output of the corresponding SNN: evaluating the plausibility of the hypothesis. However: there is a lack of effective training mechanisms for SNNs: which can be inhibitory for some applications: including computer vision tasks. / As of 2019 SNNs lag behind ANNs in terms of accuracy: but the gap is decreasing: and has vanished on some tasks.When using SNNs for image based data we need to convert static images into binary spike trains coding. Types of encodings: / Temporal coding generates one spike per neuron in which spike latency is inversely proportional to the pixel intensity. / Rate coding converts pixel intensity into a spike train where the number of spikes is proportional to the pixel intensity. / Direct coding uses a trainable layer to generate float value for each time-step. We have a learnable layer which converts each pixel at certain time step in float number and then threshold is used on the generated floating numbers to see if they will be 1 or 0. / Phase coding encodes temporal information into spike patterns based on a global oscillator. / Burst coding transmits the burst of spikes in a small-time duration: increasing the reliability of synaptic communication between neurons. /  /  / == Software == / A diverse range of application software can simulate SNNs. This software can be classified according to its uses: /  /  / === SNN simulation === /  These simulate complex neural models with a high level of detail and accuracy. Large networks usually require lengthy processing. Candidates include:Brian – developed by Romain Brette and Dan Goodman at the École Normale Supérieure; / GENESIS (the GEneral NEural SImulation System) – developed in James Bower's laboratory at Caltech; / NEST – developed by the NEST Initiative; / NEURON – mainly developed by Michael Hines: John W. Moore and Ted Carnevale in Yale University and Duke University; / RAVSim (Runtime Tool)  – mainly developed by Sanaullah in Bielefeld University of Applied Sciences and Arts; /  /  / == Hardware == / Future neuromorphic architectures will comprise billions of such nanosynapses: which require a clear understanding of the physical mechanisms responsible for plasticity. Experimental systems based on ferroelectric tunnel junctions have been used to show that STDP can be harnessed from heterogeneous polarization switching. Through combined scanning probe imaging: electrical transport and atomic-scale molecular dynamics: conductance variations can be modelled by nucleation-dominated reversal of domains. Simulations show that arrays of ferroelectric nanosynapses can autonomously learn to recognize patterns in a predictable way: opening the path towards unsupervised learning. / Akida is a completely digital event-based neural processing device with 1.2 million artificial neurons and 10 billion artificial synapses developed by BrainChip. Utilizing event-based possessing: it analyzes essential inputs at specific points. Results are stored in the on-chip memory units. / Neurogrid is a board that can simulate spiking neural networks directly in hardware. (Stanford University) / SpiNNaker (Spiking Neural Network Architecture) uses ARM processors as the building blocks of a massively parallel computing platform based on a six-layer thalamocortical model. (University of Manchester) The SpiNNaker system is based on numerical models running in real time on custom digital multicore chips using the ARM architecture."
0.13191050000023097,What are some of the challenges in using SNNs?,"Some challenges in using SNNs include the non-differentiability of the spiking nonlinearity and the implementation of the optimization algorithm. The all-or-nothing behavior of the binary spiking nonlinearity makes neurons unsuitable for gradient-based optimization, and standard Backpropagation can be computationally expensive. ","Score: 0.827459004961394 / This helps to exclude rare dependencies. Finally: data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.DNNs must consider many training parameters: such as the size (number of layers and number of units per layer): the learning rate: and initial weights. Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources. Various tricks: such as batching (computing the gradient on several training examples at once rather than individual examples) speed up computation. Large processing capabilities of many-core architectures (such as GPUs or the Intel Xeon Phi) have produced significant speedups in training: because of the suitability of such processing architectures for the matrix and vector computations.Alternatively: engineers may look for other types of neural networks with more straightforward and convergent training algorithms. CMAC (cerebellar model articulation controller) is one such kind of neural network. It doesn't require learning rates or randomized initial weights. The training process can be guaranteed to converge in one step with a new batch of data: and the computational complexity of the training algorithm is linear with respect to the number of neurons involved. /  /  / == Hardware == / Since the 2010s: advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer. By 2019: graphic processing units (GPUs): often with AI-specific enhancements: had displaced CPUs as the dominant method of training large-scale commercial cloud AI. OpenAI estimated the hardware computation used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017): and found a 300:000-fold increase in the amount of computation required: with a doubling-time trendline of 3.4 months.Special electronic circuits called deep learning processors were designed to speed up deep learning algorithms. Deep learning processors include neural processing units (NPUs) in Huawei cellphones and cloud computing servers such as tensor processing units (TPU) in the Google Cloud Platform. Cerebras Systems has also built a dedicated system to handle large deep learning models: the CS-2: based on the largest processor in the industry: the second-generation Wafer Scale Engine (WSE-2).Atomically thin semiconductors are considered promising for energy-efficient deep learning hardware where the same basic device structure is used for both logic operations and data storage. / In 2020: Marega et al. published experiments with a large-area active channel material for developing logic-in-memory devices and circuits based on floating-gate field-effect transistors (FGFETs).In 2021: J. Feldmann et al. proposed an integrated photonic hardware accelerator for parallel convolutional processing. The authors identify two key advantages of integrated photonics over its electronic counterparts: (1) massively parallel data transfer through wavelength division multiplexing in conjunction with frequency combs: and (2) extremely high data modulation speeds. Their system can execute trillions of multiply-accumulate operations per second: indicating the potential of integrated photonics in data-heavy AI applications. /  /  / == Applications == /  /  / === Automatic speech recognition === /  / Large-scale automatic speech recognition is the first and most convincing successful case of deep learning. LSTM RNNs can learn ""Very Deep Learning"" tasks that involve multi-second intervals containing speech events separated by thousands of discrete time steps: where one time step corresponds to about 10 ms. LSTM with forget gates is competitive with traditional speech recognizers on certain tasks.The initial success in speech recognition was based on small-scale recognition tasks based on TIMIT. The data set contains 630 speakers from eight major dialects of American English: where each speaker reads 10 sentences. Its small size lets many configurations be tried. More importantly: the TIMIT task concerns phone-sequence recognition: which: unlike word-sequence recognition: allows weak phone bigram language models. This lets the strength of the acoustic modeling aspects of speech recognition be more easily analyzed. The error rates listed below: including these early results and measured as percent phone error rates (PER): have been summarized since 1991.","Score: 0.827236146971377 / Recently: This phenomenon is achieved mostly achieved using Compartmental neuron models. / The simpler versions are of neuron models with adaptive thresholds: indirect way of achieving SFA: equips SNNs with improved learning capabilities: even with constrained synaptic plasticity: and elevates computational efficiency. This feature lessens the demand on network layers by decreasing the need for spike processing: thus cutting down on computational load and memory access time—essential aspects of neural computation. / Moreover: SNNs utilizing neurons capable of SFA achieve levels of accuracy that rival those of conventional artificial neural networks: including those based on long short-term memory models: while also requiring fewer neurons for comparable computational tasks. This efficiency not only streamlines the computational workflow but also conserves space and energy: offering a pragmatic step forward in the practical application of SNNs for complex computing tasks: all while maintaining a commitment to technical integrity. /  /  / == Applications == / SNNs can in principle apply to the same applications as traditional ANNs. In addition: SNNs can model the central nervous system of biological organisms: such as an insect seeking food without prior knowledge of the environment. Due to their relative realism: they can be used to study the operation of biological neural circuits. Starting with a hypothesis about the topology of a biological neuronal circuit and its function: recordings of this circuit can be compared to the output of the corresponding SNN: evaluating the plausibility of the hypothesis. However: there is a lack of effective training mechanisms for SNNs: which can be inhibitory for some applications: including computer vision tasks. / As of 2019 SNNs lag behind ANNs in terms of accuracy: but the gap is decreasing: and has vanished on some tasks.When using SNNs for image based data we need to convert static images into binary spike trains coding. Types of encodings: / Temporal coding generates one spike per neuron in which spike latency is inversely proportional to the pixel intensity. / Rate coding converts pixel intensity into a spike train where the number of spikes is proportional to the pixel intensity. / Direct coding uses a trainable layer to generate float value for each time-step. We have a learnable layer which converts each pixel at certain time step in float number and then threshold is used on the generated floating numbers to see if they will be 1 or 0. / Phase coding encodes temporal information into spike patterns based on a global oscillator. / Burst coding transmits the burst of spikes in a small-time duration: increasing the reliability of synaptic communication between neurons. /  /  / == Software == / A diverse range of application software can simulate SNNs. This software can be classified according to its uses: /  /  / === SNN simulation === /  These simulate complex neural models with a high level of detail and accuracy. Large networks usually require lengthy processing. Candidates include:Brian – developed by Romain Brette and Dan Goodman at the École Normale Supérieure; / GENESIS (the GEneral NEural SImulation System) – developed in James Bower's laboratory at Caltech; / NEST – developed by the NEST Initiative; / NEURON – mainly developed by Michael Hines: John W. Moore and Ted Carnevale in Yale University and Duke University; / RAVSim (Runtime Tool)  – mainly developed by Sanaullah in Bielefeld University of Applied Sciences and Arts; /  /  / == Hardware == / Future neuromorphic architectures will comprise billions of such nanosynapses: which require a clear understanding of the physical mechanisms responsible for plasticity. Experimental systems based on ferroelectric tunnel junctions have been used to show that STDP can be harnessed from heterogeneous polarization switching. Through combined scanning probe imaging: electrical transport and atomic-scale molecular dynamics: conductance variations can be modelled by nucleation-dominated reversal of domains. Simulations show that arrays of ferroelectric nanosynapses can autonomously learn to recognize patterns in a predictable way: opening the path towards unsupervised learning. / Akida is a completely digital event-based neural processing device with 1.2 million artificial neurons and 10 billion artificial synapses developed by BrainChip. Utilizing event-based possessing: it analyzes essential inputs at specific points. Results are stored in the on-chip memory units. / Neurogrid is a board that can simulate spiking neural networks directly in hardware. (Stanford University) / SpiNNaker (Spiking Neural Network Architecture) uses ARM processors as the building blocks of a massively parallel computing platform based on a six-layer thalamocortical model. (University of Manchester) The SpiNNaker system is based on numerical models running in real time on custom digital multicore chips using the ARM architecture."
0.16661670000758022,What are some applications of SNNs?,"SNNs can apply to the same applications as traditional artificial neural networks (ANNs), and in addition, they can model the central nervous system of biological organisms. However, a lack of effective training mechanisms for SNNs can be inhibitory for some applications, like computer vision tasks. ","Score: 0.8380110806491388 / Recently: This phenomenon is achieved mostly achieved using Compartmental neuron models. / The simpler versions are of neuron models with adaptive thresholds: indirect way of achieving SFA: equips SNNs with improved learning capabilities: even with constrained synaptic plasticity: and elevates computational efficiency. This feature lessens the demand on network layers by decreasing the need for spike processing: thus cutting down on computational load and memory access time—essential aspects of neural computation. / Moreover: SNNs utilizing neurons capable of SFA achieve levels of accuracy that rival those of conventional artificial neural networks: including those based on long short-term memory models: while also requiring fewer neurons for comparable computational tasks. This efficiency not only streamlines the computational workflow but also conserves space and energy: offering a pragmatic step forward in the practical application of SNNs for complex computing tasks: all while maintaining a commitment to technical integrity. /  /  / == Applications == / SNNs can in principle apply to the same applications as traditional ANNs. In addition: SNNs can model the central nervous system of biological organisms: such as an insect seeking food without prior knowledge of the environment. Due to their relative realism: they can be used to study the operation of biological neural circuits. Starting with a hypothesis about the topology of a biological neuronal circuit and its function: recordings of this circuit can be compared to the output of the corresponding SNN: evaluating the plausibility of the hypothesis. However: there is a lack of effective training mechanisms for SNNs: which can be inhibitory for some applications: including computer vision tasks. / As of 2019 SNNs lag behind ANNs in terms of accuracy: but the gap is decreasing: and has vanished on some tasks.When using SNNs for image based data we need to convert static images into binary spike trains coding. Types of encodings: / Temporal coding generates one spike per neuron in which spike latency is inversely proportional to the pixel intensity. / Rate coding converts pixel intensity into a spike train where the number of spikes is proportional to the pixel intensity. / Direct coding uses a trainable layer to generate float value for each time-step. We have a learnable layer which converts each pixel at certain time step in float number and then threshold is used on the generated floating numbers to see if they will be 1 or 0. / Phase coding encodes temporal information into spike patterns based on a global oscillator. / Burst coding transmits the burst of spikes in a small-time duration: increasing the reliability of synaptic communication between neurons. /  /  / == Software == / A diverse range of application software can simulate SNNs. This software can be classified according to its uses: /  /  / === SNN simulation === /  These simulate complex neural models with a high level of detail and accuracy. Large networks usually require lengthy processing. Candidates include:Brian – developed by Romain Brette and Dan Goodman at the École Normale Supérieure; / GENESIS (the GEneral NEural SImulation System) – developed in James Bower's laboratory at Caltech; / NEST – developed by the NEST Initiative; / NEURON – mainly developed by Michael Hines: John W. Moore and Ted Carnevale in Yale University and Duke University; / RAVSim (Runtime Tool)  – mainly developed by Sanaullah in Bielefeld University of Applied Sciences and Arts; /  /  / == Hardware == / Future neuromorphic architectures will comprise billions of such nanosynapses: which require a clear understanding of the physical mechanisms responsible for plasticity. Experimental systems based on ferroelectric tunnel junctions have been used to show that STDP can be harnessed from heterogeneous polarization switching. Through combined scanning probe imaging: electrical transport and atomic-scale molecular dynamics: conductance variations can be modelled by nucleation-dominated reversal of domains. Simulations show that arrays of ferroelectric nanosynapses can autonomously learn to recognize patterns in a predictable way: opening the path towards unsupervised learning. / Akida is a completely digital event-based neural processing device with 1.2 million artificial neurons and 10 billion artificial synapses developed by BrainChip. Utilizing event-based possessing: it analyzes essential inputs at specific points. Results are stored in the on-chip memory units. / Neurogrid is a board that can simulate spiking neural networks directly in hardware. (Stanford University) / SpiNNaker (Spiking Neural Network Architecture) uses ARM processors as the building blocks of a massively parallel computing platform based on a six-layer thalamocortical model. (University of Manchester) The SpiNNaker system is based on numerical models running in real time on custom digital multicore chips using the ARM architecture.","Score: 0.8316265531374682 / spatial regression models) whenever the geo-spatial datasets' variables depict non-linear relations. Examples of SNNs are the OSFA spatial neural networks: SVANNs and GWNNs. /  /  / === Neocognitron === / The neocognitron is a hierarchical: multilayered network that was modeled after the visual cortex. It uses multiple types of units: (originally two: called simple and complex cells): as a cascading model for use in pattern recognition tasks. Local features are extracted by S-cells whose deformation is tolerated by C-cells. Local features in the input are integrated gradually and classified at higher layers. Among the various kinds of neocognitron are systems that can detect multiple patterns in the same input by using back propagation to achieve selective attention. It has been used for pattern recognition tasks and inspired convolutional neural networks. /  /  / === Compound hierarchical-deep models === / Compound hierarchical-deep models compose deep networks with non-parametric Bayesian models. Features can be learned using deep architectures such as DBNs: deep Boltzmann machines (DBM): deep auto encoders: convolutional variants: ssRBMs: deep coding networks: DBNs with sparse feature learning: RNNs: conditional DBNs: denoising autoencoders. This provides a better representation: allowing faster learning and more accurate classification with high-dimensional data. However: these architectures are poor at learning novel classes with few examples: because all network units are involved in representing the input (a distributed representation) and must be adjusted together (high degree of freedom). Limiting the degree of freedom reduces the number of parameters to learn: facilitating learning of new classes from few examples. Hierarchical Bayesian (HB) models allow learning from few examples: for example for computer vision: statistics and cognitive science. / Compound HD architectures aim to integrate characteristics of both HB and deep networks. The compound HDP-DBM architecture is a hierarchical Dirichlet process (HDP) as a hierarchical model: incorporating DBM architecture. It is a full generative model: generalized from abstract concepts flowing through the model layers: which is able to synthesize new examples in novel classes that look ""reasonably"" natural. All the levels are learned jointly by maximizing a joint log-probability score.In a DBM with three hidden layers: the probability of a visible input ''ν'' is: /  / p(ν:ψ)=1Z∑hexp⁡(∑ijWij(1)νihj1+∑jℓWjℓ(2)hj1hℓ2+∑ℓmWℓm(3)hℓ2hm3):{\displaystyle p({\boldsymbol {\nu }}:\psi )={\frac {1}{Z}}\sum _{h}\exp \left(\sum _{ij}W_{ij}^{(1)}\nu _{i}h_{j}^{1}+\sum _{j\ell }W_{j\ell }^{(2)}h_{j}^{1}h_{\ell }^{2}+\sum _{\ell m}W_{\ell m}^{(3)}h_{\ell }^{2}h_{m}^{3}\right):}where h={h(1):h(2):h(3)}{\displaystyle {\boldsymbol {h}}=\{{\boldsymbol {h}}^{(1)}:{\boldsymbol {h}}^{(2)}:{\boldsymbol {h}}^{(3)}\}} is the set of hidden units: and ψ={W(1):W(2):W(3)}{\displaystyle \psi =\{{\boldsymbol {W}}^{(1)}:{\boldsymbol {W}}^{(2)}:{\boldsymbol {W}}^{(3)}\}} are the model parameters: representing visible-hidden and hidden-hidden symmetric interaction terms. / A learned DBM model is an undirected model that defines the joint distribution P(ν:h1:h2:h3){\displaystyle P(\nu :h^{1}:h^{2}:h^{3})}. One way to express what has been learned is the conditional model P(ν:h1:h2∣h3){\displaystyle P(\nu :h^{1}:h^{2}\mid h^{3})} and a prior term P(h3){\displaystyle P(h^{3})}."
0.14901379999355413,What developments have improved the efficiency and computational power of SNNs?,"Incorporating additional neuron dynamics like Spike Frequency Adaptation (SFA) into neuron models is one such development. SFA offers computational benefits by reducing power usage and increasing coding efficiency, especially in repetitive or intense stimuli. This adaptation also enhances signal clarity and introduces short-term memory at the neuron level, refining information processing accuracy and efficiency.","Score: 0.8415989600255036 / This helps to exclude rare dependencies. Finally: data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.DNNs must consider many training parameters: such as the size (number of layers and number of units per layer): the learning rate: and initial weights. Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources. Various tricks: such as batching (computing the gradient on several training examples at once rather than individual examples) speed up computation. Large processing capabilities of many-core architectures (such as GPUs or the Intel Xeon Phi) have produced significant speedups in training: because of the suitability of such processing architectures for the matrix and vector computations.Alternatively: engineers may look for other types of neural networks with more straightforward and convergent training algorithms. CMAC (cerebellar model articulation controller) is one such kind of neural network. It doesn't require learning rates or randomized initial weights. The training process can be guaranteed to converge in one step with a new batch of data: and the computational complexity of the training algorithm is linear with respect to the number of neurons involved. /  /  / == Hardware == / Since the 2010s: advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer. By 2019: graphic processing units (GPUs): often with AI-specific enhancements: had displaced CPUs as the dominant method of training large-scale commercial cloud AI. OpenAI estimated the hardware computation used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017): and found a 300:000-fold increase in the amount of computation required: with a doubling-time trendline of 3.4 months.Special electronic circuits called deep learning processors were designed to speed up deep learning algorithms. Deep learning processors include neural processing units (NPUs) in Huawei cellphones and cloud computing servers such as tensor processing units (TPU) in the Google Cloud Platform. Cerebras Systems has also built a dedicated system to handle large deep learning models: the CS-2: based on the largest processor in the industry: the second-generation Wafer Scale Engine (WSE-2).Atomically thin semiconductors are considered promising for energy-efficient deep learning hardware where the same basic device structure is used for both logic operations and data storage. / In 2020: Marega et al. published experiments with a large-area active channel material for developing logic-in-memory devices and circuits based on floating-gate field-effect transistors (FGFETs).In 2021: J. Feldmann et al. proposed an integrated photonic hardware accelerator for parallel convolutional processing. The authors identify two key advantages of integrated photonics over its electronic counterparts: (1) massively parallel data transfer through wavelength division multiplexing in conjunction with frequency combs: and (2) extremely high data modulation speeds. Their system can execute trillions of multiply-accumulate operations per second: indicating the potential of integrated photonics in data-heavy AI applications. /  /  / == Applications == /  /  / === Automatic speech recognition === /  / Large-scale automatic speech recognition is the first and most convincing successful case of deep learning. LSTM RNNs can learn ""Very Deep Learning"" tasks that involve multi-second intervals containing speech events separated by thousands of discrete time steps: where one time step corresponds to about 10 ms. LSTM with forget gates is competitive with traditional speech recognizers on certain tasks.The initial success in speech recognition was based on small-scale recognition tasks based on TIMIT. The data set contains 630 speakers from eight major dialects of American English: where each speaker reads 10 sentences. Its small size lets many configurations be tried. More importantly: the TIMIT task concerns phone-sequence recognition: which: unlike word-sequence recognition: allows weak phone bigram language models. This lets the strength of the acoustic modeling aspects of speech recognition be more easily analyzed. The error rates listed below: including these early results and measured as percent phone error rates (PER): have been summarized since 1991.","Score: 0.8380387642890517 / Industrial applications of deep learning to large-scale speech recognition started around 2010. / In 2006: publications by Geoff Hinton: Ruslan Salakhutdinov: Osindero and Teh showed how a many-layered feedforward neural network could be effectively pre-trained one layer at a time: treating each layer in turn as an unsupervised restricted Boltzmann machine: then fine-tuning it using supervised backpropagation. The papers referred to learning for deep belief nets. / The 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech: and the possibility that given more capable hardware and large-scale data sets that deep neural nets might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets. However: it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large: context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems. The nature of the recognition errors produced by the two types of systems was characteristically different: offering technical insights into how to integrate deep learning into the existing highly efficient: run-time speech decoding system deployed by all major speech recognition systems. Analysis around 2009–2010: contrasting the GMM (and other generative speech models) vs. DNN models: stimulated early industrial investment in deep learning for speech recognition.  That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models. / In 2010: researchers extended deep learning from TIMIT to large vocabulary speech recognition: by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees.Deep learning is part of state-of-the-art systems in various disciplines: particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification): as well as a range of large-vocabulary speech recognition tasks have steadily improved. Convolutional neural networks were superseded for ASR by CTC for LSTM. but are more successful in computer vision. / Advances in hardware have driven renewed interest in deep learning. In 2009: Nvidia was involved in what was called the ""big bang"" of deep learning: ""as deep-learning neural networks were trained with Nvidia graphics processing units (GPUs)"". That year: Andrew Ng determined that GPUs could increase the speed of deep-learning systems by about 100 times. In particular: GPUs are well-suited for the matrix/vector computations involved in machine learning. GPUs speed up training algorithms by orders of magnitude: reducing running times from weeks to days. Further: specialized hardware and algorithm optimizations can be used for efficient processing of deep learning models. /  /  / === Deep learning revolution === / In the late 2000s: deep learning started to outperform other methods in machine learning competitions. / In 2009: a long short-term memory trained by connectionist temporal classification (Alex Graves: Santiago Fernández: Faustino Gomez: and Jürgen Schmidhuber: 2006) was the first RNN to win pattern recognition contests: winning three competitions in connected handwriting recognition. Google later used CTC-trained LSTM for speech recognition on the smartphone.Significant impacts in image or object recognition were felt from 2011 to 2012. Although CNNs trained by backpropagation had been around for decades: and GPU implementations of NNs for years: including CNNs: faster implementations of CNNs on GPUs were needed to progress on computer vision. In 2011: the DanNet by Dan Ciresan: Ueli Meier: Jonathan Masci: Luca Maria Gambardella: and Jürgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest: outperforming traditional methods by a factor of 3. Also in 2011: DanNet won the ICDAR Chinese handwriting contest: and in May 2012: it won the ISBI image segmentation contest. Until 2011: CNNs did not play a major role at computer vision conferences: but in June 2012: a paper by Ciresan et al. at the leading conference CVPR showed how max-pooling CNNs on GPU can dramatically improve many vision benchmark records."
0.12212270000600256,What is the Unified Modeling Language (UML)? ,"The Unified Modeling Language (UML) is a general-purpose visual modeling language that provides a standard way to visualize the design of a system. It offers a standard notation for many types of diagrams which can be roughly categorized into behavior diagrams, interaction diagrams, and structure diagrams.","Score: 0.9062096390337643 / The unified modeling language (UML) is a general-purpose visual modeling language that is intended to provide a standard way to visualize the design of a system.UML provides a standard notation for many types of diagrams which can be roughly divided into three main groups: behavior diagrams: interaction diagrams: and structure diagrams.  / The creation of UML was originally motivated by the desire to standardize the disparate notational systems and approaches to software design. It was developed at Rational Software in 1994–1995: with further development led by them through 1996.In 1997: UML was adopted as a standard by the Object Management Group (OMG): and has been managed by this organization ever since. In 2005: UML was also published by the International Organization for Standardization (ISO) and the International Electrotechnical Commission (IEC) as the ISO/IEC 19501 standard. Since then the standard has been periodically revised to cover the latest revision of UML.In software engineering: most practitioners do not use UML: but instead produce informal hand drawn diagrams; these diagrams: however: often include elements from UML.: 536  /  /  / == History == /  /  / === Before UML 1.0 === / UML has been evolved since the second half of the 1990s and has its roots in the object-oriented programming methods developed in the late 1980s and early 1990s. The timeline (see image) shows the highlights of the history of object-oriented modeling methods and notation. / It is originally based on the notations of the Booch method: the object-modeling technique (OMT) and object-oriented software engineering (OOSE): which it has integrated into a single language.Rational Software Corporation hired James Rumbaugh from General Electric in 1994 and after that the company became the source for two of the most popular object-oriented modeling approaches of the day: Rumbaugh's object-modeling technique (OMT) and Grady Booch's method. They were soon assisted in their efforts by Ivar Jacobson: the creator of the object-oriented software engineering (OOSE) method: who joined them at Rational in 1995. /  /  / === UML 1.x === / Under the technical leadership of those three (Rumbaugh: Jacobson and Booch): a consortium called the UML Partners was organized in 1996 to complete the Unified Modeling Language (UML) specification: and propose it to the Object Management Group (OMG) for standardization. The partnership also contained additional interested parties (for example HP: DEC: IBM and Microsoft). The UML Partners' UML 1.0 draft was proposed to the OMG in January 1997 by the consortium. During the same month the UML Partners formed a group: designed to define the exact meaning of language constructs: chaired by Cris Kobryn and administered by Ed Eykholt: to finalize the specification and integrate it with other standardization efforts. The result of this work: UML 1.1: was submitted to the OMG in August 1997 and adopted by the OMG in November 1997.After the first release a task force was formed to improve the language: which released several minor revisions: 1.3: 1.4: and 1.5.The standards it produced (as well as the original standard) have been noted as being ambiguous and inconsistent. /  /  / ==== Cardinality notation ==== / As with database Chen: Bachman: and ISO ER diagrams: class models are specified to use ""look-across"" cardinalities: even though several authors (Merise: Elmasri & Navathe amongst others) prefer same-side or ""look-here"" for roles and both minimum and maximum cardinalities. Recent researchers (Feinerer: Dullea et al.) have shown that the ""look-across"" technique used by UML and ER diagrams is less effective and less coherent when applied to n-ary relationships of order strictly greater than 2. / Feinerer says: ""Problems arise if we operate under the look-across semantics as used for UML associations. Hartmann investigates this situation and shows how and why different transformations fail."": and: ""As we will see on the next few pages: the look-across interpretation introduces several difficulties which prevent the extension of simple mechanisms from binary to n-ary associations.""","Score: 0.9051377397105612 / Unified Modeling Language (UML) is a general-purpose modeling language that is an industry standard for specifying software-intensive systems. UML 2.0: the current version: supports thirteen different diagram techniques: and has widespread tool support. / Service-oriented modeling framework (SOMF) is a holistic language for designing enterprise and application level architecture models in the space of enterprise architecture: virtualization: service-oriented architecture (SOA): cloud computing: and more. / Architecture description language (ADL) is a language used to describe and represent the systems architecture of a system. / Architecture Analysis & Design Language (AADL) is a modeling language that supports early and repeated analyses of a system's architecture with respect to performance-critical properties through an extendable notation: a tool framework: and precisely defined semantics.Examples of graphical modeling languages in other fields of science. /  / EAST-ADL is a  Domain-Specific Modeling language dedicated to automotive system design. / Energy Systems Language (ESL): a language that aims to model ecological energetics & global economics. / IEC 61499 defines Domain-Specific Modeling language dedicated to distribute industrial process measurement and control systems. /  /  / === Textual types === / Information models can also be expressed in formalized natural languages: such as Gellish. Gellish has natural language variants such as Gellish Formal English and Gellish Formal Dutch (Gellish Formeel Nederlands): etc. Gellish Formal English is an information representation language or semantic modeling language that is defined in the Gellish English Dictionary-Taxonomy: which has the form of a Taxonomy-Ontology (similarly for Dutch). Gellish Formal English is not only suitable to express knowledge: requirements and dictionaries: taxonomies and ontologies: but also information about individual things. All that information is expressed in one language and therefore it can all be integrated: independent of the question whether it is stored in central or distributed or in federated databases. Information models in Gellish Formal English consists of collections of Gellish Formal English expressions: that use natural language terms and formalized phrases. For example: a geographic information model might consist of a number of Gellish Formal English expressions: such as: /  / - the Eiffel tower <is located in> Paris / - Paris <is classified as a> city /  / whereas information requirements and knowledge can be expressed for example as follows: /  / - tower <shall be located in a> geographical area / - city <is a kind of> geographical area /  / Such Gellish Formal English expressions use names of concepts (such as ""city"") and phrases that represent relation types (such as ⟨is located in⟩ and ⟨is classified as a⟩) that should be selected from the Gellish English Dictionary-Taxonomy (or of your own domain dictionary). The Gellish English Dictionary-Taxonomy enables the creation of semantically rich information models: because the dictionary contains more than 600 standard relation types and contains definitions of more than 40000 concepts. An information model in Gellish can express facts or make statements: queries and answers. /  /  / === More specific types === / In the field of computer science recently more specific types of modeling languages have emerged. /  /  / ==== Algebraic ==== / Algebraic Modeling Languages (AML) are high-level programming languages for describing and solving high complexity problems for large scale mathematical computation (i.e. large scale optimization type problems). One particular advantage of AMLs like AIMMS: AMPL: GAMS: Gekko: Mosel: OPL and OptimJ is the similarity of its syntax to the mathematical notation of optimization problems. This allows for a very concise and readable definition of problems in the domain of optimization: which is supported by certain language elements like sets: indices: algebraic expressions: powerful sparse index and data handling variables: constraints with arbitrary names. The algebraic formulation of a model does not contain any hints how to process it. /  /  / ==== Behavioral ==== / Behavioral languages are designed to describe the observable behavior of complex systems consisting of components that / execute concurrently. These languages focus on the description of key concepts such as: concurrency: nondeterminism: synchronization: and communication. The semantic foundations of Behavioral languages are process calculus or process algebra. /  /  / ==== Discipline-specific ==== / A discipline-specific modeling (DspM) language is focused on deliverables affiliated with a specific software development life cycle stage. Therefore: such language offers a distinct vocabulary: syntax: and notation for each stage: such as discovery: analysis: design: architecture: contraction: etc. For example: for the analysis phase of a project: the modeler employs specific analysis notation to deliver an analysis proposition diagram. During the design phase: however: logical design notation is used to depict relationship between software entities."
0.5402836000139359,What was the motivation behind the creation of UML? ,The creation of UML was primarily motivated by the desire to standardize the disparate notational systems and approaches to software design. ,"Score: 0.8677566874948197 / The unified modeling language (UML) is a general-purpose visual modeling language that is intended to provide a standard way to visualize the design of a system.UML provides a standard notation for many types of diagrams which can be roughly divided into three main groups: behavior diagrams: interaction diagrams: and structure diagrams.  / The creation of UML was originally motivated by the desire to standardize the disparate notational systems and approaches to software design. It was developed at Rational Software in 1994–1995: with further development led by them through 1996.In 1997: UML was adopted as a standard by the Object Management Group (OMG): and has been managed by this organization ever since. In 2005: UML was also published by the International Organization for Standardization (ISO) and the International Electrotechnical Commission (IEC) as the ISO/IEC 19501 standard. Since then the standard has been periodically revised to cover the latest revision of UML.In software engineering: most practitioners do not use UML: but instead produce informal hand drawn diagrams; these diagrams: however: often include elements from UML.: 536  /  /  / == History == /  /  / === Before UML 1.0 === / UML has been evolved since the second half of the 1990s and has its roots in the object-oriented programming methods developed in the late 1980s and early 1990s. The timeline (see image) shows the highlights of the history of object-oriented modeling methods and notation. / It is originally based on the notations of the Booch method: the object-modeling technique (OMT) and object-oriented software engineering (OOSE): which it has integrated into a single language.Rational Software Corporation hired James Rumbaugh from General Electric in 1994 and after that the company became the source for two of the most popular object-oriented modeling approaches of the day: Rumbaugh's object-modeling technique (OMT) and Grady Booch's method. They were soon assisted in their efforts by Ivar Jacobson: the creator of the object-oriented software engineering (OOSE) method: who joined them at Rational in 1995. /  /  / === UML 1.x === / Under the technical leadership of those three (Rumbaugh: Jacobson and Booch): a consortium called the UML Partners was organized in 1996 to complete the Unified Modeling Language (UML) specification: and propose it to the Object Management Group (OMG) for standardization. The partnership also contained additional interested parties (for example HP: DEC: IBM and Microsoft). The UML Partners' UML 1.0 draft was proposed to the OMG in January 1997 by the consortium. During the same month the UML Partners formed a group: designed to define the exact meaning of language constructs: chaired by Cris Kobryn and administered by Ed Eykholt: to finalize the specification and integrate it with other standardization efforts. The result of this work: UML 1.1: was submitted to the OMG in August 1997 and adopted by the OMG in November 1997.After the first release a task force was formed to improve the language: which released several minor revisions: 1.3: 1.4: and 1.5.The standards it produced (as well as the original standard) have been noted as being ambiguous and inconsistent. /  /  / ==== Cardinality notation ==== / As with database Chen: Bachman: and ISO ER diagrams: class models are specified to use ""look-across"" cardinalities: even though several authors (Merise: Elmasri & Navathe amongst others) prefer same-side or ""look-here"" for roles and both minimum and maximum cardinalities. Recent researchers (Feinerer: Dullea et al.) have shown that the ""look-across"" technique used by UML and ER diagrams is less effective and less coherent when applied to n-ary relationships of order strictly greater than 2. / Feinerer says: ""Problems arise if we operate under the look-across semantics as used for UML associations. Hartmann investigates this situation and shows how and why different transformations fail."": and: ""As we will see on the next few pages: the look-across interpretation introduces several difficulties which prevent the extension of simple mechanisms from binary to n-ary associations.""","Score: 0.8601505855171236 / Some people (including Jacobson) feel that UML's size hinders learning (and therefore using) it.MS Visual Studio dropped support for UML in 2016 due to lack of usage.According to Google Trends UML has been on steady decline since 2004. /  /  / == See also == / Applications of UML / Business Process Model and Notation (BPMN) / C4 model / Department of Defense Architecture Framework / DOT (graph description language) / List of Unified Modeling Language tools / MODAF / Model-based testing / Model-driven engineering / Object-oriented role analysis and modeling / Process Specification Language / Systems Modeling Language (SysML) /  /  / == References == /  /  / == Further reading == / Ambler: Scott William (2004). The Object Primer: Agile Model Driven Development with UML 2. Cambridge University Press. ISBN 0-521-54018-6. Archived from the original on 31 January 2010. Retrieved 29 April 2006. / Chonoles: Michael Jesse; James A. Schardt (2003). UML 2 for Dummies. Wiley Publishing. ISBN 0-7645-2614-6. / Fowler: Martin (2004). UML Distilled: A Brief Guide to the Standard Object Modeling Language (3rd ed.). Addison-Wesley. ISBN 0-321-19368-7. / Jacobson: Ivar; Grady Booch; James Rumbaugh (1998). The Unified Software Development Process. Addison Wesley Longman. ISBN 0-201-57169-2. / Martin: Robert Cecil (2003). UML for Java Programmers. Prentice Hall. ISBN 0-13-142848-9. / Noran: Ovidiu S. ""Business Modelling: UML vs. IDEF"" (PDF). Retrieved 14 November 2022. / Horst Kargl. ""Interactive UML Metamodel with additional Examples"". / Penker: Magnus; Hans-Erik Eriksson (2000). Business Modeling with UML. John Wiley & Sons. ISBN 0-471-29551-5. / Douglass: Bruce Powel. ""Bruce Douglass: Real-Time Agile Systems and Software Development"" (web). Retrieved 1 January 2019. / Douglass: Bruce (2014). Real-Time UML Workshop 2nd Edition. Newnes. ISBN 978-0-471-29551-8. / Douglass: Bruce (2004). Real-Time UML 3rd Edition. Newnes. ISBN 978-0321160768. / Douglass: Bruce (2002). Real-Time Design Patterns. Addison-Wesley Professional. ISBN 978-0201699562. / Douglass: Bruce (2009). Real-Time Agility. Addison-Wesley Professional. ISBN 978-0321545497. / Douglass: Bruce (2010). Design Patterns for Embedded Systems in C. Newnes. ISBN 978-1856177078. /  /  / == External links == /  / Official website  / Current Version Specification"
0.11924579998594709,When and by whom was UML developed? ,"UML was developed at Rational Software in 1994–1995, with further development led by them through 1996. ","Score: 0.8548654033193523 / Some people (including Jacobson) feel that UML's size hinders learning (and therefore using) it.MS Visual Studio dropped support for UML in 2016 due to lack of usage.According to Google Trends UML has been on steady decline since 2004. /  /  / == See also == / Applications of UML / Business Process Model and Notation (BPMN) / C4 model / Department of Defense Architecture Framework / DOT (graph description language) / List of Unified Modeling Language tools / MODAF / Model-based testing / Model-driven engineering / Object-oriented role analysis and modeling / Process Specification Language / Systems Modeling Language (SysML) /  /  / == References == /  /  / == Further reading == / Ambler: Scott William (2004). The Object Primer: Agile Model Driven Development with UML 2. Cambridge University Press. ISBN 0-521-54018-6. Archived from the original on 31 January 2010. Retrieved 29 April 2006. / Chonoles: Michael Jesse; James A. Schardt (2003). UML 2 for Dummies. Wiley Publishing. ISBN 0-7645-2614-6. / Fowler: Martin (2004). UML Distilled: A Brief Guide to the Standard Object Modeling Language (3rd ed.). Addison-Wesley. ISBN 0-321-19368-7. / Jacobson: Ivar; Grady Booch; James Rumbaugh (1998). The Unified Software Development Process. Addison Wesley Longman. ISBN 0-201-57169-2. / Martin: Robert Cecil (2003). UML for Java Programmers. Prentice Hall. ISBN 0-13-142848-9. / Noran: Ovidiu S. ""Business Modelling: UML vs. IDEF"" (PDF). Retrieved 14 November 2022. / Horst Kargl. ""Interactive UML Metamodel with additional Examples"". / Penker: Magnus; Hans-Erik Eriksson (2000). Business Modeling with UML. John Wiley & Sons. ISBN 0-471-29551-5. / Douglass: Bruce Powel. ""Bruce Douglass: Real-Time Agile Systems and Software Development"" (web). Retrieved 1 January 2019. / Douglass: Bruce (2014). Real-Time UML Workshop 2nd Edition. Newnes. ISBN 978-0-471-29551-8. / Douglass: Bruce (2004). Real-Time UML 3rd Edition. Newnes. ISBN 978-0321160768. / Douglass: Bruce (2002). Real-Time Design Patterns. Addison-Wesley Professional. ISBN 978-0201699562. / Douglass: Bruce (2009). Real-Time Agility. Addison-Wesley Professional. ISBN 978-0321545497. / Douglass: Bruce (2010). Design Patterns for Embedded Systems in C. Newnes. ISBN 978-1856177078. /  /  / == External links == /  / Official website  / Current Version Specification","Score: 0.8521712886304089 / The unified modeling language (UML) is a general-purpose visual modeling language that is intended to provide a standard way to visualize the design of a system.UML provides a standard notation for many types of diagrams which can be roughly divided into three main groups: behavior diagrams: interaction diagrams: and structure diagrams.  / The creation of UML was originally motivated by the desire to standardize the disparate notational systems and approaches to software design. It was developed at Rational Software in 1994–1995: with further development led by them through 1996.In 1997: UML was adopted as a standard by the Object Management Group (OMG): and has been managed by this organization ever since. In 2005: UML was also published by the International Organization for Standardization (ISO) and the International Electrotechnical Commission (IEC) as the ISO/IEC 19501 standard. Since then the standard has been periodically revised to cover the latest revision of UML.In software engineering: most practitioners do not use UML: but instead produce informal hand drawn diagrams; these diagrams: however: often include elements from UML.: 536  /  /  / == History == /  /  / === Before UML 1.0 === / UML has been evolved since the second half of the 1990s and has its roots in the object-oriented programming methods developed in the late 1980s and early 1990s. The timeline (see image) shows the highlights of the history of object-oriented modeling methods and notation. / It is originally based on the notations of the Booch method: the object-modeling technique (OMT) and object-oriented software engineering (OOSE): which it has integrated into a single language.Rational Software Corporation hired James Rumbaugh from General Electric in 1994 and after that the company became the source for two of the most popular object-oriented modeling approaches of the day: Rumbaugh's object-modeling technique (OMT) and Grady Booch's method. They were soon assisted in their efforts by Ivar Jacobson: the creator of the object-oriented software engineering (OOSE) method: who joined them at Rational in 1995. /  /  / === UML 1.x === / Under the technical leadership of those three (Rumbaugh: Jacobson and Booch): a consortium called the UML Partners was organized in 1996 to complete the Unified Modeling Language (UML) specification: and propose it to the Object Management Group (OMG) for standardization. The partnership also contained additional interested parties (for example HP: DEC: IBM and Microsoft). The UML Partners' UML 1.0 draft was proposed to the OMG in January 1997 by the consortium. During the same month the UML Partners formed a group: designed to define the exact meaning of language constructs: chaired by Cris Kobryn and administered by Ed Eykholt: to finalize the specification and integrate it with other standardization efforts. The result of this work: UML 1.1: was submitted to the OMG in August 1997 and adopted by the OMG in November 1997.After the first release a task force was formed to improve the language: which released several minor revisions: 1.3: 1.4: and 1.5.The standards it produced (as well as the original standard) have been noted as being ambiguous and inconsistent. /  /  / ==== Cardinality notation ==== / As with database Chen: Bachman: and ISO ER diagrams: class models are specified to use ""look-across"" cardinalities: even though several authors (Merise: Elmasri & Navathe amongst others) prefer same-side or ""look-here"" for roles and both minimum and maximum cardinalities. Recent researchers (Feinerer: Dullea et al.) have shown that the ""look-across"" technique used by UML and ER diagrams is less effective and less coherent when applied to n-ary relationships of order strictly greater than 2. / Feinerer says: ""Problems arise if we operate under the look-across semantics as used for UML associations. Hartmann investigates this situation and shows how and why different transformations fail."": and: ""As we will see on the next few pages: the look-across interpretation introduces several difficulties which prevent the extension of simple mechanisms from binary to n-ary associations."""
0.17333389999112114,Who manages UML and when was it adopted as a standard? ,"UML was adopted as a standard by the Object Management Group (OMG) in 1997 and has been managed by this organization ever since. In 2005, UML was also published by the International Organization for Standardization (ISO) and the International Electrotechnical Commission (IEC) as the ISO/IEC 19501 standard.","Score: 0.8500995120245242 / The unified modeling language (UML) is a general-purpose visual modeling language that is intended to provide a standard way to visualize the design of a system.UML provides a standard notation for many types of diagrams which can be roughly divided into three main groups: behavior diagrams: interaction diagrams: and structure diagrams.  / The creation of UML was originally motivated by the desire to standardize the disparate notational systems and approaches to software design. It was developed at Rational Software in 1994–1995: with further development led by them through 1996.In 1997: UML was adopted as a standard by the Object Management Group (OMG): and has been managed by this organization ever since. In 2005: UML was also published by the International Organization for Standardization (ISO) and the International Electrotechnical Commission (IEC) as the ISO/IEC 19501 standard. Since then the standard has been periodically revised to cover the latest revision of UML.In software engineering: most practitioners do not use UML: but instead produce informal hand drawn diagrams; these diagrams: however: often include elements from UML.: 536  /  /  / == History == /  /  / === Before UML 1.0 === / UML has been evolved since the second half of the 1990s and has its roots in the object-oriented programming methods developed in the late 1980s and early 1990s. The timeline (see image) shows the highlights of the history of object-oriented modeling methods and notation. / It is originally based on the notations of the Booch method: the object-modeling technique (OMT) and object-oriented software engineering (OOSE): which it has integrated into a single language.Rational Software Corporation hired James Rumbaugh from General Electric in 1994 and after that the company became the source for two of the most popular object-oriented modeling approaches of the day: Rumbaugh's object-modeling technique (OMT) and Grady Booch's method. They were soon assisted in their efforts by Ivar Jacobson: the creator of the object-oriented software engineering (OOSE) method: who joined them at Rational in 1995. /  /  / === UML 1.x === / Under the technical leadership of those three (Rumbaugh: Jacobson and Booch): a consortium called the UML Partners was organized in 1996 to complete the Unified Modeling Language (UML) specification: and propose it to the Object Management Group (OMG) for standardization. The partnership also contained additional interested parties (for example HP: DEC: IBM and Microsoft). The UML Partners' UML 1.0 draft was proposed to the OMG in January 1997 by the consortium. During the same month the UML Partners formed a group: designed to define the exact meaning of language constructs: chaired by Cris Kobryn and administered by Ed Eykholt: to finalize the specification and integrate it with other standardization efforts. The result of this work: UML 1.1: was submitted to the OMG in August 1997 and adopted by the OMG in November 1997.After the first release a task force was formed to improve the language: which released several minor revisions: 1.3: 1.4: and 1.5.The standards it produced (as well as the original standard) have been noted as being ambiguous and inconsistent. /  /  / ==== Cardinality notation ==== / As with database Chen: Bachman: and ISO ER diagrams: class models are specified to use ""look-across"" cardinalities: even though several authors (Merise: Elmasri & Navathe amongst others) prefer same-side or ""look-here"" for roles and both minimum and maximum cardinalities. Recent researchers (Feinerer: Dullea et al.) have shown that the ""look-across"" technique used by UML and ER diagrams is less effective and less coherent when applied to n-ary relationships of order strictly greater than 2. / Feinerer says: ""Problems arise if we operate under the look-across semantics as used for UML associations. Hartmann investigates this situation and shows how and why different transformations fail."": and: ""As we will see on the next few pages: the look-across interpretation introduces several difficulties which prevent the extension of simple mechanisms from binary to n-ary associations.""","Score: 0.8483746071977559 / Some people (including Jacobson) feel that UML's size hinders learning (and therefore using) it.MS Visual Studio dropped support for UML in 2016 due to lack of usage.According to Google Trends UML has been on steady decline since 2004. /  /  / == See also == / Applications of UML / Business Process Model and Notation (BPMN) / C4 model / Department of Defense Architecture Framework / DOT (graph description language) / List of Unified Modeling Language tools / MODAF / Model-based testing / Model-driven engineering / Object-oriented role analysis and modeling / Process Specification Language / Systems Modeling Language (SysML) /  /  / == References == /  /  / == Further reading == / Ambler: Scott William (2004). The Object Primer: Agile Model Driven Development with UML 2. Cambridge University Press. ISBN 0-521-54018-6. Archived from the original on 31 January 2010. Retrieved 29 April 2006. / Chonoles: Michael Jesse; James A. Schardt (2003). UML 2 for Dummies. Wiley Publishing. ISBN 0-7645-2614-6. / Fowler: Martin (2004). UML Distilled: A Brief Guide to the Standard Object Modeling Language (3rd ed.). Addison-Wesley. ISBN 0-321-19368-7. / Jacobson: Ivar; Grady Booch; James Rumbaugh (1998). The Unified Software Development Process. Addison Wesley Longman. ISBN 0-201-57169-2. / Martin: Robert Cecil (2003). UML for Java Programmers. Prentice Hall. ISBN 0-13-142848-9. / Noran: Ovidiu S. ""Business Modelling: UML vs. IDEF"" (PDF). Retrieved 14 November 2022. / Horst Kargl. ""Interactive UML Metamodel with additional Examples"". / Penker: Magnus; Hans-Erik Eriksson (2000). Business Modeling with UML. John Wiley & Sons. ISBN 0-471-29551-5. / Douglass: Bruce Powel. ""Bruce Douglass: Real-Time Agile Systems and Software Development"" (web). Retrieved 1 January 2019. / Douglass: Bruce (2014). Real-Time UML Workshop 2nd Edition. Newnes. ISBN 978-0-471-29551-8. / Douglass: Bruce (2004). Real-Time UML 3rd Edition. Newnes. ISBN 978-0321160768. / Douglass: Bruce (2002). Real-Time Design Patterns. Addison-Wesley Professional. ISBN 978-0201699562. / Douglass: Bruce (2009). Real-Time Agility. Addison-Wesley Professional. ISBN 978-0321545497. / Douglass: Bruce (2010). Design Patterns for Embedded Systems in C. Newnes. ISBN 978-1856177078. /  /  / == External links == /  / Official website  / Current Version Specification"
0.1410191000031773,What are the main groups that UML diagrams can be divided into?,"UML diagrams can be largely divided into three main groups: behavior diagrams, interaction diagrams, and structure diagrams.","Score: 0.8491722256588504 / UML diagrams represent two different views of a system model: / Static (or structural) view: emphasizes the static structure of the system using objects: attributes: operations and relationships. It includes class diagrams and composite structure diagrams. / Dynamic (or behavioral) view: emphasizes the dynamic behavior of the system by showing collaborations among objects and changes to the internal states of objects. This view includes sequence diagrams: activity diagrams and state machine diagrams.UML models can be exchanged among UML tools by using the XML Metadata Interchange (XMI) format. / In UML: one of the key tools for behavior modeling is the use-case model: caused by OOSE. Use cases are a way of specifying required usages of a system. Typically: they are used to capture the requirements of a system: that is: what a system is supposed to do. /  /  / == Diagrams == / UML 2 has many types of diagrams: which are divided into two categories. Some types represent structural information: and the rest represent general types of behavior: including a few that represent different aspects of interactions. These diagrams can be categorized hierarchically as shown in the following class diagram: / These diagrams may all contain comments or notes explaining usage: constraint: or intent. /  /  / === Structure diagrams === / Structure diagrams represent the static aspects of the system. It emphasizes the things that must be present in the system being modeled. Since structure diagrams represent the structure: they are used extensively in documenting the software architecture of software systems. For example: the component diagram describes how a software system is split up into components and shows the dependencies among these components. /  / 			 / 			 / 		 / 		 / 			 / 			 / 		 /  /  / === Behavior diagrams === / Behavior diagrams represent the dynamic aspect of the system. It emphasizes what must happen in the system being modeled. Since behavior diagrams illustrate the behavior of a system: they are used extensively to describe the functionality of software systems. As an example: the activity diagram describes the business and operational step-by-step activities of the components in a system. /  / 			 / 			 / 		 / 		 / 			 / 			 / 		 /  /  / ==== Interaction diagrams ==== / Interaction diagrams: a subset of behavior diagrams: emphasize the flow of control and data among the things in the system being modeled. For example: the sequence diagram shows how objects communicate with each other regarding a sequence of messages. /  / 			 / 			 / 		 / 		 / 			 / 			 / 		 /  /  / == Metamodeling == /  / The Object Management Group (OMG) has developed a metamodeling architecture to define the UML: called the Meta-Object Facility. MOF is designed as a four-layered architecture: as shown in the image at right. It provides a meta-meta model at the top: called the M3 layer. This M3-model is the language used by Meta-Object Facility to build metamodels: called M2-models. / The most prominent example of a Layer 2 Meta-Object Facility model is the UML metamodel: which describes the UML itself. These M2-models describe elements of the M1-layer: and thus M1-models. These would be: for example: models written in UML. The last layer is the M0-layer or data layer. It is used to describe runtime instances of the system.The meta-model can be extended using a mechanism called stereotyping. This has been criticized as being insufficient/untenable by Brian Henderson-Sellers and Cesar Gonzalez-Perez in ""Uses and Abuses of the Stereotype Mechanism in UML 1.x and 2.0"". /  /  / == Adoption == / Back in 2013 UML has been marketed by OMG for many contexts: but aimed primarily at software development with limited success.It has been treated: at times: as a design silver bullet: which leads to problems. UML misuse includes overuse (designing every part of the system with it: which is unnecessary) and assuming that novices can design with it.It is considered a large language: with many constructs. Some people (including Jacobson) feel that UML's size hinders learning (and therefore using) it.MS Visual Studio dropped support for UML in 2016 due to lack of usage.According to Google Trends UML has been on steady decline since 2004. /  /  / == See also == / Applications of UML / Business Process Model and Notation (BPMN) / C4 model / Department of Defense Architecture Framework / DOT (graph description language) / List of Unified Modeling Language tools / MODAF / Model-based testing / Model-driven engineering / Object-oriented role analysis and modeling / Process Specification Language / Systems Modeling Language (SysML) /  /  / == References == /  /  / == Further reading == / Ambler: Scott William (2004). The Object Primer: Agile Model Driven Development with UML 2. Cambridge University Press.","Score: 0.8392918405374088 / The unified modeling language (UML) is a general-purpose visual modeling language that is intended to provide a standard way to visualize the design of a system.UML provides a standard notation for many types of diagrams which can be roughly divided into three main groups: behavior diagrams: interaction diagrams: and structure diagrams.  / The creation of UML was originally motivated by the desire to standardize the disparate notational systems and approaches to software design. It was developed at Rational Software in 1994–1995: with further development led by them through 1996.In 1997: UML was adopted as a standard by the Object Management Group (OMG): and has been managed by this organization ever since. In 2005: UML was also published by the International Organization for Standardization (ISO) and the International Electrotechnical Commission (IEC) as the ISO/IEC 19501 standard. Since then the standard has been periodically revised to cover the latest revision of UML.In software engineering: most practitioners do not use UML: but instead produce informal hand drawn diagrams; these diagrams: however: often include elements from UML.: 536  /  /  / == History == /  /  / === Before UML 1.0 === / UML has been evolved since the second half of the 1990s and has its roots in the object-oriented programming methods developed in the late 1980s and early 1990s. The timeline (see image) shows the highlights of the history of object-oriented modeling methods and notation. / It is originally based on the notations of the Booch method: the object-modeling technique (OMT) and object-oriented software engineering (OOSE): which it has integrated into a single language.Rational Software Corporation hired James Rumbaugh from General Electric in 1994 and after that the company became the source for two of the most popular object-oriented modeling approaches of the day: Rumbaugh's object-modeling technique (OMT) and Grady Booch's method. They were soon assisted in their efforts by Ivar Jacobson: the creator of the object-oriented software engineering (OOSE) method: who joined them at Rational in 1995. /  /  / === UML 1.x === / Under the technical leadership of those three (Rumbaugh: Jacobson and Booch): a consortium called the UML Partners was organized in 1996 to complete the Unified Modeling Language (UML) specification: and propose it to the Object Management Group (OMG) for standardization. The partnership also contained additional interested parties (for example HP: DEC: IBM and Microsoft). The UML Partners' UML 1.0 draft was proposed to the OMG in January 1997 by the consortium. During the same month the UML Partners formed a group: designed to define the exact meaning of language constructs: chaired by Cris Kobryn and administered by Ed Eykholt: to finalize the specification and integrate it with other standardization efforts. The result of this work: UML 1.1: was submitted to the OMG in August 1997 and adopted by the OMG in November 1997.After the first release a task force was formed to improve the language: which released several minor revisions: 1.3: 1.4: and 1.5.The standards it produced (as well as the original standard) have been noted as being ambiguous and inconsistent. /  /  / ==== Cardinality notation ==== / As with database Chen: Bachman: and ISO ER diagrams: class models are specified to use ""look-across"" cardinalities: even though several authors (Merise: Elmasri & Navathe amongst others) prefer same-side or ""look-here"" for roles and both minimum and maximum cardinalities. Recent researchers (Feinerer: Dullea et al.) have shown that the ""look-across"" technique used by UML and ER diagrams is less effective and less coherent when applied to n-ary relationships of order strictly greater than 2. / Feinerer says: ""Problems arise if we operate under the look-across semantics as used for UML associations. Hartmann investigates this situation and shows how and why different transformations fail."": and: ""As we will see on the next few pages: the look-across interpretation introduces several difficulties which prevent the extension of simple mechanisms from binary to n-ary associations."""
