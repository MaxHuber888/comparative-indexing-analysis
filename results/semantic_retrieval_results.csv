query_time,query,answer,response_0,response_1
0.7972130000125617,What is an AI accelerator?,"An AI accelerator, also referred to as deep learning processor, or neural processing unit (NPU), is a class of specialized hardware accelerator or computer system designed to accelerate artificial intelligence and machine learning applications, including artificial neural networks and machine vision.",Cosine Similarity: 0.8629087313331788 / An AI accelerator: deep learning processor: or neural processing unit (NPU) is a class of specialized hardware accelerator or computer system designed to accelerate artificial intelligence and machine learning applications: including artificial neural networks and machine vision.,"Cosine Similarity: 0.8617515818136974 / == Nomenclature == /  / As of 2016: the field is still in flux and vendors are pushing their own marketing term for what amounts to an ""AI accelerator"": in the hope that their designs and APIs will become the dominant design."
0.719247400003951,Where are AI accelerators often used?,"AI accelerators are used in various devices, including mobile devices such as neural processing units (NPUs) in Apple iPhones or Huawei cellphones, and personal computers such as Apple silicon Macs, to cloud computing servers such as tensor processing units (TPU) in the Google Cloud Platform. ",Cosine Similarity: 0.8788130765421266 / Table 2 lists several typical benchmarks for AI accelerators.,Cosine Similarity: 0.872348057373247 / Graphics processing units designed by companies such as Nvidia and AMD often include AI-specific hardware: and are commonly used as AI accelerators: both for training and inference.
0.7121677999966778,How have GPUs been utilized for AI acceleration?,"Graphics processing units or GPUs, that are specialized hardware for the manipulation of images and calculation of local image properties, have been increasingly used for machine learning tasks. GPU developers have also incorporated neural network-specific hardware to further accelerate these tasks.",Cosine Similarity: 0.9127457492293591 / As GPUs have been increasingly applied to AI acceleration: GPU manufacturers have incorporated neural network-specific hardware to further accelerate these tasks.,Cosine Similarity: 0.9023623672880091 / GPUs continue to be used in large-scale AI applications.
0.6935569999914151,What has been the progress of AI accelerator technology in the 1990s?,"In the 1990s, attempts were made to create high-throughput parallel systems for workstations aimed at various applications, including neural network simulations. FPGA-based accelerators for both inference and training were first explored in the 1990s.",Cosine Similarity: 0.8637032937462458 / As deep learning and artificial intelligence workloads rose in prominence in the 2010s: specialized hardware units were developed or adapted from existing products to accelerate these tasks.,Cosine Similarity: 0.8435222471438308 / As early as 1993: digital signal processors were used as neural network accelerators to accelerate optical character recognition software.By 1988: Wei Zhang et al.
0.7027099000115413,What is the difference between CPUs and AI accelerators in terms of performing AI-related tasks?,"While CPUs are used for running AI workloads and are superior for DNNs with small or medium-scale parallelism, for sparse DNNs and in low-batch-size scenarios, specialized AI accelerators like GPUs and FPGAs perform far better due to their optimized memory use and the use of lower precision arithmetic to accelerate calculation and increase throughput of computation. In fact, ASICs, a specific design of accelerators, may provide up to 10 times the efficiency of CPUs for AI-related tasks.",Cosine Similarity: 0.8790633181325637 / === Emergence of dedicated AI accelerator ASICs === /  / While GPUs and FPGAs perform far better than CPUs for AI-related tasks: a factor of up to 10 in efficiency may be gained with a more specific design: via an application-specific integrated circuit (ASIC).,Cosine Similarity: 0.863767899999669 / Due to the increasing performance of CPUs: they are also used for running AI workloads.
0.6854800999863073,What is the BERT model?,"The Bidirectional Encoder Representations from Transformers (BERT) is a language model based on the transformer architecture, introduced in October 2018 by researchers at Google. It is known for its significant improvement over previous models. BERT was originally implemented in the English language and has become a ubiquitous baseline in Natural Language Processing (NLP) experiments.",Cosine Similarity: 0.9017703695964713 / Bidirectional Encoder Representations from Transformers (BERT) is a language model based on the transformer architecture: notable for its dramatic improvement over previous state of the art models.,Cosine Similarity: 0.8943952231011779 / Unlike previous models: BERT is a deeply bidirectional: unsupervised language representation: pre-trained using only a plain text corpus.
0.6543107000179589,How is BERT pretrained?,"BERT was pre-trained simultaneously on two tasks: language modeling and next sentence prediction. In language modeling, a portion of the tokens in the text were selected for prediction and were replaced with a [MASK] token, a random word token, or not replaced. The next sentence prediction task involved predicting if two spans of text appeared sequentially in the training corpus, outputting either [IsNext] or [NotNext].",Cosine Similarity: 0.8779266397260788 / Unlike previous models: BERT is a deeply bidirectional: unsupervised language representation: pre-trained using only a plain text corpus.,Cosine Similarity: 0.8755866936969097 / After pre-training: BERT can be fine-tuned with fewer resources on smaller datasets to optimize its performance on specific tasks such as NLP tasks (language inference: text classification) and sequence-to-sequence based language generation tasks (question-answering: conversational response generation).
0.730031000013696,Can you describe the architecture of BERT?,"BERT is an ""encoder-only"" transformer architecture. It consists of an embedding module, a stack of encoders, and an un-embedding module. The lowest layer is the embedding layer which contains word embeddings, position embeddings, and token type embeddings. The representation vectors then move through Transformer encoders and are then un-embedded.",Cosine Similarity: 0.881961817477122 / === Architecture details === /  / This section describes BERTBASE.,"Cosine Similarity: 0.8671212767626246 / == Design == /  / BERT is an ""encoder-only"" transformer architecture."
0.6877316000172868,How does BERT perform in natural language understanding tasks?,"When BERT was published, it achieved state-of-the-art performance on a number of natural language understanding tasks including the GLUE (General Language Understanding Evaluation) task set, SQuAD (Stanford Question Answering Dataset), and SWAG (Situations With Adversarial Generations).",Cosine Similarity: 0.9126384487213499 / == Performance == /  / When BERT was published: it achieved state-of-the-art performance on a number of natural language understanding tasks: /  / GLUE (General Language Understanding Evaluation) task set (consisting of 9 tasks) /  / SQuAD (Stanford Question Answering Dataset) v1.1 and v2.0 /  / SWAG (Situations With Adversarial Generations) /  /  /  /  /  / == Analysis == /  / The reasons for BERT's state-of-the-art performance on these natural language understanding tasks are not yet well understood.,"Cosine Similarity: 0.8903923246358857 / A 2020 literature survey concluded that ""in a little over a year: BERT has become a ubiquitous baseline in Natural Language Processing (NLP) experiments counting over 150 research publications analyzing and improving the model."
0.6739417999924626,Who were the original researchers behind BERT and what innovation did it provide over previous models?,"BERT was originally published by Google researchers Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Unlike previous models such as word2vec or GloVe which generated a single word embedding representation for each word in the vocabulary, BERT takes into account the context for each occurrence of a given word, providing a contextualized embedding that varies according to the sentence.",Cosine Similarity: 0.8757627793226866 / == History == /  / BERT was originally published by Google researchers Jacob Devlin: Ming-Wei Chang: Kenton Lee: and Kristina Toutanova.,"Cosine Similarity: 0.8653543618649632 / A 2020 literature survey concluded that ""in a little over a year: BERT has become a ubiquitous baseline in Natural Language Processing (NLP) experiments counting over 150 research publications analyzing and improving the model."
0.6185662999923807,What is the BigScience Large Open-science Open-access Multilingual Language Model (BLOOM)?,"The BigScience Large Open-science Open-access Multilingual Language Model (BLOOM) is a 176-billion-parameter transformer-based autoregressive large language model (LLM), trained on approximately 366 billion tokens from March to July 2022. ",Cosine Similarity: 0.9543515695558433 / BigScience Large Open-science Open-access Multilingual Language Model (BLOOM) is a 176-billion-parameter transformer-based autoregressive large language model (LLM).,Cosine Similarity: 0.8631262837867925 / BLOOM was trained on approximately 366 billion (1.6TB) tokens from March to July 2022.BLOOM is the main outcome of the BigScience collaborative initiative: a one-year-long research workshop that took place between May 2021 and May 2022.
0.6110635999939404,What is unique about BLOOM's licensing?,"BLOOM, its code base, and the data used to train it are all distributed under free licenses, which makes the resources open and accessible to the general public.",Cosine Similarity: 0.8087444892313346 / BigScience Large Open-science Open-access Multilingual Language Model (BLOOM) is a 176-billion-parameter transformer-based autoregressive large language model (LLM).,Cosine Similarity: 0.8050392125807003 / BLOOM was trained on approximately 366 billion (1.6TB) tokens from March to July 2022.BLOOM is the main outcome of the BigScience collaborative initiative: a one-year-long research workshop that took place between May 2021 and May 2022.
0.7626002999895718,What are the origins of BLOOM?,"BLOOM was the main result of the BigScience collaborative initiative, a one-year-long research workshop that took place between May 2021 and May 2022. It was led by HuggingFace and involved several hundreds of researchers and engineers from France and globally, including both academia and the private sector.",Cosine Similarity: 0.8385294438531115 / BLOOM was trained on approximately 366 billion (1.6TB) tokens from March to July 2022.BLOOM is the main outcome of the BigScience collaborative initiative: a one-year-long research workshop that took place between May 2021 and May 2022.,Cosine Similarity: 0.8377483477459923 / BLOOM's training corpus: named ROOTS: combines data extracted from the then-latest version of the web-based OSCAR corpus (38% of ROOTS) and newly collected data extracted from a manually selected and documented list of language data sources.
0.633903999987524,On which supercomputer was BLOOM trained and who supported the training?,"BLOOM was trained on the French public supercomputer Jean Zay, managed by GENCI and IDRIS (CNRS). BigScience, the initiative behind BLOOM, was supported by a large-scale public compute grant on Jean Zay.",Cosine Similarity: 0.8573338598853251 / BLOOM was trained on approximately 366 billion (1.6TB) tokens from March to July 2022.BLOOM is the main outcome of the BigScience collaborative initiative: a one-year-long research workshop that took place between May 2021 and May 2022.,Cosine Similarity: 0.8330551798800658 / BigScience was supported by a large-scale public compute grant on the French public supercomputer Jean Zay: managed by GENCI and IDRIS (CNRS): on which it was trained.
0.628807599976426,"Can you tell me more about BLOOM's training corpus, ROOTS?","ROOTS is the training corpus for BLOOM and combines data extracted from the web-based OSCAR corpus (38% of ROOTS) and newly collected data from a manually selected list of language data sources. It includes data in 46 natural languages, with English forming 30% of the whole dataset to as little as 0.00002% for Chi Tumbuka, as well as data in 13 different programming languages.",Cosine Similarity: 0.9150917433813257 / BLOOM's training corpus: named ROOTS: combines data extracted from the then-latest version of the web-based OSCAR corpus (38% of ROOTS) and newly collected data extracted from a manually selected and documented list of language data sources.,Cosine Similarity: 0.841160095484106 / BLOOM was trained on approximately 366 billion (1.6TB) tokens from March to July 2022.BLOOM is the main outcome of the BigScience collaborative initiative: a one-year-long research workshop that took place between May 2021 and May 2022.
0.6145252999849617,What is Chinchilla and when was it presented?,"Chinchilla is a family of large language models developed by the research team at DeepMind, presented in March 2022.","Cosine Similarity: 0.8591267680532445 / It is named ""chinchilla"" because it is a further development over a previous model family named Gopher.",Cosine Similarity: 0.8533154845246735 / Chinchilla is a family of large language models developed by the research team at DeepMind: presented in March 2022.
0.6568964000034612,How does Chinchilla compare to the previous model named Gopher?,"Chinchilla is a further development over Gopher. It requires less computer power for inference and fine-tuning, has an average accuracy of 67.5% on the MMLU benchmark which is 7% higher than Gopher, and has 70 billion parameters with four times as much data.","Cosine Similarity: 0.9264452353054111 / It is named ""chinchilla"" because it is a further development over a previous model family named Gopher.",Cosine Similarity: 0.8888068093282753 / The Chinchilla family is the same as the Gopher family: but trained with AdamW instead of Adam optimizer.
0.5693286999885458,What hypothesis was used to train the Chinchilla by DeepMind?,"The hypothesis used to train Chinchilla by DeepMind is that if one doubles the model size, one must also have twice the number of training tokens.",Cosine Similarity: 0.9680472880073929 / This hypothesis has been used to train Chinchilla by DeepMind.,Cosine Similarity: 0.8738123371802412 / Chinchilla is a family of large language models developed by the research team at DeepMind: presented in March 2022.
0.6211097000050358,How are the Chinchilla and Gopher families similar in terms of architecture?,"Both the Gopher and Chinchilla families are transformer models. They are essentially the same as GPT-2, with different sizes and minor modifications. Gopher uses RMSNorm instead of LayerNorm, and relative positional encoding rather than absolute. Chinchilla is the same as Gopher but trained with AdamW instead of Adam optimizer.",Cosine Similarity: 0.9013982326301927 / == Architecture == /  / Both the Gopher family and Chinchilla family are families of transformer models.,Cosine Similarity: 0.8850712634625507 / Similar naming conventions apply for the Chinchilla family.
0.657849899987923,"What is the status of Chinchilla as of January 12, 2023?","As of January 12, 2023, Chinchilla was still in the testing phase.",Cosine Similarity: 0.8763450195597666 / Chinchilla was still in the testing phase as of January 12: 2023.Chinchilla contributes to developing an effective training paradigm for large autoregressive language models with limited compute resources.,Cosine Similarity: 0.854621732537518 / Chinchilla is a family of large language models developed by the research team at DeepMind: presented in March 2022.
0.6120050000026822,What are dilution and dropout techniques in artificial neural networks?,"Dilution and dropout techniques in artificial neural networks are regularization techniques aimed at reducing overfitting by preventing complex co-adaptations on training data. In these processes, dilution refers to thinning weights, while dropout refers to randomly ""dropping out"", or omitting, units during the training process of a neural network.","Cosine Similarity: 0.910014929266841 / Dilution and dropout (also called DropConnect) are regularization techniques for reducing overfitting in artificial neural networks by preventing complex co-adaptations on training data.They are an efficient way of performing model averaging with neural networks. Dilution refers to thinning weights: while dropout refers to randomly ""dropping out"": or omitting: units (both hidden and visible) during the training process of a neural network. Both trigger the same type of regularization. == Types and uses == /  / Dilution is usually split in weak dilution and strong dilution. Weak dilution describes the process in which the finite fraction of removed connections is small: and strong dilution refers to when this fraction is large. There is no clear distinction on where the limit between strong and weak dilution is: and often the distinction is dependent on the precedent of a specific use-case and has implications for how to solve for exact solutions. Sometimes dilution is used for adding damping noise to the inputs. In that case: weak dilution refers to adding a small amount of damping noise: while strong dilution refers to adding a greater amount of damping noise. Both can be rewritten as variants of weight dilution. These techniques are also sometimes referred to as random pruning of weights: but this is usually a non-recurring one-way operation. The network is pruned: and then kept if it is an improvement over the previous model. Dilution and dropout both refer to an iterative process. The pruning of weights typically does not imply that the network continues learning: while in dilution/dropout: the network continues to learn after the technique is applied. == Generalized linear network == /  / Output from a layer of linear nodes: in an artificial neural net can be described as /  /  /  / yi{\displaystyle y_{i}} – output from node i{\displaystyle i} /  / wij{\displaystyle w_{ij}} – real weight before dilution: also called the Hebb connection strength /  / xj{\displaystyle x_{j}} – input from node j{\displaystyle j}This can be written in vector notation as /  /  /  / y{\displaystyle \mathbf {y} } – output vector /  / W{\displaystyle \mathbf {W} } – weight matrix /  / x{\displaystyle \mathbf {x} } – input vectorEquations (1) and (2) are used in the subsequent sections. == Weak dilution == /  / During weak dilution: the finite fraction of removed connections (the weights) is small: giving rise to a tiny uncertainty. This edge-case can be solved exactly with mean field theory. In weak dilution the impact on the weights can be described as /  /  /  / wij^{\displaystyle {\hat {w_{ij}}}} – diluted weight /  / wij{\displaystyle w_{ij}} – real weight before dilution /  / P(c){\displaystyle P(c)} – the probability of c{\displaystyle c}: the probability of keeping a weightThe interpretation of probability P(c){\displaystyle P(c)} can also be changed from keeping a weight into pruning a weight. In vector notation this can be written as /  /  /  / where the function g⁡(⋅){\displaystyle \operatorname {g} (\cdot )} imposes the previous dilution. In weak dilution only a small and fixed fraction of the weights are diluted. When the number of terms in the sum goes to infinite (the weights for each node) it is still infinite (the fraction is fixed): thus mean field theory can be applied. In the notation from Hertz et al. this would be written as /  /  /  / ⟨hi⟩{\displaystyle \left\langle h_{i}\right\rangle } the mean field temperature /  / c{\displaystyle c} – a scaling factor for the temperature from the probability of keeping the weight /  / wij{\displaystyle w_{ij}} – real weight before dilution: also called the Hebb connection strength /  / ⟨Sj⟩{\displaystyle \left\langle S_{j}\right\rangle } – the mean stable equilibrium statesThere are some assumptions for this to hold: which are not listed here. == Strong dilution == /  / When the dilution is strong: the finite fraction of removed connections (the weights) is large: giving rise to a huge uncertainty. == Dropout == /  / Dropout is a special case of the previous weight equation (3): where the aforementioned equation is adjusted to remove a whole row in the vector matrix: and not only random weights /  /  /  / P(c){\displaystyle P(c)} – the probability c{\displaystyle c} to keep a row in the weight matrix /  / wj{\displaystyle \mathbf {w} _{j}} – real row in the weight matrix before dropout /  / wj^{\displaystyle {\hat {\mathbf {w} _{j}}}} – diluted row in the weight matrixBecause dropout removes a whole row from the vector matrix: the previous (unlisted) assumptions for weak dilution and the use of mean field theory are not applicable. The process by which the node is driven to zero: whether by setting the weights to zero: by “removing the node”: or by some other means: does not impact the end result and does not create a new and unique case. If the neural net is processed by a high-performance digital array-multiplicator: then it is likely more effective to drive the value to zero late in the process graph. If the net is processed by a constrained processor: perhaps even an analog neuromorph processor: then it is likely a more power-efficient solution is to drive the value to zero early in the process graph. == Google's patent == /  / Although there have been examples of randomly removing connections between neurons in a neural network to improve models: this technique was first introduced with the name dropout by Geoffrey Hinton: et al. in 2012. Google currently holds the patent for the dropout technique. == See also == /  / AlexNet /  / Convolutional neural network § Dropout /  /  /  /  /  / == Notes == /  /  /  /  /  / == References == ","Cosine Similarity: 0.8662556445581432 / This is the biggest contribution of the dropout method: although it effectively generates 2n{\displaystyle 2^{n}} neural nets: and as such allows for model combination: at test time only a single network needs to be tested.By avoiding training all nodes on all training data: dropout decreases overfitting. The method also significantly improves training speed. This makes the model combination practical: even for deep neural networks. The technique seems to reduce node interactions: leading them to learn more robust features that better generalize to new data. ==== DropConnect ==== /  / DropConnect is the generalization of dropout in which each connection: rather than each output unit: can be dropped with probability 1−p{\displaystyle 1-p}. Each unit thus receives input from a random subset of units in the previous layer.DropConnect is similar to dropout as it introduces dynamic sparsity within the model: but differs in that the sparsity is on the weights: rather than the output vectors of a layer. In other words: the fully connected layer with DropConnect becomes a sparsely connected layer in which the connections are chosen at random during the training stage. ==== Stochastic pooling ==== /  / A major drawback to Dropout is that it does not have the same benefits for convolutional layers: where the neurons are not fully connected. Even before Dropout: in 2013 a technique called stochastic pooling: the conventional deterministic pooling operations were replaced with a stochastic procedure: where the activation within each pooling region is picked randomly according to a multinomial distribution: given by the activities within the pooling region. This approach is free of hyperparameters and can be combined with other regularization approaches: such as dropout and data augmentation. An alternate view of stochastic pooling is that it is equivalent to standard max pooling but with many copies of an input image: each having small local deformations. This is similar to explicit elastic deformations of the input images: which delivers excellent performance on the MNIST data set. Using stochastic pooling in a multilayer model gives an exponential number of deformations since the selections in higher layers are independent of those below. ==== Artificial data ==== /  /  /  / Because the degree of model overfitting is determined by both its power and the amount of training it receives: providing a convolutional network with more training examples can reduce overfitting. Because there is often not enough available data to train: especially considering that some part should be spared for later testing: two approaches are to either generate new data from scratch (if possible) or perturb existing data to create new ones. The latter one is used since mid-1990s. For example: input images can be cropped: rotated: or rescaled to create new examples with the same labels as the original training set. === Explicit === /  /  /  /  /  / ==== Early stopping ==== /  /  /  / One of the simplest methods to prevent overfitting of a network is to simply stop the training before overfitting has had a chance to occur. It comes with the disadvantage that the learning process is halted. ==== Number of parameters ==== /  / Another simple way to prevent overfitting is to limit the number of parameters: typically by limiting the number of hidden units in each layer or limiting network depth. For convolutional networks: the filter size also affects the number of parameters. Limiting the number of parameters restricts the predictive power of the network directly: reducing the complexity of the function that it can perform on the data: and thus limits the amount of overfitting. This is equivalent to a ""zero norm"". ==== Weight decay ==== /  / A simple form of added regularizer is weight decay: which simply adds an additional error: proportional to the sum of weights (L1 norm) or squared magnitude (L2 norm) of the weight vector: to the error at each node. The level of acceptable model complexity can be reduced by increasing the proportionality constant('alpha' hyperparameter): thus increasing the penalty for large weight vectors. L2 regularization is the most common form of regularization. It can be implemented by penalizing the squared magnitude of all parameters directly in the objective. The L2 regularization has the intuitive interpretation of heavily penalizing peaky weight vectors and preferring diffuse weight vectors. Due to multiplicative interactions between weights and inputs this has the useful property of encouraging the network to use all of its inputs a little rather than some of its inputs a lot. L1 regularization is also common. It makes the weight vectors sparse during optimization. In other words: neurons with L1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the noisy inputs. L1 with L2 regularization can be combined; this is called elastic net regularization. ==== Max norm constraints ==== /  / Another form of regularization is to enforce an absolute upper bound on the magnitude of the weight vector for every neuron and use projected gradient descent to enforce the constraint. In practice: this corresponds to performing the parameter update as normal: and then enforcing the constraint by clamping the weight vector w→{\displaystyle {\vec {w}}} of every neuron to satisfy ‖w→‖2<c{\displaystyle \|{\vec {w}}\|_{2}<c}. Typical values of c{\displaystyle c} are order of 3–4. Some papers report improvements when using this form of regularization. == Hierarchical coordinate frames == /  / Pooling loses the precise spatial relationships between high-level parts (such as nose and mouth in a face image). These relationships are needed for identity recognition. Overlapping the pools so that each feature occurs in multiple pools: helps retain the information. Translation alone cannot extrapolate the understanding of geometric relationships to a radically new viewpoint: such as a different orientation or scale. On the other hand: people are very good at extrapolating; after seeing a new shape once they can recognize it from a different viewpoint.An earlier common way to deal with this problem is to train the network on transformed data in different orientations: scales: lighting: etc. so that the network can cope with these variations. This is computationally intensive for large data-sets. The alternative is to use a hierarchy of coordinate frames and use a group of neurons to represent a conjunction of the shape of the feature and its pose relative to the retina. "
0.6021092000009958,How are weak dilution and strong dilution differentiated in neural networks?,"In neural networks, weak dilution is when the finite fraction of removed connections is small, often adding a small amount of damping noise to the inputs. Conversely, strong dilution refers to when this fraction of removed connections is large, often accompanied by adding a greater amount of damping noise.","Cosine Similarity: 0.8664838912985595 / Dilution and dropout (also called DropConnect) are regularization techniques for reducing overfitting in artificial neural networks by preventing complex co-adaptations on training data.They are an efficient way of performing model averaging with neural networks. Dilution refers to thinning weights: while dropout refers to randomly ""dropping out"": or omitting: units (both hidden and visible) during the training process of a neural network. Both trigger the same type of regularization. == Types and uses == /  / Dilution is usually split in weak dilution and strong dilution. Weak dilution describes the process in which the finite fraction of removed connections is small: and strong dilution refers to when this fraction is large. There is no clear distinction on where the limit between strong and weak dilution is: and often the distinction is dependent on the precedent of a specific use-case and has implications for how to solve for exact solutions. Sometimes dilution is used for adding damping noise to the inputs. In that case: weak dilution refers to adding a small amount of damping noise: while strong dilution refers to adding a greater amount of damping noise. Both can be rewritten as variants of weight dilution. These techniques are also sometimes referred to as random pruning of weights: but this is usually a non-recurring one-way operation. The network is pruned: and then kept if it is an improvement over the previous model. Dilution and dropout both refer to an iterative process. The pruning of weights typically does not imply that the network continues learning: while in dilution/dropout: the network continues to learn after the technique is applied. == Generalized linear network == /  / Output from a layer of linear nodes: in an artificial neural net can be described as /  /  /  / yi{\displaystyle y_{i}} – output from node i{\displaystyle i} /  / wij{\displaystyle w_{ij}} – real weight before dilution: also called the Hebb connection strength /  / xj{\displaystyle x_{j}} – input from node j{\displaystyle j}This can be written in vector notation as /  /  /  / y{\displaystyle \mathbf {y} } – output vector /  / W{\displaystyle \mathbf {W} } – weight matrix /  / x{\displaystyle \mathbf {x} } – input vectorEquations (1) and (2) are used in the subsequent sections. == Weak dilution == /  / During weak dilution: the finite fraction of removed connections (the weights) is small: giving rise to a tiny uncertainty. This edge-case can be solved exactly with mean field theory. In weak dilution the impact on the weights can be described as /  /  /  / wij^{\displaystyle {\hat {w_{ij}}}} – diluted weight /  / wij{\displaystyle w_{ij}} – real weight before dilution /  / P(c){\displaystyle P(c)} – the probability of c{\displaystyle c}: the probability of keeping a weightThe interpretation of probability P(c){\displaystyle P(c)} can also be changed from keeping a weight into pruning a weight. In vector notation this can be written as /  /  /  / where the function g⁡(⋅){\displaystyle \operatorname {g} (\cdot )} imposes the previous dilution. In weak dilution only a small and fixed fraction of the weights are diluted. When the number of terms in the sum goes to infinite (the weights for each node) it is still infinite (the fraction is fixed): thus mean field theory can be applied. In the notation from Hertz et al. this would be written as /  /  /  / ⟨hi⟩{\displaystyle \left\langle h_{i}\right\rangle } the mean field temperature /  / c{\displaystyle c} – a scaling factor for the temperature from the probability of keeping the weight /  / wij{\displaystyle w_{ij}} – real weight before dilution: also called the Hebb connection strength /  / ⟨Sj⟩{\displaystyle \left\langle S_{j}\right\rangle } – the mean stable equilibrium statesThere are some assumptions for this to hold: which are not listed here. == Strong dilution == /  / When the dilution is strong: the finite fraction of removed connections (the weights) is large: giving rise to a huge uncertainty. == Dropout == /  / Dropout is a special case of the previous weight equation (3): where the aforementioned equation is adjusted to remove a whole row in the vector matrix: and not only random weights /  /  /  / P(c){\displaystyle P(c)} – the probability c{\displaystyle c} to keep a row in the weight matrix /  / wj{\displaystyle \mathbf {w} _{j}} – real row in the weight matrix before dropout /  / wj^{\displaystyle {\hat {\mathbf {w} _{j}}}} – diluted row in the weight matrixBecause dropout removes a whole row from the vector matrix: the previous (unlisted) assumptions for weak dilution and the use of mean field theory are not applicable. The process by which the node is driven to zero: whether by setting the weights to zero: by “removing the node”: or by some other means: does not impact the end result and does not create a new and unique case. If the neural net is processed by a high-performance digital array-multiplicator: then it is likely more effective to drive the value to zero late in the process graph. If the net is processed by a constrained processor: perhaps even an analog neuromorph processor: then it is likely a more power-efficient solution is to drive the value to zero early in the process graph. == Google's patent == /  / Although there have been examples of randomly removing connections between neurons in a neural network to improve models: this technique was first introduced with the name dropout by Geoffrey Hinton: et al. in 2012. Google currently holds the patent for the dropout technique. == See also == /  / AlexNet /  / Convolutional neural network § Dropout /  /  /  /  /  / == Notes == /  /  /  /  /  / == References == ",Cosine Similarity: 0.8315013426197113 / There are two main types of neural network.
0.6228436000237707,How is dilution related to adding damping noise to inputs?,"Dilution can be used for adding damping noise to the inputs within a neural network. Weak dilution refers to adding a small amount of damping noise, while strong dilution refers to adding a larger amount of damping noise. These can be considered as variants of weight dilution.","Cosine Similarity: 0.8086204531314339 / Dilution and dropout (also called DropConnect) are regularization techniques for reducing overfitting in artificial neural networks by preventing complex co-adaptations on training data.They are an efficient way of performing model averaging with neural networks. Dilution refers to thinning weights: while dropout refers to randomly ""dropping out"": or omitting: units (both hidden and visible) during the training process of a neural network. Both trigger the same type of regularization. == Types and uses == /  / Dilution is usually split in weak dilution and strong dilution. Weak dilution describes the process in which the finite fraction of removed connections is small: and strong dilution refers to when this fraction is large. There is no clear distinction on where the limit between strong and weak dilution is: and often the distinction is dependent on the precedent of a specific use-case and has implications for how to solve for exact solutions. Sometimes dilution is used for adding damping noise to the inputs. In that case: weak dilution refers to adding a small amount of damping noise: while strong dilution refers to adding a greater amount of damping noise. Both can be rewritten as variants of weight dilution. These techniques are also sometimes referred to as random pruning of weights: but this is usually a non-recurring one-way operation. The network is pruned: and then kept if it is an improvement over the previous model. Dilution and dropout both refer to an iterative process. The pruning of weights typically does not imply that the network continues learning: while in dilution/dropout: the network continues to learn after the technique is applied. == Generalized linear network == /  / Output from a layer of linear nodes: in an artificial neural net can be described as /  /  /  / yi{\displaystyle y_{i}} – output from node i{\displaystyle i} /  / wij{\displaystyle w_{ij}} – real weight before dilution: also called the Hebb connection strength /  / xj{\displaystyle x_{j}} – input from node j{\displaystyle j}This can be written in vector notation as /  /  /  / y{\displaystyle \mathbf {y} } – output vector /  / W{\displaystyle \mathbf {W} } – weight matrix /  / x{\displaystyle \mathbf {x} } – input vectorEquations (1) and (2) are used in the subsequent sections. == Weak dilution == /  / During weak dilution: the finite fraction of removed connections (the weights) is small: giving rise to a tiny uncertainty. This edge-case can be solved exactly with mean field theory. In weak dilution the impact on the weights can be described as /  /  /  / wij^{\displaystyle {\hat {w_{ij}}}} – diluted weight /  / wij{\displaystyle w_{ij}} – real weight before dilution /  / P(c){\displaystyle P(c)} – the probability of c{\displaystyle c}: the probability of keeping a weightThe interpretation of probability P(c){\displaystyle P(c)} can also be changed from keeping a weight into pruning a weight. In vector notation this can be written as /  /  /  / where the function g⁡(⋅){\displaystyle \operatorname {g} (\cdot )} imposes the previous dilution. In weak dilution only a small and fixed fraction of the weights are diluted. When the number of terms in the sum goes to infinite (the weights for each node) it is still infinite (the fraction is fixed): thus mean field theory can be applied. In the notation from Hertz et al. this would be written as /  /  /  / ⟨hi⟩{\displaystyle \left\langle h_{i}\right\rangle } the mean field temperature /  / c{\displaystyle c} – a scaling factor for the temperature from the probability of keeping the weight /  / wij{\displaystyle w_{ij}} – real weight before dilution: also called the Hebb connection strength /  / ⟨Sj⟩{\displaystyle \left\langle S_{j}\right\rangle } – the mean stable equilibrium statesThere are some assumptions for this to hold: which are not listed here. == Strong dilution == /  / When the dilution is strong: the finite fraction of removed connections (the weights) is large: giving rise to a huge uncertainty. == Dropout == /  / Dropout is a special case of the previous weight equation (3): where the aforementioned equation is adjusted to remove a whole row in the vector matrix: and not only random weights /  /  /  / P(c){\displaystyle P(c)} – the probability c{\displaystyle c} to keep a row in the weight matrix /  / wj{\displaystyle \mathbf {w} _{j}} – real row in the weight matrix before dropout /  / wj^{\displaystyle {\hat {\mathbf {w} _{j}}}} – diluted row in the weight matrixBecause dropout removes a whole row from the vector matrix: the previous (unlisted) assumptions for weak dilution and the use of mean field theory are not applicable. The process by which the node is driven to zero: whether by setting the weights to zero: by “removing the node”: or by some other means: does not impact the end result and does not create a new and unique case. If the neural net is processed by a high-performance digital array-multiplicator: then it is likely more effective to drive the value to zero late in the process graph. If the net is processed by a constrained processor: perhaps even an analog neuromorph processor: then it is likely a more power-efficient solution is to drive the value to zero early in the process graph. == Google's patent == /  / Although there have been examples of randomly removing connections between neurons in a neural network to improve models: this technique was first introduced with the name dropout by Geoffrey Hinton: et al. in 2012. Google currently holds the patent for the dropout technique. == See also == /  / AlexNet /  / Convolutional neural network § Dropout /  /  /  /  /  / == Notes == /  /  /  /  /  / == References == ",Cosine Similarity: 0.8071145780319734 / Given noisy measurements of a generic dynamic system described by the equation above: PINNs can be designed to solve two classes of problems: /  /  /  / data-driven solution /  / data-driven discoveryof partial differential equations.
0.5813462999940384,How does the dropout technique differ from dilution in terms of implementation?,"The dropout technique can be considered a special case of dilution where the equation is adjusted to remove a whole row in the vector matrix, rather than just random weights. This process doesn’t rely on whether the weights are set to zero, the node is removed, or any other means. The end result remains the same.","Cosine Similarity: 0.8737338881207368 / Dilution and dropout (also called DropConnect) are regularization techniques for reducing overfitting in artificial neural networks by preventing complex co-adaptations on training data.They are an efficient way of performing model averaging with neural networks. Dilution refers to thinning weights: while dropout refers to randomly ""dropping out"": or omitting: units (both hidden and visible) during the training process of a neural network. Both trigger the same type of regularization. == Types and uses == /  / Dilution is usually split in weak dilution and strong dilution. Weak dilution describes the process in which the finite fraction of removed connections is small: and strong dilution refers to when this fraction is large. There is no clear distinction on where the limit between strong and weak dilution is: and often the distinction is dependent on the precedent of a specific use-case and has implications for how to solve for exact solutions. Sometimes dilution is used for adding damping noise to the inputs. In that case: weak dilution refers to adding a small amount of damping noise: while strong dilution refers to adding a greater amount of damping noise. Both can be rewritten as variants of weight dilution. These techniques are also sometimes referred to as random pruning of weights: but this is usually a non-recurring one-way operation. The network is pruned: and then kept if it is an improvement over the previous model. Dilution and dropout both refer to an iterative process. The pruning of weights typically does not imply that the network continues learning: while in dilution/dropout: the network continues to learn after the technique is applied. == Generalized linear network == /  / Output from a layer of linear nodes: in an artificial neural net can be described as /  /  /  / yi{\displaystyle y_{i}} – output from node i{\displaystyle i} /  / wij{\displaystyle w_{ij}} – real weight before dilution: also called the Hebb connection strength /  / xj{\displaystyle x_{j}} – input from node j{\displaystyle j}This can be written in vector notation as /  /  /  / y{\displaystyle \mathbf {y} } – output vector /  / W{\displaystyle \mathbf {W} } – weight matrix /  / x{\displaystyle \mathbf {x} } – input vectorEquations (1) and (2) are used in the subsequent sections. == Weak dilution == /  / During weak dilution: the finite fraction of removed connections (the weights) is small: giving rise to a tiny uncertainty. This edge-case can be solved exactly with mean field theory. In weak dilution the impact on the weights can be described as /  /  /  / wij^{\displaystyle {\hat {w_{ij}}}} – diluted weight /  / wij{\displaystyle w_{ij}} – real weight before dilution /  / P(c){\displaystyle P(c)} – the probability of c{\displaystyle c}: the probability of keeping a weightThe interpretation of probability P(c){\displaystyle P(c)} can also be changed from keeping a weight into pruning a weight. In vector notation this can be written as /  /  /  / where the function g⁡(⋅){\displaystyle \operatorname {g} (\cdot )} imposes the previous dilution. In weak dilution only a small and fixed fraction of the weights are diluted. When the number of terms in the sum goes to infinite (the weights for each node) it is still infinite (the fraction is fixed): thus mean field theory can be applied. In the notation from Hertz et al. this would be written as /  /  /  / ⟨hi⟩{\displaystyle \left\langle h_{i}\right\rangle } the mean field temperature /  / c{\displaystyle c} – a scaling factor for the temperature from the probability of keeping the weight /  / wij{\displaystyle w_{ij}} – real weight before dilution: also called the Hebb connection strength /  / ⟨Sj⟩{\displaystyle \left\langle S_{j}\right\rangle } – the mean stable equilibrium statesThere are some assumptions for this to hold: which are not listed here. == Strong dilution == /  / When the dilution is strong: the finite fraction of removed connections (the weights) is large: giving rise to a huge uncertainty. == Dropout == /  / Dropout is a special case of the previous weight equation (3): where the aforementioned equation is adjusted to remove a whole row in the vector matrix: and not only random weights /  /  /  / P(c){\displaystyle P(c)} – the probability c{\displaystyle c} to keep a row in the weight matrix /  / wj{\displaystyle \mathbf {w} _{j}} – real row in the weight matrix before dropout /  / wj^{\displaystyle {\hat {\mathbf {w} _{j}}}} – diluted row in the weight matrixBecause dropout removes a whole row from the vector matrix: the previous (unlisted) assumptions for weak dilution and the use of mean field theory are not applicable. The process by which the node is driven to zero: whether by setting the weights to zero: by “removing the node”: or by some other means: does not impact the end result and does not create a new and unique case. If the neural net is processed by a high-performance digital array-multiplicator: then it is likely more effective to drive the value to zero late in the process graph. If the net is processed by a constrained processor: perhaps even an analog neuromorph processor: then it is likely a more power-efficient solution is to drive the value to zero early in the process graph. == Google's patent == /  / Although there have been examples of randomly removing connections between neurons in a neural network to improve models: this technique was first introduced with the name dropout by Geoffrey Hinton: et al. in 2012. Google currently holds the patent for the dropout technique. == See also == /  / AlexNet /  / Convolutional neural network § Dropout /  /  /  /  /  / == Notes == /  /  /  /  /  / == References == ","Cosine Similarity: 0.8419400618493653 / This is the biggest contribution of the dropout method: although it effectively generates 2n{\displaystyle 2^{n}} neural nets: and as such allows for model combination: at test time only a single network needs to be tested.By avoiding training all nodes on all training data: dropout decreases overfitting. The method also significantly improves training speed. This makes the model combination practical: even for deep neural networks. The technique seems to reduce node interactions: leading them to learn more robust features that better generalize to new data. ==== DropConnect ==== /  / DropConnect is the generalization of dropout in which each connection: rather than each output unit: can be dropped with probability 1−p{\displaystyle 1-p}. Each unit thus receives input from a random subset of units in the previous layer.DropConnect is similar to dropout as it introduces dynamic sparsity within the model: but differs in that the sparsity is on the weights: rather than the output vectors of a layer. In other words: the fully connected layer with DropConnect becomes a sparsely connected layer in which the connections are chosen at random during the training stage. ==== Stochastic pooling ==== /  / A major drawback to Dropout is that it does not have the same benefits for convolutional layers: where the neurons are not fully connected. Even before Dropout: in 2013 a technique called stochastic pooling: the conventional deterministic pooling operations were replaced with a stochastic procedure: where the activation within each pooling region is picked randomly according to a multinomial distribution: given by the activities within the pooling region. This approach is free of hyperparameters and can be combined with other regularization approaches: such as dropout and data augmentation. An alternate view of stochastic pooling is that it is equivalent to standard max pooling but with many copies of an input image: each having small local deformations. This is similar to explicit elastic deformations of the input images: which delivers excellent performance on the MNIST data set. Using stochastic pooling in a multilayer model gives an exponential number of deformations since the selections in higher layers are independent of those below. ==== Artificial data ==== /  /  /  / Because the degree of model overfitting is determined by both its power and the amount of training it receives: providing a convolutional network with more training examples can reduce overfitting. Because there is often not enough available data to train: especially considering that some part should be spared for later testing: two approaches are to either generate new data from scratch (if possible) or perturb existing data to create new ones. The latter one is used since mid-1990s. For example: input images can be cropped: rotated: or rescaled to create new examples with the same labels as the original training set. === Explicit === /  /  /  /  /  / ==== Early stopping ==== /  /  /  / One of the simplest methods to prevent overfitting of a network is to simply stop the training before overfitting has had a chance to occur. It comes with the disadvantage that the learning process is halted. ==== Number of parameters ==== /  / Another simple way to prevent overfitting is to limit the number of parameters: typically by limiting the number of hidden units in each layer or limiting network depth. For convolutional networks: the filter size also affects the number of parameters. Limiting the number of parameters restricts the predictive power of the network directly: reducing the complexity of the function that it can perform on the data: and thus limits the amount of overfitting. This is equivalent to a ""zero norm"". ==== Weight decay ==== /  / A simple form of added regularizer is weight decay: which simply adds an additional error: proportional to the sum of weights (L1 norm) or squared magnitude (L2 norm) of the weight vector: to the error at each node. The level of acceptable model complexity can be reduced by increasing the proportionality constant('alpha' hyperparameter): thus increasing the penalty for large weight vectors. L2 regularization is the most common form of regularization. It can be implemented by penalizing the squared magnitude of all parameters directly in the objective. The L2 regularization has the intuitive interpretation of heavily penalizing peaky weight vectors and preferring diffuse weight vectors. Due to multiplicative interactions between weights and inputs this has the useful property of encouraging the network to use all of its inputs a little rather than some of its inputs a lot. L1 regularization is also common. It makes the weight vectors sparse during optimization. In other words: neurons with L1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the noisy inputs. L1 with L2 regularization can be combined; this is called elastic net regularization. ==== Max norm constraints ==== /  / Another form of regularization is to enforce an absolute upper bound on the magnitude of the weight vector for every neuron and use projected gradient descent to enforce the constraint. In practice: this corresponds to performing the parameter update as normal: and then enforcing the constraint by clamping the weight vector w→{\displaystyle {\vec {w}}} of every neuron to satisfy ‖w→‖2<c{\displaystyle \|{\vec {w}}\|_{2}<c}. Typical values of c{\displaystyle c} are order of 3–4. Some papers report improvements when using this form of regularization. == Hierarchical coordinate frames == /  / Pooling loses the precise spatial relationships between high-level parts (such as nose and mouth in a face image). These relationships are needed for identity recognition. Overlapping the pools so that each feature occurs in multiple pools: helps retain the information. Translation alone cannot extrapolate the understanding of geometric relationships to a radically new viewpoint: such as a different orientation or scale. On the other hand: people are very good at extrapolating; after seeing a new shape once they can recognize it from a different viewpoint.An earlier common way to deal with this problem is to train the network on transformed data in different orientations: scales: lighting: etc. so that the network can cope with these variations. This is computationally intensive for large data-sets. The alternative is to use a hierarchy of coordinate frames and use a group of neurons to represent a conjunction of the shape of the feature and its pose relative to the retina. "
0.6528943000012077,Who introduced the dropout technique and who currently holds the patent?,"The dropout technique was first introduced by Geoffrey Hinton and others in 2012 for neural networks. Currently, Google holds the patent for this technique.","Cosine Similarity: 0.8054510185549845 / The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks.The probabilistic interpretation was introduced by researchers including Hopfield: Widrow and Narendra and popularized in surveys such as the one by Bishop. == History == /  / There are two types of artificial neural network (ANN): feedforward neural networks (FNNs) and recurrent neural networks (RNNs). RNNs have cycles in their connectivity structure: FNNs don't. In the 1920s: Wilhelm Lenz and Ernst Ising created and analyzed the Ising model which is essentially a non-learning RNN architecture consisting of neuron-like threshold elements. In 1972: Shun'ichi Amari made this architecture adaptive. His learning RNN was popularised by John Hopfield in 1982. RNNs have become central for speech recognition and language processing. Charles Tappert writes that Frank Rosenblatt developed and explored all of the basic ingredients of the deep learning systems of today: referring to Rosenblatt's 1962 book which introduced multilayer perceptron (MLP) with 3 layers: an input layer: a hidden layer with randomized weights that did not learn: and an output layer. It also introduced variants: including a version with four-layer perceptrons where the last two layers have learned weights (and thus a proper multilayer perceptron). : section 16  In addition: term deep learning was proposed in 1986 by Rina Dechter although the history of its appearance is apparently more complicated.The first general: working learning algorithm for supervised: deep: feedforward: multilayer perceptrons was published by Alexey Ivakhnenko and Lapa in 1967. A 1971 paper described a deep network with eight layers trained by the group method of data handling.The first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. In computer experiments conducted by Amari's student Saito: a five layer MLP with two modifiable layers learned  internal representations to classify non-linearily separable pattern classes. In 1987 Matthew Brand reported that wide 12-layer nonlinear perceptrons could be fully end-to-end trained to reproduce logic functions of nontrivial circuit depth via gradient descent on small batches of random input/output samples: but concluded that training time on contemporary hardware (sub-megaflop computers) made the technique impractical: and proposed using fixed random early layers as an input hash for a single modifiable layer. Instead: subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique. In 1970: Seppo Linnainmaa published the reverse mode of automatic differentiation of discrete connected networks of nested differentiable functions. This became known as backpropagation. It is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes. The terminology ""back-propagating errors"" was actually introduced in 1962 by Rosenblatt: but he did not know how to implement this: although Henry J. Kelley had a continuous precursor of backpropagation already in 1960 in the context of control theory. In 1982: Paul Werbos applied backpropagation to MLPs in the way that has become standard. In 1985: David E. Rumelhart et al. published an experimental analysis of the technique.Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers began with the Neocognitron introduced by Kunihiko Fukushima in 1980. In 1969: he also introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for CNNs and deep learning in general. CNNs have become an essential tool for computer vision. The term Deep Learning was introduced to the machine learning community by Rina Dechter in 1986: and to artificial neural networks by Igor Aizenberg and colleagues in 2000: in the context of Boolean threshold neurons.In 1988: Wei Zhang et al. applied the backpropagation algorithm  /  / to a convolutional neural network (a simplified Neocognitron with convolutional interconnections between the image feature layers and the last fully connected layer) for alphabet recognition. They also proposed an implementation of the CNN with an optical computing system. In 1989: Yann LeCun et al. applied backpropagation to a CNN with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked: training required 3 days. Subsequently: Wei Zhang: et al. modified their model by removing the last fully connected layer and applied it for medical image object segmentation in 1991 and breast cancer detection in mammograms in 1994. LeNet-5 (1998): a 7-level CNN by Yann LeCun et al.: that classifies digits: was applied by several banks to recognize hand-written numbers on checks  digitized in 32x32 pixel images. In the 1980s: backpropagation did not work well for deep learning with long credit assignment paths. To overcome this problem: Jürgen Schmidhuber (1992) proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning. It uses predictive coding  to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN: by distilling a higher level chunker network into a lower level automatizer network. In 1993: a chunker solved a deep learning task whose depth exceeded 1000.In 1992: Jürgen Schmidhuber also published an alternative to RNNs which is now called a linear Transformer or a  Transformer with linearized self-attention (save for a normalization operator). It learns internal spotlights of attention: a slow feedforward neural network learns by gradient descent to control the fast weights of another neural network through outer products of self-generated activation patterns FROM and TO (which are now called key and value for self-attention). This fast weight attention mapping is applied to a query pattern. The modern Transformer was introduced by Ashish Vaswani et al. in their 2017 paper ""Attention Is All You Need"". It combines this with a softmax operator and a projection matrix. Transformers have increasingly become the model of choice for natural language processing. Many modern large language models such as ChatGPT: GPT-4: and BERT use it. Transformers are also increasingly being used in computer vision.In 1991: Jürgen Schmidhuber also published adversarial neural networks that contest with each other in the form of a zero-sum game: where one network's gain is the other network's loss. The first network is a generative model that models a probability distribution over output patterns. ",Cosine Similarity: 0.8045691302783383 / published an experimental analysis of the technique.
0.6138136999798007,What is a Feedforward Neural Network (FNN)?,"A feedforward neural network (FNN) is one of two broad types of artificial neural networks, characterized by the direction of the flow of information between its layers. Its flow is uni-directional, meaning that the information in the model flows in only one direction—forward—from the input nodes, through the hidden nodes (if any) and to the output nodes, without any cycles or loops. This is in contrast to recurrent neural networks, which have a bi-directional flow.",Cosine Similarity: 0.9310060711153301 / A feedforward neural network (FNN) is one of the two broad types of artificial neural network: characterized by direction of the flow of information between its layers.,"Cosine Similarity: 0.869330027610164 / Modern feedforward networks are trained using the backpropagation method and are colloquially referred to as the ""vanilla"" neural networks."
0.608263000001898,What is the main method of training modern feedforward networks?,Modern feedforward networks are trained using the backpropagation method.,"Cosine Similarity: 0.8910718592946939 / Modern feedforward networks are trained using the backpropagation method and are colloquially referred to as the ""vanilla"" neural networks.","Cosine Similarity: 0.8598474391621156 / The standard method is called ""backpropagation through time"" or BPTT: a generalization of back-propagation for feedforward networks."
0.6656837000045925,Who first utilized a deep-learning feedforward neural network?,The first deep-learning feedforward network was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa in 1965.,Cosine Similarity: 0.8958766409931617 / This extreme learning machine was not yet a deep learning network.In 1965: the first  deep-learning feedforward network: not yet using stochastic gradient descent: was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa: at the time called the Group Method of Data Handling.In 1967: a deep-learning network: using stochastic gradient descent for the first time: was able to classify non-linearily separable pattern classes: as reported Shun'ichi Amari.,Cosine Similarity: 0.8752957759679603 / The first deep learning MLP trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari.
0.5980351000034716,What are the functions of the feedforward neural network?,"In a feedforward neural network, information flows in only one direction—forward—from the input nodes, through the hidden nodes (if any) and to the output nodes, without any cycles or loops.",Cosine Similarity: 0.8754841309144197 / A feedforward neural network (FNN) is one of the two broad types of artificial neural network: characterized by direction of the flow of information between its layers.,Cosine Similarity: 0.8605466176057706 / Feedforward networks can be constructed with various types of units: such as binary McCulloch–Pitts neurons: the simplest of which is the perceptron.
0.631358899991028,How does learning occur in a feedforward neural network?,"Learning occurs in a feedforward neural network by changing connection weights after each piece of data is processed. This is based on the amount of error in the output compared to the expected result. This is an example of supervised learning, and is carried out through backpropagation.",Cosine Similarity: 0.8722316640859168 / At each time step: the input is propagated in a standard feedforward fashion: and then a backpropagation-like learning rule is applied (not performing gradient descent).,"Cosine Similarity: 0.8690709891204057 / Modern feedforward networks are trained using the backpropagation method and are colloquially referred to as the ""vanilla"" neural networks."
0.5705076000012923,What is the Gemini family and who developed it?,"The Gemini family comprises of multimodal large language models developed by Google DeepMind. It serves as the successor to LaMDA and PaLM 2 and includes Gemini Ultra, Gemini Pro, and Gemini Nano. ",Cosine Similarity: 0.8623523593844753 / Gemini is a family of multimodal large language models developed by Google DeepMind: serving as the successor to LaMDA and PaLM 2.,"Cosine Similarity: 0.8340216655046525 / Professor Alexei Efros of the University of California: Berkeley praised the potential of Gemini's multimodal approach: while scientist Melanie Mitchell of the Santa Fe Institute called Gemini ""very sophisticated""."
0.6624299999966752,When was the Gemini family announced and who were they positioned as a competitor to?,"The Gemini family was announced on December 6, 2023, and they were positioned as a competitor to OpenAI's GPT-4. ",Cosine Similarity: 0.8385270106388311 / Comprising Gemini Ultra: Gemini Pro: and Gemini Nano: it was announced on December 6: 2023: positioned as a competitor to OpenAI's GPT-4.,"Cosine Similarity: 0.8313008223876488 / In August 2023: Dylan Patel and Daniel Nishball of research firm SemiAnalysis penned a blog post declaring that the release of Gemini would ""eat the world"" and outclass GPT-4: prompting OpenAI CEO Sam Altman to ridicule the duo on X (formerly Twitter)."
0.6592137000116054,What is unique about the Gemini family of language models?,"Unlike other Large Language Models, Gemini is unique in that it is not trained on a text corpus alone. It is designed to be multimodal, meaning it can process multiple types of data simultaneously, including text, images, audio, video, and computer code.",Cosine Similarity: 0.8988712223220535 / Gemini is a family of multimodal large language models developed by Google DeepMind: serving as the successor to LaMDA and PaLM 2.,"Cosine Similarity: 0.8791687542249789 / Gemini's dataset is multimodal and multilingual: consisting of ""web documents: books: and code: and includ[ing] image: audio: and video data"".Demis Hassabis claims that training Gemini 1 used ""roughly the same amount of compute: maybe slightly more than what was rumored for GPT-4"".The second generation of Gemini (""Gemini 1.5"") has one model published so far: Gemini 1.5 Pro."
0.5865856000164058,How has Gemini been integrated into Google's products and services?,"Upon launch, Gemini Pro and Nano were integrated into Bard and the Pixel 8 Pro smartphone, respectively, while Gemini Ultra was set to power ""Bard Advanced"". Google also intended to incorporate Gemini into other products including Search, Ads, Chrome, Duet AI on Google Workspace, and AlphaCode 2.",Cosine Similarity: 0.8979167106824555 / Other products that Google intended to incorporate Gemini into included Search: Ads: Chrome: Duet AI on Google Workspace: and AlphaCode 2.,Cosine Similarity: 0.8731848418408052 / Gemini Pro was made available to Google Cloud customers on AI Studio and Vertex AI on December 13: while Gemini Nano will be made available to Android developers as well.
0.669672799995169,What were some notable achievements of Gemini Ultra and Gemini Pro?,"Gemini Ultra has outperformed GPT-4, Anthropic's Claude 2, Inflection AI's Inflection-2, Meta's LLaMA 2, and xAI's Grok 1 on a variety of industry benchmarks, and was the first language model to outperform human experts on the 57-subject Massive Multitask Language Understanding (MMLU) test, obtaining a score of 90%. Gemini Pro was also said to have outperformed GPT-3.5.","Cosine Similarity: 0.8847319075387637 / It comprised three models: Gemini Ultra: designed for ""highly complex tasks""; Gemini Pro: designed for ""a wide range of tasks""; and Gemini Nano: designed for ""on-device tasks"".","Cosine Similarity: 0.8772030724259565 / Gemini Pro also received a global launch.In February: Google launched ""Gemini 1.5"" in a limited capacity: positioned as a more powerful and capable model than 1.0 Ultra."
0.7603561000141781,What are Generative pre-trained transformers (GPT)?,"Generative pre-trained transformers (GPT) are a type of large language model (LLM) and a prominent framework for generative artificial intelligence. They are artificial neural networks used in natural language processing tasks. GPTs are based on the transformer architecture, pre-trained on large data sets of unlabelled text, and capable of generating novel human-like content.","Cosine Similarity: 0.9281260591751644 / Generative pre-trained transformers (GPT) are a type of large language model (LLM) and a prominent framework for generative artificial intelligence.They are artificial neural networks that are used in natural language processing tasks. GPTs are based on the transformer architecture: pre-trained on large data sets of unlabelled text: and able to generate novel human-like content. As of 2023: most LLMs have these characteristics and are sometimes referred to broadly as GPTs.The first GPT was introduced in 2018 by OpenAI. OpenAI has released very influential GPT foundation models that have been sequentially numbered: to comprise its ""GPT-n"" series. Each of these was significantly more capable than the previous: due to increased size (number of trainable parameters) and training. The most recent of these: GPT-4: was released in March 2023. Such models have been the basis for their more task-specific GPT systems: including models fine-tuned for instruction following—which in turn power the ChatGPT chatbot service.The term ""GPT"" is also used in the names and descriptions of such models developed by others. For example: other GPT foundation models include a series of models created by EleutherAI: and seven models created by Cerebras in 2023. Also: companies in different industries have developed task-specific GPTs in their respective fields: such as Salesforce's ""EinsteinGPT"" (for CRM) and Bloomberg's ""BloombergGPT"" (for finance). == History == /  /  /  /  /  / === Initial developments === /  / Generative pretraining (GP) was a long-established concept in machine learning applications. It was originally used as a form of semi-supervised learning: as the model is trained first on an unlabelled dataset (pretraining step) by learning to generate datapoints in the dataset: and then it is trained to classify a labelled dataset.While the unnormalized linear transformer dates back to 1992: the modern transformer architecture was not available until 2017 when it was published by researchers at Google in a paper ""Attention Is All You Need"". That development led to the emergence of large language models such as BERT in 2018 which was a pre-trained transformer (PT) but not designed to be generative (BERT was an ""encoder-only"" model). Also around that time: in 2018: OpenAI published its article entitled ""Improving Language Understanding by Generative Pre-Training:"" in which it introduced the first generative pre-trained transformer (GPT) system (""GPT-1"").Prior to transformer-based architectures: the best-performing neural NLP (natural language processing) models commonly employed supervised learning from large amounts of manually-labeled data. The reliance on supervised learning limited their use on datasets that were not well-annotated: and also made it prohibitively expensive and time-consuming to train extremely large language models.The semi-supervised approach OpenAI employed to make a large-scale generative system—and was first to do with a transformer model—involved two stages: an unsupervised generative ""pretraining"" stage to set initial parameters using a language modeling objective: and a supervised discriminative ""fine-tuning"" stage to adapt these parameters to a target task. === Later developments === /  / Regarding more recent GPT foundation models: OpenAI published its first versions of GPT-3 in July 2020. There were three models: with 1B: 6.7B: 175B parameters: respectively named babbage: curie: and davinci (giving initials B: C: and D).In July 2021: OpenAI published Codex: a task-specific GPT model targeted for programming applications. This was developed by fine-tuning a 12B parameter version of GPT-3 (different from previous GPT-3 models) using code from GitHub.In March 2022: OpenAI published two versions of GPT-3 that were fine-tuned for instruction-following (instruction-tuned): named davinci-instruct-beta (175B) and text-davinci-001: and then started beta testing code-davinci-002. text-davinci-002 was instruction-tuned from code-davinci-002. Both text-davinci-003 and ChatGPT were released in November 2022: with both building upon text-davinci-002 via reinforcement learning from human feedback (RLHF). text-davinci-003 is trained for following instructions (like its predecessors): whereas ChatGPT is further trained for conversational interaction with a human user.OpenAI's most recent GPT foundation model: GPT-4: was released on March 14: 2023. It can be accessed directly by users via a premium version of ChatGPT: and is available to developers for incorporation into other products and services via OpenAI's API. Other producers of GPT foundation models include EleutherAI (with a series of models starting in March 2021) and Cerebras (with seven models released in March 2023). == Foundational models == /  / A foundational model is an AI model trained on broad data at scale such that it can be adapted to a wide range of downstream tasks.Thus far: the most notable GPT foundation models have been from OpenAI's GPT-n series. The most recent from that is GPT-4: for which OpenAI declined to publish the size or training details (citing ""the competitive landscape and the safety implications of large-scale models""). Other such models include Google's PaLM: a broad foundation model that has been compared to GPT-3 and has recently been made available to developers via an API: and Together's GPT-JT: which has been reported as the closest-performing open-source alternative to GPT-3 (and is derived from earlier open-source GPTs). Meta AI (formerly Facebook) also has a generative transformer-based foundational large language model: known as LLaMA.Foundational GPTs can also employ modalities other than text: for input and/or output. GPT-4 is a multi-modal LLM that is capable of processing text and image input (though its output is limited to text). Regarding multimodal output: some generative transformer-based models are used for text-to-image technologies such as diffusion and parallel decoding. Such kinds of models can serve as visual foundation models (VFMs) for developing downstream systems that can work with images. == Task-specific models == /  / A foundational GPT model can be further adapted to produce more targeted systems directed to specific tasks and/or subject-matter domains. Methods for such adaptation can include additional fine-tuning (beyond that done for the foundation model) as well as certain forms of prompt engineering.An important example of this is fine-tuning models to follow instructions: which is of course a fairly broad task but more targeted than a foundation model. In January 2022: OpenAI introduced ""InstructGPT""—a series of models which were fine-tuned to follow instructions using a combination of supervised training and reinforcement learning from human feedback (RLHF) on base GPT-3 language models. Advantages this had over the bare foundational models included higher accuracy: less negative/toxic sentiment: and generally better alignment with user needs. ","Cosine Similarity: 0.8715853940561352 / Hence: OpenAI began using this as the basis for its API service offerings.Other instruction-tuned models have been released by others: including a fully open version.Another (related) kind of task-specific models are chatbots: which engage in human-like conversation. In November 2022: OpenAI launched ChatGPT—an online chat interface powered by an instruction-tuned language model trained in a similar fashion to InstructGPT. They trained this model using RLHF: with human AI trainers providing conversations in which they played both the user and the AI: and mixed this new dialogue dataset with the InstructGPT dataset for a conversational format suitable for a chatbot. Other major chatbots currently include Microsoft's Bing Chat: which uses OpenAI's GPT-4 (as part of a broader close collaboration between OpenAI and Microsoft): and Google's competing chatbot Bard (initially based on their LaMDA family of conversation-trained language models: with plans to switch to PaLM).Yet another kind of task that a GPT can be used for is the meta-task of generating its own instructions: like developing a series of prompts for 'itself' to be able to effectuate a more general goal given by a human user. This is known as an AI agent: and more specifically a recursive one because it uses results from its previous self-instructions to help it form its subsequent prompts; the first major example of this was Auto-GPT (which uses OpenAI's GPT models): and others have since been developed as well. === Multimodality === /  / Generative transformer-based systems can also be targeted to tasks involving modalities beyond text. For example: Microsoft’s “Visual ChatGPT” combines ChatGPT with visual foundation models (VFMs) to enable input or output comprising images as well as text. Also: advances in text-to-speech technology offer powerful tools for audio content creation when used in conjunction with foundational GPT language models. === Domain-specificity === /  / GPT systems can be directed toward particular fields or domains. Some reported examples of such models and apps are as follows:  /  /  /  / EinsteinGPT – for sales and marketing domains: to aid with customer relationship management (uses GPT-3.5) /  / BloombergGPT – for the financial domain: to aid with financial news and information (uses ""freely available"" AI methods: combined with their proprietary data) /  / Khanmigo – described as a GPT version for tutoring: in the education domain: it aids students using Khan Academy by guiding them through their studies without directly providing answers (powered by GPT-4) /  / SlackGPT – for the Slack instant-messaging service: to aid with navigating and summarizing discussions on it (uses OpenAI's API) /  / BioGPT – for the biomedical domain: to aid with biomedical literature text generation and mining (uses GPT-2)Sometimes domain-specificity is accomplished via software plug-ins or add-ons. For example: several different companies have developed particular plugins that interact directly with OpenAI's ChatGPT interface: and Google Workspace has available add-ons such as “GPT for Sheets and Docs”—which is reported to aid use of spreadsheet functionality in Google Sheets.In November 2023: OpenAI announced that it's enabling ChatGPT Plus subscribers to create custom versions of ChatGPT (being called GPTs). These can be tailored for specific domains via prompt engineering: curated datasets: and/or targeted interaction with external tools. Users who register as verified builders are able to publish their custom GPTs for other users: with monetization potential. (This is notably distinct from OpenAI's API service: as this is based internally within OpenAI's platform.) == Brand issues == /  / OpenAI: which created the first generative pre-trained transformer (GPT) in 2018: has recently asserted that “GPT” should be regarded as a brand of OpenAI. In April 2023: OpenAI revised the brand guidelines in its terms of service to indicate that other businesses using its API to run their artificial intelligence (AI) services would no longer be able to include “GPT” in such names or branding. In May 2023: OpenAI engaged a brand management service to notify its API customers of this policy: although these notifications stopped short of making overt legal claims (such as allegations of trademark infringement or demands to cease and desist). As of November 2023: OpenAI still prohibits its API licensees from naming their own products with ""GPT:"" but it has begun enabling its ChatGPT Plus subscribers to make ""custom versions of ChatGPT"" that are being called GPTs on the OpenAI site. OpenAI's terms of service says that its subscribers may use ""GPT"" in the names of these: although it's ""discouraged. ""Relatedly: OpenAI has applied to the United States Patent and Trademark Office (USPTO) to seek domestic trademark registration for the term “GPT” in the field of AI. OpenAI sought to expedite handling of its application: but the USPTO declined that request in April 2023. In May 2023: the USPTO responded to the application with a determination that ""GPT"" was both descriptive and generic. As of November 2023: OpenAI continues to pursue its argument through the available processes. Regardless: failure to obtain a registered U.S. trademark does not preclude some level of common-law trademark rights in the U.S.: and/or trademark rights in other countries.For any given type or scope of trademark protection in the U.S.: OpenAI would need to establish that the term is actually “distinctive” to their specific offerings in addition to being a broader technical term for the kind of technology. Some media reports suggested that OpenAI may be able to obtain trademark registration based indirectly on the fame of its GPT-based chatbot product: ChatGPT: for which OpenAI has separately sought protection (and which it has sought to enforce more strongly). Other reports have indicated that registration for the bare term “GPT” seems unlikely to be granted: as it is used frequently as a common term to refer simply to AI systems that involve generative pre-trained transformers. In any event: to whatever extent exclusive rights in the term may occur the U.S.: others would need to avoid using it for similar products or services in ways likely to cause confusion. If such rights ever became broad enough to implicate other well-established uses in the field: the trademark doctrine of descriptive fair use could still preserve some room to continue non-brand-related usage. == Selected bibliography == /  / This section lists the main official publications from OpenAI and Microsoft on their GPT models. "
0.6379938999889418,Who introduced the first GPT and when?,The first GPT was introduced by OpenAI in 2018.,"Cosine Similarity: 0.8245778932365523 / ""GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models"".","Cosine Similarity: 0.8245778932365523 / ""GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models""."
0.6333882999897469,What are some task-specific GPT systems developed by OpenAI? ,"OpenAI has developed task-specific GPT systems including models fine-tuned for instruction following, which power the ChatGPT chatbot service.","Cosine Similarity: 0.888538260380696 / Hence: OpenAI began using this as the basis for its API service offerings.Other instruction-tuned models have been released by others: including a fully open version.Another (related) kind of task-specific models are chatbots: which engage in human-like conversation. In November 2022: OpenAI launched ChatGPT—an online chat interface powered by an instruction-tuned language model trained in a similar fashion to InstructGPT. They trained this model using RLHF: with human AI trainers providing conversations in which they played both the user and the AI: and mixed this new dialogue dataset with the InstructGPT dataset for a conversational format suitable for a chatbot. Other major chatbots currently include Microsoft's Bing Chat: which uses OpenAI's GPT-4 (as part of a broader close collaboration between OpenAI and Microsoft): and Google's competing chatbot Bard (initially based on their LaMDA family of conversation-trained language models: with plans to switch to PaLM).Yet another kind of task that a GPT can be used for is the meta-task of generating its own instructions: like developing a series of prompts for 'itself' to be able to effectuate a more general goal given by a human user. This is known as an AI agent: and more specifically a recursive one because it uses results from its previous self-instructions to help it form its subsequent prompts; the first major example of this was Auto-GPT (which uses OpenAI's GPT models): and others have since been developed as well. === Multimodality === /  / Generative transformer-based systems can also be targeted to tasks involving modalities beyond text. For example: Microsoft’s “Visual ChatGPT” combines ChatGPT with visual foundation models (VFMs) to enable input or output comprising images as well as text. Also: advances in text-to-speech technology offer powerful tools for audio content creation when used in conjunction with foundational GPT language models. === Domain-specificity === /  / GPT systems can be directed toward particular fields or domains. Some reported examples of such models and apps are as follows:  /  /  /  / EinsteinGPT – for sales and marketing domains: to aid with customer relationship management (uses GPT-3.5) /  / BloombergGPT – for the financial domain: to aid with financial news and information (uses ""freely available"" AI methods: combined with their proprietary data) /  / Khanmigo – described as a GPT version for tutoring: in the education domain: it aids students using Khan Academy by guiding them through their studies without directly providing answers (powered by GPT-4) /  / SlackGPT – for the Slack instant-messaging service: to aid with navigating and summarizing discussions on it (uses OpenAI's API) /  / BioGPT – for the biomedical domain: to aid with biomedical literature text generation and mining (uses GPT-2)Sometimes domain-specificity is accomplished via software plug-ins or add-ons. For example: several different companies have developed particular plugins that interact directly with OpenAI's ChatGPT interface: and Google Workspace has available add-ons such as “GPT for Sheets and Docs”—which is reported to aid use of spreadsheet functionality in Google Sheets.In November 2023: OpenAI announced that it's enabling ChatGPT Plus subscribers to create custom versions of ChatGPT (being called GPTs). These can be tailored for specific domains via prompt engineering: curated datasets: and/or targeted interaction with external tools. Users who register as verified builders are able to publish their custom GPTs for other users: with monetization potential. (This is notably distinct from OpenAI's API service: as this is based internally within OpenAI's platform.) == Brand issues == /  / OpenAI: which created the first generative pre-trained transformer (GPT) in 2018: has recently asserted that “GPT” should be regarded as a brand of OpenAI. In April 2023: OpenAI revised the brand guidelines in its terms of service to indicate that other businesses using its API to run their artificial intelligence (AI) services would no longer be able to include “GPT” in such names or branding. In May 2023: OpenAI engaged a brand management service to notify its API customers of this policy: although these notifications stopped short of making overt legal claims (such as allegations of trademark infringement or demands to cease and desist). As of November 2023: OpenAI still prohibits its API licensees from naming their own products with ""GPT:"" but it has begun enabling its ChatGPT Plus subscribers to make ""custom versions of ChatGPT"" that are being called GPTs on the OpenAI site. OpenAI's terms of service says that its subscribers may use ""GPT"" in the names of these: although it's ""discouraged. ""Relatedly: OpenAI has applied to the United States Patent and Trademark Office (USPTO) to seek domestic trademark registration for the term “GPT” in the field of AI. OpenAI sought to expedite handling of its application: but the USPTO declined that request in April 2023. In May 2023: the USPTO responded to the application with a determination that ""GPT"" was both descriptive and generic. As of November 2023: OpenAI continues to pursue its argument through the available processes. Regardless: failure to obtain a registered U.S. trademark does not preclude some level of common-law trademark rights in the U.S.: and/or trademark rights in other countries.For any given type or scope of trademark protection in the U.S.: OpenAI would need to establish that the term is actually “distinctive” to their specific offerings in addition to being a broader technical term for the kind of technology. Some media reports suggested that OpenAI may be able to obtain trademark registration based indirectly on the fame of its GPT-based chatbot product: ChatGPT: for which OpenAI has separately sought protection (and which it has sought to enforce more strongly). Other reports have indicated that registration for the bare term “GPT” seems unlikely to be granted: as it is used frequently as a common term to refer simply to AI systems that involve generative pre-trained transformers. In any event: to whatever extent exclusive rights in the term may occur the U.S.: others would need to avoid using it for similar products or services in ways likely to cause confusion. If such rights ever became broad enough to implicate other well-established uses in the field: the trademark doctrine of descriptive fair use could still preserve some room to continue non-brand-related usage. == Selected bibliography == /  / This section lists the main official publications from OpenAI and Microsoft on their GPT models. ","Cosine Similarity: 0.8811017956869593 / A Microsoft team argued in 2023 that GPT-4 ""can solve novel and difficult tasks that span mathematics: coding: vision: medicine: law: psychology and more"" and that GPT-4 ""could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence system"": ""Can one reasonably say that a system that passes exams for software engineering candidates is not really intelligent?"""
0.658759399986593,What companies have developed GPT foundation models other than OpenAI?,EleutherAI and Cerebras have developed GPT foundation models aside from OpenAI.,"Cosine Similarity: 0.8500406890821465 / Hence: OpenAI began using this as the basis for its API service offerings.Other instruction-tuned models have been released by others: including a fully open version.Another (related) kind of task-specific models are chatbots: which engage in human-like conversation. In November 2022: OpenAI launched ChatGPT—an online chat interface powered by an instruction-tuned language model trained in a similar fashion to InstructGPT. They trained this model using RLHF: with human AI trainers providing conversations in which they played both the user and the AI: and mixed this new dialogue dataset with the InstructGPT dataset for a conversational format suitable for a chatbot. Other major chatbots currently include Microsoft's Bing Chat: which uses OpenAI's GPT-4 (as part of a broader close collaboration between OpenAI and Microsoft): and Google's competing chatbot Bard (initially based on their LaMDA family of conversation-trained language models: with plans to switch to PaLM).Yet another kind of task that a GPT can be used for is the meta-task of generating its own instructions: like developing a series of prompts for 'itself' to be able to effectuate a more general goal given by a human user. This is known as an AI agent: and more specifically a recursive one because it uses results from its previous self-instructions to help it form its subsequent prompts; the first major example of this was Auto-GPT (which uses OpenAI's GPT models): and others have since been developed as well. === Multimodality === /  / Generative transformer-based systems can also be targeted to tasks involving modalities beyond text. For example: Microsoft’s “Visual ChatGPT” combines ChatGPT with visual foundation models (VFMs) to enable input or output comprising images as well as text. Also: advances in text-to-speech technology offer powerful tools for audio content creation when used in conjunction with foundational GPT language models. === Domain-specificity === /  / GPT systems can be directed toward particular fields or domains. Some reported examples of such models and apps are as follows:  /  /  /  / EinsteinGPT – for sales and marketing domains: to aid with customer relationship management (uses GPT-3.5) /  / BloombergGPT – for the financial domain: to aid with financial news and information (uses ""freely available"" AI methods: combined with their proprietary data) /  / Khanmigo – described as a GPT version for tutoring: in the education domain: it aids students using Khan Academy by guiding them through their studies without directly providing answers (powered by GPT-4) /  / SlackGPT – for the Slack instant-messaging service: to aid with navigating and summarizing discussions on it (uses OpenAI's API) /  / BioGPT – for the biomedical domain: to aid with biomedical literature text generation and mining (uses GPT-2)Sometimes domain-specificity is accomplished via software plug-ins or add-ons. For example: several different companies have developed particular plugins that interact directly with OpenAI's ChatGPT interface: and Google Workspace has available add-ons such as “GPT for Sheets and Docs”—which is reported to aid use of spreadsheet functionality in Google Sheets.In November 2023: OpenAI announced that it's enabling ChatGPT Plus subscribers to create custom versions of ChatGPT (being called GPTs). These can be tailored for specific domains via prompt engineering: curated datasets: and/or targeted interaction with external tools. Users who register as verified builders are able to publish their custom GPTs for other users: with monetization potential. (This is notably distinct from OpenAI's API service: as this is based internally within OpenAI's platform.) == Brand issues == /  / OpenAI: which created the first generative pre-trained transformer (GPT) in 2018: has recently asserted that “GPT” should be regarded as a brand of OpenAI. In April 2023: OpenAI revised the brand guidelines in its terms of service to indicate that other businesses using its API to run their artificial intelligence (AI) services would no longer be able to include “GPT” in such names or branding. In May 2023: OpenAI engaged a brand management service to notify its API customers of this policy: although these notifications stopped short of making overt legal claims (such as allegations of trademark infringement or demands to cease and desist). As of November 2023: OpenAI still prohibits its API licensees from naming their own products with ""GPT:"" but it has begun enabling its ChatGPT Plus subscribers to make ""custom versions of ChatGPT"" that are being called GPTs on the OpenAI site. OpenAI's terms of service says that its subscribers may use ""GPT"" in the names of these: although it's ""discouraged. ""Relatedly: OpenAI has applied to the United States Patent and Trademark Office (USPTO) to seek domestic trademark registration for the term “GPT” in the field of AI. OpenAI sought to expedite handling of its application: but the USPTO declined that request in April 2023. In May 2023: the USPTO responded to the application with a determination that ""GPT"" was both descriptive and generic. As of November 2023: OpenAI continues to pursue its argument through the available processes. Regardless: failure to obtain a registered U.S. trademark does not preclude some level of common-law trademark rights in the U.S.: and/or trademark rights in other countries.For any given type or scope of trademark protection in the U.S.: OpenAI would need to establish that the term is actually “distinctive” to their specific offerings in addition to being a broader technical term for the kind of technology. Some media reports suggested that OpenAI may be able to obtain trademark registration based indirectly on the fame of its GPT-based chatbot product: ChatGPT: for which OpenAI has separately sought protection (and which it has sought to enforce more strongly). Other reports have indicated that registration for the bare term “GPT” seems unlikely to be granted: as it is used frequently as a common term to refer simply to AI systems that involve generative pre-trained transformers. In any event: to whatever extent exclusive rights in the term may occur the U.S.: others would need to avoid using it for similar products or services in ways likely to cause confusion. If such rights ever became broad enough to implicate other well-established uses in the field: the trademark doctrine of descriptive fair use could still preserve some room to continue non-brand-related usage. == Selected bibliography == /  / This section lists the main official publications from OpenAI and Microsoft on their GPT models. ",Cosine Similarity: 0.8348515469111635 / Larger sized models: such as GPT-3: however: can be prompt-engineered to achieve similar results.
0.6305652000010014,What are the foundational models?,"Foundational models are AI models trained on broad data at scale so that they can be adapted to a wide range of downstream tasks. Notable GPT foundational models have been from OpenAI's GPT-n series. Other such models include Google's PaLM, Together's GPT-JT, and Meta AI's transformer-based large language model known as LLaMA.",Cosine Similarity: 0.8471852443776828 / == See also == /  / Foundation models /  /  /  /  /  / == Notes == /  /  /  /  /  / == References == /  /  /  /  /  / == Further reading == /  / Jurafsky: Dan: Martin: James.,Cosine Similarity: 0.8471852443776828 / == See also == /  / Foundation models /  /  /  /  /  / == Notes == /  /  /  /  /  / == References == /  /  /  /  /  / == Further reading == /  / Jurafsky: Dan: Martin: James.
0.6184186000027694,What is a Graph Neural Network (GNN)?,"A Graph Neural Network (GNN) is a type of artificial neural network designed for processing data that can be represented as graphs. It falls under the broader field of ""geometric deep learning"". GNNs use pairwise message passing as the key design element, enabling graph nodes to iteratively update their representations by exchanging information with their neighbors.","Cosine Similarity: 0.908572947022274 / A graph neural network (GNN) belongs to a class of artificial neural networks for processing data that can be represented as graphs.In the more general subject of ""geometric deep learning"": certain existing neural network architectures can be interpreted as GNNs operating on suitably defined graphs. A convolutional neural network layer: in the context of computer vision: can be seen as a GNN applied to graphs whose nodes are pixels and only adjacent pixels are connected by edges in the graph. A transformer layer: in natural language processing: can be seen as a GNN applied to complete graphs whose nodes are words or tokens in a passage of natural language text. The key design element of GNNs is the use of pairwise message passing: such that graph nodes iteratively update their representations by exchanging information with their neighbors. Since their inception: several different GNN architectures have been proposed: which implement different flavors of message passing:  started by recursive or convolutional constructive approaches. As of 2022: whether it is possible to define GNN architectures ""going beyond"" message passing: or if every GNN can be built on message passing over suitably defined graphs: is an open research question.Relevant application domains for GNNs include Natural Language Processing: social networks: citation networks: molecular biology: chemistry: physics and NP-hard combinatorial optimization problems.Several open source libraries implementing graph neural networks are available: such as PyTorch Geometric (PyTorch): TensorFlow GNN (TensorFlow): jraph (Google JAX): and GraphNeuralNetworks.jl/GeometricFlux.jl (Julia: Flux). == Architecture == /  / The architecture of a generic GNN implements the following fundamental layers: /  / Permutation equivariant: a permutation equivariant layer maps a representation of a graph into an updated representation of the same graph. In the literature: permutation equivariant layers are implemented via pairwise message passing between graph nodes. Intuitively: in a message passing layer: nodes update their representations by aggregating the messages received from their immediate neighbours. As such: each message passing layer increases the receptive field of the GNN by one hop. Local pooling: a local pooling layer coarsens the graph via downsampling. Local pooling is used to increase the receptive field of a GNN: in a similar fashion to pooling layers in convolutional neural networks. Examples include k-nearest neighbours pooling: top-k pooling: and self-attention pooling. Global pooling: a global pooling layer: also known as readout layer: provides fixed-size representation of the whole graph. The global pooling layer must be permutation invariant: such that permutations in the ordering of graph nodes and edges do not alter the final output. Examples include element-wise sum: mean or maximum.It has been demonstrated that GNNs cannot be more expressive than the Weisfeiler–Leman Graph Isomorphism Test. In practice: this means that there exist different graph structures (e.g.: molecules with the same atoms but different bonds) that cannot be distinguished by GNNs. More powerful GNNs operating on higher-dimension geometries such as simplicial complexes can be designed. As of 2022: whether or not future architectures will overcome the message passing primitive is an open research question. == Message passing layers == /  / Message passing layers are permutation-equivariant layers mapping a graph into an updated representation of the same graph. Formally: they can be expressed as message passing neural networks (MPNNs).Let G=(V:E){\displaystyle G=(V:E)} be a graph: where V{\displaystyle V} is the node set and E{\displaystyle E} is the edge set. Let Nu{\displaystyle N_{u}} be the neighbourhood of some node u∈V{\displaystyle u\in V}. Additionally: let xu{\displaystyle \mathbf {x} _{u}} be the features of node u∈V{\displaystyle u\in V}: and euv{\displaystyle \mathbf {e} _{uv}} be the features of edge (u:v)∈E{\displaystyle (u:v)\in E}. An MPNN layer can be expressed as follows: /  / hu=ϕ(xu:⨁v∈Nuψ(xu:xv:euv)){\displaystyle \mathbf {h} _{u}=\phi \left(\mathbf {x} _{u}:\bigoplus _{v\in N_{u}}\psi (\mathbf {x} _{u}:\mathbf {x} _{v}:\mathbf {e} _{uv})\right)}where ϕ{\displaystyle \phi } and ψ{\displaystyle \psi } are differentiable functions (e.g.: artificial neural networks): and ⨁{\displaystyle \bigoplus } is a permutation invariant aggregation operator that can accept an arbitrary number of inputs (e.g.: element-wise sum: mean: or max). In particular: ϕ{\displaystyle \phi } and ψ{\displaystyle \psi } are referred to as update and message functions: respectively. Intuitively: in an MPNN computational block: graph nodes update their representations by aggregating the messages received from their neighbours. The outputs of one or more MPNN layers are node representations hu{\displaystyle \mathbf {h} _{u}} for each node u∈V{\displaystyle u\in V} in the graph. Node representations can be employed for any downstream task: such as node/graph classification or edge prediction. Graph nodes in an MPNN update their representation aggregating information from their immediate neighbours. As such: stacking n{\displaystyle n} MPNN layers means that one node will be able to communicate with nodes that are at most n{\displaystyle n} ""hops"" away. In principle: to ensure that every node receives information from every other node: one would need to stack a number of MPNN layers equal to the graph diameter. However: stacking many MPNN layers may cause issues such as oversmoothing and oversquashing. Oversmoothing refers to the issue of node representations becoming indistinguishable. Oversquashing refers to the bottleneck that is created by squeezing long-range dependencies into fixed-size representations. Countermeasures such as skip connections (as in residual neural networks): gated update rules and jumping knowledge can mitigate oversmoothing. Modifying the final layer to be a fully-adjacent layer: i.e.: by considering the graph as a complete graph: can mitigate oversquashing in problems where long-range dependencies are required.Other ""flavours"" of MPNN have been developed in the literature: such as graph convolutional networks and graph attention networks: whose definitions can be expressed in terms of the MPNN formalism. === Graph convolutional network === /  / The graph convolutional network (GCN) was first introduced by Thomas Kipf and Max Welling in 2017.A GCN layer defines a first-order approximation of a localized spectral filter on graphs. GCNs can be understood as a generalization of convolutional neural networks to graph-structured data. The formal expression of a GCN layer reads as follows: /  /  /  / H=σ(D~−12A~D~−12XΘ){\displaystyle \mathbf {H} =\sigma \left({\tilde {\mathbf {D} }}^{-{\frac {1}{2}}}{\tilde {\mathbf {A} }}{\tilde {\mathbf {D} }}^{-{\frac {1}{2}}}\mathbf {X} \mathbf {\Theta } \right)}where H{\displaystyle \mathbf {H} } is the matrix of node representations hu{\displaystyle \mathbf {h} _{u}}: X{\displaystyle \mathbf {X} } is the matrix of node features xu{\displaystyle \mathbf {x} _{u}}: σ(⋅){\displaystyle \sigma (\cdot )} is an activation function (e.g.: ReLU): A~{\displaystyle {\tilde {\mathbf {A} }}} is the graph adjacency matrix with the addition of self-loops: D~{\displaystyle {\tilde {\mathbf {D} }}} is the graph degree matrix with the addition of self-loops: and Θ{\displaystyle \mathbf {\Theta } } is a matrix of trainable parameters. ",Cosine Similarity: 0.8772983574891406 / === Social networks === /  /  /  / Social networks are a major application domain for GNNs due to their natural representation as social graphs.GNNs are used to develop recommender systems based on both social relations and item relations. === Combinatorial optimization === /  /  /  / GNNs are used as fundamental building blocks for several combinatorial optimization algorithms. Examples include computing shortest paths or Eulerian circuits for a given graph: deriving chip placements superior or competitive to handcrafted human solutions: and improving expert-designed branching rules in branch and bound. === Cyber security === /  /  /  / When viewed as a graph: a network of computers can be analyzed with GNNs for anomaly detection. Anomalies within provenance graphs often correlate to malicious activity within the network. GNNs have been used to identify these anomalies on individual nodes and within paths to detect malicious processes: or on the edge level to detect lateral movement. == References == /  /  /  /  /  / == External links == /  / https://distill.pub/2021/gnn-intro/ 
0.6530125999997836,How can convolutional neural networks and transformer layers be interpreted in the context of GNNs?,"In geometric deep learning, convolutional neural networks and transformer layers can be interpreted as GNNs operating on specifically defined graphs. For instance, a convolutional neural network layer, in computer vision context, can be seen as a GNN applied to graphs whose nodes are pixels and where only adjacent pixels are connected by edges. Similarly, a transformer layer in natural language processing can be seen as a GNN applied to complete graphs whose nodes are words or tokens in a text passage.","Cosine Similarity: 0.8823101846517819 / A graph neural network (GNN) belongs to a class of artificial neural networks for processing data that can be represented as graphs.In the more general subject of ""geometric deep learning"": certain existing neural network architectures can be interpreted as GNNs operating on suitably defined graphs. A convolutional neural network layer: in the context of computer vision: can be seen as a GNN applied to graphs whose nodes are pixels and only adjacent pixels are connected by edges in the graph. A transformer layer: in natural language processing: can be seen as a GNN applied to complete graphs whose nodes are words or tokens in a passage of natural language text. The key design element of GNNs is the use of pairwise message passing: such that graph nodes iteratively update their representations by exchanging information with their neighbors. Since their inception: several different GNN architectures have been proposed: which implement different flavors of message passing:  started by recursive or convolutional constructive approaches. As of 2022: whether it is possible to define GNN architectures ""going beyond"" message passing: or if every GNN can be built on message passing over suitably defined graphs: is an open research question.Relevant application domains for GNNs include Natural Language Processing: social networks: citation networks: molecular biology: chemistry: physics and NP-hard combinatorial optimization problems.Several open source libraries implementing graph neural networks are available: such as PyTorch Geometric (PyTorch): TensorFlow GNN (TensorFlow): jraph (Google JAX): and GraphNeuralNetworks.jl/GeometricFlux.jl (Julia: Flux). == Architecture == /  / The architecture of a generic GNN implements the following fundamental layers: /  / Permutation equivariant: a permutation equivariant layer maps a representation of a graph into an updated representation of the same graph. In the literature: permutation equivariant layers are implemented via pairwise message passing between graph nodes. Intuitively: in a message passing layer: nodes update their representations by aggregating the messages received from their immediate neighbours. As such: each message passing layer increases the receptive field of the GNN by one hop. Local pooling: a local pooling layer coarsens the graph via downsampling. Local pooling is used to increase the receptive field of a GNN: in a similar fashion to pooling layers in convolutional neural networks. Examples include k-nearest neighbours pooling: top-k pooling: and self-attention pooling. Global pooling: a global pooling layer: also known as readout layer: provides fixed-size representation of the whole graph. The global pooling layer must be permutation invariant: such that permutations in the ordering of graph nodes and edges do not alter the final output. Examples include element-wise sum: mean or maximum.It has been demonstrated that GNNs cannot be more expressive than the Weisfeiler–Leman Graph Isomorphism Test. In practice: this means that there exist different graph structures (e.g.: molecules with the same atoms but different bonds) that cannot be distinguished by GNNs. More powerful GNNs operating on higher-dimension geometries such as simplicial complexes can be designed. As of 2022: whether or not future architectures will overcome the message passing primitive is an open research question. == Message passing layers == /  / Message passing layers are permutation-equivariant layers mapping a graph into an updated representation of the same graph. Formally: they can be expressed as message passing neural networks (MPNNs).Let G=(V:E){\displaystyle G=(V:E)} be a graph: where V{\displaystyle V} is the node set and E{\displaystyle E} is the edge set. Let Nu{\displaystyle N_{u}} be the neighbourhood of some node u∈V{\displaystyle u\in V}. Additionally: let xu{\displaystyle \mathbf {x} _{u}} be the features of node u∈V{\displaystyle u\in V}: and euv{\displaystyle \mathbf {e} _{uv}} be the features of edge (u:v)∈E{\displaystyle (u:v)\in E}. An MPNN layer can be expressed as follows: /  / hu=ϕ(xu:⨁v∈Nuψ(xu:xv:euv)){\displaystyle \mathbf {h} _{u}=\phi \left(\mathbf {x} _{u}:\bigoplus _{v\in N_{u}}\psi (\mathbf {x} _{u}:\mathbf {x} _{v}:\mathbf {e} _{uv})\right)}where ϕ{\displaystyle \phi } and ψ{\displaystyle \psi } are differentiable functions (e.g.: artificial neural networks): and ⨁{\displaystyle \bigoplus } is a permutation invariant aggregation operator that can accept an arbitrary number of inputs (e.g.: element-wise sum: mean: or max). In particular: ϕ{\displaystyle \phi } and ψ{\displaystyle \psi } are referred to as update and message functions: respectively. Intuitively: in an MPNN computational block: graph nodes update their representations by aggregating the messages received from their neighbours. The outputs of one or more MPNN layers are node representations hu{\displaystyle \mathbf {h} _{u}} for each node u∈V{\displaystyle u\in V} in the graph. Node representations can be employed for any downstream task: such as node/graph classification or edge prediction. Graph nodes in an MPNN update their representation aggregating information from their immediate neighbours. As such: stacking n{\displaystyle n} MPNN layers means that one node will be able to communicate with nodes that are at most n{\displaystyle n} ""hops"" away. In principle: to ensure that every node receives information from every other node: one would need to stack a number of MPNN layers equal to the graph diameter. However: stacking many MPNN layers may cause issues such as oversmoothing and oversquashing. Oversmoothing refers to the issue of node representations becoming indistinguishable. Oversquashing refers to the bottleneck that is created by squeezing long-range dependencies into fixed-size representations. Countermeasures such as skip connections (as in residual neural networks): gated update rules and jumping knowledge can mitigate oversmoothing. Modifying the final layer to be a fully-adjacent layer: i.e.: by considering the graph as a complete graph: can mitigate oversquashing in problems where long-range dependencies are required.Other ""flavours"" of MPNN have been developed in the literature: such as graph convolutional networks and graph attention networks: whose definitions can be expressed in terms of the MPNN formalism. === Graph convolutional network === /  / The graph convolutional network (GCN) was first introduced by Thomas Kipf and Max Welling in 2017.A GCN layer defines a first-order approximation of a localized spectral filter on graphs. GCNs can be understood as a generalization of convolutional neural networks to graph-structured data. The formal expression of a GCN layer reads as follows: /  /  /  / H=σ(D~−12A~D~−12XΘ){\displaystyle \mathbf {H} =\sigma \left({\tilde {\mathbf {D} }}^{-{\frac {1}{2}}}{\tilde {\mathbf {A} }}{\tilde {\mathbf {D} }}^{-{\frac {1}{2}}}\mathbf {X} \mathbf {\Theta } \right)}where H{\displaystyle \mathbf {H} } is the matrix of node representations hu{\displaystyle \mathbf {h} _{u}}: X{\displaystyle \mathbf {X} } is the matrix of node features xu{\displaystyle \mathbf {x} _{u}}: σ(⋅){\displaystyle \sigma (\cdot )} is an activation function (e.g.: ReLU): A~{\displaystyle {\tilde {\mathbf {A} }}} is the graph adjacency matrix with the addition of self-loops: D~{\displaystyle {\tilde {\mathbf {D} }}} is the graph degree matrix with the addition of self-loops: and Θ{\displaystyle \mathbf {\Theta } } is a matrix of trainable parameters. ",Cosine Similarity: 0.8510114142225683 / === Social networks === /  /  /  / Social networks are a major application domain for GNNs due to their natural representation as social graphs.GNNs are used to develop recommender systems based on both social relations and item relations. === Combinatorial optimization === /  /  /  / GNNs are used as fundamental building blocks for several combinatorial optimization algorithms. Examples include computing shortest paths or Eulerian circuits for a given graph: deriving chip placements superior or competitive to handcrafted human solutions: and improving expert-designed branching rules in branch and bound. === Cyber security === /  /  /  / When viewed as a graph: a network of computers can be analyzed with GNNs for anomaly detection. Anomalies within provenance graphs often correlate to malicious activity within the network. GNNs have been used to identify these anomalies on individual nodes and within paths to detect malicious processes: or on the edge level to detect lateral movement. == References == /  /  /  /  /  / == External links == /  / https://distill.pub/2021/gnn-intro/ 
0.6384224999928847,What is the role of message passing in GNNs?,"In GNNs, message passing is the key design element wherein graph nodes update their representations by exchanging information with their neighbors iteratively. Different GNN architectures implement different kinds of message passing. This aspect of defining GNN architectures ""going beyond"" message passing or basing every GNN on message passing over appropriately defined graphs is still a subject of open research.","Cosine Similarity: 0.8431094634659627 / A graph neural network (GNN) belongs to a class of artificial neural networks for processing data that can be represented as graphs.In the more general subject of ""geometric deep learning"": certain existing neural network architectures can be interpreted as GNNs operating on suitably defined graphs. A convolutional neural network layer: in the context of computer vision: can be seen as a GNN applied to graphs whose nodes are pixels and only adjacent pixels are connected by edges in the graph. A transformer layer: in natural language processing: can be seen as a GNN applied to complete graphs whose nodes are words or tokens in a passage of natural language text. The key design element of GNNs is the use of pairwise message passing: such that graph nodes iteratively update their representations by exchanging information with their neighbors. Since their inception: several different GNN architectures have been proposed: which implement different flavors of message passing:  started by recursive or convolutional constructive approaches. As of 2022: whether it is possible to define GNN architectures ""going beyond"" message passing: or if every GNN can be built on message passing over suitably defined graphs: is an open research question.Relevant application domains for GNNs include Natural Language Processing: social networks: citation networks: molecular biology: chemistry: physics and NP-hard combinatorial optimization problems.Several open source libraries implementing graph neural networks are available: such as PyTorch Geometric (PyTorch): TensorFlow GNN (TensorFlow): jraph (Google JAX): and GraphNeuralNetworks.jl/GeometricFlux.jl (Julia: Flux). == Architecture == /  / The architecture of a generic GNN implements the following fundamental layers: /  / Permutation equivariant: a permutation equivariant layer maps a representation of a graph into an updated representation of the same graph. In the literature: permutation equivariant layers are implemented via pairwise message passing between graph nodes. Intuitively: in a message passing layer: nodes update their representations by aggregating the messages received from their immediate neighbours. As such: each message passing layer increases the receptive field of the GNN by one hop. Local pooling: a local pooling layer coarsens the graph via downsampling. Local pooling is used to increase the receptive field of a GNN: in a similar fashion to pooling layers in convolutional neural networks. Examples include k-nearest neighbours pooling: top-k pooling: and self-attention pooling. Global pooling: a global pooling layer: also known as readout layer: provides fixed-size representation of the whole graph. The global pooling layer must be permutation invariant: such that permutations in the ordering of graph nodes and edges do not alter the final output. Examples include element-wise sum: mean or maximum.It has been demonstrated that GNNs cannot be more expressive than the Weisfeiler–Leman Graph Isomorphism Test. In practice: this means that there exist different graph structures (e.g.: molecules with the same atoms but different bonds) that cannot be distinguished by GNNs. More powerful GNNs operating on higher-dimension geometries such as simplicial complexes can be designed. As of 2022: whether or not future architectures will overcome the message passing primitive is an open research question. == Message passing layers == /  / Message passing layers are permutation-equivariant layers mapping a graph into an updated representation of the same graph. Formally: they can be expressed as message passing neural networks (MPNNs).Let G=(V:E){\displaystyle G=(V:E)} be a graph: where V{\displaystyle V} is the node set and E{\displaystyle E} is the edge set. Let Nu{\displaystyle N_{u}} be the neighbourhood of some node u∈V{\displaystyle u\in V}. Additionally: let xu{\displaystyle \mathbf {x} _{u}} be the features of node u∈V{\displaystyle u\in V}: and euv{\displaystyle \mathbf {e} _{uv}} be the features of edge (u:v)∈E{\displaystyle (u:v)\in E}. An MPNN layer can be expressed as follows: /  / hu=ϕ(xu:⨁v∈Nuψ(xu:xv:euv)){\displaystyle \mathbf {h} _{u}=\phi \left(\mathbf {x} _{u}:\bigoplus _{v\in N_{u}}\psi (\mathbf {x} _{u}:\mathbf {x} _{v}:\mathbf {e} _{uv})\right)}where ϕ{\displaystyle \phi } and ψ{\displaystyle \psi } are differentiable functions (e.g.: artificial neural networks): and ⨁{\displaystyle \bigoplus } is a permutation invariant aggregation operator that can accept an arbitrary number of inputs (e.g.: element-wise sum: mean: or max). In particular: ϕ{\displaystyle \phi } and ψ{\displaystyle \psi } are referred to as update and message functions: respectively. Intuitively: in an MPNN computational block: graph nodes update their representations by aggregating the messages received from their neighbours. The outputs of one or more MPNN layers are node representations hu{\displaystyle \mathbf {h} _{u}} for each node u∈V{\displaystyle u\in V} in the graph. Node representations can be employed for any downstream task: such as node/graph classification or edge prediction. Graph nodes in an MPNN update their representation aggregating information from their immediate neighbours. As such: stacking n{\displaystyle n} MPNN layers means that one node will be able to communicate with nodes that are at most n{\displaystyle n} ""hops"" away. In principle: to ensure that every node receives information from every other node: one would need to stack a number of MPNN layers equal to the graph diameter. However: stacking many MPNN layers may cause issues such as oversmoothing and oversquashing. Oversmoothing refers to the issue of node representations becoming indistinguishable. Oversquashing refers to the bottleneck that is created by squeezing long-range dependencies into fixed-size representations. Countermeasures such as skip connections (as in residual neural networks): gated update rules and jumping knowledge can mitigate oversmoothing. Modifying the final layer to be a fully-adjacent layer: i.e.: by considering the graph as a complete graph: can mitigate oversquashing in problems where long-range dependencies are required.Other ""flavours"" of MPNN have been developed in the literature: such as graph convolutional networks and graph attention networks: whose definitions can be expressed in terms of the MPNN formalism. === Graph convolutional network === /  / The graph convolutional network (GCN) was first introduced by Thomas Kipf and Max Welling in 2017.A GCN layer defines a first-order approximation of a localized spectral filter on graphs. GCNs can be understood as a generalization of convolutional neural networks to graph-structured data. The formal expression of a GCN layer reads as follows: /  /  /  / H=σ(D~−12A~D~−12XΘ){\displaystyle \mathbf {H} =\sigma \left({\tilde {\mathbf {D} }}^{-{\frac {1}{2}}}{\tilde {\mathbf {A} }}{\tilde {\mathbf {D} }}^{-{\frac {1}{2}}}\mathbf {X} \mathbf {\Theta } \right)}where H{\displaystyle \mathbf {H} } is the matrix of node representations hu{\displaystyle \mathbf {h} _{u}}: X{\displaystyle \mathbf {X} } is the matrix of node features xu{\displaystyle \mathbf {x} _{u}}: σ(⋅){\displaystyle \sigma (\cdot )} is an activation function (e.g.: ReLU): A~{\displaystyle {\tilde {\mathbf {A} }}} is the graph adjacency matrix with the addition of self-loops: D~{\displaystyle {\tilde {\mathbf {D} }}} is the graph degree matrix with the addition of self-loops: and Θ{\displaystyle \mathbf {\Theta } } is a matrix of trainable parameters. ",Cosine Similarity: 0.7990779017096216 / In particular: let A{\displaystyle \mathbf {A} } be the graph adjacency matrix: then: one can define A~=A+I{\displaystyle {\tilde {\mathbf {A} }}=\mathbf {A} +\mathbf {I} } and D~ii=∑j∈VA~ij{\displaystyle {\tilde {\mathbf {D} }}_{ii}=\sum _{j\in V}{\tilde {A}}_{ij}}: where I{\displaystyle \mathbf {I} } denotes the identity matrix.This normalization ensures that the eigenvalues of D~−12A~D~−12{\displaystyle {\tilde {\mathbf {D} }}^{-{\frac {1}{2}}}{\tilde {\mathbf {A} }}{\tilde {\mathbf {D} }}^{-{\frac {1}{2}}}} are bounded in the range [0:1]{\displaystyle [0:1]}: avoiding numerical instabilities and exploding/vanishing gradients. A limitation of GCNs is that they do not allow multidimensional edge features euv{\displaystyle \mathbf {e} _{uv}}. It is however possible to associate scalar weights wuv{\displaystyle w_{uv}} to each edge by imposing Auv=wuv{\displaystyle A_{uv}=w_{uv}}: i.e.: by setting each nonzero entry in the adjacency matrix equal to the weight of the corresponding edge. === Graph attention network === /  / The graph attention network (GAT) was introduced by Petar Veličković et al. in 2018.Graph attention network is a combination of a graph neural network and an attention layer. The implementation of attention layer in graphical neural networks helps provide attention or focus to the important information from the data instead of focusing on the whole data. A multi-head GAT layer can be expressed as follows: /  /  /  / hu=‖k=1Kσ(∑v∈NuαuvWkxv){\displaystyle \mathbf {h} _{u}={\overset {K}{\underset {k=1}{\Big \Vert }}}\sigma \left(\sum _{v\in N_{u}}\alpha _{uv}\mathbf {W} ^{k}\mathbf {x} _{v}\right)}where K{\displaystyle K} is the number of attention heads: ‖{\displaystyle {\Big \Vert }} denotes vector concatenation: σ(⋅){\displaystyle \sigma (\cdot )} is an activation function (e.g.: ReLU): αij{\displaystyle \alpha _{ij}} are attention coefficients: and Wk{\displaystyle W^{k}} is a matrix of trainable parameters for the k{\displaystyle k}-th attention head. For the final GAT layer: the outputs from each attention head are averaged before the application of the activation function. Formally: the final GAT layer can be written as: /  /  /  / hu=σ(1K∑k=1K∑v∈NuαuvWkxv){\displaystyle \mathbf {h} _{u}=\sigma \left({\frac {1}{K}}\sum _{k=1}^{K}\sum _{v\in N_{u}}\alpha _{uv}\mathbf {W} ^{k}\mathbf {x} _{v}\right)}Attention in Machine Learning is a technique that mimics cognitive attention. In the context of learning on graphs: the attention coefficient αuv{\displaystyle \alpha _{uv}} measures how important is node u∈V{\displaystyle u\in V} to node v∈V{\displaystyle v\in V}. Normalized attention coefficients are computed as follows: /  /  /  / αuv=exp⁡(LeakyReLU(aT[Whu‖Whv‖euv]))∑z∈Nuexp⁡(LeakyReLU(aT[Whu‖Whz‖euz])){\displaystyle \alpha _{uv}={\frac {\exp({\text{LeakyReLU}}\left(\mathbf {a} ^{T}[\mathbf {W} \mathbf {h} _{u}\Vert \mathbf {W} \mathbf {h} _{v}\Vert \mathbf {e} _{uv}]\right))}{\sum _{z\in N_{u}}\exp({\text{LeakyReLU}}\left(\mathbf {a} ^{T}[\mathbf {W} \mathbf {h} _{u}\Vert \mathbf {W} \mathbf {h} _{z}\Vert \mathbf {e} _{uz}]\right))}}}where a{\displaystyle \mathbf {a} } is a vector of learnable weights: ⋅T{\displaystyle \cdot ^{T}} indicates transposition: and LeakyReLU{\displaystyle {\text{LeakyReLU}}} is a modified ReLU activation function. Attention coefficients are normalized to make them easily comparable across different nodes.A GCN can be seen as a special case of a GAT where attention coefficients are not learnable: but fixed and equal to the edge weights wuv{\displaystyle w_{uv}}. === Gated graph sequence neural network === /  / The gated graph sequence neural network (GGS-NN) was introduced by Yujia Li et al. in 2015. The GGS-NN extends the GNN formulation by Scarselli et al. to output sequences. The message passing framework is implemented as an update rule to a gated recurrent unit (GRU) cell. A GGS-NN can be expressed as follows: /  /  /  / hu(0)=xu‖0{\displaystyle \mathbf {h} _{u}^{(0)}=\mathbf {x} _{u}\:\Vert \:\mathbf {0} } /  / mu(l+1)=∑v∈NuΘhv{\displaystyle \mathbf {m} _{u}^{(l+1)}=\sum _{v\in N_{u}}\mathbf {\Theta } \mathbf {h} _{v}} /  / hu(l+1)=GRU(mu(l+1):hu(l)){\displaystyle \mathbf {h} _{u}^{(l+1)}={\text{GRU}}(\mathbf {m} _{u}^{(l+1)}:\mathbf {h} _{u}^{(l)})}where ‖{\displaystyle \Vert } denotes vector concatenation: 0{\displaystyle \mathbf {0} } is a vector of zeros: Θ{\displaystyle \mathbf {\Theta } } is a matrix of learnable parameters: GRU{\displaystyle {\text{GRU}}} is a GRU cell: and l{\displaystyle l} denotes the sequence index. In a GGS-NN: the node representations are regarded as the hidden states of a GRU cell. The initial node features xu(0){\displaystyle \mathbf {x} _{u}^{(0)}} are zero-padded up to the hidden state dimension of the GRU cell. The same GRU cell is used for updating representations for each node. == Local pooling layers == /  / Local pooling layers coarsen the graph via downsampling. We present here several learnable local pooling strategies that have been proposed. For each cases: the input is the initial graph is represented by a matrix X{\displaystyle \mathbf {X} } of node features: and the graph adjacency matrix A{\displaystyle \mathbf {A} }. The output is the new matrix X′{\displaystyle \mathbf {X} '}of node features: and the new graph adjacency matrix A′{\displaystyle \mathbf {A} '}. === Top-k pooling === /  / We first set /  / y=Xp‖p‖{\displaystyle \mathbf {y} ={\frac {\mathbf {X} \mathbf {p} }{\Vert \mathbf {p} \Vert }}} /  / where p{\displaystyle \mathbf {p} } is a learnable projection vector. The projection vector p{\displaystyle \mathbf {p} } computes a scalar projection value for each graph node. The top-k pooling layer  can then be formalised as follows: /  /  /  / X′=(X⊙sigmoid(y))i{\displaystyle \mathbf {X} '=(\mathbf {X} \odot {\text{sigmoid}}(\mathbf {y} ))_{\mathbf {i} }}A′=Ai:i{\displaystyle \mathbf {A} '=\mathbf {A} _{\mathbf {i} :\mathbf {i} }}where i=topk(y){\displaystyle \mathbf {i} ={\text{top}}_{k}(\mathbf {y} )} is the subset of nodes with the top-k highest projection scores: ⊙{\displaystyle \odot } denotes element-wise matrix multiplication: and sigmoid(⋅){\displaystyle {\text{sigmoid}}(\cdot )} is the sigmoid function. In other words: the nodes with the top-k highest projection scores are retained in the new adjacency matrix A′{\displaystyle \mathbf {A} '}. The sigmoid(⋅){\displaystyle {\text{sigmoid}}(\cdot )} operation makes the projection vector p{\displaystyle \mathbf {p} } trainable by backpropagation: which otherwise would produce discrete outputs. === Self-attention pooling === /  / We first set /  /  /  / y=GNN(X:A){\displaystyle \mathbf {y} ={\text{GNN}}(\mathbf {X} :\mathbf {A} )}where GNN{\displaystyle {\text{GNN}}} is a generic permutation equivariant GNN layer (e.g.: GCN: GAT: MPNN). The Self-attention pooling layer can then be formalised as follows: /  /  /  / X′=(X⊙y)i{\displaystyle \mathbf {X} '=(\mathbf {X} \odot \mathbf {y} )_{\mathbf {i} }}A′=Ai:i{\displaystyle \mathbf {A} '=\mathbf {A} _{\mathbf {i} :\mathbf {i} }}where i=topk(y){\displaystyle \mathbf {i} ={\text{top}}_{k}(\mathbf {y} )} is the subset of nodes with the top-k highest projection scores: ⊙{\displaystyle \odot } denotes element-wise matrix multiplication. The self-attention pooling layer can be seen as an extension of the top-k pooling layer. Differently from top-k pooling: the self-attention scores computed in self-attention pooling account both for the graph features and the graph topology. == Applications == /  /  /  /  /  / === Protein folding === /  /  /  / Graph neural networks are one of the main building blocks of AlphaFold: an artificial intelligence program developed by Google's DeepMind for solving the protein folding problem in biology. AlphaFold achieved first place in several CASP competitions. 
0.6064898999757133,What are the possible application domains for GNNs?,"GNNs are applicable in various fields such as Natural Language Processing, social networks, citation networks, molecular biology, chemistry, physics, and NP-hard combinatorial optimization problems.",Cosine Similarity: 0.8939418247341557 / === Social networks === /  /  /  / Social networks are a major application domain for GNNs due to their natural representation as social graphs.GNNs are used to develop recommender systems based on both social relations and item relations. === Combinatorial optimization === /  /  /  / GNNs are used as fundamental building blocks for several combinatorial optimization algorithms. Examples include computing shortest paths or Eulerian circuits for a given graph: deriving chip placements superior or competitive to handcrafted human solutions: and improving expert-designed branching rules in branch and bound. === Cyber security === /  /  /  / When viewed as a graph: a network of computers can be analyzed with GNNs for anomaly detection. Anomalies within provenance graphs often correlate to malicious activity within the network. GNNs have been used to identify these anomalies on individual nodes and within paths to detect malicious processes: or on the edge level to detect lateral movement. == References == /  /  /  /  /  / == External links == /  / https://distill.pub/2021/gnn-intro/ ,Cosine Similarity: 0.8571703654666757 / == Applications == /  / SNNs can in principle apply to the same applications as traditional ANNs.
0.6026454999810085,What are some examples of libraries implementing graph neural networks?,"Several open-source libraries implement graph neural networks. Some examples include PyTorch Geometric (PyTorch), TensorFlow GNN (TensorFlow), jraph (Google JAX), and GraphNeuralNetworks.jl/GeometricFlux.jl (Julia, Flux).",Cosine Similarity: 0.8425705723059446 / Examples of SNNs are the OSFA spatial neural networks: SVANNs and GWNNs.,Cosine Similarity: 0.8393548908791703 / Examples include the ADALINE memristor-based neural network.
0.6152209999854676,Who was behind the first implementation of artificial neural networks (ANNs)?,The first implementation of ANNs was by psychologist Frank Rosenblatt.,Cosine Similarity: 0.8903378134220932 / While some of the computational implementations ANNs relate to earlier discoveries in mathematics: the first implementation of ANNs was by psychologist Frank Rosenblatt: who developed the perceptron.,Cosine Similarity: 0.8736337543274314 / Artificial neural networks (ANNs) are models created using machine learning to perform a number of tasks.
0.5981526999967173,What is the AlexNet and what is its significance in the field of artificial neural networks?,"AlexNet is a deep neural network developed in the 2010s. It greatly outperformed other image recognition models and is thought to have launched the ongoing AI spring, contributing to further increased interest in ANNs.",Cosine Similarity: 0.8833351619636349 / The 2010s: saw the development of a deep neural network (a neural network with many layers) called AlexNet.,"Cosine Similarity: 0.8479654389092733 / In machine learning: an artificial neural network (also neural network or neural net: abbreviated ANN or NN) is a model inspired by the neuronal organization found in the biological neural networks in animal brains.An ANN is made of connected units or nodes called artificial neurons: which loosely model the neurons in a brain.These are connected by edges: which model the synapses in a brain. An artificial neuron receives signals from connected neurons: then processes them and sends a signal to other connected neurons. The ""signal"" is a real number: and the output of each neuron is computed by some non-linear function of the sum of its inputs: called the activation function. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Typically: neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer): possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least 2 hidden layers.Artificial neural networks are used for predictive modeling: adaptive control: and other applications where they can be trained via a dataset. They are also used to solve problems in artificial intelligence. Networks can learn from experience: and can derive conclusions from a complex and seemingly unrelated set of information. == Training == /  / Neural networks are typically trained through empirical risk minimization. This method is based on the idea of optimizing the network's parameters to minimize the difference: or empirical risk: between the predicted output and the actual target values in a given dataset. Gradient based methods such as backpropagation are usually used to estimate the parameters of the network. During the training phase: ANNs learn from labeled training data by iteratively updating their parameters to minimize a defined loss function. This method allows the network to generalize to unseen data. == History == /  /  /  / Historically: digital computers evolved from the von Neumann model: and operate via the execution of explicit instructions via access to memory by a number of processors. Neural networks: on the other hand: originated from efforts to model information processing in biological systems through the framework of connectionism. Unlike the von Neumann model: connectionist computing does not separate memory and processing. The simplest kind of feedforward neural network (FNN) is a linear network: which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated at each node. The mean squared errors between these calculated outputs and the given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement.Warren McCulloch and Walter Pitts (1943) also considered a non-learning computational model for neural networks.In the late 1940s: D. O. Hebb created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Hebbian learning is considered to be a 'typical' unsupervised learning rule and its later variants were early models for long term potentiation. These ideas started being applied to computational models in 1948 with Turing's ""unorganized machines"". Farley and Wesley A. Clark were the first to simulate a Hebbian network in 1954 at MIT. They used computational machines: then called ""calculators"". Other neural network computational machines were created by Rochester: Holland: Habit: and Duda in 1956. In 1958: psychologist Frank Rosenblatt invented the perceptron: the first implemented artificial neural network: funded by the United States Office of Naval Research. The invention of the perceptron raised public excitement for research in Artificial Neural Networks: causing the US government to drastically increase funding into deep learning research. This led to ""the golden age of AI"" fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence. For example: in 1957 Herbert Simon famously said:It is not my aim to surprise or shock you—but the simplest way I can summarize is to say that there are now in the world machines that think: that learn and that create. Moreover: their ability to do these things is going to increase rapidly until—in a visible future—the range of problems they can handle will be coextensive with the range to which the human mind has been applied.However: this wasn't the case as research stagnated in the United States following the work of Minsky and Papert (1969): who discovered that basic perceptrons were incapable of processing the exclusive-or circuit and that computers lacked sufficient power to train useful neural networks. This: along with other factors such as the 1973 Lighthill report by James Lighthill stating that research in Artificial Intelligence has not ""produced the major impact that was then promised:"" shutting funding in research into the field of AI in all but two universities in the UK and in many major institutions across the world. This ushered an era called the AI Winter with reduced research into connectionism due to a decrease in government funding and an increased stress on symbolic artificial intelligence in the United States and other Western countries.During the AI Winter era: however: research outside the United States continued: especially in Eastern Europe. By the time Minsky and Papert's book on Perceptrons came out: methods for training multilayer perceptrons (MLPs) were already known. The first deep learning MLP was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa in 1965: as the Group Method of Data Handling. The first deep learning MLP trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. In computer experiments conducted by Amari's student Saito: a five layer MLP with two modifiable layers learned useful internal representations to classify non-linearily separable pattern classes.Self-organizing maps (SOMs) were described by Teuvo Kohonen in 1982. "
0.646718800009694,How does a linear neural network work?,A linear network consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated in each node. The mean squared errors between these calculated outputs and a given target values are minimized by creating an adjustment to the weights.,Cosine Similarity: 0.8800778350422115 / == Linear neural network == /  / The simplest kind of feedforward neural network is a linear network: which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights.,Cosine Similarity: 0.8715547605769353 / == History == /  /  /  /  /  / === Linear neural network === /  / The simplest kind of feedforward neural network is a linear network: which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights.
0.5792300000030082,Who developed the perceptron and what was its function?,"The perceptron, an algorithm for pattern recognition, was developed by Rosenblatt. It described circuitries not in the basic perceptron, including the exclusive-or circuit, which could not be processed by neural networks at the time.",Cosine Similarity: 0.8734685326134922 / While some of the computational implementations ANNs relate to earlier discoveries in mathematics: the first implementation of ANNs was by psychologist Frank Rosenblatt: who developed the perceptron.,Cosine Similarity: 0.8630893964523684 / Other neural network computational machines were created by Rochester: Holland: Habit and Duda (1956).Rosenblatt (1958) created the perceptron: an algorithm for pattern recognition.
0.640063500002725,What is the role of the backpropagation algorithm in artificial neural networks?,The backpropagation algorithm is an efficient application of the Leibniz chain rule to networks of differentiable nodes. It's crucial for adjusting the weight of various nodes in the network to improve prediction accuracy.,"Cosine Similarity: 0.8684958524198227 / The standard method is called ""backpropagation through time"" or BPTT: a generalization of back-propagation for feedforward networks.",Cosine Similarity: 0.8664360838356405 / It has been implemented using a perceptron network whose connection weights were trained with back propagation (supervised learning).
0.6195397999836132,What is a modeling language?,"A modeling language is an artificial language that can be used to express data, information or knowledge or systems in a structure defined by a consistent set of rules. These rules help interpret the meaning of components in the structure. They can be graphical or textual, and can be used in various fields including computer science, information management, business process modeling, software engineering, and systems engineering.",Cosine Similarity: 0.914687100815317 / A modeling language is any artificial language that can be used to express data: information or knowledge or systems in a structure that is defined by a consistent set of rules.,Cosine Similarity: 0.8925810989497038 / == Overview == /  / A modeling language can be graphical or textual.
0.6427265999955125,What is the difference between graphical and textual modeling languages?,"Graphical modeling languages use a diagram technique with named symbols to represent concepts and lines to depict relationships between symbols and other graphical notations to represent constraints. On the other hand, textual modeling languages use standardized keywords along with parameters or natural language terms and phrases to form computer interpretable expressions.",Cosine Similarity: 0.9051518814472439 / == Overview == /  / A modeling language can be graphical or textual.,Cosine Similarity: 0.899585752045093 / Textual modeling languages may use standardized keywords accompanied by parameters or natural language terms and phrases to make computer-interpretable expressions.An example of a graphical modeling language and a corresponding textual modeling language is EXPRESS.
0.612823200004641,How do executable modeling languages benefit programmers?,"Executable modeling languages aim to boost the productivity of skilled programmers. They allow programmers to tackle more complex problems, like parallel computing and distributed systems. However, their use does not imply that the need for programmers is eliminated. ",Cosine Similarity: 0.9165911371248336 / On the contrary: executable modeling languages are intended to amplify the productivity of skilled programmers: so that they can address more challenging problems: such as parallel computing and distributed systems.,Cosine Similarity: 0.891676430265478 / Not all modeling languages are executable: and for those that are: the use of them doesn't necessarily mean that programmers are no longer required.
0.6443007999914698,What is an example of a graphical modeling language and its corresponding textual modeling language?,EXPRESS is an example of a graphical modeling language and its corresponding textual modeling language.,Cosine Similarity: 0.9229048816758114 / Textual modeling languages may use standardized keywords accompanied by parameters or natural language terms and phrases to make computer-interpretable expressions.An example of a graphical modeling language and a corresponding textual modeling language is EXPRESS.,Cosine Similarity: 0.8936775245559133 / == Overview == /  / A modeling language can be graphical or textual.
0.6162242999998853,What is the role of modeling languages in system development?,"Modeling languages can be used to specify system requirements, structures, and behaviors, offering a precise specification of systems to better understand the system being modeled by stakeholders like customers, operators, analysts, and designers. More mature modeling languages that are precise, consistent, and executable can automate system verification and validation, simulation, and code generation.",Cosine Similarity: 0.9016968620814614 / Modeling languages can be used to specify: /  /  /  / system requirements: /  / structures and /  / behaviors.Modeling languages are intended to be used to precisely specify systems so that stakeholders (e.g.: customers: operators: analysts: designers) can better understand the system being modeled.,Cosine Similarity: 0.877227532761954 / Because a modeling language is visual and at a higher-level of abstraction than code: using models encourages the generation of a shared vision that may prevent problems of differing interpretation later in development.
0.6396986999898218,What is a neural network and how closely is it related to artificial neural networks?,"A neural network, also known as a neuronal network, is an interconnected population of neurons that typically contain multiple neural circuits. These networks are studied to understand the organization and functioning of nervous systems. Artificial neural networks are machine learning models, which are very closely related to biological neural networks as they are inspired and designed based on the mechanisms used by neural circuits in biological networks.",Cosine Similarity: 0.8843319019180118 / Closely related are artificial neural networks: machine learning models inspired by biological neural networks.,Cosine Similarity: 0.8772490600953857 / Artificial neural networks are computational models inspired by biological neural networks: and are used to approximate functions that are generally unknown.
0.5849572000151966,What are some applications of artificial neural networks in the field of artificial intelligence?,"In the field of artificial intelligence, artificial neural networks have been applied successfully to several areas including speech recognition, image analysis, and adaptive control. They are also used to create software agents in computer and video games or in the construction of autonomous robots.",Cosine Similarity: 0.9192549042407006 / In the artificial intelligence field: artificial neural networks have been applied successfully to speech recognition: image analysis and adaptive control: in order to construct software agents (in computer and video games) or autonomous robots.,Cosine Similarity: 0.916234620109775 / Artificial neural networks are used to solve artificial intelligence problems.
0.6433297000185121,What theories did Alexander Bain and William James propose regarding neural networks?,"Alexander Bain proposed that every activity led to the firing of a certain set of neurons and when activities were repeated, the connections between those neurons strengthened, leading to the formation of memory. William James suggested that memories and actions resulted from electrical currents flowing among the neurons in the brain and did not require individual neural connections for each memory or action.",Cosine Similarity: 0.9072373827684261 / == History == /  / The preliminary theoretical base for contemporary neural networks was independently proposed by Alexander Bain (1873) and William James (1890).,Cosine Similarity: 0.8994783859171377 / == History == /  /  /  / The theoretical base for contemporary neural networks was independently proposed by Alexander Bain in 1873 and William James in 1890.
0.6349308000062592,Who were McCulloch and Pitts and what was their contribution to neural network research?,"McCulloch and Pitts were researchers who, in 1943, created a computational model for neural networks based on mathematics and algorithms. They called this model threshold logic. Their work contributed significantly to the evolution of neural network research, which split into two distinct approaches - one focusing on biological processes in the brain, and the other on the application of neural networks to artificial intelligence.",Cosine Similarity: 0.9029659710027307 / McCulloch and Pitts  (1943) also created a computational model for neural networks based on mathematics and algorithms.,Cosine Similarity: 0.8715404634913221 / == Perceptrons and other early neural networks == /  / Warren McCulloch and Walter Pitts (1943) also considered a non-learning computational model for neural networks.
0.6610996999952476,What are some recent improvements in the research of neural networks?,"Recent improvements in the research of neural networks have mostly been concerned with the exploration of the role of neuromodulators, such as dopamine, acetylcholine, and serotonin, on behavior and learning. Biophysical models, like BCM theory, have been instrumental for understanding synaptic plasticity mechanisms, and have influenced both computer science and neuroscience.",Cosine Similarity: 0.8781005133567509 / Consequently: optical neural networks have garnered increased attention in the research community.,Cosine Similarity: 0.8748302185897376 / Many improvements to the approach have been made in subsequent decades.In 1987: using a stochastic gradient descent within a (wide 12-layer nonlinear) feed-forward network: Matthew Brand has trained it to reproduce logic functions of nontrivial circuit depth: using small batches of random input/output samples.
0.6762014999985695,What is a neural network?,"A neural network refers to a group of interconnected units, or neurons, that send signals to each other. These neurons can be either biological cells or mathematical models. A complex network of neurons can execute complex tasks. There are two main types of such networks - biological neural networks seen in brains and complex nervous systems, and artificial neural networks, which are mathematical models used in machine learning for approximating nonlinear functions.",Cosine Similarity: 0.9107501240243987 / A neural network is a group of interconnected units called neurons that send signals to one another.,Cosine Similarity: 0.9073564838119914 / A neural network: also called a neuronal network: is an interconnected population of neurons (typically containing multiple neural circuits).
0.7049643000063952,What is the difference between a biological neural network and an artificial neural network?,"A biological neural network comprises biological neurons that are chemically connected to each other by synapses. These neurons send and receive electrochemical signals. An artificial neural network, on the other hand, is a mathematical model that approximates non-linear functions. These networks are mostly used in software form to help solve artificial intelligence problems.",Cosine Similarity: 0.891099138464239 / One approach focused on biological processes while the other focused on the application of neural networks to artificial intelligence.,Cosine Similarity: 0.8894354894128711 / One approach focused on biological processes in the brain and the other focused on the application of neural networks to artificial intelligence.
0.6253542999911588,How do neurons in an artificial neural network operate?,"In artificial neural networks, neurons are arranged into layers, with information passing through from the input layer to the output layer, through one or more intermediate or hidden layers. Each neuron receives an input signal - a number based on the outputs of neurons from the previous layer. The signal a neuron puts out is computed based on this number and its activation function. The strengths, or weights, of the connections between neurons shape the behavior of the network. Training a network involves modifying these weights to reach the desired output.",Cosine Similarity: 0.8886905394796153 / Neurons in an artificial neural network are usually arranged into layers: with information passing from the first layer (the input layer) through one or more intermediate layers (hidden layers) to the final layer (the output layer).,Cosine Similarity: 0.8850331790421813 / They consist of artificial neurons: which are mathematical functions that are designed to be analogous to the mechanisms used by neural circuits.
0.6311951999960002,How do signals in a biological neural network function?,"In a biological neural network, each neuron sends and receives electrochemical signals known as action potentials. Depending on its role, a neuron can either amplify and propagate or suppress these signals. These signals travel through the nervous system to muscle cells, inducing contraction and subsequent motion.",Cosine Similarity: 0.8851280413600239 / Biological neural networks are studied to understand the organization and functioning of nervous systems.,Cosine Similarity: 0.8737424818482615 / == Biological neural network == /  /  /  / A biological neural network is a population of biological neurons chemically connected to each other by synapses.
0.5901522999920417,How has the application of artificial neural networks evolved?,"Artificial neural networks were initially developed to model biological neural networks under the concept of connectionism in the 1930s. However, with the development of the perceptron, a simple artificial neural network, around 1943 and subsequent implementations in hardware, they started being utilized more for machine learning applications, and thereby deviated considerably from their biological counterparts.",Cosine Similarity: 0.8890980675765376 / In the artificial intelligence field: artificial neural networks have been applied successfully to speech recognition: image analysis and adaptive control: in order to construct software agents (in computer and video games) or autonomous robots.,Cosine Similarity: 0.8881036807219347 / While early artificial neural networks were physical machines: today they are almost always implemented in software.
0.6241431000235025,What is an optical neural network?,An optical neural network is a physical implementation of an artificial neural network with optical components. It uses the strength of the optical interconnect for implementing neuronal communications.,Cosine Similarity: 0.9401052749720873 / An optical neural network is a physical implementation of an artificial neural network with optical components.,Cosine Similarity: 0.9346792926378972 / An optical neural network is a physical implementation of an artificial neural network  with optical components.
0.590669799974421,What are some types of artificial neural networks that have been implemented as optical neural networks?,"Some artificial neural networks that have been implemented as optical neural networks include the Hopfield neural network and the Kohonen self-organizing map, often with liquid crystal spatial light modulators.",Cosine Similarity: 0.9011510403857975 / This research led to extensive research on alternative methods using the strength of the optical interconnect for implementing neuronal communications.Some artificial neural networks that have been implemented as optical neural networks include the Hopfield neural network and the Kohonen self-organizing map with liquid crystal spatial light modulators  Optical neural networks can also be based on the principles of neuromorphic engineering: creating neuromorphic photonic systems.,Cosine Similarity: 0.9010625871253152 / An optical neural network is a physical implementation of an artificial neural network with optical components.
0.5953599000058603,How do biological neural networks differ from optical neural networks?,"Biological neural networks function on an electrochemical basis, while optical neural networks use electromagnetic waves. The mechanisms for dynamically changing the state of the neurons in a biological network include short-term and long-term synaptic plasticity.",Cosine Similarity: 0.9182291253547887 / == Electrochemical vs. optical neural networks == /  / Biological neural networks function on an electrochemical basis: while optical neural networks use electromagnetic waves.,Cosine Similarity: 0.9035197252980909 / Optical interfaces to biological neural networks can be created with optogenetics: but is not the same as an optical neural networks.
0.6474647999857552,What are the two primary methods of optical neural computing currently under research?,"The two primary methods of optical neural computing under research are silicon photonics-based and free-space optics. Silicon photonics offer superior speed, while free-space optics deliver massive parallelism.",Cosine Similarity: 0.9435276986611388 / Presently: two primary methods of optical neural computing are under research: silicon photonics-based and free-space optics.,Cosine Similarity: 0.8779604375291484 / There are two main types of neural network.
0.632468499999959,What was significant about the Programmable Optical Array/Analogic Computer (POAC)?,"The Programmable Optical Array/Analogic Computer (POAC) was a model of an optical neural network implemented in 2000. It utilized a Joint Fourier Transform Correlator (JTC) and Bacteriorhodopsin (BR) as a holographic optical memory. It promised full parallelism, large array size, and the speed of light for implementing an optical CNN.",Cosine Similarity: 0.9082818001941217 / == Other Implementations == /  / In 2007 there was one model of Optical Neural Network: the Programmable Optical Array/Analogic Computer (POAC).,Cosine Similarity: 0.9042666921797458 / However: POAC is a general purpose and programmable array computer that has a wide range of applications including: /  /  /  / image processing /  / pattern recognition /  / target tracking /  / real-time video processing /  / document security /  / optical switching /  /  /  /  /  / == See also == /  / Optical computing /  / Quantum neural network /  /  /  /  /  / == References ==
0.654128900001524,What are Physics-informed neural networks (PINNs)?,"Physics-informed neural networks (PINNs) are universal function approximators that can integrate the knowledge of any physical laws governing a given data-set in the learning process. They can be described by partial differential equations (PDEs) and help overcome the low data availability of some biological and engineering systems. They increase the correctness of the function approximation and enhance the information content of the available data, making learning algorithms capture the right solution and generalize well.",Cosine Similarity: 0.9430293620038922 / Physics-informed neural networks (PINNs) are a type of universal function approximators that can embed the knowledge of any physical laws that govern a given data-set in the learning process: and can be described by partial differential equations (PDEs).,Cosine Similarity: 0.9284975639621684 / On the other hand: physics-informed neural networks (PINNs) leverage governing physical equations in neural network training.
0.6389416000165511,How do PINNs work in terms of function approximation?,"PINNs work by leveraging governing physical equations in neural network training. They are designed to be trained to satisfy the given training data and the imposed governing equations. This means that a neural network can be guided with training data that do not necessarily need to be large and complete. Thus, even with sparse and incomplete data, PINN may be used for finding an optimal solution with high fidelity.",Cosine Similarity: 0.8671805535169768 / Physics-informed neural networks (PINNs) are a type of universal function approximators that can embed the knowledge of any physical laws that govern a given data-set in the learning process: and can be described by partial differential equations (PDEs).,Cosine Similarity: 0.8655300160160322 / Namely: PINNs are designed to be trained to satisfy the given training data as well as the imposed governing equations.
0.6057059000013396,What is the significance of automatic differentiation (AD) in PINNs?,Automatic differentiation (AD) is exploited in PINNs to compute the required derivatives in the partial differential equations. It's a new class of differentiation techniques widely used to derive neural networks and is considered superior to numerical or symbolic differentiation.,Cosine Similarity: 0.8684474653837502 / In addition: they allow for exploiting automatic differentiation (AD) to compute the required derivatives in the partial differential equations: a new class of differentiation techniques widely used to derive neural networks assessed to be superior to numerical or symbolic differentiation.,Cosine Similarity: 0.8411381205633838 / This network can be differentiated using automatic differentiation.
0.6264435000193771,How can PINNs be applied for piece-wise function approximation?,"For problems with strong non-linearity or sharp gradients, lightweight PINNs are used for piece-wise approximation, which increases accuracy substantially and decreases computational load. Distributed physics-informed neural networks (DPINNs) and Distributed physics-informed extreme learning machines (DPIELMs) are used for better approximation, solving PDEs in much larger discrete subdomains.",Cosine Similarity: 0.8603142806231153 / With the capability of approximating strong non-linearity extremely light weight PINNs are used to solve PDEs in much larger discrete subdomains that increases accuracy substantially and decreases computational load as well.,Cosine Similarity: 0.858930382508165 / Piece-wise approximation has been an old practice in the field of numerical approximation.
0.7094977000087965,What limitations do PINNs have?,"PINNs struggle to approximate translation and discontinuous behavior. They fail when solving differential equations with slight advective dominance and are not successful in solving chaotic equations. One of the reasons for this is the soft-constraining of Dirichlet and Neumann boundary conditions which pose multi-objective optimization problems. This necessitates the need for manually weighing the loss terms for optimization. Also, there is the risk of getting stuck at a local optimum often.",Cosine Similarity: 0.8705060115195212 / == Limitations == /  / Translation and discontinuous behavior are hard to approximate using PINNs.,Cosine Similarity: 0.8685282681205501 / This limitation of regular PINNs imposes high computational costs: specifically for a comprehensive investigation of geometric parameters in industrial designs.
0.64478699999745,Who were the first to publish ideas on quantum neural computation?,The first ideas on quantum neural computation were published independently in 1995 by Subhash Kak and Ron Chrisley.,Cosine Similarity: 0.8427313556526409 / Quantum neural networks are computational neural network models which are based on the principles of quantum mechanics.The first ideas on quantum neural computation were published independently in 1995 by Subhash Kak and Ron Chrisley: engaging with the theory of quantum mind: which posits that quantum effects play a role in cognitive function. However: typical research in quantum neural networks involves combining classical artificial neural network models (which are widely used in machine learning for the important task of pattern recognition) with the advantages of quantum information in order to develop more efficient algorithms. One important motivation for these investigations is the difficulty to train classical neural networks: especially in big data applications. The hope is that features of quantum computing such as quantum parallelism or the effects of interference and entanglement can be used as resources. Since the technological implementation of a quantum computer is still in a premature stage: such quantum neural network models are mostly theoretical proposals that await their full implementation in physical experiments. Most Quantum neural networks are developed as feed-forward networks. Similar to their classical counterparts: this structure intakes input from one layer of qubits: and passes that input onto another layer of qubits. This layer of qubits evaluates this information and passes on the output to the next layer. Eventually the path leads to the final layer of qubits. The layers do not have to be of the same width: meaning they don't have to have the same number of qubits as the layer before or after it. This structure is trained on which path to take similar to classical artificial neural networks. This is discussed in a lower section. Quantum neural networks refer to three different categories: Quantum computer with classical data: classical computer with quantum data: and quantum computer with quantum data. == Examples == /  / Quantum neural network research is still in its infancy: and a conglomeration of proposals and ideas of varying scope and mathematical rigor have been put forward. Most of them are based on the idea of replacing classical binary or McCulloch-Pitts neurons with a qubit (which can be called a “quron”): resulting in neural units that can be in a superposition of the state ‘firing’ and ‘resting’. === Quantum perceptrons === /  / A lot of proposals attempt to find a quantum equivalent for the perceptron unit from which neural nets are constructed. A problem is that nonlinear activation functions do not immediately correspond to the mathematical structure of quantum theory: since a quantum evolution is described by linear operations and leads to probabilistic observation. Ideas to imitate the perceptron activation function with a quantum mechanical formalism reach from special measurements  to postulating non-linear quantum operators (a mathematical framework that is disputed). A direct implementation of the activation function using the circuit-based model of quantum computation has recently been proposed by Schuld: Sinayskiy and Petruccione based on the quantum phase estimation algorithm. === Quantum networks === /  / At a larger scale: researchers have attempted to generalize neural networks to the quantum setting. One way of constructing a quantum neuron is to first generalise classical neurons and then generalising them further to make unitary gates. Interactions between neurons can be controlled quantumly: with unitary gates: or classically: via measurement of the network states. This high-level theoretical technique can be applied broadly: by taking different types of networks and different implementations of quantum neurons: such as photonically implemented neurons and quantum reservoir processor (quantum version of reservoir computing). Most learning algorithms follow the classical model of training an artificial neural network to learn the input-output function of a given training set and use classical feedback loops to update parameters of the quantum system until they converge to an optimal configuration. Learning as a parameter optimisation problem has also been approached by adiabatic models of quantum computing.Quantum neural networks can be applied to algorithmic design: given qubits with tunable mutual interactions: one can attempt to learn interactions following the classical backpropagation rule from a training set of desired input-output relations: taken to be the desired output algorithm's behavior. The quantum network thus ‘learns’ an algorithm. === Quantum associative memory === /  / The first quantum associative memory algorithm was introduced by Dan Ventura and Tony Martinez in 1999. The authors do not attempt to translate the structure of artificial neural network models into quantum theory: but propose an algorithm for a circuit-based quantum computer that simulates associative memory. The memory states (in Hopfield neural networks saved in the weights of the neural connections) are written into a superposition: and a Grover-like quantum search algorithm retrieves the memory state closest to a given input. As such: this is not a fully content-addressable memory: since only incomplete patterns can be retrieved. The first truly content-addressable quantum memory: which can retrieve patterns also from corrupted inputs: was proposed by Carlo A. Trugenberger. Both memories can store an exponential (in terms of n qubits) number of patterns but can be used only once due to the no-cloning theorem and their destruction upon measurement. Trugenberger: however: has shown that his proababilistic model of quantum associative memory can be efficiently implemented and re-used multiples times for any polynomial number of stored patterns: a large advantage with respect to classical associative memories. === Classical neural networks inspired by quantum theory === /  / A substantial amount of interest has been given to a “quantum-inspired” model that uses ideas from quantum theory to implement a neural network based on fuzzy logic. == Training == /  / Quantum Neural Networks can be theoretically trained similarly to training classical/artificial neural networks. A key difference lies in communication between the layers of a neural networks. For classical neural networks: at the end of a given operation: the current perceptron copies its output to the next layer of perceptron(s) in the network. However: in a quantum neural network: where each perceptron is a qubit: this would violate the no-cloning theorem. A proposed generalized solution to this is to replace the classical fan-out method with an arbitrary unitary that spreads out: but does not copy: the output of one qubit to the next layer of qubits. ,"Cosine Similarity: 0.8416989841949849 / Farley and Clark (1954) first used computational machines: then called ""calculators"": to simulate a Hebbian network."
0.6525132000097074,What is one of the main motivations for investigating quantum neural networks?,"One important motivation for investigating quantum neural networks is the challenge of training classical neural networks, particularly in big data applications.",Cosine Similarity: 0.8683272152437007 / Quantum neural networks are computational neural network models which are based on the principles of quantum mechanics.The first ideas on quantum neural computation were published independently in 1995 by Subhash Kak and Ron Chrisley: engaging with the theory of quantum mind: which posits that quantum effects play a role in cognitive function. However: typical research in quantum neural networks involves combining classical artificial neural network models (which are widely used in machine learning for the important task of pattern recognition) with the advantages of quantum information in order to develop more efficient algorithms. One important motivation for these investigations is the difficulty to train classical neural networks: especially in big data applications. The hope is that features of quantum computing such as quantum parallelism or the effects of interference and entanglement can be used as resources. Since the technological implementation of a quantum computer is still in a premature stage: such quantum neural network models are mostly theoretical proposals that await their full implementation in physical experiments. Most Quantum neural networks are developed as feed-forward networks. Similar to their classical counterparts: this structure intakes input from one layer of qubits: and passes that input onto another layer of qubits. This layer of qubits evaluates this information and passes on the output to the next layer. Eventually the path leads to the final layer of qubits. The layers do not have to be of the same width: meaning they don't have to have the same number of qubits as the layer before or after it. This structure is trained on which path to take similar to classical artificial neural networks. This is discussed in a lower section. Quantum neural networks refer to three different categories: Quantum computer with classical data: classical computer with quantum data: and quantum computer with quantum data. == Examples == /  / Quantum neural network research is still in its infancy: and a conglomeration of proposals and ideas of varying scope and mathematical rigor have been put forward. Most of them are based on the idea of replacing classical binary or McCulloch-Pitts neurons with a qubit (which can be called a “quron”): resulting in neural units that can be in a superposition of the state ‘firing’ and ‘resting’. === Quantum perceptrons === /  / A lot of proposals attempt to find a quantum equivalent for the perceptron unit from which neural nets are constructed. A problem is that nonlinear activation functions do not immediately correspond to the mathematical structure of quantum theory: since a quantum evolution is described by linear operations and leads to probabilistic observation. Ideas to imitate the perceptron activation function with a quantum mechanical formalism reach from special measurements  to postulating non-linear quantum operators (a mathematical framework that is disputed). A direct implementation of the activation function using the circuit-based model of quantum computation has recently been proposed by Schuld: Sinayskiy and Petruccione based on the quantum phase estimation algorithm. === Quantum networks === /  / At a larger scale: researchers have attempted to generalize neural networks to the quantum setting. One way of constructing a quantum neuron is to first generalise classical neurons and then generalising them further to make unitary gates. Interactions between neurons can be controlled quantumly: with unitary gates: or classically: via measurement of the network states. This high-level theoretical technique can be applied broadly: by taking different types of networks and different implementations of quantum neurons: such as photonically implemented neurons and quantum reservoir processor (quantum version of reservoir computing). Most learning algorithms follow the classical model of training an artificial neural network to learn the input-output function of a given training set and use classical feedback loops to update parameters of the quantum system until they converge to an optimal configuration. Learning as a parameter optimisation problem has also been approached by adiabatic models of quantum computing.Quantum neural networks can be applied to algorithmic design: given qubits with tunable mutual interactions: one can attempt to learn interactions following the classical backpropagation rule from a training set of desired input-output relations: taken to be the desired output algorithm's behavior. The quantum network thus ‘learns’ an algorithm. === Quantum associative memory === /  / The first quantum associative memory algorithm was introduced by Dan Ventura and Tony Martinez in 1999. The authors do not attempt to translate the structure of artificial neural network models into quantum theory: but propose an algorithm for a circuit-based quantum computer that simulates associative memory. The memory states (in Hopfield neural networks saved in the weights of the neural connections) are written into a superposition: and a Grover-like quantum search algorithm retrieves the memory state closest to a given input. As such: this is not a fully content-addressable memory: since only incomplete patterns can be retrieved. The first truly content-addressable quantum memory: which can retrieve patterns also from corrupted inputs: was proposed by Carlo A. Trugenberger. Both memories can store an exponential (in terms of n qubits) number of patterns but can be used only once due to the no-cloning theorem and their destruction upon measurement. Trugenberger: however: has shown that his proababilistic model of quantum associative memory can be efficiently implemented and re-used multiples times for any polynomial number of stored patterns: a large advantage with respect to classical associative memories. === Classical neural networks inspired by quantum theory === /  / A substantial amount of interest has been given to a “quantum-inspired” model that uses ideas from quantum theory to implement a neural network based on fuzzy logic. == Training == /  / Quantum Neural Networks can be theoretically trained similarly to training classical/artificial neural networks. A key difference lies in communication between the layers of a neural networks. For classical neural networks: at the end of a given operation: the current perceptron copies its output to the next layer of perceptron(s) in the network. However: in a quantum neural network: where each perceptron is a qubit: this would violate the no-cloning theorem. A proposed generalized solution to this is to replace the classical fan-out method with an arbitrary unitary that spreads out: but does not copy: the output of one qubit to the next layer of qubits. ,Cosine Similarity: 0.8474597193740226 / Biological neural networks are studied to understand the organization and functioning of nervous systems.
0.6495565999939572,How does the structure of a quantum neural network compare to that of a classical artificial neural network? ,"Most Quantum neural networks are developed as feed-forward networks similar to their classical counterparts. The structure intakes input from one layer of qubits and passes that input onto another layer after evaluation. However, the layers in quantum neural networks do not need to have the same number of qubits as the layer before or after it.",Cosine Similarity: 0.8924504735244098 / Quantum neural networks are computational neural network models which are based on the principles of quantum mechanics.The first ideas on quantum neural computation were published independently in 1995 by Subhash Kak and Ron Chrisley: engaging with the theory of quantum mind: which posits that quantum effects play a role in cognitive function. However: typical research in quantum neural networks involves combining classical artificial neural network models (which are widely used in machine learning for the important task of pattern recognition) with the advantages of quantum information in order to develop more efficient algorithms. One important motivation for these investigations is the difficulty to train classical neural networks: especially in big data applications. The hope is that features of quantum computing such as quantum parallelism or the effects of interference and entanglement can be used as resources. Since the technological implementation of a quantum computer is still in a premature stage: such quantum neural network models are mostly theoretical proposals that await their full implementation in physical experiments. Most Quantum neural networks are developed as feed-forward networks. Similar to their classical counterparts: this structure intakes input from one layer of qubits: and passes that input onto another layer of qubits. This layer of qubits evaluates this information and passes on the output to the next layer. Eventually the path leads to the final layer of qubits. The layers do not have to be of the same width: meaning they don't have to have the same number of qubits as the layer before or after it. This structure is trained on which path to take similar to classical artificial neural networks. This is discussed in a lower section. Quantum neural networks refer to three different categories: Quantum computer with classical data: classical computer with quantum data: and quantum computer with quantum data. == Examples == /  / Quantum neural network research is still in its infancy: and a conglomeration of proposals and ideas of varying scope and mathematical rigor have been put forward. Most of them are based on the idea of replacing classical binary or McCulloch-Pitts neurons with a qubit (which can be called a “quron”): resulting in neural units that can be in a superposition of the state ‘firing’ and ‘resting’. === Quantum perceptrons === /  / A lot of proposals attempt to find a quantum equivalent for the perceptron unit from which neural nets are constructed. A problem is that nonlinear activation functions do not immediately correspond to the mathematical structure of quantum theory: since a quantum evolution is described by linear operations and leads to probabilistic observation. Ideas to imitate the perceptron activation function with a quantum mechanical formalism reach from special measurements  to postulating non-linear quantum operators (a mathematical framework that is disputed). A direct implementation of the activation function using the circuit-based model of quantum computation has recently been proposed by Schuld: Sinayskiy and Petruccione based on the quantum phase estimation algorithm. === Quantum networks === /  / At a larger scale: researchers have attempted to generalize neural networks to the quantum setting. One way of constructing a quantum neuron is to first generalise classical neurons and then generalising them further to make unitary gates. Interactions between neurons can be controlled quantumly: with unitary gates: or classically: via measurement of the network states. This high-level theoretical technique can be applied broadly: by taking different types of networks and different implementations of quantum neurons: such as photonically implemented neurons and quantum reservoir processor (quantum version of reservoir computing). Most learning algorithms follow the classical model of training an artificial neural network to learn the input-output function of a given training set and use classical feedback loops to update parameters of the quantum system until they converge to an optimal configuration. Learning as a parameter optimisation problem has also been approached by adiabatic models of quantum computing.Quantum neural networks can be applied to algorithmic design: given qubits with tunable mutual interactions: one can attempt to learn interactions following the classical backpropagation rule from a training set of desired input-output relations: taken to be the desired output algorithm's behavior. The quantum network thus ‘learns’ an algorithm. === Quantum associative memory === /  / The first quantum associative memory algorithm was introduced by Dan Ventura and Tony Martinez in 1999. The authors do not attempt to translate the structure of artificial neural network models into quantum theory: but propose an algorithm for a circuit-based quantum computer that simulates associative memory. The memory states (in Hopfield neural networks saved in the weights of the neural connections) are written into a superposition: and a Grover-like quantum search algorithm retrieves the memory state closest to a given input. As such: this is not a fully content-addressable memory: since only incomplete patterns can be retrieved. The first truly content-addressable quantum memory: which can retrieve patterns also from corrupted inputs: was proposed by Carlo A. Trugenberger. Both memories can store an exponential (in terms of n qubits) number of patterns but can be used only once due to the no-cloning theorem and their destruction upon measurement. Trugenberger: however: has shown that his proababilistic model of quantum associative memory can be efficiently implemented and re-used multiples times for any polynomial number of stored patterns: a large advantage with respect to classical associative memories. === Classical neural networks inspired by quantum theory === /  / A substantial amount of interest has been given to a “quantum-inspired” model that uses ideas from quantum theory to implement a neural network based on fuzzy logic. == Training == /  / Quantum Neural Networks can be theoretically trained similarly to training classical/artificial neural networks. A key difference lies in communication between the layers of a neural networks. For classical neural networks: at the end of a given operation: the current perceptron copies its output to the next layer of perceptron(s) in the network. However: in a quantum neural network: where each perceptron is a qubit: this would violate the no-cloning theorem. A proposed generalized solution to this is to replace the classical fan-out method with an arbitrary unitary that spreads out: but does not copy: the output of one qubit to the next layer of qubits. ,Cosine Similarity: 0.8622086244081695 / Using this fan-out Unitary (Uf{\displaystyle U_{f}}) with a dummy state qubit in a known state (Ex.|0⟩{\displaystyle |0\rangle } in the computational basis): also known as an Ancilla bit: the information from the qubit can be transferred to the next layer of qubits. This process adheres to the quantum operation requirement of reversibility.Using this quantum feed-forward network: deep neural networks can be executed and trained efficiently. A deep neural network is essentially a network with many hidden-layers: as seen in the sample model neural network above. Since the Quantum neural network being discussed uses fan-out Unitary operators: and each operator only acts on its respective input: only two layers are used at any given time. In other words: no Unitary operator is acting on the entire network at any given time: meaning the number of qubits required for a given step depends on the number of inputs in a given layer. Since Quantum Computers are notorious for their ability to run multiple iterations in a short period of time: the efficiency of a quantum neural network is solely dependent on the number of qubits in any given layer: and not on the depth of the network. === Cost functions === /  / To determine the effectiveness of a neural network: a cost function is used: which essentially measures the proximity of the network's output to the expected or desired output. In a Classical Neural Network: the weights (w{\displaystyle w}) and biases (b{\displaystyle b}) at each step determine the outcome of the cost function C(w:b){\displaystyle C(w:b)}. When training a Classical Neural network: the weights and biases are adjusted after each iteration: and given equation 1 below: where y(x){\displaystyle y(x)} is the desired output and aout(x){\displaystyle a^{\text{out}}(x)} is the actual output: the cost function is optimized when C(w:b){\displaystyle C(w:b)}= 0. For a quantum neural network: the cost function is determined by measuring the fidelity of the outcome state (ρout{\displaystyle \rho ^{\text{out}}}) with the desired outcome state (ϕout{\displaystyle \phi ^{\text{out}}}): seen in Equation 2 below. In this case: the Unitary operators are adjusted after each iteration: and the cost function is optimized when C = 1. Equation 1 C(w:b)=1N∑x||y(x)−aout(x)||2{\displaystyle C(w:b)={1 \over N}\sum _{x}{||y(x)-a^{\text{out}}(x)|| \over 2}} /  /  /  / Equation 2 C=1N∑xN⟨ϕout|ρout|ϕout⟩{\displaystyle C={1 \over N}\sum _{x}^{N}{\langle \phi ^{\text{out}}|\rho ^{\text{out}}|\phi ^{\text{out}}\rangle }} /  /  /  /  /  / == See also == /  / Differentiable programming /  / Optical neural network /  / Holographic associative memory /  / Quantum cognition /  / Quantum machine learning /  /  /  /  /  / == References == /  /  /  /  /  / == External links == /  / Recent review of quantum neural networks by M. Schuld: I. Sinayskiy and F. Petruccione /  / Review of quantum neural networks by Wei /  / Article by P. Gralewicz on the plausibility of quantum computing in biological neural networks /  / Training a neural net to recognize images 
0.6014927999931388,What categories does the term 'quantum neural networks' refer to?,"The term 'quantum neural networks' refers to three different categories: Quantum computer with classical data, classical computer with quantum data, and quantum computer with quantum data.",Cosine Similarity: 0.8863654775486663 / Quantum neural networks are computational neural network models which are based on the principles of quantum mechanics.The first ideas on quantum neural computation were published independently in 1995 by Subhash Kak and Ron Chrisley: engaging with the theory of quantum mind: which posits that quantum effects play a role in cognitive function. However: typical research in quantum neural networks involves combining classical artificial neural network models (which are widely used in machine learning for the important task of pattern recognition) with the advantages of quantum information in order to develop more efficient algorithms. One important motivation for these investigations is the difficulty to train classical neural networks: especially in big data applications. The hope is that features of quantum computing such as quantum parallelism or the effects of interference and entanglement can be used as resources. Since the technological implementation of a quantum computer is still in a premature stage: such quantum neural network models are mostly theoretical proposals that await their full implementation in physical experiments. Most Quantum neural networks are developed as feed-forward networks. Similar to their classical counterparts: this structure intakes input from one layer of qubits: and passes that input onto another layer of qubits. This layer of qubits evaluates this information and passes on the output to the next layer. Eventually the path leads to the final layer of qubits. The layers do not have to be of the same width: meaning they don't have to have the same number of qubits as the layer before or after it. This structure is trained on which path to take similar to classical artificial neural networks. This is discussed in a lower section. Quantum neural networks refer to three different categories: Quantum computer with classical data: classical computer with quantum data: and quantum computer with quantum data. == Examples == /  / Quantum neural network research is still in its infancy: and a conglomeration of proposals and ideas of varying scope and mathematical rigor have been put forward. Most of them are based on the idea of replacing classical binary or McCulloch-Pitts neurons with a qubit (which can be called a “quron”): resulting in neural units that can be in a superposition of the state ‘firing’ and ‘resting’. === Quantum perceptrons === /  / A lot of proposals attempt to find a quantum equivalent for the perceptron unit from which neural nets are constructed. A problem is that nonlinear activation functions do not immediately correspond to the mathematical structure of quantum theory: since a quantum evolution is described by linear operations and leads to probabilistic observation. Ideas to imitate the perceptron activation function with a quantum mechanical formalism reach from special measurements  to postulating non-linear quantum operators (a mathematical framework that is disputed). A direct implementation of the activation function using the circuit-based model of quantum computation has recently been proposed by Schuld: Sinayskiy and Petruccione based on the quantum phase estimation algorithm. === Quantum networks === /  / At a larger scale: researchers have attempted to generalize neural networks to the quantum setting. One way of constructing a quantum neuron is to first generalise classical neurons and then generalising them further to make unitary gates. Interactions between neurons can be controlled quantumly: with unitary gates: or classically: via measurement of the network states. This high-level theoretical technique can be applied broadly: by taking different types of networks and different implementations of quantum neurons: such as photonically implemented neurons and quantum reservoir processor (quantum version of reservoir computing). Most learning algorithms follow the classical model of training an artificial neural network to learn the input-output function of a given training set and use classical feedback loops to update parameters of the quantum system until they converge to an optimal configuration. Learning as a parameter optimisation problem has also been approached by adiabatic models of quantum computing.Quantum neural networks can be applied to algorithmic design: given qubits with tunable mutual interactions: one can attempt to learn interactions following the classical backpropagation rule from a training set of desired input-output relations: taken to be the desired output algorithm's behavior. The quantum network thus ‘learns’ an algorithm. === Quantum associative memory === /  / The first quantum associative memory algorithm was introduced by Dan Ventura and Tony Martinez in 1999. The authors do not attempt to translate the structure of artificial neural network models into quantum theory: but propose an algorithm for a circuit-based quantum computer that simulates associative memory. The memory states (in Hopfield neural networks saved in the weights of the neural connections) are written into a superposition: and a Grover-like quantum search algorithm retrieves the memory state closest to a given input. As such: this is not a fully content-addressable memory: since only incomplete patterns can be retrieved. The first truly content-addressable quantum memory: which can retrieve patterns also from corrupted inputs: was proposed by Carlo A. Trugenberger. Both memories can store an exponential (in terms of n qubits) number of patterns but can be used only once due to the no-cloning theorem and their destruction upon measurement. Trugenberger: however: has shown that his proababilistic model of quantum associative memory can be efficiently implemented and re-used multiples times for any polynomial number of stored patterns: a large advantage with respect to classical associative memories. === Classical neural networks inspired by quantum theory === /  / A substantial amount of interest has been given to a “quantum-inspired” model that uses ideas from quantum theory to implement a neural network based on fuzzy logic. == Training == /  / Quantum Neural Networks can be theoretically trained similarly to training classical/artificial neural networks. A key difference lies in communication between the layers of a neural networks. For classical neural networks: at the end of a given operation: the current perceptron copies its output to the next layer of perceptron(s) in the network. However: in a quantum neural network: where each perceptron is a qubit: this would violate the no-cloning theorem. A proposed generalized solution to this is to replace the classical fan-out method with an arbitrary unitary that spreads out: but does not copy: the output of one qubit to the next layer of qubits. ,Cosine Similarity: 0.8508973807867916 / Neural computers.
0.6945341999817174,How is the training method of classical and quantum neural networks different?,"Quantum Neural Networks can be theoretically trained similarly to training classical/artificial neural networks. The key difference lies in the communication between the layers of a neural network. In a quantum neural network, this is done by replacing the classical fan-out method with an arbitrary unitary that spreads out, but does not copy, the output of one qubit to the next layer of qubits. This process adheres to the quantum operation requirement of reversibility.",Cosine Similarity: 0.8654137336306037 / Quantum neural networks are computational neural network models which are based on the principles of quantum mechanics.The first ideas on quantum neural computation were published independently in 1995 by Subhash Kak and Ron Chrisley: engaging with the theory of quantum mind: which posits that quantum effects play a role in cognitive function. However: typical research in quantum neural networks involves combining classical artificial neural network models (which are widely used in machine learning for the important task of pattern recognition) with the advantages of quantum information in order to develop more efficient algorithms. One important motivation for these investigations is the difficulty to train classical neural networks: especially in big data applications. The hope is that features of quantum computing such as quantum parallelism or the effects of interference and entanglement can be used as resources. Since the technological implementation of a quantum computer is still in a premature stage: such quantum neural network models are mostly theoretical proposals that await their full implementation in physical experiments. Most Quantum neural networks are developed as feed-forward networks. Similar to their classical counterparts: this structure intakes input from one layer of qubits: and passes that input onto another layer of qubits. This layer of qubits evaluates this information and passes on the output to the next layer. Eventually the path leads to the final layer of qubits. The layers do not have to be of the same width: meaning they don't have to have the same number of qubits as the layer before or after it. This structure is trained on which path to take similar to classical artificial neural networks. This is discussed in a lower section. Quantum neural networks refer to three different categories: Quantum computer with classical data: classical computer with quantum data: and quantum computer with quantum data. == Examples == /  / Quantum neural network research is still in its infancy: and a conglomeration of proposals and ideas of varying scope and mathematical rigor have been put forward. Most of them are based on the idea of replacing classical binary or McCulloch-Pitts neurons with a qubit (which can be called a “quron”): resulting in neural units that can be in a superposition of the state ‘firing’ and ‘resting’. === Quantum perceptrons === /  / A lot of proposals attempt to find a quantum equivalent for the perceptron unit from which neural nets are constructed. A problem is that nonlinear activation functions do not immediately correspond to the mathematical structure of quantum theory: since a quantum evolution is described by linear operations and leads to probabilistic observation. Ideas to imitate the perceptron activation function with a quantum mechanical formalism reach from special measurements  to postulating non-linear quantum operators (a mathematical framework that is disputed). A direct implementation of the activation function using the circuit-based model of quantum computation has recently been proposed by Schuld: Sinayskiy and Petruccione based on the quantum phase estimation algorithm. === Quantum networks === /  / At a larger scale: researchers have attempted to generalize neural networks to the quantum setting. One way of constructing a quantum neuron is to first generalise classical neurons and then generalising them further to make unitary gates. Interactions between neurons can be controlled quantumly: with unitary gates: or classically: via measurement of the network states. This high-level theoretical technique can be applied broadly: by taking different types of networks and different implementations of quantum neurons: such as photonically implemented neurons and quantum reservoir processor (quantum version of reservoir computing). Most learning algorithms follow the classical model of training an artificial neural network to learn the input-output function of a given training set and use classical feedback loops to update parameters of the quantum system until they converge to an optimal configuration. Learning as a parameter optimisation problem has also been approached by adiabatic models of quantum computing.Quantum neural networks can be applied to algorithmic design: given qubits with tunable mutual interactions: one can attempt to learn interactions following the classical backpropagation rule from a training set of desired input-output relations: taken to be the desired output algorithm's behavior. The quantum network thus ‘learns’ an algorithm. === Quantum associative memory === /  / The first quantum associative memory algorithm was introduced by Dan Ventura and Tony Martinez in 1999. The authors do not attempt to translate the structure of artificial neural network models into quantum theory: but propose an algorithm for a circuit-based quantum computer that simulates associative memory. The memory states (in Hopfield neural networks saved in the weights of the neural connections) are written into a superposition: and a Grover-like quantum search algorithm retrieves the memory state closest to a given input. As such: this is not a fully content-addressable memory: since only incomplete patterns can be retrieved. The first truly content-addressable quantum memory: which can retrieve patterns also from corrupted inputs: was proposed by Carlo A. Trugenberger. Both memories can store an exponential (in terms of n qubits) number of patterns but can be used only once due to the no-cloning theorem and their destruction upon measurement. Trugenberger: however: has shown that his proababilistic model of quantum associative memory can be efficiently implemented and re-used multiples times for any polynomial number of stored patterns: a large advantage with respect to classical associative memories. === Classical neural networks inspired by quantum theory === /  / A substantial amount of interest has been given to a “quantum-inspired” model that uses ideas from quantum theory to implement a neural network based on fuzzy logic. == Training == /  / Quantum Neural Networks can be theoretically trained similarly to training classical/artificial neural networks. A key difference lies in communication between the layers of a neural networks. For classical neural networks: at the end of a given operation: the current perceptron copies its output to the next layer of perceptron(s) in the network. However: in a quantum neural network: where each perceptron is a qubit: this would violate the no-cloning theorem. A proposed generalized solution to this is to replace the classical fan-out method with an arbitrary unitary that spreads out: but does not copy: the output of one qubit to the next layer of qubits. ,Cosine Similarity: 0.8390525636712032 / There are two main types of neural network.
0.7627561999834143,What is the ReLU activation function in artificial neural networks?,"The ReLU (rectified linear unit) activation function in artificial neural networks is defined as the positive part of its argument. It's also known as a ramp function, analogous to half-wave rectification in electrical engineering. This activation function was introduced by Kunihiko Fukushima in 1969. ",Cosine Similarity: 0.9101190155215582 / In the context of artificial neural networks: the rectifier or ReLU (rectified linear unit) activation function is an activation function defined as the positive part of its argument: /  /  /  / f(x)=x+=max(0:x)=x+|x|2={xif x>0:0otherwise:{\displaystyle f(x)=x^{+}=\max(0:x)={\frac {x+|x|}{2}}={\begin{cases}x&{\text{if }}x>0:\\0&{\text{otherwise}}:\end{cases}}}where x is the input to a neuron.,Cosine Similarity: 0.8774502590379399 / This has become the most cited neural network of the 21st century.In 2011: Xavier Glorot: Antoine Bordes and Yoshua Bengio found that the ReLU of Kunihiko Fukushima also helps to overcome the vanishing gradient problem: compared to widely used activation functions prior to 2011.
0.6191740999929607,How does ReLU help in neural networks?,"ReLU helps in neural networks by enabling better training of deeper networks, compared to the widely used activation functions prior to 2011 like the logistic sigmoid and the hyperbolic tangent. ReLU has found applications in computer vision and speech recognition using deep neural nets and computational neuroscience.",Cosine Similarity: 0.8864417029619176 / In recent developments of deep learning the rectified linear unit (ReLU) is more frequently used as one of the possible ways to overcome the numerical problems related to the sigmoids.,Cosine Similarity: 0.8769018380713517 / In the context of artificial neural networks: the rectifier or ReLU (rectified linear unit) activation function is an activation function defined as the positive part of its argument: /  /  /  / f(x)=x+=max(0:x)=x+|x|2={xif x>0:0otherwise:{\displaystyle f(x)=x^{+}=\max(0:x)={\frac {x+|x|}{2}}={\begin{cases}x&{\text{if }}x>0:\\0&{\text{otherwise}}:\end{cases}}}where x is the input to a neuron.
0.5750038999831304,What are some advantages of the ReLU function?,The ReLU function offers the advantages of sparse activation where only about 50% of hidden units are activated. The feature helps in better gradient propagation and efficient computation. It is also scale-invariant.,Cosine Similarity: 0.8779862752338166 / In recent developments of deep learning the rectified linear unit (ReLU) is more frequently used as one of the possible ways to overcome the numerical problems related to the sigmoids.,Cosine Similarity: 0.873648808865454 / Rectified linear units: compared to sigmoid function or similar activation functions: allow faster and effective training of deep neural architectures on large and complex datasets.
0.6068857999925967,What are some potential problems of ReLU?,"Some potential problems of ReLU include the fact that it is non-differentiable at zero. It's also not zero-centered, is unbounded, and may suffer from the dying ReLU problem where neurons become inactive for virtually all inputs, a form of the vanishing gradient problem.",Cosine Similarity: 0.8802473782675038 / Dying ReLU problem: ReLU (rectified linear unit) neurons can sometimes be pushed into states in which they become inactive for essentially all inputs.,Cosine Similarity: 0.8623897158440227 / In recent developments of deep learning the rectified linear unit (ReLU) is more frequently used as one of the possible ways to overcome the numerical problems related to the sigmoids.
0.645966900017811,Can you name and explain an variant of ReLU?,"A variant of ReLU is the Leaky ReLU. It allows a small, positive gradient when the unit is not active, helping to mitigate the vanishing gradient problem.",Cosine Similarity: 0.8805938229365408 / == Variants == /  /  /  /  /  / === Piecewise-linear variants === /  /  /  /  /  / ==== Leaky ReLU ==== /  / Leaky ReLUs allow a small: positive gradient when the unit is not active: helping to mitigate the vanishing gradient problem.,Cosine Similarity: 0.8577687620901735 / It was inspired by Swish: itself a variant of ReLU.
0.6139021999842953,What is a recurrent neural network (RNN)?,"A recurrent neural network (RNN) is one of the two broad types of artificial neural network, characterized by the flow of information between its layers. It is a bi-directional artificial neural network, meaning it allows the output from some nodes to affect subsequent input to the same nodes. ","Cosine Similarity: 0.9089460001738422 / A recurrent neural network (RNN) is one of the two broad types of artificial neural network: characterized by direction of the flow of information between its layers.In contrast to the uni-directional feedforward neural network: it is a bi-directional artificial neural network: meaning that it allows the output from some nodes to affect subsequent input to the same nodes. Their ability to use internal state (memory) to process arbitrary sequences of inputs makes them applicable to tasks such as unsegmented: connected handwriting recognition or speech recognition. The term ""recurrent neural network"" is used to refer to the class of networks with an infinite impulse response: whereas ""convolutional neural network"" refers to the class of finite impulse response. Both classes of networks exhibit temporal dynamic behavior. A finite impulse recurrent network is a directed acyclic graph that can be unrolled and replaced with a strictly feedforward neural network: while an infinite impulse recurrent network is a directed cyclic graph that can not be unrolled. Additional stored states and the storage under direct control by the network can be added to both infinite-impulse and finite-impulse networks. Another network or graph can also replace the storage if that incorporates time delays or has feedback loops. Such controlled states are referred to as gated states or gated memory and are part of long short-term memory networks (LSTMs) and gated recurrent units. This is also called Feedforward Neural Network (FNN). Recurrent neural networks are theoretically Turing complete and can run arbitrary programs to process arbitrary sequences of inputs. == History == /  / The Ising model (1925) by Wilhelm Lenz and Ernst Ising /  / was the first RNN architecture that did not learn. Shun'ichi Amari made it adaptive in 1972. This was also called the Hopfield network (1982). See also David Rumelhart's work in 1986. In 1993: a neural history compressor system solved a ""Very Deep Learning"" task that required more than 1000 subsequent layers in an RNN unfolded in time. === LSTM === /  / Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1997 and set accuracy records in multiple applications domains.Around 2007: LSTM started to revolutionize speech recognition: outperforming traditional models in certain speech applications. In 2009: a Connectionist Temporal Classification (CTC)-trained LSTM network was the first RNN to win pattern recognition contests when it won several competitions in connected handwriting recognition. In 2014: the Chinese company Baidu used CTC-trained RNNs to break the 2S09 Switchboard Hub5'00 speech recognition dataset benchmark without using any traditional speech processing methods.LSTM also improved large-vocabulary speech recognition and text-to-speech synthesis and was used in Google Android. In 2015: Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM.LSTM broke records for improved machine translation: Language Modeling and Multilingual Language Processing. LSTM combined with convolutional neural networks (CNNs) improved automatic image captioning. == Architectures == /  /  /  / RNNs come in many variants. === Fully recurrent === /  / Fully recurrent neural networks (FRNN) connect the outputs of all neurons to the inputs of all neurons. This is the most general neural network topology because all other topologies can be represented by setting some connection weights to zero to simulate the lack of connections between those neurons. The illustration to the right may be misleading to many because practical neural network topologies are frequently organized in ""layers"" and the drawing gives that appearance. However: what appears to be layers are: in fact: different steps in time of the same fully recurrent neural network. The left-most item in the illustration shows the recurrent connections as the arc labeled 'v'. It is ""unfolded"" in time to produce the appearance of layers. === Elman networks and Jordan networks === /  / An Elman network is a three-layer network (arranged horizontally as x: y: and z in the illustration) with the addition of a set of context units (u in the illustration). The middle (hidden) layer is connected to these context units fixed with a weight of one. At each time step: the input is fed forward and a learning rule is applied. The fixed back-connections save a copy of the previous values of the hidden units in the context units (since they propagate over the connections before the learning rule is applied). Thus the network can maintain a sort of state: allowing it to perform such tasks as sequence-prediction that are beyond the power of a standard multilayer perceptron. Jordan networks are similar to Elman networks. The context units are fed from the output layer instead of the hidden layer. The context units in a Jordan network are also called the state layer. They have a recurrent connection to themselves.Elman and Jordan networks are also known as ""Simple recurrent networks"" (SRN). Elman network /  / ht=σh(Whxt+Uhht−1+bh)yt=σy(Wyht+by){\displaystyle {\begin{aligned}h_{t}&=\sigma _{h}(W_{h}x_{t}+U_{h}h_{t-1}+b_{h})\\y_{t}&=\sigma _{y}(W_{y}h_{t}+b_{y})\end{aligned}}} /  / Jordan network /  / ht=σh(Whxt+Uhyt−1+bh)yt=σy(Wyht+by){\displaystyle {\begin{aligned}h_{t}&=\sigma _{h}(W_{h}x_{t}+U_{h}y_{t-1}+b_{h})\\y_{t}&=\sigma _{y}(W_{y}h_{t}+b_{y})\end{aligned}}}Variables and functions /  /  /  / xt{\displaystyle x_{t}}: input vector /  / ht{\displaystyle h_{t}}: hidden layer vector /  / yt{\displaystyle y_{t}}: output vector /  / W{\displaystyle W}: U{\displaystyle U} and b{\displaystyle b}: parameter matrices and vector /  / σh{\displaystyle \sigma _{h}} and σy{\displaystyle \sigma _{y}}: Activation functions /  /  /  /  /  / === Hopfield === /  /  /  / The Hopfield network is an RNN in which all connections across layers are equally sized. It requires stationary inputs and is thus not a general RNN: as it does not process sequences of patterns. However: it guarantees that it will converge. If the connections are trained using Hebbian learning: then the Hopfield network can perform as robust content-addressable memory: resistant to connection alteration. ==== Bidirectional associative memory ==== /  /  /  / Introduced by Bart Kosko: a bidirectional associative memory (BAM) network is a variant of a Hopfield network that stores associative data as a vector. The bi-directionality comes from passing information through a matrix and its transpose. Typically: bipolar encoding is preferred to binary encoding of the associative pairs. Recently: stochastic BAM models using Markov stepping were optimized for increased network stability and relevance to real-world applications.A BAM network has two layers: either of which can be driven as an input to recall an association and produce an output on the other layer. === Echo state === /  /  /  / Echo state networks (ESN) have a sparsely connected random hidden layer. ",Cosine Similarity: 0.8870303040787318 / == Recurrent neural network == /  /  /  / Recurrent neural networks (RNN) propagate data forward: but also backwards: from later processing stages to earlier stages.
0.6773978999990504,What is the difference between recurrent neural networks and convolutional neural networks in terms of impulse response?,"The term ""recurrent neural network"" is used to refer to the class of networks with an infinite impulse response, whereas ""convolutional neural network"" refers to the class of finite impulse response. Both classes of networks exhibit temporal dynamic behavior.","Cosine Similarity: 0.8596040627266243 / A recurrent neural network (RNN) is one of the two broad types of artificial neural network: characterized by direction of the flow of information between its layers.In contrast to the uni-directional feedforward neural network: it is a bi-directional artificial neural network: meaning that it allows the output from some nodes to affect subsequent input to the same nodes. Their ability to use internal state (memory) to process arbitrary sequences of inputs makes them applicable to tasks such as unsegmented: connected handwriting recognition or speech recognition. The term ""recurrent neural network"" is used to refer to the class of networks with an infinite impulse response: whereas ""convolutional neural network"" refers to the class of finite impulse response. Both classes of networks exhibit temporal dynamic behavior. A finite impulse recurrent network is a directed acyclic graph that can be unrolled and replaced with a strictly feedforward neural network: while an infinite impulse recurrent network is a directed cyclic graph that can not be unrolled. Additional stored states and the storage under direct control by the network can be added to both infinite-impulse and finite-impulse networks. Another network or graph can also replace the storage if that incorporates time delays or has feedback loops. Such controlled states are referred to as gated states or gated memory and are part of long short-term memory networks (LSTMs) and gated recurrent units. This is also called Feedforward Neural Network (FNN). Recurrent neural networks are theoretically Turing complete and can run arbitrary programs to process arbitrary sequences of inputs. == History == /  / The Ising model (1925) by Wilhelm Lenz and Ernst Ising /  / was the first RNN architecture that did not learn. Shun'ichi Amari made it adaptive in 1972. This was also called the Hopfield network (1982). See also David Rumelhart's work in 1986. In 1993: a neural history compressor system solved a ""Very Deep Learning"" task that required more than 1000 subsequent layers in an RNN unfolded in time. === LSTM === /  / Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1997 and set accuracy records in multiple applications domains.Around 2007: LSTM started to revolutionize speech recognition: outperforming traditional models in certain speech applications. In 2009: a Connectionist Temporal Classification (CTC)-trained LSTM network was the first RNN to win pattern recognition contests when it won several competitions in connected handwriting recognition. In 2014: the Chinese company Baidu used CTC-trained RNNs to break the 2S09 Switchboard Hub5'00 speech recognition dataset benchmark without using any traditional speech processing methods.LSTM also improved large-vocabulary speech recognition and text-to-speech synthesis and was used in Google Android. In 2015: Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM.LSTM broke records for improved machine translation: Language Modeling and Multilingual Language Processing. LSTM combined with convolutional neural networks (CNNs) improved automatic image captioning. == Architectures == /  /  /  / RNNs come in many variants. === Fully recurrent === /  / Fully recurrent neural networks (FRNN) connect the outputs of all neurons to the inputs of all neurons. This is the most general neural network topology because all other topologies can be represented by setting some connection weights to zero to simulate the lack of connections between those neurons. The illustration to the right may be misleading to many because practical neural network topologies are frequently organized in ""layers"" and the drawing gives that appearance. However: what appears to be layers are: in fact: different steps in time of the same fully recurrent neural network. The left-most item in the illustration shows the recurrent connections as the arc labeled 'v'. It is ""unfolded"" in time to produce the appearance of layers. === Elman networks and Jordan networks === /  / An Elman network is a three-layer network (arranged horizontally as x: y: and z in the illustration) with the addition of a set of context units (u in the illustration). The middle (hidden) layer is connected to these context units fixed with a weight of one. At each time step: the input is fed forward and a learning rule is applied. The fixed back-connections save a copy of the previous values of the hidden units in the context units (since they propagate over the connections before the learning rule is applied). Thus the network can maintain a sort of state: allowing it to perform such tasks as sequence-prediction that are beyond the power of a standard multilayer perceptron. Jordan networks are similar to Elman networks. The context units are fed from the output layer instead of the hidden layer. The context units in a Jordan network are also called the state layer. They have a recurrent connection to themselves.Elman and Jordan networks are also known as ""Simple recurrent networks"" (SRN). Elman network /  / ht=σh(Whxt+Uhht−1+bh)yt=σy(Wyht+by){\displaystyle {\begin{aligned}h_{t}&=\sigma _{h}(W_{h}x_{t}+U_{h}h_{t-1}+b_{h})\\y_{t}&=\sigma _{y}(W_{y}h_{t}+b_{y})\end{aligned}}} /  / Jordan network /  / ht=σh(Whxt+Uhyt−1+bh)yt=σy(Wyht+by){\displaystyle {\begin{aligned}h_{t}&=\sigma _{h}(W_{h}x_{t}+U_{h}y_{t-1}+b_{h})\\y_{t}&=\sigma _{y}(W_{y}h_{t}+b_{y})\end{aligned}}}Variables and functions /  /  /  / xt{\displaystyle x_{t}}: input vector /  / ht{\displaystyle h_{t}}: hidden layer vector /  / yt{\displaystyle y_{t}}: output vector /  / W{\displaystyle W}: U{\displaystyle U} and b{\displaystyle b}: parameter matrices and vector /  / σh{\displaystyle \sigma _{h}} and σy{\displaystyle \sigma _{y}}: Activation functions /  /  /  /  /  / === Hopfield === /  /  /  / The Hopfield network is an RNN in which all connections across layers are equally sized. It requires stationary inputs and is thus not a general RNN: as it does not process sequences of patterns. However: it guarantees that it will converge. If the connections are trained using Hebbian learning: then the Hopfield network can perform as robust content-addressable memory: resistant to connection alteration. ==== Bidirectional associative memory ==== /  /  /  / Introduced by Bart Kosko: a bidirectional associative memory (BAM) network is a variant of a Hopfield network that stores associative data as a vector. The bi-directionality comes from passing information through a matrix and its transpose. Typically: bipolar encoding is preferred to binary encoding of the associative pairs. Recently: stochastic BAM models using Markov stepping were optimized for increased network stability and relevance to real-world applications.A BAM network has two layers: either of which can be driven as an input to recall an association and produce an output on the other layer. === Echo state === /  /  /  / Echo state networks (ESN) have a sparsely connected random hidden layer. ","Cosine Similarity: 0.8420478590335143 / The weights of output neurons are the only part of the network that can change (be trained).ESNs are good at reproducing certain time series. A variant for spiking neurons is known as a liquid state machine. === Independently RNN (IndRNN) === /  / The independently recurrent neural network (IndRNN) addresses the gradient vanishing and exploding problems in the traditional fully connected RNN. Each neuron in one layer only receives its own past state as context information (instead of full connectivity to all other neurons in this layer) and thus neurons are independent of each other's history. The gradient backpropagation can be regulated to avoid gradient vanishing and exploding in order to keep long or short-term memory. The cross-neuron information is explored in the next layers. IndRNN can be robustly trained with non-saturated nonlinear functions such as ReLU. Deep networks can be trained using skip connections. === Recursive === /  /  /  / A recursive neural network is created by applying the same set of weights recursively over a differentiable graph-like structure by traversing the structure in topological order. Such networks are typically also trained by the reverse mode of automatic differentiation. They can process distributed representations of structure: such as logical terms. A special case of recursive neural networks is the RNN whose structure corresponds to a linear chain. Recursive neural networks have been applied to natural language processing. The Recursive Neural Tensor Network uses a tensor-based composition function for all nodes in the tree. === Neural history compressor === /  / The neural history compressor is an unsupervised stack of RNNs. At the input level: it learns to predict its next input from the previous inputs. Only unpredictable inputs of some RNN in the hierarchy become inputs to the next higher level RNN: which therefore recomputes its internal state only rarely. Each higher level RNN thus studies a compressed representation of the information in the RNN below. This is done such that the input sequence can be precisely reconstructed from the representation at the highest level. The system effectively minimizes the description length or the negative logarithm of the probability of the data. Given a lot of learnable predictability in the incoming data sequence: the highest level RNN can use supervised learning to easily classify even deep sequences with long intervals between important events. It is possible to distill the RNN hierarchy into two RNNs: the ""conscious"" chunker (higher level) and the ""subconscious"" automatizer (lower level). Once the chunker has learned to predict and compress inputs that are unpredictable by the automatizer: then the automatizer can be forced in the next learning phase to predict or imitate through additional units the hidden units of the more slowly changing chunker. This makes it easy for the automatizer to learn appropriate: rarely changing memories across long intervals. In turn: this helps the automatizer to make many of its once unpredictable inputs predictable: such that the chunker can focus on the remaining unpredictable events.A generative model partially overcame the vanishing gradient problem of automatic differentiation or backpropagation in neural networks in 1992. In 1993: such a system solved a ""Very Deep Learning"" task that required more than 1000 subsequent layers in an RNN unfolded in time. === Second order RNNs === /  / Second-order RNNs use higher order weights wijk{\displaystyle w{}_{ijk}} instead of the standard wij{\displaystyle w{}_{ij}} weights: and states can be a product. This allows a direct mapping to a finite-state machine both in training: stability: and representation. Long short-term memory is an example of this but has no such formal mappings or proof of stability. === Long short-term memory === /  /  /  / Long short-term memory (LSTM) is a deep learning system that avoids the vanishing gradient problem. LSTM is normally augmented by recurrent gates called ""forget gates"". LSTM prevents backpropagated errors from vanishing or exploding. Instead: errors can flow backward through unlimited numbers of virtual layers unfolded in space. That is: LSTM can learn tasks that require memories of events that happened thousands or even millions of discrete time steps earlier. Problem-specific LSTM-like topologies can be evolved. LSTM works even given long delays between significant events and can handle signals that mix low and high-frequency components. Many applications use stacks of LSTM RNNs and train them by connectionist temporal classification (CTC) to find an RNN weight matrix that maximizes the probability of the label sequences in a training set: given the corresponding input sequences. CTC achieves both alignment and recognition. LSTM can learn to recognize context-sensitive languages unlike previous models based on hidden Markov models (HMM) and similar concepts. === Gated recurrent unit === /  /  /  / Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks introduced in 2014. They are used in the full form and several simplified variants. Their performance on polyphonic music modeling and speech signal modeling was found to be similar to that of long short-term memory. They have fewer parameters than LSTM: as they lack an output gate. === Bi-directional === /  /  /  / Bi-directional RNNs use a finite sequence to predict or label each element of the sequence based on the element's past and future contexts. This is done by concatenating the outputs of two RNNs: one processing the sequence from left to right: and the other one from right to left. The combined outputs are the predictions of the teacher-given target signals. This technique has been proven to be especially useful when combined with LSTM RNNs. === Continuous-time === /  / A continuous-time recurrent neural network (CTRNN) uses a system of ordinary differential equations to model the effects on a neuron of the incoming inputs. For a neuron i{\displaystyle i} in the network with activation yi{\displaystyle y_{i}}: the rate of change of activation is given by: /  /  /  / τiy˙i=−yi+∑j=1nwjiσ(yj−Θj)+Ii(t){\displaystyle \tau _{i}{\dot {y}}_{i}=-y_{i}+\sum _{j=1}^{n}w_{ji}\sigma (y_{j}-\Theta _{j})+I_{i}(t)}Where: /  /  /  / τi{\displaystyle \tau _{i}} : Time constant of postsynaptic node /  / yi{\displaystyle y_{i}} : Activation of postsynaptic node /  / y˙i{\displaystyle {\dot {y}}_{i}} : Rate of change of activation of postsynaptic node /  / wji{\displaystyle w{}_{ji}} : Weight of connection from pre to postsynaptic node /  / σ(x){\displaystyle \sigma (x)} : Sigmoid of x e.g. σ(x)=1/(1+e−x){\displaystyle \sigma (x)=1/(1+e^{-x})}. yj{\displaystyle y_{j}} : Activation of presynaptic node /  / Θj{\displaystyle \Theta _{j}} : Bias of presynaptic node /  / Ii(t){\displaystyle I_{i}(t)} : Input (if any) to nodeCTRNNs have been applied to evolutionary robotics where they have been used to address vision: co-operation: and minimal cognitive behaviour.Note that: by the Shannon sampling theorem: discrete-time recurrent neural networks can be viewed as continuous-time recurrent neural networks where the differential equations have transformed into equivalent difference equations. "
0.6251702999870759,What is a significant feature of LSTM networks related to states?,"It can add additional stored states and the storage under direct control by the network, referred to as gated states or gated memory.","Cosine Similarity: 0.8571314456895754 / LSTM recurrent neural networks can learn ""very deep learning"" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before.","Cosine Similarity: 0.8484361095677612 / The weights of output neurons are the only part of the network that can change (be trained).ESNs are good at reproducing certain time series. A variant for spiking neurons is known as a liquid state machine. === Independently RNN (IndRNN) === /  / The independently recurrent neural network (IndRNN) addresses the gradient vanishing and exploding problems in the traditional fully connected RNN. Each neuron in one layer only receives its own past state as context information (instead of full connectivity to all other neurons in this layer) and thus neurons are independent of each other's history. The gradient backpropagation can be regulated to avoid gradient vanishing and exploding in order to keep long or short-term memory. The cross-neuron information is explored in the next layers. IndRNN can be robustly trained with non-saturated nonlinear functions such as ReLU. Deep networks can be trained using skip connections. === Recursive === /  /  /  / A recursive neural network is created by applying the same set of weights recursively over a differentiable graph-like structure by traversing the structure in topological order. Such networks are typically also trained by the reverse mode of automatic differentiation. They can process distributed representations of structure: such as logical terms. A special case of recursive neural networks is the RNN whose structure corresponds to a linear chain. Recursive neural networks have been applied to natural language processing. The Recursive Neural Tensor Network uses a tensor-based composition function for all nodes in the tree. === Neural history compressor === /  / The neural history compressor is an unsupervised stack of RNNs. At the input level: it learns to predict its next input from the previous inputs. Only unpredictable inputs of some RNN in the hierarchy become inputs to the next higher level RNN: which therefore recomputes its internal state only rarely. Each higher level RNN thus studies a compressed representation of the information in the RNN below. This is done such that the input sequence can be precisely reconstructed from the representation at the highest level. The system effectively minimizes the description length or the negative logarithm of the probability of the data. Given a lot of learnable predictability in the incoming data sequence: the highest level RNN can use supervised learning to easily classify even deep sequences with long intervals between important events. It is possible to distill the RNN hierarchy into two RNNs: the ""conscious"" chunker (higher level) and the ""subconscious"" automatizer (lower level). Once the chunker has learned to predict and compress inputs that are unpredictable by the automatizer: then the automatizer can be forced in the next learning phase to predict or imitate through additional units the hidden units of the more slowly changing chunker. This makes it easy for the automatizer to learn appropriate: rarely changing memories across long intervals. In turn: this helps the automatizer to make many of its once unpredictable inputs predictable: such that the chunker can focus on the remaining unpredictable events.A generative model partially overcame the vanishing gradient problem of automatic differentiation or backpropagation in neural networks in 1992. In 1993: such a system solved a ""Very Deep Learning"" task that required more than 1000 subsequent layers in an RNN unfolded in time. === Second order RNNs === /  / Second-order RNNs use higher order weights wijk{\displaystyle w{}_{ijk}} instead of the standard wij{\displaystyle w{}_{ij}} weights: and states can be a product. This allows a direct mapping to a finite-state machine both in training: stability: and representation. Long short-term memory is an example of this but has no such formal mappings or proof of stability. === Long short-term memory === /  /  /  / Long short-term memory (LSTM) is a deep learning system that avoids the vanishing gradient problem. LSTM is normally augmented by recurrent gates called ""forget gates"". LSTM prevents backpropagated errors from vanishing or exploding. Instead: errors can flow backward through unlimited numbers of virtual layers unfolded in space. That is: LSTM can learn tasks that require memories of events that happened thousands or even millions of discrete time steps earlier. Problem-specific LSTM-like topologies can be evolved. LSTM works even given long delays between significant events and can handle signals that mix low and high-frequency components. Many applications use stacks of LSTM RNNs and train them by connectionist temporal classification (CTC) to find an RNN weight matrix that maximizes the probability of the label sequences in a training set: given the corresponding input sequences. CTC achieves both alignment and recognition. LSTM can learn to recognize context-sensitive languages unlike previous models based on hidden Markov models (HMM) and similar concepts. === Gated recurrent unit === /  /  /  / Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks introduced in 2014. They are used in the full form and several simplified variants. Their performance on polyphonic music modeling and speech signal modeling was found to be similar to that of long short-term memory. They have fewer parameters than LSTM: as they lack an output gate. === Bi-directional === /  /  /  / Bi-directional RNNs use a finite sequence to predict or label each element of the sequence based on the element's past and future contexts. This is done by concatenating the outputs of two RNNs: one processing the sequence from left to right: and the other one from right to left. The combined outputs are the predictions of the teacher-given target signals. This technique has been proven to be especially useful when combined with LSTM RNNs. === Continuous-time === /  / A continuous-time recurrent neural network (CTRNN) uses a system of ordinary differential equations to model the effects on a neuron of the incoming inputs. For a neuron i{\displaystyle i} in the network with activation yi{\displaystyle y_{i}}: the rate of change of activation is given by: /  /  /  / τiy˙i=−yi+∑j=1nwjiσ(yj−Θj)+Ii(t){\displaystyle \tau _{i}{\dot {y}}_{i}=-y_{i}+\sum _{j=1}^{n}w_{ji}\sigma (y_{j}-\Theta _{j})+I_{i}(t)}Where: /  /  /  / τi{\displaystyle \tau _{i}} : Time constant of postsynaptic node /  / yi{\displaystyle y_{i}} : Activation of postsynaptic node /  / y˙i{\displaystyle {\dot {y}}_{i}} : Rate of change of activation of postsynaptic node /  / wji{\displaystyle w{}_{ji}} : Weight of connection from pre to postsynaptic node /  / σ(x){\displaystyle \sigma (x)} : Sigmoid of x e.g. σ(x)=1/(1+e−x){\displaystyle \sigma (x)=1/(1+e^{-x})}. yj{\displaystyle y_{j}} : Activation of presynaptic node /  / Θj{\displaystyle \Theta _{j}} : Bias of presynaptic node /  / Ii(t){\displaystyle I_{i}(t)} : Input (if any) to nodeCTRNNs have been applied to evolutionary robotics where they have been used to address vision: co-operation: and minimal cognitive behaviour.Note that: by the Shannon sampling theorem: discrete-time recurrent neural networks can be viewed as continuous-time recurrent neural networks where the differential equations have transformed into equivalent difference equations. "
0.6162313999957405,What was the first RNN architecture that did not learn and who made it adaptive?,"The Ising model by Wilhelm Lenz and Ernst Ising, made in 1925, was the first RNN architecture that did not learn. It was made adaptive by Shun'ichi Amari in 1972.",Cosine Similarity: 0.8227136101255107 / This extreme learning machine was not yet a deep learning network.In 1965: the first  deep-learning feedforward network: not yet using stochastic gradient descent: was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa: at the time called the Group Method of Data Handling.In 1967: a deep-learning network: using stochastic gradient descent for the first time: was able to classify non-linearily separable pattern classes: as reported Shun'ichi Amari.,"Cosine Similarity: 0.8194156001093421 / The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks.The probabilistic interpretation was introduced by researchers including Hopfield: Widrow and Narendra and popularized in surveys such as the one by Bishop. == History == /  / There are two types of artificial neural network (ANN): feedforward neural networks (FNNs) and recurrent neural networks (RNNs). RNNs have cycles in their connectivity structure: FNNs don't. In the 1920s: Wilhelm Lenz and Ernst Ising created and analyzed the Ising model which is essentially a non-learning RNN architecture consisting of neuron-like threshold elements. In 1972: Shun'ichi Amari made this architecture adaptive. His learning RNN was popularised by John Hopfield in 1982. RNNs have become central for speech recognition and language processing. Charles Tappert writes that Frank Rosenblatt developed and explored all of the basic ingredients of the deep learning systems of today: referring to Rosenblatt's 1962 book which introduced multilayer perceptron (MLP) with 3 layers: an input layer: a hidden layer with randomized weights that did not learn: and an output layer. It also introduced variants: including a version with four-layer perceptrons where the last two layers have learned weights (and thus a proper multilayer perceptron). : section 16  In addition: term deep learning was proposed in 1986 by Rina Dechter although the history of its appearance is apparently more complicated.The first general: working learning algorithm for supervised: deep: feedforward: multilayer perceptrons was published by Alexey Ivakhnenko and Lapa in 1967. A 1971 paper described a deep network with eight layers trained by the group method of data handling.The first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. In computer experiments conducted by Amari's student Saito: a five layer MLP with two modifiable layers learned  internal representations to classify non-linearily separable pattern classes. In 1987 Matthew Brand reported that wide 12-layer nonlinear perceptrons could be fully end-to-end trained to reproduce logic functions of nontrivial circuit depth via gradient descent on small batches of random input/output samples: but concluded that training time on contemporary hardware (sub-megaflop computers) made the technique impractical: and proposed using fixed random early layers as an input hash for a single modifiable layer. Instead: subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique. In 1970: Seppo Linnainmaa published the reverse mode of automatic differentiation of discrete connected networks of nested differentiable functions. This became known as backpropagation. It is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes. The terminology ""back-propagating errors"" was actually introduced in 1962 by Rosenblatt: but he did not know how to implement this: although Henry J. Kelley had a continuous precursor of backpropagation already in 1960 in the context of control theory. In 1982: Paul Werbos applied backpropagation to MLPs in the way that has become standard. In 1985: David E. Rumelhart et al. published an experimental analysis of the technique.Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers began with the Neocognitron introduced by Kunihiko Fukushima in 1980. In 1969: he also introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for CNNs and deep learning in general. CNNs have become an essential tool for computer vision. The term Deep Learning was introduced to the machine learning community by Rina Dechter in 1986: and to artificial neural networks by Igor Aizenberg and colleagues in 2000: in the context of Boolean threshold neurons.In 1988: Wei Zhang et al. applied the backpropagation algorithm  /  / to a convolutional neural network (a simplified Neocognitron with convolutional interconnections between the image feature layers and the last fully connected layer) for alphabet recognition. They also proposed an implementation of the CNN with an optical computing system. In 1989: Yann LeCun et al. applied backpropagation to a CNN with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked: training required 3 days. Subsequently: Wei Zhang: et al. modified their model by removing the last fully connected layer and applied it for medical image object segmentation in 1991 and breast cancer detection in mammograms in 1994. LeNet-5 (1998): a 7-level CNN by Yann LeCun et al.: that classifies digits: was applied by several banks to recognize hand-written numbers on checks  digitized in 32x32 pixel images. In the 1980s: backpropagation did not work well for deep learning with long credit assignment paths. To overcome this problem: Jürgen Schmidhuber (1992) proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning. It uses predictive coding  to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN: by distilling a higher level chunker network into a lower level automatizer network. In 1993: a chunker solved a deep learning task whose depth exceeded 1000.In 1992: Jürgen Schmidhuber also published an alternative to RNNs which is now called a linear Transformer or a  Transformer with linearized self-attention (save for a normalization operator). It learns internal spotlights of attention: a slow feedforward neural network learns by gradient descent to control the fast weights of another neural network through outer products of self-generated activation patterns FROM and TO (which are now called key and value for self-attention). This fast weight attention mapping is applied to a query pattern. The modern Transformer was introduced by Ashish Vaswani et al. in their 2017 paper ""Attention Is All You Need"". It combines this with a softmax operator and a projection matrix. Transformers have increasingly become the model of choice for natural language processing. Many modern large language models such as ChatGPT: GPT-4: and BERT use it. Transformers are also increasingly being used in computer vision.In 1991: Jürgen Schmidhuber also published adversarial neural networks that contest with each other in the form of a zero-sum game: where one network's gain is the other network's loss. The first network is a generative model that models a probability distribution over output patterns. "
0.6238374999957159,What is the function of an Elman network in a recurrent neural network?,"An Elman network is a three-layer network with the addition of a set of context units. It can maintain a sort of state, allowing it to perform tasks such as sequence prediction that are beyond the power of a standard multilayer perceptron.",Cosine Similarity: 0.8446668886648547 / They are often implemented as recurrent networks.,"Cosine Similarity: 0.8360461729877251 / A recurrent neural network (RNN) is one of the two broad types of artificial neural network: characterized by direction of the flow of information between its layers.In contrast to the uni-directional feedforward neural network: it is a bi-directional artificial neural network: meaning that it allows the output from some nodes to affect subsequent input to the same nodes. Their ability to use internal state (memory) to process arbitrary sequences of inputs makes them applicable to tasks such as unsegmented: connected handwriting recognition or speech recognition. The term ""recurrent neural network"" is used to refer to the class of networks with an infinite impulse response: whereas ""convolutional neural network"" refers to the class of finite impulse response. Both classes of networks exhibit temporal dynamic behavior. A finite impulse recurrent network is a directed acyclic graph that can be unrolled and replaced with a strictly feedforward neural network: while an infinite impulse recurrent network is a directed cyclic graph that can not be unrolled. Additional stored states and the storage under direct control by the network can be added to both infinite-impulse and finite-impulse networks. Another network or graph can also replace the storage if that incorporates time delays or has feedback loops. Such controlled states are referred to as gated states or gated memory and are part of long short-term memory networks (LSTMs) and gated recurrent units. This is also called Feedforward Neural Network (FNN). Recurrent neural networks are theoretically Turing complete and can run arbitrary programs to process arbitrary sequences of inputs. == History == /  / The Ising model (1925) by Wilhelm Lenz and Ernst Ising /  / was the first RNN architecture that did not learn. Shun'ichi Amari made it adaptive in 1972. This was also called the Hopfield network (1982). See also David Rumelhart's work in 1986. In 1993: a neural history compressor system solved a ""Very Deep Learning"" task that required more than 1000 subsequent layers in an RNN unfolded in time. === LSTM === /  / Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1997 and set accuracy records in multiple applications domains.Around 2007: LSTM started to revolutionize speech recognition: outperforming traditional models in certain speech applications. In 2009: a Connectionist Temporal Classification (CTC)-trained LSTM network was the first RNN to win pattern recognition contests when it won several competitions in connected handwriting recognition. In 2014: the Chinese company Baidu used CTC-trained RNNs to break the 2S09 Switchboard Hub5'00 speech recognition dataset benchmark without using any traditional speech processing methods.LSTM also improved large-vocabulary speech recognition and text-to-speech synthesis and was used in Google Android. In 2015: Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM.LSTM broke records for improved machine translation: Language Modeling and Multilingual Language Processing. LSTM combined with convolutional neural networks (CNNs) improved automatic image captioning. == Architectures == /  /  /  / RNNs come in many variants. === Fully recurrent === /  / Fully recurrent neural networks (FRNN) connect the outputs of all neurons to the inputs of all neurons. This is the most general neural network topology because all other topologies can be represented by setting some connection weights to zero to simulate the lack of connections between those neurons. The illustration to the right may be misleading to many because practical neural network topologies are frequently organized in ""layers"" and the drawing gives that appearance. However: what appears to be layers are: in fact: different steps in time of the same fully recurrent neural network. The left-most item in the illustration shows the recurrent connections as the arc labeled 'v'. It is ""unfolded"" in time to produce the appearance of layers. === Elman networks and Jordan networks === /  / An Elman network is a three-layer network (arranged horizontally as x: y: and z in the illustration) with the addition of a set of context units (u in the illustration). The middle (hidden) layer is connected to these context units fixed with a weight of one. At each time step: the input is fed forward and a learning rule is applied. The fixed back-connections save a copy of the previous values of the hidden units in the context units (since they propagate over the connections before the learning rule is applied). Thus the network can maintain a sort of state: allowing it to perform such tasks as sequence-prediction that are beyond the power of a standard multilayer perceptron. Jordan networks are similar to Elman networks. The context units are fed from the output layer instead of the hidden layer. The context units in a Jordan network are also called the state layer. They have a recurrent connection to themselves.Elman and Jordan networks are also known as ""Simple recurrent networks"" (SRN). Elman network /  / ht=σh(Whxt+Uhht−1+bh)yt=σy(Wyht+by){\displaystyle {\begin{aligned}h_{t}&=\sigma _{h}(W_{h}x_{t}+U_{h}h_{t-1}+b_{h})\\y_{t}&=\sigma _{y}(W_{y}h_{t}+b_{y})\end{aligned}}} /  / Jordan network /  / ht=σh(Whxt+Uhyt−1+bh)yt=σy(Wyht+by){\displaystyle {\begin{aligned}h_{t}&=\sigma _{h}(W_{h}x_{t}+U_{h}y_{t-1}+b_{h})\\y_{t}&=\sigma _{y}(W_{y}h_{t}+b_{y})\end{aligned}}}Variables and functions /  /  /  / xt{\displaystyle x_{t}}: input vector /  / ht{\displaystyle h_{t}}: hidden layer vector /  / yt{\displaystyle y_{t}}: output vector /  / W{\displaystyle W}: U{\displaystyle U} and b{\displaystyle b}: parameter matrices and vector /  / σh{\displaystyle \sigma _{h}} and σy{\displaystyle \sigma _{y}}: Activation functions /  /  /  /  /  / === Hopfield === /  /  /  / The Hopfield network is an RNN in which all connections across layers are equally sized. It requires stationary inputs and is thus not a general RNN: as it does not process sequences of patterns. However: it guarantees that it will converge. If the connections are trained using Hebbian learning: then the Hopfield network can perform as robust content-addressable memory: resistant to connection alteration. ==== Bidirectional associative memory ==== /  /  /  / Introduced by Bart Kosko: a bidirectional associative memory (BAM) network is a variant of a Hopfield network that stores associative data as a vector. The bi-directionality comes from passing information through a matrix and its transpose. Typically: bipolar encoding is preferred to binary encoding of the associative pairs. Recently: stochastic BAM models using Markov stepping were optimized for increased network stability and relevance to real-world applications.A BAM network has two layers: either of which can be driven as an input to recall an association and produce an output on the other layer. === Echo state === /  /  /  / Echo state networks (ESN) have a sparsely connected random hidden layer. "
0.712161700008437,What is a residual neural network?,"A residual neural network, also known as a residual network or ResNet, is a deep learning model in which the weight layers learn residual functions in relation to the layer inputs. It behaves like a highway network, with gates that are opened through strongly positive bias weights. This system allows deep learning models with numerous layers to train more easily and achieve better accuracy. ","Cosine Similarity: 0.8940533777192977 / A residual neural network (also referred to as a residual network or ResNet) is a deep learning model in which the weight layers learn residual functions with reference to the layer inputs.It behaves like a highway network whose gates are opened through strongly positive bias weights. This enables deep learning models with tens or hundreds of layers to train easily and approach better accuracy when going deeper. The identity skip connections: often referred to as ""residual connections"": are also used in the 1997 LSTM networks: Transformer models (e.g.: BERT: GPT models such as ChatGPT): the AlphaGo Zero system: the AlphaStar system: and the AlphaFold system. Residual networks were developed by Kaiming He: Xiangyu Zhang: Shaoqing Ren: and Jian Sun: who won the 2015 ImageNet competition. == Formulation == /  /  /  /  /  / === Background === /  / The AlexNet model developed in 2012 for ImageNet was an eight-layer convolutional neural network. The neural networks developed in 2014 by the Visual Geometry Group (VGG) at the University of Oxford approached a depth of 19 layers by stacking 3-by-3 convolutional layers. However: stacking more layers led to a steep reduction in training accuracy: which is referred to as the ""degradation"" problem.A deeper network should not produce a higher training loss than its shallower counterpart: if this deeper network can be constructed by its shallower counterpart stacked with extra layers. If the extra layers can be set as identity mappings: the deeper network would represent the same function as the shallower counterpart. It is hypothesized that the optimizer is not able to approach identity mappings for the parameterized layers. === Residual learning === /  / In a multi-layer neural network model: consider a subnetwork with a certain number (e.g.: 2 or 3) of stacked layers. Denote the underlying function performed by this subnetwork as H(x){\textstyle H(x)}: where x{\textstyle x} is the input to this subnetwork. The idea of ""Residual Learning"" re-parameterizes this subnetwork and lets the parameter layers represent a residual function F(x):=H(x)−x{\textstyle F(x):=H(x)-x}. The output y{\textstyle y} of this subnetwork is represented as: /  /  /  / y=F(x)+x{\displaystyle {\begin{aligned}y&=F(x)+x\end{aligned}}}This is also the principle of the 1997 LSTM cell computing yt+1=F(xt)+xt{\textstyle y_{t+1}=F(x_{t})+x_{t}}: which becomes y=F(x)+x{\textstyle y=F(x)+x} during backpropagation through time. The function F(x){\textstyle F(x)} is often represented by matrix multiplication interlaced with activation functions and normalization operations (e.g.: Batch Normalization or Layer Normalization). This subnetwork is referred to as a ""Residual Block"". A deep residual network is constructed by stacking a series of residual blocks. The operation of ""+ x{\textstyle +\ x}"" in ""y=F(x)+x{\textstyle y=F(x)+x}"" is approached by a skip connection that performs identity mapping and connects the input of a residual block with its output. This connection is often referred to as a ""Residual Connection"" in later work. === Signal propagation === /  / The introduction of identity mappings facilitates signal propagation in both forward and backward paths. ==== Forward propagation ==== /  / If the output of the ℓ{\textstyle \ell }-th residual block is the input to the (ℓ+1){\textstyle (\ell +1)}-th residual block (i.e.: assuming no activation function between blocks): we have: /  / xℓ+1=F(xℓ)+xℓ{\displaystyle {\begin{aligned}x_{\ell +1}&=F(x_{\ell })+x_{\ell }\end{aligned}}}Applying this formulation recursively: e.g.: xℓ+2=F(xℓ+1)+xℓ+1=F(xℓ+1)+F(xℓ)+xℓ{\displaystyle {\begin{aligned}x_{\ell +2}=F(x_{\ell +1})+x_{\ell +1}=F(x_{\ell +1})+F(x_{\ell })+x_{\ell }\end{aligned}}}: we have: /  /  /  / xL=xℓ+∑i=lL−1F(xi){\displaystyle {\begin{aligned}x_{L}&=x_{\ell }+\sum _{i=l}^{L-1}F(x_{i})\\\end{aligned}}}where L{\textstyle L} is the index of any later residual block (e.g.: the last block) and ℓ{\textstyle \ell } is the index of any earlier block. This formulation suggests that there is always a signal that is directly sent from a shallower block ℓ{\textstyle \ell } to a deeper block L{\textstyle L}. ==== Backward propagation ==== /  / The Residual Learning formulation provides the added benefit of addressing the vanishing gradient problem to some extent. However: it is crucial to acknowledge that the vanishing gradient issue is not the root cause of the degradation problem: as it has already been tackled through the use of normalization layers. Taking the derivative w.r.t. xℓ{\textstyle x_{\ell }} according to the above forward propagation: we have: /  / ∂E∂xℓ=∂E∂xL∂xL∂xℓ=∂E∂xL(1+∂∂xℓ∑i=lL−1F(xi))=∂E∂xL+∂E∂xL∂∂xℓ∑i=lL−1F(xi){\displaystyle {\begin{aligned}{\frac {\partial {\mathcal {E}}}{\partial x_{\ell }}}&={\frac {\partial {\mathcal {E}}}{\partial x_{L}}}{\frac {\partial x_{L}}{\partial x_{\ell }}}\\&={\frac {\partial {\mathcal {E}}}{\partial x_{L}}}\left(1+{\frac {\partial }{\partial x_{\ell }}}\sum _{i=l}^{L-1}F(x_{i})\right)\\&={\frac {\partial {\mathcal {E}}}{\partial x_{L}}}+{\frac {\partial {\mathcal {E}}}{\partial x_{L}}}{\frac {\partial }{\partial x_{\ell }}}\sum _{i=l}^{L-1}F(x_{i})\\\end{aligned}}}Here E{\textstyle {\mathcal {E}}} is the loss function to be minimized. This formulation suggests that the gradient computation of a shallower layer  /  / ∂E∂xℓ{\textstyle {\frac {\partial {\mathcal {E}}}{\partial x_{\ell }}}} /  / always has a term ∂E∂xL{\textstyle {\frac {\partial {\mathcal {E}}}{\partial x_{L}}}} that is directly added. Even if the gradients of the F(xi){\textstyle F(x_{i})} terms are small: the total gradient ∂E∂xℓ{\textstyle {\frac {\partial {\mathcal {E}}}{\partial x_{\ell }}}} is not vanishing thanks to the added term ∂E∂xL{\textstyle {\frac {\partial {\mathcal {E}}}{\partial x_{L}}}}. == Variants of residual blocks == /  /  /  /  /  / === Basic block === /  / A Basic Block is the simplest building block studied in the original ResNet. This block consists of two sequential 3x3 convolutional layers and a residual connection. The input and output dimensions of both layers are equal. === Bottleneck block === /  / A Bottleneck Block consists of three sequential convolutional layers and a residual connection. The first layer in this block is a 1x1 convolution for dimension reduction: e.g.: to 1/4 of the input dimension; the second layer performs a 3x3 convolution; the last layer is another 1x1 convolution for dimension restoration. The models of ResNet-50: ResNet-101: and ResNet-152 in  are all based on Bottleneck Blocks. === Pre-activation block === /  / The Pre-activation Residual Block applies the activation functions (e.g.: non-linearity and normalization) before applying the residual function F{\textstyle F}. Formally: the computation of a Pre-activation Residual Block can be written as: /  /  /  / xℓ+1=F(ϕ(xℓ))+xℓ{\displaystyle {\begin{aligned}x_{\ell +1}&=F(\phi (x_{\ell }))+x_{\ell }\end{aligned}}}where ϕ{\textstyle \phi } can be any non-linearity activation (e.g.: ReLU) or normalization (e.g.: LayerNorm) operation. This design reduces the number of non-identity mappings between Residual Blocks. This design was used to train models with 200 to over 1000 layers.Since GPT-2: the Transformer Blocks have been dominantly implemented as Pre-activation Blocks. This is often referred to as ""pre-normalization"" in the literature of Transformer models. === Transformer block === /  / A Transformer Block is a stack of two Residual Blocks. Each Residual Block has a Residual Connection. The first Residual Block is a Multi-Head Attention Block: which performs (self-)attention computation followed by a linear projection. ",Cosine Similarity: 0.8569417145916696 / By defining the residual f(t:x){\displaystyle f(t:x)} as /  / f:=ut+N[u]=0{\displaystyle f:=u_{t}+N[u]=0}: /  / and approximating u(t:x){\displaystyle u(t:x)} by a deep neural network.
1.1107231999922078,Who are the people behind the development of residual networks?,"Residual networks were developed by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. They were the winners of the 2015 ImageNet competition.",Cosine Similarity: 0.8261592902289592 / 7 months later: Kaiming He: Xiangyu Zhang;  Shaoqing Ren: and Jian Sun won the ImageNet 2015 competition with an open-gated or gateless Highway network variant called Residual neural network.,Cosine Similarity: 0.818483514051068 / He proposed recurrent residual connections to solve this problem.
0.6148075000091922,What is the degradation problem in neural networks?,"The degradation problem refers to the steep reduction in training accuracy that occurs when more layers are stacked in a neural network. Despite the assumption that a deeper network should not produce a higher training loss than its shallower counterpart, the addition of extra layers often contributes to this issue.",Cosine Similarity: 0.8440572281782217 / In some cases: large numbers of neurons in a network can become stuck in dead states: effectively decreasing the model capacity.,Cosine Similarity: 0.8391798784167959 / Some researchers perceive the root problem to be a weak discriminative network that fails to notice the pattern of omission: while others assign blame to a bad choice of objective function.
0.6180212000035681,Can you explain 'Residual Learning' in a neural network?,"In a residual learning, a residual function is represented by the parameter layers in a multi-layer neural network, re-parameterizing the subnetwork. The residual function is often denoted F(x):=H(x)-x, where H(x) is the underlying function performed by the network and x is the input to the subnetwork. The output y of this network is given by y=F(x)+x. This concept is also applied in the LSTM cell computing.","Cosine Similarity: 0.8804828155822587 / A residual neural network (also referred to as a residual network or ResNet) is a deep learning model in which the weight layers learn residual functions with reference to the layer inputs.It behaves like a highway network whose gates are opened through strongly positive bias weights. This enables deep learning models with tens or hundreds of layers to train easily and approach better accuracy when going deeper. The identity skip connections: often referred to as ""residual connections"": are also used in the 1997 LSTM networks: Transformer models (e.g.: BERT: GPT models such as ChatGPT): the AlphaGo Zero system: the AlphaStar system: and the AlphaFold system. Residual networks were developed by Kaiming He: Xiangyu Zhang: Shaoqing Ren: and Jian Sun: who won the 2015 ImageNet competition. == Formulation == /  /  /  /  /  / === Background === /  / The AlexNet model developed in 2012 for ImageNet was an eight-layer convolutional neural network. The neural networks developed in 2014 by the Visual Geometry Group (VGG) at the University of Oxford approached a depth of 19 layers by stacking 3-by-3 convolutional layers. However: stacking more layers led to a steep reduction in training accuracy: which is referred to as the ""degradation"" problem.A deeper network should not produce a higher training loss than its shallower counterpart: if this deeper network can be constructed by its shallower counterpart stacked with extra layers. If the extra layers can be set as identity mappings: the deeper network would represent the same function as the shallower counterpart. It is hypothesized that the optimizer is not able to approach identity mappings for the parameterized layers. === Residual learning === /  / In a multi-layer neural network model: consider a subnetwork with a certain number (e.g.: 2 or 3) of stacked layers. Denote the underlying function performed by this subnetwork as H(x){\textstyle H(x)}: where x{\textstyle x} is the input to this subnetwork. The idea of ""Residual Learning"" re-parameterizes this subnetwork and lets the parameter layers represent a residual function F(x):=H(x)−x{\textstyle F(x):=H(x)-x}. The output y{\textstyle y} of this subnetwork is represented as: /  /  /  / y=F(x)+x{\displaystyle {\begin{aligned}y&=F(x)+x\end{aligned}}}This is also the principle of the 1997 LSTM cell computing yt+1=F(xt)+xt{\textstyle y_{t+1}=F(x_{t})+x_{t}}: which becomes y=F(x)+x{\textstyle y=F(x)+x} during backpropagation through time. The function F(x){\textstyle F(x)} is often represented by matrix multiplication interlaced with activation functions and normalization operations (e.g.: Batch Normalization or Layer Normalization). This subnetwork is referred to as a ""Residual Block"". A deep residual network is constructed by stacking a series of residual blocks. The operation of ""+ x{\textstyle +\ x}"" in ""y=F(x)+x{\textstyle y=F(x)+x}"" is approached by a skip connection that performs identity mapping and connects the input of a residual block with its output. This connection is often referred to as a ""Residual Connection"" in later work. === Signal propagation === /  / The introduction of identity mappings facilitates signal propagation in both forward and backward paths. ==== Forward propagation ==== /  / If the output of the ℓ{\textstyle \ell }-th residual block is the input to the (ℓ+1){\textstyle (\ell +1)}-th residual block (i.e.: assuming no activation function between blocks): we have: /  / xℓ+1=F(xℓ)+xℓ{\displaystyle {\begin{aligned}x_{\ell +1}&=F(x_{\ell })+x_{\ell }\end{aligned}}}Applying this formulation recursively: e.g.: xℓ+2=F(xℓ+1)+xℓ+1=F(xℓ+1)+F(xℓ)+xℓ{\displaystyle {\begin{aligned}x_{\ell +2}=F(x_{\ell +1})+x_{\ell +1}=F(x_{\ell +1})+F(x_{\ell })+x_{\ell }\end{aligned}}}: we have: /  /  /  / xL=xℓ+∑i=lL−1F(xi){\displaystyle {\begin{aligned}x_{L}&=x_{\ell }+\sum _{i=l}^{L-1}F(x_{i})\\\end{aligned}}}where L{\textstyle L} is the index of any later residual block (e.g.: the last block) and ℓ{\textstyle \ell } is the index of any earlier block. This formulation suggests that there is always a signal that is directly sent from a shallower block ℓ{\textstyle \ell } to a deeper block L{\textstyle L}. ==== Backward propagation ==== /  / The Residual Learning formulation provides the added benefit of addressing the vanishing gradient problem to some extent. However: it is crucial to acknowledge that the vanishing gradient issue is not the root cause of the degradation problem: as it has already been tackled through the use of normalization layers. Taking the derivative w.r.t. xℓ{\textstyle x_{\ell }} according to the above forward propagation: we have: /  / ∂E∂xℓ=∂E∂xL∂xL∂xℓ=∂E∂xL(1+∂∂xℓ∑i=lL−1F(xi))=∂E∂xL+∂E∂xL∂∂xℓ∑i=lL−1F(xi){\displaystyle {\begin{aligned}{\frac {\partial {\mathcal {E}}}{\partial x_{\ell }}}&={\frac {\partial {\mathcal {E}}}{\partial x_{L}}}{\frac {\partial x_{L}}{\partial x_{\ell }}}\\&={\frac {\partial {\mathcal {E}}}{\partial x_{L}}}\left(1+{\frac {\partial }{\partial x_{\ell }}}\sum _{i=l}^{L-1}F(x_{i})\right)\\&={\frac {\partial {\mathcal {E}}}{\partial x_{L}}}+{\frac {\partial {\mathcal {E}}}{\partial x_{L}}}{\frac {\partial }{\partial x_{\ell }}}\sum _{i=l}^{L-1}F(x_{i})\\\end{aligned}}}Here E{\textstyle {\mathcal {E}}} is the loss function to be minimized. This formulation suggests that the gradient computation of a shallower layer  /  / ∂E∂xℓ{\textstyle {\frac {\partial {\mathcal {E}}}{\partial x_{\ell }}}} /  / always has a term ∂E∂xL{\textstyle {\frac {\partial {\mathcal {E}}}{\partial x_{L}}}} that is directly added. Even if the gradients of the F(xi){\textstyle F(x_{i})} terms are small: the total gradient ∂E∂xℓ{\textstyle {\frac {\partial {\mathcal {E}}}{\partial x_{\ell }}}} is not vanishing thanks to the added term ∂E∂xL{\textstyle {\frac {\partial {\mathcal {E}}}{\partial x_{L}}}}. == Variants of residual blocks == /  /  /  /  /  / === Basic block === /  / A Basic Block is the simplest building block studied in the original ResNet. This block consists of two sequential 3x3 convolutional layers and a residual connection. The input and output dimensions of both layers are equal. === Bottleneck block === /  / A Bottleneck Block consists of three sequential convolutional layers and a residual connection. The first layer in this block is a 1x1 convolution for dimension reduction: e.g.: to 1/4 of the input dimension; the second layer performs a 3x3 convolution; the last layer is another 1x1 convolution for dimension restoration. The models of ResNet-50: ResNet-101: and ResNet-152 in  are all based on Bottleneck Blocks. === Pre-activation block === /  / The Pre-activation Residual Block applies the activation functions (e.g.: non-linearity and normalization) before applying the residual function F{\textstyle F}. Formally: the computation of a Pre-activation Residual Block can be written as: /  /  /  / xℓ+1=F(ϕ(xℓ))+xℓ{\displaystyle {\begin{aligned}x_{\ell +1}&=F(\phi (x_{\ell }))+x_{\ell }\end{aligned}}}where ϕ{\textstyle \phi } can be any non-linearity activation (e.g.: ReLU) or normalization (e.g.: LayerNorm) operation. This design reduces the number of non-identity mappings between Residual Blocks. This design was used to train models with 200 to over 1000 layers.Since GPT-2: the Transformer Blocks have been dominantly implemented as Pre-activation Blocks. This is often referred to as ""pre-normalization"" in the literature of Transformer models. === Transformer block === /  / A Transformer Block is a stack of two Residual Blocks. Each Residual Block has a Residual Connection. The first Residual Block is a Multi-Head Attention Block: which performs (self-)attention computation followed by a linear projection. ","Cosine Similarity: 0.846965412185867 / The second network learns by gradient descent to predict the reactions of the environment to these patterns.This was called ""artificial curiosity"". In 2014: this principle was used in a generative adversarial network (GAN) by Ian Goodfellow et al. Here the environmental reaction is 1 or 0 depending on whether the first network's output is in a given set. This can be used to create realistic deepfakes. Excellent image quality is achieved by Nvidia's StyleGAN (2018) based on the Progressive GAN by Tero Karras et al. Here the GAN generator is grown from small to large scale in a pyramidal fashion. Sepp Hochreiter's diploma thesis (1991) was called ""one of the most important documents in the history of machine learning"" by his supervisor Schmidhuber. It not only tested the neural history compressor: but also identified and analyzed the vanishing gradient problem. Hochreiter proposed recurrent residual connections to solve this problem. This led to the deep learning method called long short-term memory (LSTM): published in 1997. LSTM recurrent neural networks can learn ""very deep learning"" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. The ""vanilla LSTM"" with forget gate was introduced in 1999 by Felix Gers: Schmidhuber and Fred Cummins. LSTM has become the  most cited neural network of the 20th century. In 2015: Rupesh Kumar Srivastava: Klaus Greff: and Schmidhuber used LSTM principles to create the Highway network: a feedforward neural network with hundreds of layers: much deeper than previous networks. 7 months later: Kaiming He: Xiangyu Zhang;  Shaoqing Ren: and Jian Sun won the ImageNet 2015 competition with an open-gated or gateless Highway network variant called Residual neural network. This has become the most cited neural network of the 21st century.In 1994: André de Carvalho: together with Mike Fairhurst and David Bisset: published experimental results of a multi-layer boolean neural network: also known as a weightless neural network: composed of a 3-layers self-organising feature extraction neural network module (SOFT) followed by a multi-layer classification neural network module (GSN): which were independently trained. Each layer in the feature extraction module extracted features with growing complexity regarding the previous layer.In 1995: Brendan Frey demonstrated that it was possible to train (over two days) a network containing six fully connected layers and several hundred hidden units using the wake-sleep algorithm: co-developed with Peter Dayan and Hinton.Since 1997: Sven Behnke extended the feed-forward hierarchical convolutional approach in the Neural Abstraction Pyramid by lateral and backward connections in order to flexibly incorporate context into decisions and iteratively resolve local ambiguities. Simpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) were a popular choice in the 1990s and 2000s: because of artificial neural networks' computational cost and a lack of understanding of how the brain wires its biological networks. Both shallow and deep learning (e.g.: recurrent nets) of ANNs for speech recognition have been explored for many years. These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively. Key difficulties have been analyzed: including gradient diminishing and weak temporal correlation structure in neural predictive models. Additional difficulties were the lack of training data and limited computing power. Most speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government's NSA and DARPA: SRI studied deep neural networks (DNNs) in speech and speaker recognition. The speaker recognition team led by Larry Heck reported significant success with deep neural networks in speech processing in the 1998 National Institute of Standards and Technology Speaker Recognition evaluation. The SRI deep neural network was then deployed in the Nuance Verifier: representing the first major industrial application of deep learning. The principle of elevating ""raw"" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the ""raw"" spectrogram or linear filter-bank features in the late 1990s: showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech: waveforms: later produced excellent larger-scale results.Speech recognition was taken over by LSTM. In 2003: LSTM started to become competitive with traditional speech recognizers on certain tasks. In 2006: Alex Graves: Santiago Fernández: Faustino Gomez: and Schmidhuber combined it with connectionist temporal classification (CTC) in stacks of LSTM RNNs. In 2015: Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM: which they made available through Google Voice Search.The impact of deep learning in industry began in the early 2000s: when CNNs already processed an estimated 10% to 20% of all the checks written in the US: according to Yann LeCun. Industrial applications of deep learning to large-scale speech recognition started around 2010. In 2006: publications by Geoff Hinton: Ruslan Salakhutdinov: Osindero and Teh showed how a many-layered feedforward neural network could be effectively pre-trained one layer at a time: treating each layer in turn as an unsupervised restricted Boltzmann machine: then fine-tuning it using supervised backpropagation. The papers referred to learning for deep belief nets. The 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech: and the possibility that given more capable hardware and large-scale data sets that deep neural nets might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets. However: it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large: context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems. The nature of the recognition errors produced by the two types of systems was characteristically different: offering technical insights into how to integrate deep learning into the existing highly efficient: run-time speech decoding system deployed by all major speech recognition systems. "
0.6485504000156652,What is a Transformer Block?,"A Transformer Block is a stack of two Residual Blocks, each with a Residual Connection. It consists first of a Multi-Head Attention Block, which performs (self-)attention computation followed by a linear projection. The second block is a feed-forward Multi-Layer Perceptron (MLP) Block, which increases and then reduces the dimension through linear projections. The GPT-3 model, for instance, has 96 Transformer Blocks, amounting to a depth of about 400 projection layers. Very deep Transformer models cannot be successfully trained without Residual Connections.",Cosine Similarity: 0.834020340611883 / These encoders are the Transformer encoders.,Cosine Similarity: 0.8099119067547103 / Though the original transformer has both encoder and decoder blocks: BERT is an encoder-only model.
0.6249296999885701,What is a Siamese neural network?,"A Siamese neural network, also known as a twin neural network, is an artificial neural network that uses the same weights while operating on two different input vectors to compute comparable output vectors. It is often used for comparing similar instances in different type sets and in applications such as face recognition and matching queries with indexed documents.",Cosine Similarity: 0.9288532746876514 / A Siamese neural network (sometimes called a twin neural network) is an artificial neural network that uses the same weights while working in tandem on two different input vectors to compute comparable output vectors.,"Cosine Similarity: 0.8534266564591101 / == See also == /  / Artificial neural network /  / Triplet loss /  /  /  /  /  / == Further reading == /  / Chicco: Davide (2020): ""Siamese neural networks: an overview"": Artificial Neural Networks: Methods in Molecular Biology: vol."
0.6462389999942388,How is learning conducted in twin networks?,"Learning in twin networks can be achieved through two methods: triplet loss or contrastive loss. For learning by triplet loss, a baseline vector, positive vector, and negative vector are used where the negative vector pushes learning in the network and the positive vector acts as a regularizer. For learning by contrastive loss, there must be a weight decay to regularize the weights.",Cosine Similarity: 0.8817015476153833 / == Learning == /  / Learning in twin networks can be done with triplet loss or contrastive loss.,Cosine Similarity: 0.8614900889897564 / The twin network might be the same: but the implementation can be quite different.
0.5970295000006445,What is a common goal during learning and what is a frequently used metric?,The common goal during learning is to minimize a distance metric for similar objects and to maximize it for distinct ones. The most common distance metric used is the Euclidean distance. ,Cosine Similarity: 0.8652766722662109 / === Predefined metrics: Euclidean distance metric === /  / The common learning goal is to minimize a distance metric for similar objects and maximize for distinct ones.,"Cosine Similarity: 0.801419498661771 / One broad category of evaluation dataset is question answering datasets: consisting of pairs of questions and correct answers: for example: (""Have the San Jose Sharks won the Stanley Cup?"
0.6226248999882955,How is a 'half-twin' network different from a twin network?,"While similar to a twin network, a 'half-twin' network implements slightly different functions. The delta function (distance calculation) between the two functions implemented by 'half-twin' network varies based on whether the indexes of the two vectors are the same or different.",Cosine Similarity: 0.8936288897731698 / The twin network might be the same: but the implementation can be quite different.,Cosine Similarity: 0.8436604687932368 / === Learned metrics: half-twin networks === /  / This form also allows the twin network to be more of a half-twin: implementing a slightly different functions /  /  /  / ifi=jthenδ⁡[f⁡(x(i)):g⁡(x(j))]is smallotherwiseδ⁡[f⁡(x(i)):g⁡(x(j))]is large{\displaystyle {\begin{aligned}{\text{if}}\:i=j\:{\text{then}}&\:\operatorname {\delta } \left[\operatorname {f} \left(x^{(i)}\right):\:\operatorname {g} \left(x^{(j)}\right)\right]\:{\text{is small}}\\{\text{otherwise}}&\:\operatorname {\delta } \left[\operatorname {f} \left(x^{(i)}\right):\:\operatorname {g} \left(x^{(j)}\right)\right]\:{\text{is large}}\end{aligned}}} /  / i:j{\displaystyle i:j} are indexes into a set of vectors /  / f⁡(⋅):g⁡(⋅){\displaystyle \operatorname {f} (\cdot ):\operatorname {g} (\cdot )}function implemented by the half-twin network /  / δ⁡(⋅){\displaystyle \operatorname {\delta } (\cdot )}function implemented by the network joining outputs from the twin network /  /  /  /  /  / == Twin networks for object tracking == /  / Twin networks have been used in object tracking because of its unique two tandem inputs and similarity measurement.
0.6159084000100847,How are twin networks used in object tracking?,"Twin networks have been used in object tracking due to their unique two tandem inputs and similarity measurement capabilities. One input of the twin network is a user pre-selected exemplar image, and the other is a larger search image. The twin network's job is to locate the exemplar image within the search image by measuring the similarity between the exemplar and each part of the search image, thereby producing a map of similarity scores.",Cosine Similarity: 0.8816733309668741 / In object tracking: one input of the twin network is user pre-selected exemplar image: the other input is a larger search image: which twin network's job is to locate exemplar inside of search image.,Cosine Similarity: 0.8769626662520936 / The perhaps most well-known application of twin networks are face recognition: where known images of people are precomputed and compared to an image from a turnstile or similar.
0.5994080000091344,What are Spiking neural networks (SNNs)?,Spiking neural networks (SNNs) are artificial neural networks that closely mimic natural neural networks. They incorporate the concept of time into their operating model and transmit information only when a neuron's membrane potential reaches a specific threshold value. This model represents a significant departure from typical multi-layer perceptron networks that transmit information at each propagation cycle.,Cosine Similarity: 0.9372498682420481 / Spiking neural networks (SNNs) are artificial neural networks (ANN) that more closely mimic natural neural networks.,Cosine Similarity: 0.8912305541511015 / === Spiking === /  / Spiking neural networks (SNN) explicitly consider the timing of inputs.
0.5860667999950238,What is the leaky integrate-and-fire model in SNNs?,"The leaky integrate-and-fire model is the most prominent spiking neuron model. In this model, the neuron's state is considered to be its momentary activation level. Incoming spikes push this value higher or lower until the state either decays or, if the firing threshold is reached, the neuron fires. After firing, the state variable is reset to a lower value.",Cosine Similarity: 0.8969264581723909 / The leaky integrate-and-fire model (or a derivative) is commonly used as it is easier to compute than the Hodgkin–Huxley model.,Cosine Similarity: 0.8421367900375856 / In addition to neuronal and synaptic state: SNNs incorporate the concept of time into their operating model.
0.5928064000036102,What are some of the challenges in using SNNs?,"Some challenges in using SNNs include the non-differentiability of the spiking nonlinearity and the implementation of the optimization algorithm. The all-or-nothing behavior of the binary spiking nonlinearity makes neurons unsuitable for gradient-based optimization, and standard Backpropagation can be computationally expensive. ",Cosine Similarity: 0.9440533632433938 / Currently there are a few challenges when using SNNs that researchers are actively working on.,Cosine Similarity: 0.8860114358414491 / However: there is a lack of effective training mechanisms for SNNs: which can be inhibitory for some applications: including computer vision tasks.
0.656229300017003,What are some applications of SNNs?,"SNNs can apply to the same applications as traditional artificial neural networks (ANNs), and in addition, they can model the central nervous system of biological organisms. However, a lack of effective training mechanisms for SNNs can be inhibitory for some applications, like computer vision tasks. ",Cosine Similarity: 0.9048478867445852 / == Applications == /  / SNNs can in principle apply to the same applications as traditional ANNs.,Cosine Similarity: 0.8938076362980496 / Examples of SNNs are the OSFA spatial neural networks: SVANNs and GWNNs.
0.609534800023539,What developments have improved the efficiency and computational power of SNNs?,"Incorporating additional neuron dynamics like Spike Frequency Adaptation (SFA) into neuron models is one such development. SFA offers computational benefits by reducing power usage and increasing coding efficiency, especially in repetitive or intense stimuli. This adaptation also enhances signal clarity and introduces short-term memory at the neuron level, refining information processing accuracy and efficiency.",Cosine Similarity: 0.9035536066147098 / This efficiency not only streamlines the computational workflow but also conserves space and energy: offering a pragmatic step forward in the practical application of SNNs for complex computing tasks: all while maintaining a commitment to technical integrity.,Cosine Similarity: 0.8799465076822579 / Currently there are a few challenges when using SNNs that researchers are actively working on.
0.6644804000097793,What is the Unified Modeling Language (UML)? ,"The Unified Modeling Language (UML) is a general-purpose visual modeling language that provides a standard way to visualize the design of a system. It offers a standard notation for many types of diagrams which can be roughly categorized into behavior diagrams, interaction diagrams, and structure diagrams.",Cosine Similarity: 0.93423949071276 / Unified Modeling Language (UML) is a general-purpose modeling language that is an industry standard for specifying software-intensive systems.,"Cosine Similarity: 0.9183854235088511 / The unified modeling language (UML) is a general-purpose visual modeling language that is intended to provide a standard way to visualize the design of a system.UML provides a standard notation for many types of diagrams which can be roughly divided into three main groups: behavior diagrams: interaction diagrams: and structure diagrams.The creation of UML was originally motivated by the desire to standardize the disparate notational systems and approaches to software design. It was developed at Rational Software in 1994–1995: with further development led by them through 1996.In 1997: UML was adopted as a standard by the Object Management Group (OMG): and has been managed by this organization ever since. In 2005: UML was also published by the International Organization for Standardization (ISO) and the International Electrotechnical Commission (IEC) as the ISO/IEC 19501 standard. Since then the standard has been periodically revised to cover the latest revision of UML.In software engineering: most practitioners do not use UML: but instead produce informal hand drawn diagrams; these diagrams: however: often include elements from UML. : 536  /  /  /  /  /  / == History == /  /  /  /  /  / === Before UML 1.0 === /  / UML has been evolved since the second half of the 1990s and has its roots in the object-oriented programming methods developed in the late 1980s and early 1990s. The timeline (see image) shows the highlights of the history of object-oriented modeling methods and notation. It is originally based on the notations of the Booch method: the object-modeling technique (OMT) and object-oriented software engineering (OOSE): which it has integrated into a single language.Rational Software Corporation hired James Rumbaugh from General Electric in 1994 and after that the company became the source for two of the most popular object-oriented modeling approaches of the day: Rumbaugh's object-modeling technique (OMT) and Grady Booch's method. They were soon assisted in their efforts by Ivar Jacobson: the creator of the object-oriented software engineering (OOSE) method: who joined them at Rational in 1995. === UML 1.x === /  / Under the technical leadership of those three (Rumbaugh: Jacobson and Booch): a consortium called the UML Partners was organized in 1996 to complete the Unified Modeling Language (UML) specification: and propose it to the Object Management Group (OMG) for standardization. The partnership also contained additional interested parties (for example HP: DEC: IBM and Microsoft). The UML Partners' UML 1.0 draft was proposed to the OMG in January 1997 by the consortium. During the same month the UML Partners formed a group: designed to define the exact meaning of language constructs: chaired by Cris Kobryn and administered by Ed Eykholt: to finalize the specification and integrate it with other standardization efforts. The result of this work: UML 1.1: was submitted to the OMG in August 1997 and adopted by the OMG in November 1997.After the first release a task force was formed to improve the language: which released several minor revisions: 1.3: 1.4: and 1.5.The standards it produced (as well as the original standard) have been noted as being ambiguous and inconsistent. ==== Cardinality notation ==== /  / As with database Chen: Bachman: and ISO ER diagrams: class models are specified to use ""look-across"" cardinalities: even though several authors (Merise: Elmasri & Navathe amongst others) prefer same-side or ""look-here"" for roles and both minimum and maximum cardinalities. Recent researchers (Feinerer: Dullea et al.) have shown that the ""look-across"" technique used by UML and ER diagrams is less effective and less coherent when applied to n-ary relationships of order strictly greater than 2. Feinerer says: ""Problems arise if we operate under the look-across semantics as used for UML associations. Hartmann investigates this situation and shows how and why different transformations fail. "": and: ""As we will see on the next few pages: the look-across interpretation introduces several difficulties which prevent the extension of simple mechanisms from binary to n-ary associations."" === UML 2 === /  / UML 2.0 major revision replaced version 1.5 in 2005: which was developed with an enlarged consortium to improve the language further to reflect new experience on usage of its features.Although UML 2.1 was never released as a formal specification: versions 2.1.1 and 2.1.2 appeared in 2007: followed by UML 2.2 in February 2009. UML 2.3 was formally released in May 2010. UML 2.4.1 was formally released in August 2011. UML 2.5 was released in October 2012 as an ""In progress"" version and was officially released in June 2015. Formal version 2.5.1 was adopted in December 2017.There are four parts to the UML 2.x specification: /  /  /  / The Superstructure that defines the notation and semantics for diagrams and their model elements /  / The Infrastructure that defines the core metamodel on which the Superstructure is based /  / The Object Constraint Language (OCL) for defining rules for model elements /  / The UML Diagram Interchange that defines how UML 2 diagram layouts are exchangedUntil UML 2.4.1: the latest versions of these standards were: /  / UML Superstructure version 2.4.1 /  / UML Infrastructure version 2.4.1 /  / OCL version 2.3.1 /  / UML Diagram Interchange version 1.0.Since version 2.5: the UML Specification has been simplified (without Superstructure and Infrastructure): and the latest versions of these standards are now: /  / UML Specification 2.5.1 /  / OCL version 2.4It continues to be updated and improved by the revision task force: who resolve any issues with the language. == Design == /  / UML offers a way to visualize a system's architectural blueprints in a diagram: including elements such as: /  / any activities (jobs); /  / individual components of the system; /  / and how they can interact with other software components; /  / how the system will run; /  / how entities interact with others (components and interfaces); /  / external user interface.Although originally intended for object-oriented design documentation: UML has been extended to a larger set of design documentation (as listed above): and been found useful in many contexts. === Software development methods === /  / UML is not a development method by itself; however: it was designed to be compatible with the leading object-oriented software development methods of its time: for example OMT: Booch method: Objectory and especially RUP that it was originally intended to be used with when work began at Rational Software. === Modeling === /  / It is important to distinguish between the UML model and the set of diagrams of a system. "
0.5807368000096176,What was the motivation behind the creation of UML? ,The creation of UML was primarily motivated by the desire to standardize the disparate notational systems and approaches to software design. ,"Cosine Similarity: 0.8720770000789703 / The unified modeling language (UML) is a general-purpose visual modeling language that is intended to provide a standard way to visualize the design of a system.UML provides a standard notation for many types of diagrams which can be roughly divided into three main groups: behavior diagrams: interaction diagrams: and structure diagrams.The creation of UML was originally motivated by the desire to standardize the disparate notational systems and approaches to software design. It was developed at Rational Software in 1994–1995: with further development led by them through 1996.In 1997: UML was adopted as a standard by the Object Management Group (OMG): and has been managed by this organization ever since. In 2005: UML was also published by the International Organization for Standardization (ISO) and the International Electrotechnical Commission (IEC) as the ISO/IEC 19501 standard. Since then the standard has been periodically revised to cover the latest revision of UML.In software engineering: most practitioners do not use UML: but instead produce informal hand drawn diagrams; these diagrams: however: often include elements from UML. : 536  /  /  /  /  /  / == History == /  /  /  /  /  / === Before UML 1.0 === /  / UML has been evolved since the second half of the 1990s and has its roots in the object-oriented programming methods developed in the late 1980s and early 1990s. The timeline (see image) shows the highlights of the history of object-oriented modeling methods and notation. It is originally based on the notations of the Booch method: the object-modeling technique (OMT) and object-oriented software engineering (OOSE): which it has integrated into a single language.Rational Software Corporation hired James Rumbaugh from General Electric in 1994 and after that the company became the source for two of the most popular object-oriented modeling approaches of the day: Rumbaugh's object-modeling technique (OMT) and Grady Booch's method. They were soon assisted in their efforts by Ivar Jacobson: the creator of the object-oriented software engineering (OOSE) method: who joined them at Rational in 1995. === UML 1.x === /  / Under the technical leadership of those three (Rumbaugh: Jacobson and Booch): a consortium called the UML Partners was organized in 1996 to complete the Unified Modeling Language (UML) specification: and propose it to the Object Management Group (OMG) for standardization. The partnership also contained additional interested parties (for example HP: DEC: IBM and Microsoft). The UML Partners' UML 1.0 draft was proposed to the OMG in January 1997 by the consortium. During the same month the UML Partners formed a group: designed to define the exact meaning of language constructs: chaired by Cris Kobryn and administered by Ed Eykholt: to finalize the specification and integrate it with other standardization efforts. The result of this work: UML 1.1: was submitted to the OMG in August 1997 and adopted by the OMG in November 1997.After the first release a task force was formed to improve the language: which released several minor revisions: 1.3: 1.4: and 1.5.The standards it produced (as well as the original standard) have been noted as being ambiguous and inconsistent. ==== Cardinality notation ==== /  / As with database Chen: Bachman: and ISO ER diagrams: class models are specified to use ""look-across"" cardinalities: even though several authors (Merise: Elmasri & Navathe amongst others) prefer same-side or ""look-here"" for roles and both minimum and maximum cardinalities. Recent researchers (Feinerer: Dullea et al.) have shown that the ""look-across"" technique used by UML and ER diagrams is less effective and less coherent when applied to n-ary relationships of order strictly greater than 2. Feinerer says: ""Problems arise if we operate under the look-across semantics as used for UML associations. Hartmann investigates this situation and shows how and why different transformations fail. "": and: ""As we will see on the next few pages: the look-across interpretation introduces several difficulties which prevent the extension of simple mechanisms from binary to n-ary associations."" === UML 2 === /  / UML 2.0 major revision replaced version 1.5 in 2005: which was developed with an enlarged consortium to improve the language further to reflect new experience on usage of its features.Although UML 2.1 was never released as a formal specification: versions 2.1.1 and 2.1.2 appeared in 2007: followed by UML 2.2 in February 2009. UML 2.3 was formally released in May 2010. UML 2.4.1 was formally released in August 2011. UML 2.5 was released in October 2012 as an ""In progress"" version and was officially released in June 2015. Formal version 2.5.1 was adopted in December 2017.There are four parts to the UML 2.x specification: /  /  /  / The Superstructure that defines the notation and semantics for diagrams and their model elements /  / The Infrastructure that defines the core metamodel on which the Superstructure is based /  / The Object Constraint Language (OCL) for defining rules for model elements /  / The UML Diagram Interchange that defines how UML 2 diagram layouts are exchangedUntil UML 2.4.1: the latest versions of these standards were: /  / UML Superstructure version 2.4.1 /  / UML Infrastructure version 2.4.1 /  / OCL version 2.3.1 /  / UML Diagram Interchange version 1.0.Since version 2.5: the UML Specification has been simplified (without Superstructure and Infrastructure): and the latest versions of these standards are now: /  / UML Specification 2.5.1 /  / OCL version 2.4It continues to be updated and improved by the revision task force: who resolve any issues with the language. == Design == /  / UML offers a way to visualize a system's architectural blueprints in a diagram: including elements such as: /  / any activities (jobs); /  / individual components of the system; /  / and how they can interact with other software components; /  / how the system will run; /  / how entities interact with others (components and interfaces); /  / external user interface.Although originally intended for object-oriented design documentation: UML has been extended to a larger set of design documentation (as listed above): and been found useful in many contexts. === Software development methods === /  / UML is not a development method by itself; however: it was designed to be compatible with the leading object-oriented software development methods of its time: for example OMT: Booch method: Objectory and especially RUP that it was originally intended to be used with when work began at Rational Software. === Modeling === /  / It is important to distinguish between the UML model and the set of diagrams of a system. ",Cosine Similarity: 0.8590857062652056 / Unified Modeling Language (UML) is a general-purpose modeling language that is an industry standard for specifying software-intensive systems.
0.6341372999886516,When and by whom was UML developed? ,"UML was developed at Rational Software in 1994–1995, with further development led by them through 1996. ","Cosine Similarity: 0.8581550454943917 / The unified modeling language (UML) is a general-purpose visual modeling language that is intended to provide a standard way to visualize the design of a system.UML provides a standard notation for many types of diagrams which can be roughly divided into three main groups: behavior diagrams: interaction diagrams: and structure diagrams.The creation of UML was originally motivated by the desire to standardize the disparate notational systems and approaches to software design. It was developed at Rational Software in 1994–1995: with further development led by them through 1996.In 1997: UML was adopted as a standard by the Object Management Group (OMG): and has been managed by this organization ever since. In 2005: UML was also published by the International Organization for Standardization (ISO) and the International Electrotechnical Commission (IEC) as the ISO/IEC 19501 standard. Since then the standard has been periodically revised to cover the latest revision of UML.In software engineering: most practitioners do not use UML: but instead produce informal hand drawn diagrams; these diagrams: however: often include elements from UML. : 536  /  /  /  /  /  / == History == /  /  /  /  /  / === Before UML 1.0 === /  / UML has been evolved since the second half of the 1990s and has its roots in the object-oriented programming methods developed in the late 1980s and early 1990s. The timeline (see image) shows the highlights of the history of object-oriented modeling methods and notation. It is originally based on the notations of the Booch method: the object-modeling technique (OMT) and object-oriented software engineering (OOSE): which it has integrated into a single language.Rational Software Corporation hired James Rumbaugh from General Electric in 1994 and after that the company became the source for two of the most popular object-oriented modeling approaches of the day: Rumbaugh's object-modeling technique (OMT) and Grady Booch's method. They were soon assisted in their efforts by Ivar Jacobson: the creator of the object-oriented software engineering (OOSE) method: who joined them at Rational in 1995. === UML 1.x === /  / Under the technical leadership of those three (Rumbaugh: Jacobson and Booch): a consortium called the UML Partners was organized in 1996 to complete the Unified Modeling Language (UML) specification: and propose it to the Object Management Group (OMG) for standardization. The partnership also contained additional interested parties (for example HP: DEC: IBM and Microsoft). The UML Partners' UML 1.0 draft was proposed to the OMG in January 1997 by the consortium. During the same month the UML Partners formed a group: designed to define the exact meaning of language constructs: chaired by Cris Kobryn and administered by Ed Eykholt: to finalize the specification and integrate it with other standardization efforts. The result of this work: UML 1.1: was submitted to the OMG in August 1997 and adopted by the OMG in November 1997.After the first release a task force was formed to improve the language: which released several minor revisions: 1.3: 1.4: and 1.5.The standards it produced (as well as the original standard) have been noted as being ambiguous and inconsistent. ==== Cardinality notation ==== /  / As with database Chen: Bachman: and ISO ER diagrams: class models are specified to use ""look-across"" cardinalities: even though several authors (Merise: Elmasri & Navathe amongst others) prefer same-side or ""look-here"" for roles and both minimum and maximum cardinalities. Recent researchers (Feinerer: Dullea et al.) have shown that the ""look-across"" technique used by UML and ER diagrams is less effective and less coherent when applied to n-ary relationships of order strictly greater than 2. Feinerer says: ""Problems arise if we operate under the look-across semantics as used for UML associations. Hartmann investigates this situation and shows how and why different transformations fail. "": and: ""As we will see on the next few pages: the look-across interpretation introduces several difficulties which prevent the extension of simple mechanisms from binary to n-ary associations."" === UML 2 === /  / UML 2.0 major revision replaced version 1.5 in 2005: which was developed with an enlarged consortium to improve the language further to reflect new experience on usage of its features.Although UML 2.1 was never released as a formal specification: versions 2.1.1 and 2.1.2 appeared in 2007: followed by UML 2.2 in February 2009. UML 2.3 was formally released in May 2010. UML 2.4.1 was formally released in August 2011. UML 2.5 was released in October 2012 as an ""In progress"" version and was officially released in June 2015. Formal version 2.5.1 was adopted in December 2017.There are four parts to the UML 2.x specification: /  /  /  / The Superstructure that defines the notation and semantics for diagrams and their model elements /  / The Infrastructure that defines the core metamodel on which the Superstructure is based /  / The Object Constraint Language (OCL) for defining rules for model elements /  / The UML Diagram Interchange that defines how UML 2 diagram layouts are exchangedUntil UML 2.4.1: the latest versions of these standards were: /  / UML Superstructure version 2.4.1 /  / UML Infrastructure version 2.4.1 /  / OCL version 2.3.1 /  / UML Diagram Interchange version 1.0.Since version 2.5: the UML Specification has been simplified (without Superstructure and Infrastructure): and the latest versions of these standards are now: /  / UML Specification 2.5.1 /  / OCL version 2.4It continues to be updated and improved by the revision task force: who resolve any issues with the language. == Design == /  / UML offers a way to visualize a system's architectural blueprints in a diagram: including elements such as: /  / any activities (jobs); /  / individual components of the system; /  / and how they can interact with other software components; /  / how the system will run; /  / how entities interact with others (components and interfaces); /  / external user interface.Although originally intended for object-oriented design documentation: UML has been extended to a larger set of design documentation (as listed above): and been found useful in many contexts. === Software development methods === /  / UML is not a development method by itself; however: it was designed to be compatible with the leading object-oriented software development methods of its time: for example OMT: Booch method: Objectory and especially RUP that it was originally intended to be used with when work began at Rational Software. === Modeling === /  / It is important to distinguish between the UML model and the set of diagrams of a system. ",Cosine Similarity: 0.8512089789575393 / Unified Modeling Language (UML) is a general-purpose modeling language that is an industry standard for specifying software-intensive systems.
0.7067134999961127,Who manages UML and when was it adopted as a standard? ,"UML was adopted as a standard by the Object Management Group (OMG) in 1997 and has been managed by this organization ever since. In 2005, UML was also published by the International Organization for Standardization (ISO) and the International Electrotechnical Commission (IEC) as the ISO/IEC 19501 standard.","Cosine Similarity: 0.8512651692661072 / The unified modeling language (UML) is a general-purpose visual modeling language that is intended to provide a standard way to visualize the design of a system.UML provides a standard notation for many types of diagrams which can be roughly divided into three main groups: behavior diagrams: interaction diagrams: and structure diagrams.The creation of UML was originally motivated by the desire to standardize the disparate notational systems and approaches to software design. It was developed at Rational Software in 1994–1995: with further development led by them through 1996.In 1997: UML was adopted as a standard by the Object Management Group (OMG): and has been managed by this organization ever since. In 2005: UML was also published by the International Organization for Standardization (ISO) and the International Electrotechnical Commission (IEC) as the ISO/IEC 19501 standard. Since then the standard has been periodically revised to cover the latest revision of UML.In software engineering: most practitioners do not use UML: but instead produce informal hand drawn diagrams; these diagrams: however: often include elements from UML. : 536  /  /  /  /  /  / == History == /  /  /  /  /  / === Before UML 1.0 === /  / UML has been evolved since the second half of the 1990s and has its roots in the object-oriented programming methods developed in the late 1980s and early 1990s. The timeline (see image) shows the highlights of the history of object-oriented modeling methods and notation. It is originally based on the notations of the Booch method: the object-modeling technique (OMT) and object-oriented software engineering (OOSE): which it has integrated into a single language.Rational Software Corporation hired James Rumbaugh from General Electric in 1994 and after that the company became the source for two of the most popular object-oriented modeling approaches of the day: Rumbaugh's object-modeling technique (OMT) and Grady Booch's method. They were soon assisted in their efforts by Ivar Jacobson: the creator of the object-oriented software engineering (OOSE) method: who joined them at Rational in 1995. === UML 1.x === /  / Under the technical leadership of those three (Rumbaugh: Jacobson and Booch): a consortium called the UML Partners was organized in 1996 to complete the Unified Modeling Language (UML) specification: and propose it to the Object Management Group (OMG) for standardization. The partnership also contained additional interested parties (for example HP: DEC: IBM and Microsoft). The UML Partners' UML 1.0 draft was proposed to the OMG in January 1997 by the consortium. During the same month the UML Partners formed a group: designed to define the exact meaning of language constructs: chaired by Cris Kobryn and administered by Ed Eykholt: to finalize the specification and integrate it with other standardization efforts. The result of this work: UML 1.1: was submitted to the OMG in August 1997 and adopted by the OMG in November 1997.After the first release a task force was formed to improve the language: which released several minor revisions: 1.3: 1.4: and 1.5.The standards it produced (as well as the original standard) have been noted as being ambiguous and inconsistent. ==== Cardinality notation ==== /  / As with database Chen: Bachman: and ISO ER diagrams: class models are specified to use ""look-across"" cardinalities: even though several authors (Merise: Elmasri & Navathe amongst others) prefer same-side or ""look-here"" for roles and both minimum and maximum cardinalities. Recent researchers (Feinerer: Dullea et al.) have shown that the ""look-across"" technique used by UML and ER diagrams is less effective and less coherent when applied to n-ary relationships of order strictly greater than 2. Feinerer says: ""Problems arise if we operate under the look-across semantics as used for UML associations. Hartmann investigates this situation and shows how and why different transformations fail. "": and: ""As we will see on the next few pages: the look-across interpretation introduces several difficulties which prevent the extension of simple mechanisms from binary to n-ary associations."" === UML 2 === /  / UML 2.0 major revision replaced version 1.5 in 2005: which was developed with an enlarged consortium to improve the language further to reflect new experience on usage of its features.Although UML 2.1 was never released as a formal specification: versions 2.1.1 and 2.1.2 appeared in 2007: followed by UML 2.2 in February 2009. UML 2.3 was formally released in May 2010. UML 2.4.1 was formally released in August 2011. UML 2.5 was released in October 2012 as an ""In progress"" version and was officially released in June 2015. Formal version 2.5.1 was adopted in December 2017.There are four parts to the UML 2.x specification: /  /  /  / The Superstructure that defines the notation and semantics for diagrams and their model elements /  / The Infrastructure that defines the core metamodel on which the Superstructure is based /  / The Object Constraint Language (OCL) for defining rules for model elements /  / The UML Diagram Interchange that defines how UML 2 diagram layouts are exchangedUntil UML 2.4.1: the latest versions of these standards were: /  / UML Superstructure version 2.4.1 /  / UML Infrastructure version 2.4.1 /  / OCL version 2.3.1 /  / UML Diagram Interchange version 1.0.Since version 2.5: the UML Specification has been simplified (without Superstructure and Infrastructure): and the latest versions of these standards are now: /  / UML Specification 2.5.1 /  / OCL version 2.4It continues to be updated and improved by the revision task force: who resolve any issues with the language. == Design == /  / UML offers a way to visualize a system's architectural blueprints in a diagram: including elements such as: /  / any activities (jobs); /  / individual components of the system; /  / and how they can interact with other software components; /  / how the system will run; /  / how entities interact with others (components and interfaces); /  / external user interface.Although originally intended for object-oriented design documentation: UML has been extended to a larger set of design documentation (as listed above): and been found useful in many contexts. === Software development methods === /  / UML is not a development method by itself; however: it was designed to be compatible with the leading object-oriented software development methods of its time: for example OMT: Booch method: Objectory and especially RUP that it was originally intended to be used with when work began at Rational Software. === Modeling === /  / It is important to distinguish between the UML model and the set of diagrams of a system. ",Cosine Similarity: 0.8492559017872557 / UML 2.0: the current version: supports thirteen different diagram techniques: and has widespread tool support.
0.5688061999971978,What are the main groups that UML diagrams can be divided into?,"UML diagrams can be largely divided into three main groups: behavior diagrams, interaction diagrams, and structure diagrams.",Cosine Similarity: 0.8547473861008826 / UML 2.0: the current version: supports thirteen different diagram techniques: and has widespread tool support.,"Cosine Similarity: 0.8511240444877132 / The unified modeling language (UML) is a general-purpose visual modeling language that is intended to provide a standard way to visualize the design of a system.UML provides a standard notation for many types of diagrams which can be roughly divided into three main groups: behavior diagrams: interaction diagrams: and structure diagrams.The creation of UML was originally motivated by the desire to standardize the disparate notational systems and approaches to software design. It was developed at Rational Software in 1994–1995: with further development led by them through 1996.In 1997: UML was adopted as a standard by the Object Management Group (OMG): and has been managed by this organization ever since. In 2005: UML was also published by the International Organization for Standardization (ISO) and the International Electrotechnical Commission (IEC) as the ISO/IEC 19501 standard. Since then the standard has been periodically revised to cover the latest revision of UML.In software engineering: most practitioners do not use UML: but instead produce informal hand drawn diagrams; these diagrams: however: often include elements from UML. : 536  /  /  /  /  /  / == History == /  /  /  /  /  / === Before UML 1.0 === /  / UML has been evolved since the second half of the 1990s and has its roots in the object-oriented programming methods developed in the late 1980s and early 1990s. The timeline (see image) shows the highlights of the history of object-oriented modeling methods and notation. It is originally based on the notations of the Booch method: the object-modeling technique (OMT) and object-oriented software engineering (OOSE): which it has integrated into a single language.Rational Software Corporation hired James Rumbaugh from General Electric in 1994 and after that the company became the source for two of the most popular object-oriented modeling approaches of the day: Rumbaugh's object-modeling technique (OMT) and Grady Booch's method. They were soon assisted in their efforts by Ivar Jacobson: the creator of the object-oriented software engineering (OOSE) method: who joined them at Rational in 1995. === UML 1.x === /  / Under the technical leadership of those three (Rumbaugh: Jacobson and Booch): a consortium called the UML Partners was organized in 1996 to complete the Unified Modeling Language (UML) specification: and propose it to the Object Management Group (OMG) for standardization. The partnership also contained additional interested parties (for example HP: DEC: IBM and Microsoft). The UML Partners' UML 1.0 draft was proposed to the OMG in January 1997 by the consortium. During the same month the UML Partners formed a group: designed to define the exact meaning of language constructs: chaired by Cris Kobryn and administered by Ed Eykholt: to finalize the specification and integrate it with other standardization efforts. The result of this work: UML 1.1: was submitted to the OMG in August 1997 and adopted by the OMG in November 1997.After the first release a task force was formed to improve the language: which released several minor revisions: 1.3: 1.4: and 1.5.The standards it produced (as well as the original standard) have been noted as being ambiguous and inconsistent. ==== Cardinality notation ==== /  / As with database Chen: Bachman: and ISO ER diagrams: class models are specified to use ""look-across"" cardinalities: even though several authors (Merise: Elmasri & Navathe amongst others) prefer same-side or ""look-here"" for roles and both minimum and maximum cardinalities. Recent researchers (Feinerer: Dullea et al.) have shown that the ""look-across"" technique used by UML and ER diagrams is less effective and less coherent when applied to n-ary relationships of order strictly greater than 2. Feinerer says: ""Problems arise if we operate under the look-across semantics as used for UML associations. Hartmann investigates this situation and shows how and why different transformations fail. "": and: ""As we will see on the next few pages: the look-across interpretation introduces several difficulties which prevent the extension of simple mechanisms from binary to n-ary associations."" === UML 2 === /  / UML 2.0 major revision replaced version 1.5 in 2005: which was developed with an enlarged consortium to improve the language further to reflect new experience on usage of its features.Although UML 2.1 was never released as a formal specification: versions 2.1.1 and 2.1.2 appeared in 2007: followed by UML 2.2 in February 2009. UML 2.3 was formally released in May 2010. UML 2.4.1 was formally released in August 2011. UML 2.5 was released in October 2012 as an ""In progress"" version and was officially released in June 2015. Formal version 2.5.1 was adopted in December 2017.There are four parts to the UML 2.x specification: /  /  /  / The Superstructure that defines the notation and semantics for diagrams and their model elements /  / The Infrastructure that defines the core metamodel on which the Superstructure is based /  / The Object Constraint Language (OCL) for defining rules for model elements /  / The UML Diagram Interchange that defines how UML 2 diagram layouts are exchangedUntil UML 2.4.1: the latest versions of these standards were: /  / UML Superstructure version 2.4.1 /  / UML Infrastructure version 2.4.1 /  / OCL version 2.3.1 /  / UML Diagram Interchange version 1.0.Since version 2.5: the UML Specification has been simplified (without Superstructure and Infrastructure): and the latest versions of these standards are now: /  / UML Specification 2.5.1 /  / OCL version 2.4It continues to be updated and improved by the revision task force: who resolve any issues with the language. == Design == /  / UML offers a way to visualize a system's architectural blueprints in a diagram: including elements such as: /  / any activities (jobs); /  / individual components of the system; /  / and how they can interact with other software components; /  / how the system will run; /  / how entities interact with others (components and interfaces); /  / external user interface.Although originally intended for object-oriented design documentation: UML has been extended to a larger set of design documentation (as listed above): and been found useful in many contexts. === Software development methods === /  / UML is not a development method by itself; however: it was designed to be compatible with the leading object-oriented software development methods of its time: for example OMT: Booch method: Objectory and especially RUP that it was originally intended to be used with when work began at Rational Software. === Modeling === /  / It is important to distinguish between the UML model and the set of diagrams of a system. "
