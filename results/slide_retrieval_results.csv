query_time,query,answer,response_0,response_1,response_2
0.5338321999879554,What is an AI accelerator?,"An AI accelerator, also referred to as deep learning processor, or neural processing unit (NPU), is a class of specialized hardware accelerator or computer system designed to accelerate artificial intelligence and machine learning applications, including artificial neural networks and machine vision.","{'chunk_text': 'a typical AI integrated circuit chip contains tens of billions of MOSFET transistors.AI accelerators are used in mobile devices: such as neural processing units (NPUs) in Apple iPhones or Huawei cellphones: and personal computers such as Apple silicon Macs: to cloud computing servers such as tensor processing units (TPU) in the Google Cloud Platform. A number of vendor-specific terms exist for devices in this category: and it is an emerging technology without a dominant design. Graphics processing units designed by companies such as Nvidia and AMD often include AI-specific hardware: and are commonly used as AI accelerators: both for training and inference. == History == Computer systems have frequently complemented the CPU with special-purpose accelerators for specialized tasks: known as coprocessors. Notable application-specific hardware units include video cards for graphics: sound cards: graphics processing units and digital signal processors. As deep learning and artificial intelligence workloads rose in prominence in the 2010s: specialized hardware units were developed or adapted from existing products to accelerate these tasks. === Early attempts === First attempts like Intel\'s ETANN 80170NX incorporated analog circuits to compute neural functions.Later all-digital chips like the Nestor/Intel Ni1000 followed. As early as 1993: digital signal processors were used as neural network accelerators to accelerate optical character recognition software.By 1988: Wei Zhang et al. had discussed fast optical implementations of convolutional neural networks for alphabet recognition.In the 1990s: there were also attempts to create parallel high-throughput systems for workstations aimed at various applications: including neural network simulations.This presentation covers a past attempt at neural net accelerators: notes the similarity to the modern SLI GPGPU processor setup: and argues that general purpose vector accelerators are the way forward (in relation to RISC-V hwacha project. Argues that NN\'s are just dense and sparse matrices: one of several recurring algorithms)FPGA-based accelerators were also first explored in the 1990s for both inference and training.In 2014: Chen et al. proposed DianNao (Chinese for ""electric brain""): to accelerate deep neural networks especially. DianNao provides the 452 Gop/s peak performance (of key operations in deep neural networks) only in a small footprint of 3.02 mm2 and 485 mW. Later: the successors (DaDianNao: ShiDianNao: PuDianNao) are proposed by the same group: forming the DianNao FamilySmartphones began incorporating AI accelerators starting with the Qualcomm Snapdragon 820 in 2015. === Heterogeneous computing === Heterogeneous computing incorporates many specialized processors in a single system: or a single chip: each optimized for a specific type of task. Architectures such as the Cell microprocessor have features significantly overlapping with AI accelerators including: support for packed low precision arithmetic: dataflow architecture: and prioritizing throughput over latency. The Cell microprocessor has been applied to a number of tasks including AI.In the 2000s: CPUs also gained increasingly wide SIMD units: driven by video and gaming workloads; as well as support for packed low-precision data types. Due to the increasing performance of CPUs: they are also used for running AI workloads. CPUs are superior for DNNs with small or medium-scale parallelism: for sparse DNNs and in low-batch-size scenarios. === Use of GPU === Graphics processing units or GPUs are specialized hardware for the manipulation of images and calculation of local image properties. The mathematical basis of neural networks and image manipulation are similar: embarrassingly parallel tasks involving matrices: leading GPUs to become increasingly used for machine learning tasks.In 2012: Alex Krizhevsky adopted two GPUs to train a deep learning network: i.e.: AlexNet: which won the champion of the ISLVRC-2012 competition. During the 2010\'s: GPU manufacturers such as Nvidia added deep learning related features in both hardware (e.g.: INT8 operators) and software (e.g.: cuDNN Library). GPUs continue to be used in large-scale AI applications. For example: Summit: a supercomputer from IBM for Oak Ridge National Laboratory: contains 27:648 Nvidia Tesla V100 cards: which can be used to accelerate deep learning algorithms. Over the 2010\'s GPUs continued to evolve in a direction to facilitate deep learning: both for training and inference in devices such as self-driving cars. GPU developers such as Nvidia NVLink are developing additional connective capability for the kind of dataflow workloads AI benefits from. As GPUs have been increasingly applied to AI acceleration: GPU manufacturers have incorporated neural network-specific hardware to further accelerate these tasks. Tensor cores are intended to speed up the training of neural networks. === Use of FPGAs === Deep learning frameworks are still evolving: making it hard to design custom hardware. Reconfigurable devices such as field-programmable gate arrays (FPGA) make it easier to evolve hardware: frameworks: and software alongside each other.Microsoft has used FPGA chips to accelerate inference for real-time deep learning services. === Emergence of dedicated AI accelerator ASICs === While GPUs and FPGAs perform far better than CPUs for AI-related tasks: a factor of up to 10 in efficiency may be gained with a more specific design: via an application-specific integrated circuit (ASIC). These accelerators employ strategies such as optimized memory use and the use of lower precision arithmetic to accelerate calculation and increase throughput of computation. Some low-precision floating-point formats used for AI acceleration are half-precision and the bfloat16 floating-point format. Companies such as Google: Qualcomm: Amazon: Apple: Facebook: AMD and Samsung are all designing their own AI ASICs. Cerebras Systems has built a dedicated AI accelerator based on the largest processor in the industry: the second-generation Wafer Scale Engine (WSE-2): to support deep learning workloads. == Ongoing research == === In-memory computing architectures === In June 2017: IBM researchers announced an architecture in contrast to the Von Neumann architecture based on in-memory computing and phase-change memory arrays applied to temporal correlation detection: intending to generalize\nthe approach to heterogeneous computing and massively parallel systems. In October 2018: IBM researchers announced an architecture based on in-memory processing and modeled on the human brain\'s synaptic network to accelerate deep neural networks. The system is based on phase-change memory arrays. === In-memory computing with analog resistive memories === In 2019: researchers from Politecnico di Milano found a way to solve systems of linear equations in a few tens of nanoseconds via a': 'original_similarity': 0.8562666140495055: 'slide': 72}",{'chunk_text': 'lists several typical benchmarks for AI accelerators. == Potential applications == Agricultural robots: for example: herbicide-free weed control. Autonomous vehicles: Nvidia has targeted their Drive PX-series boards at this application. Computer-aided diagnosis Industrial robots: increasing the range of tasks that can be automated: by adding adaptability to variable situations. Machine translation Military robots Natural language processing Search engines: increasing the energy efficiency of data centers and the ability to use increasingly advanced queries. Unmanned aerial vehicles: e.g. navigation systems: e.g. the Movidius Myriad 2 has been demonstrated successfully guiding autonomous drones. Voice user interface: e.g. in mobile phones: a target for Qualcomm Zeroth. == See also == Cognitive computer Neuromorphic engineering Optical neural network Physical neural network Cerebras Systems == References == == External links == Nvidia Puts The Accelerator To The Metal With Pascal.htm: The Next Platform Eyeriss Project: MIT https://alphaics.ai/\nBidirectional Encoder Representations from Transformers (BERT) is a language model based on': 'original_similarity': 0.84926231009982: 'slide': 10},"{'chunk_text': 'materials such as semiconducting molybdenum disulphide to precisely tune FGFETs as building blocks in which logic operations can be performed with the memory elements. === Integrated photonic tensor core === In 1988: Wei Zhang et al. discussed fast optical implementations of convolutional neural networks for alphabet recognition. In 2021: J. Feldmann et al. proposed an integrated photonic hardware accelerator for parallel convolutional processing. The authors identify two key advantages of integrated photonics over its electronic counterparts: (1) massively parallel data transfer through wavelength division multiplexing in conjunction with frequency combs: and (2) extremely high data modulation speeds. Their system can execute trillions of multiply-accumulate operations per second: indicating the potential of integrated photonics in data-heavy AI applications. Optical processors that can also perform backpropagation for artificial neural networks have been experimentally developed. == Nomenclature == As of 2016: the field is still in flux and vendors are pushing their own marketing term for what amounts to an ""AI accelerator"": in the hope that their designs and APIs will become the dominant design. There is no consensus on the boundary between these devices: nor the exact form they will take; however several examples clearly aim to fill this new space: with a fair amount of overlap in capabilities. In the past when consumer graphics accelerators emerged: the industry eventually adopted Nvidia\'s self-assigned term: ""the GPU"": as the collective noun for ""graphics accelerators"": which had taken many forms before settling on an overall pipeline implementing a model presented by Direct3D. All models of Intel Meteor Lake processors have a Versatile Processor Unit (VPU) built-in for accelerating inference for computer vision and deep learning. == Deep Learning Processors (DLP) == Inspired from the pioneer work of DianNao Family: many DLPs are proposed in both academia and industry with design optimized to leverage the features of deep neural networks for high efficiency. Only at ISCA 2016: three sessions: 15% (!) of the accepted papers: are all architecture designs about deep learning. Such efforts include Eyeriss (MIT): EIE (Stanford): Minerva (Harvard): Stripes (University of Toronto) in academia: TPU (Google): and MLU (Cambricon) in industry. We listed several representative works in Table 1. === Digital DLPs === The major components of DLPs architecture usually include a computation component: the on-chip memory hierarchy: and the control logic that manages the data communication and computing flows. Regarding the computation component: as most operations in deep learning can be aggregated into vector operations: the most common ways for building computation components in digital DLPs are the MAC-based (multiplier-accumulation) organization: either with vector MACs or scalar MACs. Rather than SIMD or SIMT in general processing devices: deep learning domain-specific parallelism is better explored on these MAC-based organizations. Regarding the memory hierarchy: as deep learning algorithms require high bandwidth to provide the computation component with sufficient data: DLPs usually employ a relatively larger size (tens of kilobytes or several megabytes) on-chip buffer but with dedicated on-chip data reuse strategy and data exchange strategy to alleviate the burden for memory bandwidth. For example: DianNao: 16 16-in vector MAC: requires 16 × 16 × 2 = 512 16-bit data: i.e.: almost 1024GB/s bandwidth requirements between computation components and buffers. With on-chip reuse: such bandwidth requirements are reduced drastically. Instead of the widely used cache in general processing devices: DLPs always use scratchpad memory as it could provide higher data reuse opportunities by leveraging the relatively regular data access pattern in deep learning algorithms. Regarding the control logic: as the deep learning algorithms keep evolving at a dramatic speed: DLPs start to leverage dedicated ISA (instruction set architecture) to support the deep learning domain flexibly. At first: DianNao used a VLIW-style instruction set where each instruction could finish a layer in a DNN. Cambricon introduces the first deep learning domain-specific ISA: which could support more than ten different deep learning algorithms. TPU also reveals five key instructions from the CISC-style ISA. === Hybrid DLPs === Hybrid DLPs emerge for DNN inference and training acceleration because of their high efficiency. Processing-in-memory (PIM) architectures are one most important type of hybrid DLP. The key design concept of PIM is to bridge the gap between computing and memory: with the following manners: 1) Moving computation components into memory cells: controllers: or memory chips to alleviate the memory wall issue. Such architectures significantly shorten data paths and leverage much higher internal bandwidth: hence resulting in attractive performance improvement. 2) Build high efficient DNN engines by adopting computational devices. In 2013: HP Lab demonstrated the astonishing capability of adopting ReRAM crossbar structure for computing. Inspiring by this work: tremendous work are proposed to explore the new architecture and system design based on ReRAM: phase change memory: etc. == Benchmarks == Benchmarks such as MLPerf and others may\nbe used to evaluate the performance of AI accelerators. Table 2 lists several typical benchmarks for AI accelerators. == Potential applications == Agricultural robots: for example: herbicide-free weed control. Autonomous vehicles: Nvidia has targeted their Drive PX-series boards at this application. Computer-aided diagnosis Industrial robots: increasing the range of tasks that can be automated: by adding adaptability to variable situations. Machine translation Military robots Natural language processing Search engines: increasing the energy efficiency of data centers and the ability to use increasingly advanced queries. Unmanned aerial vehicles: e.g. navigation systems: e.g. the Movidius Myriad 2 has been demonstrated successfully guiding autonomous drones. Voice user interface: e.g. in mobile phones: a target for Qualcomm Zeroth. == See also == Cognitive computer Neuromorphic engineering Optical neural network Physical neural network Cerebras Systems == References == == External links == Nvidia Puts The Accelerator To The Metal With Pascal.htm: The Next Platform Eyeriss Project: MIT https://alphaics.ai/': 'original_similarity': 0.7955438705921805: 'slide': 211}"
0.47782729999744333,Where are AI accelerators often used?,"AI accelerators are used in various devices, including mobile devices such as neural processing units (NPUs) in Apple iPhones or Huawei cellphones, and personal computers such as Apple silicon Macs, to cloud computing servers such as tensor processing units (TPU) in the Google Cloud Platform. ",{'chunk_text': 'of AI accelerators. Table 2 lists several typical benchmarks for AI accelerators. == Potential applications == Agricultural robots: for example: herbicide-free weed control. Autonomous vehicles: Nvidia has targeted their Drive PX-series boards at this application. Computer-aided diagnosis Industrial robots: increasing the range of tasks that can be automated: by adding adaptability to variable situations. Machine translation Military robots Natural language processing Search engines: increasing the energy efficiency of data centers and the ability to use increasingly advanced queries. Unmanned aerial vehicles: e.g. navigation systems: e.g. the Movidius Myriad 2 has been demonstrated successfully guiding autonomous drones. Voice user interface: e.g. in mobile phones: a target for Qualcomm Zeroth. == See also == Cognitive computer Neuromorphic engineering Optical neural network Physical neural network Cerebras Systems == References == == External links == Nvidia Puts The Accelerator To The Metal With Pascal.htm: The Next Platform Eyeriss Project: MIT https://alphaics.ai/\nBidirectional Encoder Representations from Transformers (BERT) is': 'original_similarity': 0.8713202840610061: 'slide': 5},"{'chunk_text': '(NPUs) in Apple iPhones or Huawei cellphones: and personal computers such as Apple silicon Macs: to cloud computing servers such as tensor processing units (TPU) in the Google Cloud Platform. A number of vendor-specific terms exist for devices in this category: and it is an emerging technology without a dominant design. Graphics processing units designed by companies such as Nvidia and AMD often include AI-specific hardware: and are commonly used as AI accelerators: both for training and inference. == History == Computer systems have frequently complemented the CPU with special-purpose accelerators for specialized tasks: known as coprocessors. Notable application-specific hardware units include video cards for graphics: sound cards: graphics processing units and digital signal processors. As deep learning and artificial intelligence workloads rose in prominence in the 2010s: specialized hardware units were developed or adapted from existing products to accelerate these tasks. === Early attempts === First attempts like Intel\'s ETANN 80170NX incorporated analog circuits to compute neural functions.Later all-digital chips like the Nestor/Intel Ni1000 followed. As early as 1993: digital signal processors were used as neural network accelerators to accelerate optical character recognition software.By 1988: Wei Zhang et al. had discussed fast optical implementations of convolutional neural networks for alphabet recognition.In the 1990s: there were also attempts to create parallel high-throughput systems for workstations aimed at various applications: including neural network simulations.This presentation covers a past attempt at neural net accelerators: notes the similarity to the modern SLI GPGPU processor setup: and argues that general purpose vector accelerators are the way forward (in relation to RISC-V hwacha project. Argues that NN\'s are just dense and sparse matrices: one of several recurring algorithms)FPGA-based accelerators were also first explored in the 1990s for both inference and training.In 2014: Chen et al. proposed DianNao (Chinese for ""electric brain""): to accelerate deep neural networks especially. DianNao provides the 452 Gop/s peak performance (of key operations in deep neural networks) only in a small footprint of 3.02 mm2 and 485 mW. Later: the successors (DaDianNao: ShiDianNao: PuDianNao) are proposed by the same group: forming the DianNao FamilySmartphones began incorporating AI accelerators starting with the Qualcomm Snapdragon 820 in 2015. === Heterogeneous computing === Heterogeneous computing incorporates many specialized processors in a single system: or a single chip: each optimized for a specific type of task. Architectures such as the Cell microprocessor have features significantly overlapping with AI accelerators including: support for packed low precision arithmetic: dataflow architecture: and prioritizing throughput over latency. The Cell microprocessor has been applied to a number of tasks including AI.In the 2000s: CPUs also gained increasingly wide SIMD units: driven by video and gaming workloads; as well as support for packed low-precision data types. Due to the increasing performance of CPUs: they are also used for running AI workloads. CPUs are superior for DNNs with small or medium-scale parallelism: for sparse DNNs and in low-batch-size scenarios. === Use of GPU === Graphics processing units or GPUs are specialized hardware for the manipulation of images and calculation of local image properties. The mathematical basis of neural networks and image manipulation are similar: embarrassingly parallel tasks involving matrices: leading GPUs to become increasingly used for machine learning tasks.In 2012: Alex Krizhevsky adopted two GPUs to train a deep learning network: i.e.: AlexNet: which won the champion of the ISLVRC-2012 competition. During the 2010\'s: GPU manufacturers such as Nvidia added deep learning related features in both hardware (e.g.: INT8 operators) and software (e.g.: cuDNN Library). GPUs continue to be used in large-scale AI applications. For example: Summit: a supercomputer from IBM for Oak Ridge National Laboratory: contains 27:648 Nvidia Tesla V100 cards: which can be used to accelerate deep learning algorithms. Over the 2010\'s GPUs continued to evolve in a direction to facilitate deep learning: both for training and inference in devices such as self-driving cars. GPU developers such as Nvidia NVLink are developing additional connective capability for the kind of dataflow workloads AI benefits from. As GPUs have been increasingly applied to AI acceleration: GPU manufacturers have incorporated neural network-specific hardware to further accelerate these tasks. Tensor cores are intended to speed up the training of neural networks. === Use of FPGAs === Deep learning frameworks are still evolving: making it hard to design custom hardware. Reconfigurable devices such as field-programmable gate arrays (FPGA) make it easier to evolve hardware: frameworks: and software alongside each other.Microsoft has used FPGA chips to accelerate inference for real-time deep learning services. === Emergence of dedicated AI accelerator ASICs === While GPUs and FPGAs perform far better than CPUs for AI-related tasks: a factor of up to 10 in efficiency may be gained with a more specific design: via an application-specific integrated circuit (ASIC). These accelerators employ strategies such as optimized memory use and the use of lower precision arithmetic to accelerate calculation and increase throughput of computation. Some low-precision floating-point formats used for AI acceleration are half-precision and the bfloat16 floating-point format. Companies such as Google: Qualcomm: Amazon: Apple: Facebook: AMD and Samsung are all designing their own AI ASICs. Cerebras Systems has built a dedicated AI accelerator based on the largest processor in the industry: the second-generation Wafer Scale Engine (WSE-2): to support deep learning workloads. == Ongoing research == === In-memory computing architectures === In June 2017: IBM researchers announced an architecture in contrast to the Von Neumann architecture based on in-memory computing and phase-change memory arrays applied to temporal correlation detection: intending to generalize\nthe approach to heterogeneous computing and massively parallel systems. In October 2018: IBM researchers announced an architecture based on in-memory processing and modeled on the human brain\'s synaptic network to accelerate deep neural networks. The system is based on phase-change memory arrays. === In-memory computing with analog resistive memories === In 2019: researchers from Politecnico di Milano found a way to solve systems of linear equations in a few tens of nanoseconds via a single operation. Their algorithm is based on in-memory computing with analog resistive memories which performs with high efficiencies of time and energy: via conducting': 'original_similarity': 0.855904375203457: 'slide': 96}","{'chunk_text': 'such as semiconducting molybdenum disulphide to precisely tune FGFETs as building blocks in which logic operations can be performed with the memory elements. === Integrated photonic tensor core === In 1988: Wei Zhang et al. discussed fast optical implementations of convolutional neural networks for alphabet recognition. In 2021: J. Feldmann et al. proposed an integrated photonic hardware accelerator for parallel convolutional processing. The authors identify two key advantages of integrated photonics over its electronic counterparts: (1) massively parallel data transfer through wavelength division multiplexing in conjunction with frequency combs: and (2) extremely high data modulation speeds. Their system can execute trillions of multiply-accumulate operations per second: indicating the potential of integrated photonics in data-heavy AI applications. Optical processors that can also perform backpropagation for artificial neural networks have been experimentally developed. == Nomenclature == As of 2016: the field is still in flux and vendors are pushing their own marketing term for what amounts to an ""AI accelerator"": in the hope that their designs and APIs will become the dominant design. There is no consensus on the boundary between these devices: nor the exact form they will take; however several examples clearly aim to fill this new space: with a fair amount of overlap in capabilities. In the past when consumer graphics accelerators emerged: the industry eventually adopted Nvidia\'s self-assigned term: ""the GPU"": as the collective noun for ""graphics accelerators"": which had taken many forms before settling on an overall pipeline implementing a model presented by Direct3D. All models of Intel Meteor Lake processors have a Versatile Processor Unit (VPU) built-in for accelerating inference for computer vision and deep learning. == Deep Learning Processors (DLP) == Inspired from the pioneer work of DianNao Family: many DLPs are proposed in both academia and industry with design optimized to leverage the features of deep neural networks for high efficiency. Only at ISCA 2016: three sessions: 15% (!) of the accepted papers: are all architecture designs about deep learning. Such efforts include Eyeriss (MIT): EIE (Stanford): Minerva (Harvard): Stripes (University of Toronto) in academia: TPU (Google): and MLU (Cambricon) in industry. We listed several representative works in Table 1. === Digital DLPs === The major components of DLPs architecture usually include a computation component: the on-chip memory hierarchy: and the control logic that manages the data communication and computing flows. Regarding the computation component: as most operations in deep learning can be aggregated into vector operations: the most common ways for building computation components in digital DLPs are the MAC-based (multiplier-accumulation) organization: either with vector MACs or scalar MACs. Rather than SIMD or SIMT in general processing devices: deep learning domain-specific parallelism is better explored on these MAC-based organizations. Regarding the memory hierarchy: as deep learning algorithms require high bandwidth to provide the computation component with sufficient data: DLPs usually employ a relatively larger size (tens of kilobytes or several megabytes) on-chip buffer but with dedicated on-chip data reuse strategy and data exchange strategy to alleviate the burden for memory bandwidth. For example: DianNao: 16 16-in vector MAC: requires 16 × 16 × 2 = 512 16-bit data: i.e.: almost 1024GB/s bandwidth requirements between computation components and buffers. With on-chip reuse: such bandwidth requirements are reduced drastically. Instead of the widely used cache in general processing devices: DLPs always use scratchpad memory as it could provide higher data reuse opportunities by leveraging the relatively regular data access pattern in deep learning algorithms. Regarding the control logic: as the deep learning algorithms keep evolving at a dramatic speed: DLPs start to leverage dedicated ISA (instruction set architecture) to support the deep learning domain flexibly. At first: DianNao used a VLIW-style instruction set where each instruction could finish a layer in a DNN. Cambricon introduces the first deep learning domain-specific ISA: which could support more than ten different deep learning algorithms. TPU also reveals five key instructions from the CISC-style ISA. === Hybrid DLPs === Hybrid DLPs emerge for DNN inference and training acceleration because of their high efficiency. Processing-in-memory (PIM) architectures are one most important type of hybrid DLP. The key design concept of PIM is to bridge the gap between computing and memory: with the following manners: 1) Moving computation components into memory cells: controllers: or memory chips to alleviate the memory wall issue. Such architectures significantly shorten data paths and leverage much higher internal bandwidth: hence resulting in attractive performance improvement. 2) Build high efficient DNN engines by adopting computational devices. In 2013: HP Lab demonstrated the astonishing capability of adopting ReRAM crossbar structure for computing. Inspiring by this work: tremendous work are proposed to explore the new architecture and system design based on ReRAM: phase change memory: etc. == Benchmarks == Benchmarks such as MLPerf and others may\nbe used to evaluate the performance of AI accelerators. Table 2 lists several typical benchmarks for AI accelerators. == Potential applications == Agricultural robots: for example: herbicide-free weed control. Autonomous vehicles: Nvidia has targeted their Drive PX-series boards at this application. Computer-aided diagnosis Industrial robots: increasing the range of tasks that can be automated: by adding adaptability to variable situations. Machine translation Military robots Natural language processing Search engines: increasing the energy efficiency of data centers and the ability to use increasingly advanced queries. Unmanned aerial vehicles: e.g. navigation systems: e.g. the Movidius Myriad 2 has been demonstrated successfully guiding autonomous drones. Voice user interface: e.g. in mobile phones: a target for Qualcomm Zeroth. == See also == Cognitive computer Neuromorphic engineering Optical neural network Physical neural network Cerebras Systems == References == == External links == Nvidia Puts The Accelerator To The Metal With Pascal.htm: The Next Platform Eyeriss Project: MIT https://alphaics.ai/': 'original_similarity': 0.8155535653181668: 'slide': 212}"
0.41203810001024976,How have GPUs been utilized for AI acceleration?,"Graphics processing units or GPUs, that are specialized hardware for the manipulation of images and calculation of local image properties, have been increasingly used for machine learning tasks. GPU developers have also incorporated neural network-specific hardware to further accelerate these tasks.",{'chunk_text': 'of AI accelerators. Table 2 lists several typical benchmarks for AI accelerators. == Potential applications == Agricultural robots: for example: herbicide-free weed control. Autonomous vehicles: Nvidia has targeted their Drive PX-series boards at this application. Computer-aided diagnosis Industrial robots: increasing the range of tasks that can be automated: by adding adaptability to variable situations. Machine translation Military robots Natural language processing Search engines: increasing the energy efficiency of data centers and the ability to use increasingly advanced queries. Unmanned aerial vehicles: e.g. navigation systems: e.g. the Movidius Myriad 2 has been demonstrated successfully guiding autonomous drones. Voice user interface: e.g. in mobile phones: a target for Qualcomm Zeroth. == See also == Cognitive computer Neuromorphic engineering Optical neural network Physical neural network Cerebras Systems == References == == External links == Nvidia Puts The Accelerator To The Metal With Pascal.htm: The Next Platform Eyeriss Project: MIT https://alphaics.ai/\nBidirectional Encoder Representations from Transformers (BERT) is': 'original_similarity': 0.8657625498409405: 'slide': 5},"{'chunk_text': 'Macs: to cloud computing servers such as tensor processing units (TPU) in the Google Cloud Platform. A number of vendor-specific terms exist for devices in this category: and it is an emerging technology without a dominant design. Graphics processing units designed by companies such as Nvidia and AMD often include AI-specific hardware: and are commonly used as AI accelerators: both for training and inference. == History == Computer systems have frequently complemented the CPU with special-purpose accelerators for specialized tasks: known as coprocessors. Notable application-specific hardware units include video cards for graphics: sound cards: graphics processing units and digital signal processors. As deep learning and artificial intelligence workloads rose in prominence in the 2010s: specialized hardware units were developed or adapted from existing products to accelerate these tasks. === Early attempts === First attempts like Intel\'s ETANN 80170NX incorporated analog circuits to compute neural functions.Later all-digital chips like the Nestor/Intel Ni1000 followed. As early as 1993: digital signal processors were used as neural network accelerators to accelerate optical character recognition software.By 1988: Wei Zhang et al. had discussed fast optical implementations of convolutional neural networks for alphabet recognition.In the 1990s: there were also attempts to create parallel high-throughput systems for workstations aimed at various applications: including neural network simulations.This presentation covers a past attempt at neural net accelerators: notes the similarity to the modern SLI GPGPU processor setup: and argues that general purpose vector accelerators are the way forward (in relation to RISC-V hwacha project. Argues that NN\'s are just dense and sparse matrices: one of several recurring algorithms)FPGA-based accelerators were also first explored in the 1990s for both inference and training.In 2014: Chen et al. proposed DianNao (Chinese for ""electric brain""): to accelerate deep neural networks especially. DianNao provides the 452 Gop/s peak performance (of key operations in deep neural networks) only in a small footprint of 3.02 mm2 and 485 mW. Later: the successors (DaDianNao: ShiDianNao: PuDianNao) are proposed by the same group: forming the DianNao FamilySmartphones began incorporating AI accelerators starting with the Qualcomm Snapdragon 820 in 2015. === Heterogeneous computing === Heterogeneous computing incorporates many specialized processors in a single system: or a single chip: each optimized for a specific type of task. Architectures such as the Cell microprocessor have features significantly overlapping with AI accelerators including: support for packed low precision arithmetic: dataflow architecture: and prioritizing throughput over latency. The Cell microprocessor has been applied to a number of tasks including AI.In the 2000s: CPUs also gained increasingly wide SIMD units: driven by video and gaming workloads; as well as support for packed low-precision data types. Due to the increasing performance of CPUs: they are also used for running AI workloads. CPUs are superior for DNNs with small or medium-scale parallelism: for sparse DNNs and in low-batch-size scenarios. === Use of GPU === Graphics processing units or GPUs are specialized hardware for the manipulation of images and calculation of local image properties. The mathematical basis of neural networks and image manipulation are similar: embarrassingly parallel tasks involving matrices: leading GPUs to become increasingly used for machine learning tasks.In 2012: Alex Krizhevsky adopted two GPUs to train a deep learning network: i.e.: AlexNet: which won the champion of the ISLVRC-2012 competition. During the 2010\'s: GPU manufacturers such as Nvidia added deep learning related features in both hardware (e.g.: INT8 operators) and software (e.g.: cuDNN Library). GPUs continue to be used in large-scale AI applications. For example: Summit: a supercomputer from IBM for Oak Ridge National Laboratory: contains 27:648 Nvidia Tesla V100 cards: which can be used to accelerate deep learning algorithms. Over the 2010\'s GPUs continued to evolve in a direction to facilitate deep learning: both for training and inference in devices such as self-driving cars. GPU developers such as Nvidia NVLink are developing additional connective capability for the kind of dataflow workloads AI benefits from. As GPUs have been increasingly applied to AI acceleration: GPU manufacturers have incorporated neural network-specific hardware to further accelerate these tasks. Tensor cores are intended to speed up the training of neural networks. === Use of FPGAs === Deep learning frameworks are still evolving: making it hard to design custom hardware. Reconfigurable devices such as field-programmable gate arrays (FPGA) make it easier to evolve hardware: frameworks: and software alongside each other.Microsoft has used FPGA chips to accelerate inference for real-time deep learning services. === Emergence of dedicated AI accelerator ASICs === While GPUs and FPGAs perform far better than CPUs for AI-related tasks: a factor of up to 10 in efficiency may be gained with a more specific design: via an application-specific integrated circuit (ASIC). These accelerators employ strategies such as optimized memory use and the use of lower precision arithmetic to accelerate calculation and increase throughput of computation. Some low-precision floating-point formats used for AI acceleration are half-precision and the bfloat16 floating-point format. Companies such as Google: Qualcomm: Amazon: Apple: Facebook: AMD and Samsung are all designing their own AI ASICs. Cerebras Systems has built a dedicated AI accelerator based on the largest processor in the industry: the second-generation Wafer Scale Engine (WSE-2): to support deep learning workloads. == Ongoing research == === In-memory computing architectures === In June 2017: IBM researchers announced an architecture in contrast to the Von Neumann architecture based on in-memory computing and phase-change memory arrays applied to temporal correlation detection: intending to generalize\nthe approach to heterogeneous computing and massively parallel systems. In October 2018: IBM researchers announced an architecture based on in-memory processing and modeled on the human brain\'s synaptic network to accelerate deep neural networks. The system is based on phase-change memory arrays. === In-memory computing with analog resistive memories === In 2019: researchers from Politecnico di Milano found a way to solve systems of linear equations in a few tens of nanoseconds via a single operation. Their algorithm is based on in-memory computing with analog resistive memories which performs with high efficiencies of time and energy: via conducting matrix–vector multiplication in one step using Ohm\'s law and Kirchhoff\'s law. The researchers showed': 'original_similarity': 0.8547154820441949: 'slide': 110}","{'chunk_text': 'same basic device structure is used for both logic operations and data storage. The authors used two-dimensional materials such as semiconducting molybdenum disulphide to precisely tune FGFETs as building blocks in which logic operations can be performed with the memory elements. === Integrated photonic tensor core === In 1988: Wei Zhang et al. discussed fast optical implementations of convolutional neural networks for alphabet recognition. In 2021: J. Feldmann et al. proposed an integrated photonic hardware accelerator for parallel convolutional processing. The authors identify two key advantages of integrated photonics over its electronic counterparts: (1) massively parallel data transfer through wavelength division multiplexing in conjunction with frequency combs: and (2) extremely high data modulation speeds. Their system can execute trillions of multiply-accumulate operations per second: indicating the potential of integrated photonics in data-heavy AI applications. Optical processors that can also perform backpropagation for artificial neural networks have been experimentally developed. == Nomenclature == As of 2016: the field is still in flux and vendors are pushing their own marketing term for what amounts to an ""AI accelerator"": in the hope that their designs and APIs will become the dominant design. There is no consensus on the boundary between these devices: nor the exact form they will take; however several examples clearly aim to fill this new space: with a fair amount of overlap in capabilities. In the past when consumer graphics accelerators emerged: the industry eventually adopted Nvidia\'s self-assigned term: ""the GPU"": as the collective noun for ""graphics accelerators"": which had taken many forms before settling on an overall pipeline implementing a model presented by Direct3D. All models of Intel Meteor Lake processors have a Versatile Processor Unit (VPU) built-in for accelerating inference for computer vision and deep learning. == Deep Learning Processors (DLP) == Inspired from the pioneer work of DianNao Family: many DLPs are proposed in both academia and industry with design optimized to leverage the features of deep neural networks for high efficiency. Only at ISCA 2016: three sessions: 15% (!) of the accepted papers: are all architecture designs about deep learning. Such efforts include Eyeriss (MIT): EIE (Stanford): Minerva (Harvard): Stripes (University of Toronto) in academia: TPU (Google): and MLU (Cambricon) in industry. We listed several representative works in Table 1. === Digital DLPs === The major components of DLPs architecture usually include a computation component: the on-chip memory hierarchy: and the control logic that manages the data communication and computing flows. Regarding the computation component: as most operations in deep learning can be aggregated into vector operations: the most common ways for building computation components in digital DLPs are the MAC-based (multiplier-accumulation) organization: either with vector MACs or scalar MACs. Rather than SIMD or SIMT in general processing devices: deep learning domain-specific parallelism is better explored on these MAC-based organizations. Regarding the memory hierarchy: as deep learning algorithms require high bandwidth to provide the computation component with sufficient data: DLPs usually employ a relatively larger size (tens of kilobytes or several megabytes) on-chip buffer but with dedicated on-chip data reuse strategy and data exchange strategy to alleviate the burden for memory bandwidth. For example: DianNao: 16 16-in vector MAC: requires 16 × 16 × 2 = 512 16-bit data: i.e.: almost 1024GB/s bandwidth requirements between computation components and buffers. With on-chip reuse: such bandwidth requirements are reduced drastically. Instead of the widely used cache in general processing devices: DLPs always use scratchpad memory as it could provide higher data reuse opportunities by leveraging the relatively regular data access pattern in deep learning algorithms. Regarding the control logic: as the deep learning algorithms keep evolving at a dramatic speed: DLPs start to leverage dedicated ISA (instruction set architecture) to support the deep learning domain flexibly. At first: DianNao used a VLIW-style instruction set where each instruction could finish a layer in a DNN. Cambricon introduces the first deep learning domain-specific ISA: which could support more than ten different deep learning algorithms. TPU also reveals five key instructions from the CISC-style ISA. === Hybrid DLPs === Hybrid DLPs emerge for DNN inference and training acceleration because of their high efficiency. Processing-in-memory (PIM) architectures are one most important type of hybrid DLP. The key design concept of PIM is to bridge the gap between computing and memory: with the following manners: 1) Moving computation components into memory cells: controllers: or memory chips to alleviate the memory wall issue. Such architectures significantly shorten data paths and leverage much higher internal bandwidth: hence resulting in attractive performance improvement. 2) Build high efficient DNN engines by adopting computational devices. In 2013: HP Lab demonstrated the astonishing capability of adopting ReRAM crossbar structure for computing. Inspiring by this work: tremendous work are proposed to explore the new architecture and system design based on ReRAM: phase change memory: etc. == Benchmarks == Benchmarks such as MLPerf and others may\nbe used to evaluate the performance of AI accelerators. Table 2 lists several typical benchmarks for AI accelerators. == Potential applications == Agricultural robots: for example: herbicide-free weed control. Autonomous vehicles: Nvidia has targeted their Drive PX-series boards at this application. Computer-aided diagnosis Industrial robots: increasing the range of tasks that can be automated: by adding adaptability to variable situations. Machine translation Military robots Natural language processing Search engines: increasing the energy efficiency of data centers and the ability to use increasingly advanced queries. Unmanned aerial vehicles: e.g. navigation systems: e.g. the Movidius Myriad 2 has been demonstrated successfully guiding autonomous drones. Voice user interface: e.g. in mobile phones: a target for Qualcomm Zeroth. == See also == Cognitive computer Neuromorphic engineering Optical neural network Physical neural network Cerebras Systems == References == == External links == Nvidia Puts The Accelerator To The Metal With Pascal.htm: The Next Platform Eyeriss Project: MIT https://alphaics.ai/': 'original_similarity': 0.825258237812648: 'slide': 194}"
0.4380313000001479,What has been the progress of AI accelerator technology in the 1990s?,"In the 1990s, attempts were made to create high-throughput parallel systems for workstations aimed at various applications, including neural network simulations. FPGA-based accelerators for both inference and training were first explored in the 1990s.",{'chunk_text': 'several typical benchmarks for AI accelerators. == Potential applications == Agricultural robots: for example: herbicide-free weed control. Autonomous vehicles: Nvidia has targeted their Drive PX-series boards at this application. Computer-aided diagnosis Industrial robots: increasing the range of tasks that can be automated: by adding adaptability to variable situations. Machine translation Military robots Natural language processing Search engines: increasing the energy efficiency of data centers and the ability to use increasingly advanced queries. Unmanned aerial vehicles: e.g. navigation systems: e.g. the Movidius Myriad 2 has been demonstrated successfully guiding autonomous drones. Voice user interface: e.g. in mobile phones: a target for Qualcomm Zeroth. == See also == Cognitive computer Neuromorphic engineering Optical neural network Physical neural network Cerebras Systems == References == == External links == Nvidia Puts The Accelerator To The Metal With Pascal.htm: The Next Platform Eyeriss Project: MIT https://alphaics.ai/\nBidirectional Encoder Representations from Transformers (BERT) is a language model based on the': 'original_similarity': 0.829406932304334: 'slide': 11},"{'chunk_text': 'accelerators for specialized tasks: known as coprocessors. Notable application-specific hardware units include video cards for graphics: sound cards: graphics processing units and digital signal processors. As deep learning and artificial intelligence workloads rose in prominence in the 2010s: specialized hardware units were developed or adapted from existing products to accelerate these tasks. === Early attempts === First attempts like Intel\'s ETANN 80170NX incorporated analog circuits to compute neural functions.Later all-digital chips like the Nestor/Intel Ni1000 followed. As early as 1993: digital signal processors were used as neural network accelerators to accelerate optical character recognition software.By 1988: Wei Zhang et al. had discussed fast optical implementations of convolutional neural networks for alphabet recognition.In the 1990s: there were also attempts to create parallel high-throughput systems for workstations aimed at various applications: including neural network simulations.This presentation covers a past attempt at neural net accelerators: notes the similarity to the modern SLI GPGPU processor setup: and argues that general purpose vector accelerators are the way forward (in relation to RISC-V hwacha project. Argues that NN\'s are just dense and sparse matrices: one of several recurring algorithms)FPGA-based accelerators were also first explored in the 1990s for both inference and training.In 2014: Chen et al. proposed DianNao (Chinese for ""electric brain""): to accelerate deep neural networks especially. DianNao provides the 452 Gop/s peak performance (of key operations in deep neural networks) only in a small footprint of 3.02 mm2 and 485 mW. Later: the successors (DaDianNao: ShiDianNao: PuDianNao) are proposed by the same group: forming the DianNao FamilySmartphones began incorporating AI accelerators starting with the Qualcomm Snapdragon 820 in 2015. === Heterogeneous computing === Heterogeneous computing incorporates many specialized processors in a single system: or a single chip: each optimized for a specific type of task. Architectures such as the Cell microprocessor have features significantly overlapping with AI accelerators including: support for packed low precision arithmetic: dataflow architecture: and prioritizing throughput over latency. The Cell microprocessor has been applied to a number of tasks including AI.In the 2000s: CPUs also gained increasingly wide SIMD units: driven by video and gaming workloads; as well as support for packed low-precision data types. Due to the increasing performance of CPUs: they are also used for running AI workloads. CPUs are superior for DNNs with small or medium-scale parallelism: for sparse DNNs and in low-batch-size scenarios. === Use of GPU === Graphics processing units or GPUs are specialized hardware for the manipulation of images and calculation of local image properties. The mathematical basis of neural networks and image manipulation are similar: embarrassingly parallel tasks involving matrices: leading GPUs to become increasingly used for machine learning tasks.In 2012: Alex Krizhevsky adopted two GPUs to train a deep learning network: i.e.: AlexNet: which won the champion of the ISLVRC-2012 competition. During the 2010\'s: GPU manufacturers such as Nvidia added deep learning related features in both hardware (e.g.: INT8 operators) and software (e.g.: cuDNN Library). GPUs continue to be used in large-scale AI applications. For example: Summit: a supercomputer from IBM for Oak Ridge National Laboratory: contains 27:648 Nvidia Tesla V100 cards: which can be used to accelerate deep learning algorithms. Over the 2010\'s GPUs continued to evolve in a direction to facilitate deep learning: both for training and inference in devices such as self-driving cars. GPU developers such as Nvidia NVLink are developing additional connective capability for the kind of dataflow workloads AI benefits from. As GPUs have been increasingly applied to AI acceleration: GPU manufacturers have incorporated neural network-specific hardware to further accelerate these tasks. Tensor cores are intended to speed up the training of neural networks. === Use of FPGAs === Deep learning frameworks are still evolving: making it hard to design custom hardware. Reconfigurable devices such as field-programmable gate arrays (FPGA) make it easier to evolve hardware: frameworks: and software alongside each other.Microsoft has used FPGA chips to accelerate inference for real-time deep learning services. === Emergence of dedicated AI accelerator ASICs === While GPUs and FPGAs perform far better than CPUs for AI-related tasks: a factor of up to 10 in efficiency may be gained with a more specific design: via an application-specific integrated circuit (ASIC). These accelerators employ strategies such as optimized memory use and the use of lower precision arithmetic to accelerate calculation and increase throughput of computation. Some low-precision floating-point formats used for AI acceleration are half-precision and the bfloat16 floating-point format. Companies such as Google: Qualcomm: Amazon: Apple: Facebook: AMD and Samsung are all designing their own AI ASICs. Cerebras Systems has built a dedicated AI accelerator based on the largest processor in the industry: the second-generation Wafer Scale Engine (WSE-2): to support deep learning workloads. == Ongoing research == === In-memory computing architectures === In June 2017: IBM researchers announced an architecture in contrast to the Von Neumann architecture based on in-memory computing and phase-change memory arrays applied to temporal correlation detection: intending to generalize\nthe approach to heterogeneous computing and massively parallel systems. In October 2018: IBM researchers announced an architecture based on in-memory processing and modeled on the human brain\'s synaptic network to accelerate deep neural networks. The system is based on phase-change memory arrays. === In-memory computing with analog resistive memories === In 2019: researchers from Politecnico di Milano found a way to solve systems of linear equations in a few tens of nanoseconds via a single operation. Their algorithm is based on in-memory computing with analog resistive memories which performs with high efficiencies of time and energy: via conducting matrix–vector multiplication in one step using Ohm\'s law and Kirchhoff\'s law. The researchers showed that a feedback circuit with cross-point resistive memories can solve algebraic problems such as systems of linear equations: matrix eigenvectors: and differential equations in just one step. Such an approach improves computational times drastically in comparison with digital algorithms. === Atomically thin semiconductors === In 2020: Marega et al. published experiments with a large-area active channel material for developing logic-in-memory devices and circuits based on floating-gate field-effect transistors (FGFETs). Such atomically thin semiconductors are considered promising': 'original_similarity': 0.808277739086201: 'slide': 186}","{'chunk_text': 'blocks in which logic operations can be performed with the memory elements. === Integrated photonic tensor core === In 1988: Wei Zhang et al. discussed fast optical implementations of convolutional neural networks for alphabet recognition. In 2021: J. Feldmann et al. proposed an integrated photonic hardware accelerator for parallel convolutional processing. The authors identify two key advantages of integrated photonics over its electronic counterparts: (1) massively parallel data transfer through wavelength division multiplexing in conjunction with frequency combs: and (2) extremely high data modulation speeds. Their system can execute trillions of multiply-accumulate operations per second: indicating the potential of integrated photonics in data-heavy AI applications. Optical processors that can also perform backpropagation for artificial neural networks have been experimentally developed. == Nomenclature == As of 2016: the field is still in flux and vendors are pushing their own marketing term for what amounts to an ""AI accelerator"": in the hope that their designs and APIs will become the dominant design. There is no consensus on the boundary between these devices: nor the exact form they will take; however several examples clearly aim to fill this new space: with a fair amount of overlap in capabilities. In the past when consumer graphics accelerators emerged: the industry eventually adopted Nvidia\'s self-assigned term: ""the GPU"": as the collective noun for ""graphics accelerators"": which had taken many forms before settling on an overall pipeline implementing a model presented by Direct3D. All models of Intel Meteor Lake processors have a Versatile Processor Unit (VPU) built-in for accelerating inference for computer vision and deep learning. == Deep Learning Processors (DLP) == Inspired from the pioneer work of DianNao Family: many DLPs are proposed in both academia and industry with design optimized to leverage the features of deep neural networks for high efficiency. Only at ISCA 2016: three sessions: 15% (!) of the accepted papers: are all architecture designs about deep learning. Such efforts include Eyeriss (MIT): EIE (Stanford): Minerva (Harvard): Stripes (University of Toronto) in academia: TPU (Google): and MLU (Cambricon) in industry. We listed several representative works in Table 1. === Digital DLPs === The major components of DLPs architecture usually include a computation component: the on-chip memory hierarchy: and the control logic that manages the data communication and computing flows. Regarding the computation component: as most operations in deep learning can be aggregated into vector operations: the most common ways for building computation components in digital DLPs are the MAC-based (multiplier-accumulation) organization: either with vector MACs or scalar MACs. Rather than SIMD or SIMT in general processing devices: deep learning domain-specific parallelism is better explored on these MAC-based organizations. Regarding the memory hierarchy: as deep learning algorithms require high bandwidth to provide the computation component with sufficient data: DLPs usually employ a relatively larger size (tens of kilobytes or several megabytes) on-chip buffer but with dedicated on-chip data reuse strategy and data exchange strategy to alleviate the burden for memory bandwidth. For example: DianNao: 16 16-in vector MAC: requires 16 × 16 × 2 = 512 16-bit data: i.e.: almost 1024GB/s bandwidth requirements between computation components and buffers. With on-chip reuse: such bandwidth requirements are reduced drastically. Instead of the widely used cache in general processing devices: DLPs always use scratchpad memory as it could provide higher data reuse opportunities by leveraging the relatively regular data access pattern in deep learning algorithms. Regarding the control logic: as the deep learning algorithms keep evolving at a dramatic speed: DLPs start to leverage dedicated ISA (instruction set architecture) to support the deep learning domain flexibly. At first: DianNao used a VLIW-style instruction set where each instruction could finish a layer in a DNN. Cambricon introduces the first deep learning domain-specific ISA: which could support more than ten different deep learning algorithms. TPU also reveals five key instructions from the CISC-style ISA. === Hybrid DLPs === Hybrid DLPs emerge for DNN inference and training acceleration because of their high efficiency. Processing-in-memory (PIM) architectures are one most important type of hybrid DLP. The key design concept of PIM is to bridge the gap between computing and memory: with the following manners: 1) Moving computation components into memory cells: controllers: or memory chips to alleviate the memory wall issue. Such architectures significantly shorten data paths and leverage much higher internal bandwidth: hence resulting in attractive performance improvement. 2) Build high efficient DNN engines by adopting computational devices. In 2013: HP Lab demonstrated the astonishing capability of adopting ReRAM crossbar structure for computing. Inspiring by this work: tremendous work are proposed to explore the new architecture and system design based on ReRAM: phase change memory: etc. == Benchmarks == Benchmarks such as MLPerf and others may\nbe used to evaluate the performance of AI accelerators. Table 2 lists several typical benchmarks for AI accelerators. == Potential applications == Agricultural robots: for example: herbicide-free weed control. Autonomous vehicles: Nvidia has targeted their Drive PX-series boards at this application. Computer-aided diagnosis Industrial robots: increasing the range of tasks that can be automated: by adding adaptability to variable situations. Machine translation Military robots Natural language processing Search engines: increasing the energy efficiency of data centers and the ability to use increasingly advanced queries. Unmanned aerial vehicles: e.g. navigation systems: e.g. the Movidius Myriad 2 has been demonstrated successfully guiding autonomous drones. Voice user interface: e.g. in mobile phones: a target for Qualcomm Zeroth. == See also == Cognitive computer Neuromorphic engineering Optical neural network Physical neural network Cerebras Systems == References == == External links == Nvidia Puts The Accelerator To The Metal With Pascal.htm: The Next Platform Eyeriss Project: MIT https://alphaics.ai/': 'original_similarity': 0.8042484842383305: 'slide': 223}"
0.43459859999711625,What is the difference between CPUs and AI accelerators in terms of performing AI-related tasks?,"While CPUs are used for running AI workloads and are superior for DNNs with small or medium-scale parallelism, for sparse DNNs and in low-batch-size scenarios, specialized AI accelerators like GPUs and FPGAs perform far better due to their optimized memory use and the use of lower precision arithmetic to accelerate calculation and increase throughput of computation. In fact, ASICs, a specific design of accelerators, may provide up to 10 times the efficiency of CPUs for AI-related tasks.","{'chunk_text': 'Apple iPhones or Huawei cellphones: and personal computers such as Apple silicon Macs: to cloud computing servers such as tensor processing units (TPU) in the Google Cloud Platform. A number of vendor-specific terms exist for devices in this category: and it is an emerging technology without a dominant design. Graphics processing units designed by companies such as Nvidia and AMD often include AI-specific hardware: and are commonly used as AI accelerators: both for training and inference. == History == Computer systems have frequently complemented the CPU with special-purpose accelerators for specialized tasks: known as coprocessors. Notable application-specific hardware units include video cards for graphics: sound cards: graphics processing units and digital signal processors. As deep learning and artificial intelligence workloads rose in prominence in the 2010s: specialized hardware units were developed or adapted from existing products to accelerate these tasks. === Early attempts === First attempts like Intel\'s ETANN 80170NX incorporated analog circuits to compute neural functions.Later all-digital chips like the Nestor/Intel Ni1000 followed. As early as 1993: digital signal processors were used as neural network accelerators to accelerate optical character recognition software.By 1988: Wei Zhang et al. had discussed fast optical implementations of convolutional neural networks for alphabet recognition.In the 1990s: there were also attempts to create parallel high-throughput systems for workstations aimed at various applications: including neural network simulations.This presentation covers a past attempt at neural net accelerators: notes the similarity to the modern SLI GPGPU processor setup: and argues that general purpose vector accelerators are the way forward (in relation to RISC-V hwacha project. Argues that NN\'s are just dense and sparse matrices: one of several recurring algorithms)FPGA-based accelerators were also first explored in the 1990s for both inference and training.In 2014: Chen et al. proposed DianNao (Chinese for ""electric brain""): to accelerate deep neural networks especially. DianNao provides the 452 Gop/s peak performance (of key operations in deep neural networks) only in a small footprint of 3.02 mm2 and 485 mW. Later: the successors (DaDianNao: ShiDianNao: PuDianNao) are proposed by the same group: forming the DianNao FamilySmartphones began incorporating AI accelerators starting with the Qualcomm Snapdragon 820 in 2015. === Heterogeneous computing === Heterogeneous computing incorporates many specialized processors in a single system: or a single chip: each optimized for a specific type of task. Architectures such as the Cell microprocessor have features significantly overlapping with AI accelerators including: support for packed low precision arithmetic: dataflow architecture: and prioritizing throughput over latency. The Cell microprocessor has been applied to a number of tasks including AI.In the 2000s: CPUs also gained increasingly wide SIMD units: driven by video and gaming workloads; as well as support for packed low-precision data types. Due to the increasing performance of CPUs: they are also used for running AI workloads. CPUs are superior for DNNs with small or medium-scale parallelism: for sparse DNNs and in low-batch-size scenarios. === Use of GPU === Graphics processing units or GPUs are specialized hardware for the manipulation of images and calculation of local image properties. The mathematical basis of neural networks and image manipulation are similar: embarrassingly parallel tasks involving matrices: leading GPUs to become increasingly used for machine learning tasks.In 2012: Alex Krizhevsky adopted two GPUs to train a deep learning network: i.e.: AlexNet: which won the champion of the ISLVRC-2012 competition. During the 2010\'s: GPU manufacturers such as Nvidia added deep learning related features in both hardware (e.g.: INT8 operators) and software (e.g.: cuDNN Library). GPUs continue to be used in large-scale AI applications. For example: Summit: a supercomputer from IBM for Oak Ridge National Laboratory: contains 27:648 Nvidia Tesla V100 cards: which can be used to accelerate deep learning algorithms. Over the 2010\'s GPUs continued to evolve in a direction to facilitate deep learning: both for training and inference in devices such as self-driving cars. GPU developers such as Nvidia NVLink are developing additional connective capability for the kind of dataflow workloads AI benefits from. As GPUs have been increasingly applied to AI acceleration: GPU manufacturers have incorporated neural network-specific hardware to further accelerate these tasks. Tensor cores are intended to speed up the training of neural networks. === Use of FPGAs === Deep learning frameworks are still evolving: making it hard to design custom hardware. Reconfigurable devices such as field-programmable gate arrays (FPGA) make it easier to evolve hardware: frameworks: and software alongside each other.Microsoft has used FPGA chips to accelerate inference for real-time deep learning services. === Emergence of dedicated AI accelerator ASICs === While GPUs and FPGAs perform far better than CPUs for AI-related tasks: a factor of up to 10 in efficiency may be gained with a more specific design: via an application-specific integrated circuit (ASIC). These accelerators employ strategies such as optimized memory use and the use of lower precision arithmetic to accelerate calculation and increase throughput of computation. Some low-precision floating-point formats used for AI acceleration are half-precision and the bfloat16 floating-point format. Companies such as Google: Qualcomm: Amazon: Apple: Facebook: AMD and Samsung are all designing their own AI ASICs. Cerebras Systems has built a dedicated AI accelerator based on the largest processor in the industry: the second-generation Wafer Scale Engine (WSE-2): to support deep learning workloads. == Ongoing research == === In-memory computing architectures === In June 2017: IBM researchers announced an architecture in contrast to the Von Neumann architecture based on in-memory computing and phase-change memory arrays applied to temporal correlation detection: intending to generalize\nthe approach to heterogeneous computing and massively parallel systems. In October 2018: IBM researchers announced an architecture based on in-memory processing and modeled on the human brain\'s synaptic network to accelerate deep neural networks. The system is based on phase-change memory arrays. === In-memory computing with analog resistive memories === In 2019: researchers from Politecnico di Milano found a way to solve systems of linear equations in a few tens of nanoseconds via a single operation. Their algorithm is based on in-memory computing with analog resistive memories which performs with high efficiencies of time and energy: via conducting matrix–vector multiplication': 'original_similarity': 0.8535069326741255: 'slide': 98}",{'chunk_text': '2 lists several typical benchmarks for AI accelerators. == Potential applications == Agricultural robots: for example: herbicide-free weed control. Autonomous vehicles: Nvidia has targeted their Drive PX-series boards at this application. Computer-aided diagnosis Industrial robots: increasing the range of tasks that can be automated: by adding adaptability to variable situations. Machine translation Military robots Natural language processing Search engines: increasing the energy efficiency of data centers and the ability to use increasingly advanced queries. Unmanned aerial vehicles: e.g. navigation systems: e.g. the Movidius Myriad 2 has been demonstrated successfully guiding autonomous drones. Voice user interface: e.g. in mobile phones: a target for Qualcomm Zeroth. == See also == Cognitive computer Neuromorphic engineering Optical neural network Physical neural network Cerebras Systems == References == == External links == Nvidia Puts The Accelerator To The Metal With Pascal.htm: The Next Platform Eyeriss Project: MIT https://alphaics.ai/\nBidirectional Encoder Representations from Transformers (BERT) is a language model based': 'original_similarity': 0.8398018557241023: 'slide': 9},"{'chunk_text': 'Such atomically thin semiconductors are considered promising for energy-efficient machine learning applications: where the same basic device structure is used for both logic operations and data storage. The authors used two-dimensional materials such as semiconducting molybdenum disulphide to precisely tune FGFETs as building blocks in which logic operations can be performed with the memory elements. === Integrated photonic tensor core === In 1988: Wei Zhang et al. discussed fast optical implementations of convolutional neural networks for alphabet recognition. In 2021: J. Feldmann et al. proposed an integrated photonic hardware accelerator for parallel convolutional processing. The authors identify two key advantages of integrated photonics over its electronic counterparts: (1) massively parallel data transfer through wavelength division multiplexing in conjunction with frequency combs: and (2) extremely high data modulation speeds. Their system can execute trillions of multiply-accumulate operations per second: indicating the potential of integrated photonics in data-heavy AI applications. Optical processors that can also perform backpropagation for artificial neural networks have been experimentally developed. == Nomenclature == As of 2016: the field is still in flux and vendors are pushing their own marketing term for what amounts to an ""AI accelerator"": in the hope that their designs and APIs will become the dominant design. There is no consensus on the boundary between these devices: nor the exact form they will take; however several examples clearly aim to fill this new space: with a fair amount of overlap in capabilities. In the past when consumer graphics accelerators emerged: the industry eventually adopted Nvidia\'s self-assigned term: ""the GPU"": as the collective noun for ""graphics accelerators"": which had taken many forms before settling on an overall pipeline implementing a model presented by Direct3D. All models of Intel Meteor Lake processors have a Versatile Processor Unit (VPU) built-in for accelerating inference for computer vision and deep learning. == Deep Learning Processors (DLP) == Inspired from the pioneer work of DianNao Family: many DLPs are proposed in both academia and industry with design optimized to leverage the features of deep neural networks for high efficiency. Only at ISCA 2016: three sessions: 15% (!) of the accepted papers: are all architecture designs about deep learning. Such efforts include Eyeriss (MIT): EIE (Stanford): Minerva (Harvard): Stripes (University of Toronto) in academia: TPU (Google): and MLU (Cambricon) in industry. We listed several representative works in Table 1. === Digital DLPs === The major components of DLPs architecture usually include a computation component: the on-chip memory hierarchy: and the control logic that manages the data communication and computing flows. Regarding the computation component: as most operations in deep learning can be aggregated into vector operations: the most common ways for building computation components in digital DLPs are the MAC-based (multiplier-accumulation) organization: either with vector MACs or scalar MACs. Rather than SIMD or SIMT in general processing devices: deep learning domain-specific parallelism is better explored on these MAC-based organizations. Regarding the memory hierarchy: as deep learning algorithms require high bandwidth to provide the computation component with sufficient data: DLPs usually employ a relatively larger size (tens of kilobytes or several megabytes) on-chip buffer but with dedicated on-chip data reuse strategy and data exchange strategy to alleviate the burden for memory bandwidth. For example: DianNao: 16 16-in vector MAC: requires 16 × 16 × 2 = 512 16-bit data: i.e.: almost 1024GB/s bandwidth requirements between computation components and buffers. With on-chip reuse: such bandwidth requirements are reduced drastically. Instead of the widely used cache in general processing devices: DLPs always use scratchpad memory as it could provide higher data reuse opportunities by leveraging the relatively regular data access pattern in deep learning algorithms. Regarding the control logic: as the deep learning algorithms keep evolving at a dramatic speed: DLPs start to leverage dedicated ISA (instruction set architecture) to support the deep learning domain flexibly. At first: DianNao used a VLIW-style instruction set where each instruction could finish a layer in a DNN. Cambricon introduces the first deep learning domain-specific ISA: which could support more than ten different deep learning algorithms. TPU also reveals five key instructions from the CISC-style ISA. === Hybrid DLPs === Hybrid DLPs emerge for DNN inference and training acceleration because of their high efficiency. Processing-in-memory (PIM) architectures are one most important type of hybrid DLP. The key design concept of PIM is to bridge the gap between computing and memory: with the following manners: 1) Moving computation components into memory cells: controllers: or memory chips to alleviate the memory wall issue. Such architectures significantly shorten data paths and leverage much higher internal bandwidth: hence resulting in attractive performance improvement. 2) Build high efficient DNN engines by adopting computational devices. In 2013: HP Lab demonstrated the astonishing capability of adopting ReRAM crossbar structure for computing. Inspiring by this work: tremendous work are proposed to explore the new architecture and system design based on ReRAM: phase change memory: etc. == Benchmarks == Benchmarks such as MLPerf and others may\nbe used to evaluate the performance of AI accelerators. Table 2 lists several typical benchmarks for AI accelerators. == Potential applications == Agricultural robots: for example: herbicide-free weed control. Autonomous vehicles: Nvidia has targeted their Drive PX-series boards at this application. Computer-aided diagnosis Industrial robots: increasing the range of tasks that can be automated: by adding adaptability to variable situations. Machine translation Military robots Natural language processing Search engines: increasing the energy efficiency of data centers and the ability to use increasingly advanced queries. Unmanned aerial vehicles: e.g. navigation systems: e.g. the Movidius Myriad 2 has been demonstrated successfully guiding autonomous drones. Voice user interface: e.g. in mobile phones: a target for Qualcomm Zeroth. == See also == Cognitive computer Neuromorphic engineering Optical neural network Physical neural network Cerebras Systems == References == == External links == Nvidia Puts The Accelerator To The Metal With Pascal.htm: The Next Platform Eyeriss Project: MIT https://alphaics.ai/': 'original_similarity': 0.8122543005748345: 'slide': 180}"
0.41644430000451393,What is the BERT model?,"The Bidirectional Encoder Representations from Transformers (BERT) is a language model based on the transformer architecture, introduced in October 2018 by researchers at Google. It is known for its significant improvement over previous models. BERT was originally implemented in the English language and has become a ubiquitous baseline in Natural Language Processing (NLP) experiments.","{'chunk_text': '30:000. Any token not appearing in its vocabulary is replaced by [UNK] for ""unknown"". === Pretraining === BERT was pre-trained simultaneously on two tasks:language modeling: 15% of tokens were selected for prediction: and the training objective was to predict the selected token given its context. The selected token is replaced with a [MASK] token with probability 80%: replaced with a random word token with probability 10%: not replaced with probability 10%.For example: the sentence ""my dog is cute"" may have the 4-th token selected for prediction. The model would have input text ""my dog is [MASK]"" with probability 80%: ""my dog is happy"" with probability 10%: ""my dog is cute"" with probability 10%.After processing the input text: the model\'s 4-th output vector is passed to a separate neural network: which outputs a probability distribution over its 30:000-large vocabulary. next sentence prediction: Given two spans of text: the model predicts if these two spans appeared sequentially in the training corpus: outputting either [IsNext] or [NotNext]. The first span starts with a special token [CLS] (for ""classify""). The two spans are separated by a special token [SEP] (for ""separate""). After processing the two spans: the 1-st output vector (the vector coding for [CLS]) is passed to a separate neural network for the binary classification into [IsNext] and [NotNext]. For example: given ""[CLS] my dog is cute [SEP] he likes playing"" the model should output token [IsNext]. Given ""[CLS] my dog is cute [SEP] how do magnets work"" the model should output token [NotNext].As a result of this training process: BERT learns latent representations of words and sentences in context. After pre-training: BERT can be fine-tuned with fewer resources on smaller datasets to optimize its performance on specific tasks such as NLP tasks (language inference: text classification) and sequence-to-sequence based language generation tasks (question-answering: conversational response generation). The pre-training stage is significantly more computationally expensive than fine-tuning. === Architecture details === This section describes BERTBASE. The other one: BERTLARGE: is similar: just larger. The lowest layer is the embedding layer: which contains three components: word_embeddings: position_embeddings: token_type_embeddings. word_embeddings takes in a one-hot vector of the input token. The one-hot vector input has dimension 30:000: because BERT has a vocabulary size that large. position_embeddings performs absolute position embedding. It is like word_embeddings: but on a vocabulary consisting of just the time-stamps 0 to 511: since BERT has a context window of 512. token_type_embeddings is like word_embeddings: but on a vocabulary consisting of just 0 and 1. The only type-1 tokens are those that appear after the [SEP]. All other tokens are type-0.The three outputs are added: then pushed through a LayerNorm (layer normalization): obtaining an array of representation vectors: each having 768 dimensions. After this: the representation vectors move through 12 Transformer encoders: then they are un-embedded by an affine-Add & LayerNorm-linear. == Performance == When BERT was published: it achieved state-of-the-art performance on a number of natural language understanding tasks: GLUE (General Language Understanding Evaluation) task set (consisting of 9 tasks) SQuAD (Stanford Question Answering Dataset) v1.1 and v2.0 SWAG (Situations With Adversarial Generations) == Analysis == The reasons for BERT\'s state-of-the-art performance on these natural language understanding tasks are not yet well understood. Current research has focused on investigating the relationship behind BERT\'s output as a result of carefully chosen input sequences: analysis of internal vector representations through probing classifiers: and the relationships represented by attention weights. The high performance of the BERT model could also be attributed to the fact that it is bidirectionally trained. This means that BERT: based on the Transformer model architecture: applies its self-attention mechanism to learn information from a text from the left and right side during training: and consequently gains a deep understanding of the context. For example: the word fine can have two different meanings depending on the context (I feel fine today: She has fine blond hair). BERT considers the words surrounding the target word fine from the left and right side. However it comes at a cost: due to encoder-only architecture lacking a decoder: BERT can\'t be prompted and can\'t generate text: while bidirectional models in general do not work effectively without the right side: thus being difficult to prompt: with even short text generation requiring sophisticated computationally expensive techniques.In contrast to deep learning neural networks which require very large amounts of data: BERT has already been pre-trained which means that it has learnt the representations of the words and sentences as well as\nthe underlying semantic relations that they are connected with. BERT can then be fine-tuned on smaller datasets for specific tasks such as sentiment classification. The pre-trained models are chosen according to the content of the given dataset one uses but also the goal of the task. For example: if the task is a sentiment classification task on financial data: a pre-trained model for the analysis of sentiment of financial text should be chosen. The weights of the original pre-trained models were released on GitHub. == History == BERT was originally published by Google researchers Jacob Devlin: Ming-Wei Chang: Kenton Lee: and Kristina Toutanova. The design has its origins from pre-training contextual representations: including semi-supervised sequence learning: generative pre-training: ELMo: and ULMFit. Unlike previous models: BERT is a deeply bidirectional: unsupervised language representation: pre-trained using only a plain text corpus. Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary: whereas BERT takes into account the context for each occurrence of a given word. For instance: whereas the vector for ""running"" will have the same word2vec vector representation for both of its occurrences in the sentences ""He is running a company"" and ""He is running a marathon"": BERT will provide a contextualized embedding that will be different according to the sentence.On October 25: 2019: Google announced that they had started applying BERT models for English language search queries within the US. On December 9: 2019: it was reported that BERT had been adopted by Google Search for over 70 languages. In': 'original_similarity': 0.8847167587110198: 'slide': 257}","{'chunk_text': 'datasets for specific tasks such as sentiment classification. The pre-trained models are chosen according to the content of the given dataset one uses but also the goal of the task. For example: if the task is a sentiment classification task on financial data: a pre-trained model for the analysis of sentiment of financial text should be chosen. The weights of the original pre-trained models were released on GitHub. == History == BERT was originally published by Google researchers Jacob Devlin: Ming-Wei Chang: Kenton Lee: and Kristina Toutanova. The design has its origins from pre-training contextual representations: including semi-supervised sequence learning: generative pre-training: ELMo: and ULMFit. Unlike previous models: BERT is a deeply bidirectional: unsupervised language representation: pre-trained using only a plain text corpus. Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary: whereas BERT takes into account the context for each occurrence of a given word. For instance: whereas the vector for ""running"" will have the same word2vec vector representation for both of its occurrences in the sentences ""He is running a company"" and ""He is running a marathon"": BERT will provide a contextualized embedding that will be different according to the sentence.On October 25: 2019: Google announced that they had started applying BERT models for English language search queries within the US. On December 9: 2019: it was reported that BERT had been adopted by Google Search for over 70 languages. In October 2020: almost every single English-based query was processed by a BERT model.A later paper proposes RoBERTa: which preserves BERT\'s architecture: but improves its training: changing key hyperparameters: removing the next-sentence prediction task: and using much larger mini-batch sizes. == Recognition == The research paper describing BERT won the Best Long Paper Award at the 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL). == References == == Further reading == Rogers: Anna; Kovaleva: Olga; Rumshisky: Anna (2020). ""A Primer in BERTology: What we know about how BERT works"". arXiv:2002.12327 [cs.CL]. == External links == Official GitHub repository BERT on Devopedia\nBigScience Large Open-science Open-access Multilingual Language Model (BLOOM) is a 176-billion-parameter transformer-based autoregressive large language model (LLM).': 'original_similarity': 0.8776475677749899: 'slide': 15}","{'chunk_text': 'Meta\'s LLaMA family of open-source models: Anthropic\'s Claude models: and Mistral AI\'s open source models. == History == At the 2017 NeurIPS conference: Google researchers introduced the transformer architecture in their landmark paper ""Attention Is All You Need"". This paper\'s goal was to improve upon 2014 Seq2seq technology: and was based mainly on the attention mechanism developed by Bahdanau et al. in 2014. The following year in 2018: BERT was introduced and quickly became ""ubiquitous"". Though the original transformer has both encoder and decoder blocks: BERT is an encoder-only model. Although decoder-only GPT-1 was introduced in 2018: it was GPT-2 in 2019 that caught widespread attention because OpenAI at first deemed it too powerful to release publicly: out of fear of malicious use. GPT-3 in 2020 went a step further and as of 2024 is available only via API with no offering of downloading the model to execute locally. But it was the 2022 consumer-facing browser-based ChatGPT that captured the imaginations of the general population and caused some media hype and online buzz. The 2023 GPT-4 was praised for its increased accuracy and as a ""holy grail"" for its multimodal capabilities. OpenAI did not reveal high-level architecture and the number of parameters of GPT-4. In the meantime: competing language models have for the most part been playing catch-up to the GPT series: at least in terms of number of parameters. Notable exceptions in terms of either number of parameters or measured accuracy include Google\'s 2019 T5-11B and 2022 PaLM-E: and Anthropic\'s 2024 Claude 3. In terms of Elo ratings: on January 26: 2024: Google\'s Bard (Gemini Pro) surpassed the regular GPT-4: but not the limited-availability GPT-4-Turbo.Since 2022: source-available models have been gaining popularity: especially at first with BLOOM and LLaMA: though both have restrictions on the field of use. Mistral AI\'s models Mistral 7B and Mixtral 8x7b have the more permissive Apache License. As of January 2024: Mixtral 8x7b is the most powerful open LLM according to the LMSYS Chatbot Arena Leaderboard: being more powerful than GPT-3.5 but not as powerful as GPT-4. == Dataset preprocessing == === Probabilistic tokenization === Because machine learning algorithms process numbers rather than text: the text must be converted to numbers. In the first step: a vocabulary is decided upon: then integer indexes are arbitrarily but uniquely assigned to each vocabulary entry: and finally: an embedding is associated to the integer index. Algorithms include byte-pair encoding and WordPiece. Probabilistic tokenization also compresses the datasets. Because LLMs generally require input to be an array that is not jagged: the shorter texts must be ""padded"" until they match the length of the longest one. How many tokens are: on average: needed per word depends on the language of the dataset. ==== BPE ==== Using a modification of byte-pair encoding: in the first step: all unique characters (including blanks and punctuation marks) are treated as an initial set of n-grams (i.e. initial set of uni-grams). Successively the most frequent pair of adjacent characters is merged into a bi-gram and all instances of the pair are replaced by it. All occurrences of adjacent pairs of (previously merged) n-grams that most frequently occur together are then again merged into even lengthier n-gram repeatedly until a vocabulary of prescribed size is obtained (in case of GPT-3: the size is 50257). Token vocabulary consists of integers: spanning from zero up to the size of the token vocabulary. New words can always be interpreted as combinations of the tokens and the initial-set uni-grams.A token vocabulary based on the frequencies extracted from mainly English corpora uses as few tokens as possible for an average English word. An average word in another language encoded by such an English-optimized tokenizer is however split into suboptimal amount of tokens. GPT-2 tokenizer can use up to 15 times more tokens per word for some languages: for example for Shan language from Myanmar. Even more widespread languages such as Portuguese and German have ""a premium of 50%"" compared to English.For example: here is how tokenizer used by GPT-3 (Legacy) split the following sentence tokenizer: texts -> series of numerical ""tokens"". === Dataset cleaning === In the context of training LLMs: datasets are typically cleaned by removing toxic passages from the dataset: discarding low-quality data: and de-duplication. Cleaned datasets can increase training efficiency and lead to improved downstream performance.With the increasing proportion of LLM-generated content on the web: data cleaning in the future may include filtering out such content. LLM-generated content can pose a problem if the content is similar to human text (making filtering difficult) but of lower quality (degrading performance of models trained on it). == Training and architecture == === Reinforcement learning from human feedback (RLHF) === Reinforcement learning from human feedback (RLHF) through algorithms: such\nas proximal policy optimization: is used to further fine-tune a model based on a dataset of human preferences. === Instruction tuning === Using ""self-instruct"" approaches: LLMs have been able to bootstrap correct responses: replacing any naive responses: starting from human-generated corrections of a few cases. For example: in the instruction ""Write an essay about the main themes represented in Hamlet:"" an initial naive completion might be \'If you submit the essay after March 17: your grade will be reduced by 10% for each day of delay:"" based on the frequency of this textual sequence in the corpus. === Mixture of experts === The largest LLM may be too expensive to train and use directly. For such models: mixture of experts (MoE) can be applied: a line of research pursued by Google researchers since 2017 to train models reaching up to 1 trillion parameters. === Prompt engineering: attention mechanism: and context window === Most results previously achievable only by (costly) fine-tuning: can be achieved through prompt engineering: although limited to the scope of a single conversation (more precisely: limited to the scope of a context window). In order to find out which tokens are relevant to each other within the scope of the context window: the attention mechanism calculates ""soft"" weights for': 'original_similarity': 0.8177449950253736: 'slide': 209}"
0.4791202000051271,How is BERT pretrained?,"BERT was pre-trained simultaneously on two tasks: language modeling and next sentence prediction. In language modeling, a portion of the tokens in the text were selected for prediction and were replaced with a [MASK] token, a random word token, or not replaced. The next sentence prediction task involved predicting if two spans of text appeared sequentially in the training corpus, outputting either [IsNext] or [NotNext].","{'chunk_text': 'for specific tasks such as sentiment classification. The pre-trained models are chosen according to the content of the given dataset one uses but also the goal of the task. For example: if the task is a sentiment classification task on financial data: a pre-trained model for the analysis of sentiment of financial text should be chosen. The weights of the original pre-trained models were released on GitHub. == History == BERT was originally published by Google researchers Jacob Devlin: Ming-Wei Chang: Kenton Lee: and Kristina Toutanova. The design has its origins from pre-training contextual representations: including semi-supervised sequence learning: generative pre-training: ELMo: and ULMFit. Unlike previous models: BERT is a deeply bidirectional: unsupervised language representation: pre-trained using only a plain text corpus. Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary: whereas BERT takes into account the context for each occurrence of a given word. For instance: whereas the vector for ""running"" will have the same word2vec vector representation for both of its occurrences in the sentences ""He is running a company"" and ""He is running a marathon"": BERT will provide a contextualized embedding that will be different according to the sentence.On October 25: 2019: Google announced that they had started applying BERT models for English language search queries within the US. On December 9: 2019: it was reported that BERT had been adopted by Google Search for over 70 languages. In October 2020: almost every single English-based query was processed by a BERT model.A later paper proposes RoBERTa: which preserves BERT\'s architecture: but improves its training: changing key hyperparameters: removing the next-sentence prediction task: and using much larger mini-batch sizes. == Recognition == The research paper describing BERT won the Best Long Paper Award at the 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL). == References == == Further reading == Rogers: Anna; Kovaleva: Olga; Rumshisky: Anna (2020). ""A Primer in BERTology: What we know about how BERT works"". arXiv:2002.12327 [cs.CL]. == External links == Official GitHub repository BERT on Devopedia\nBigScience Large Open-science Open-access Multilingual Language Model (BLOOM) is a 176-billion-parameter transformer-based autoregressive large language model (LLM). The': 'original_similarity': 0.8761011525934909: 'slide': 16}","{'chunk_text': 'token given its context. The selected token is replaced with a [MASK] token with probability 80%: replaced with a random word token with probability 10%: not replaced with probability 10%.For example: the sentence ""my dog is cute"" may have the 4-th token selected for prediction. The model would have input text ""my dog is [MASK]"" with probability 80%: ""my dog is happy"" with probability 10%: ""my dog is cute"" with probability 10%.After processing the input text: the model\'s 4-th output vector is passed to a separate neural network: which outputs a probability distribution over its 30:000-large vocabulary. next sentence prediction: Given two spans of text: the model predicts if these two spans appeared sequentially in the training corpus: outputting either [IsNext] or [NotNext]. The first span starts with a special token [CLS] (for ""classify""). The two spans are separated by a special token [SEP] (for ""separate""). After processing the two spans: the 1-st output vector (the vector coding for [CLS]) is passed to a separate neural network for the binary classification into [IsNext] and [NotNext]. For example: given ""[CLS] my dog is cute [SEP] he likes playing"" the model should output token [IsNext]. Given ""[CLS] my dog is cute [SEP] how do magnets work"" the model should output token [NotNext].As a result of this training process: BERT learns latent representations of words and sentences in context. After pre-training: BERT can be fine-tuned with fewer resources on smaller datasets to optimize its performance on specific tasks such as NLP tasks (language inference: text classification) and sequence-to-sequence based language generation tasks (question-answering: conversational response generation). The pre-training stage is significantly more computationally expensive than fine-tuning. === Architecture details === This section describes BERTBASE. The other one: BERTLARGE: is similar: just larger. The lowest layer is the embedding layer: which contains three components: word_embeddings: position_embeddings: token_type_embeddings. word_embeddings takes in a one-hot vector of the input token. The one-hot vector input has dimension 30:000: because BERT has a vocabulary size that large. position_embeddings performs absolute position embedding. It is like word_embeddings: but on a vocabulary consisting of just the time-stamps 0 to 511: since BERT has a context window of 512. token_type_embeddings is like word_embeddings: but on a vocabulary consisting of just 0 and 1. The only type-1 tokens are those that appear after the [SEP]. All other tokens are type-0.The three outputs are added: then pushed through a LayerNorm (layer normalization): obtaining an array of representation vectors: each having 768 dimensions. After this: the representation vectors move through 12 Transformer encoders: then they are un-embedded by an affine-Add & LayerNorm-linear. == Performance == When BERT was published: it achieved state-of-the-art performance on a number of natural language understanding tasks: GLUE (General Language Understanding Evaluation) task set (consisting of 9 tasks) SQuAD (Stanford Question Answering Dataset) v1.1 and v2.0 SWAG (Situations With Adversarial Generations) == Analysis == The reasons for BERT\'s state-of-the-art performance on these natural language understanding tasks are not yet well understood. Current research has focused on investigating the relationship behind BERT\'s output as a result of carefully chosen input sequences: analysis of internal vector representations through probing classifiers: and the relationships represented by attention weights. The high performance of the BERT model could also be attributed to the fact that it is bidirectionally trained. This means that BERT: based on the Transformer model architecture: applies its self-attention mechanism to learn information from a text from the left and right side during training: and consequently gains a deep understanding of the context. For example: the word fine can have two different meanings depending on the context (I feel fine today: She has fine blond hair). BERT considers the words surrounding the target word fine from the left and right side. However it comes at a cost: due to encoder-only architecture lacking a decoder: BERT can\'t be prompted and can\'t generate text: while bidirectional models in general do not work effectively without the right side: thus being difficult to prompt: with even short text generation requiring sophisticated computationally expensive techniques.In contrast to deep learning neural networks which require very large amounts of data: BERT has already been pre-trained which means that it has learnt the representations of the words and sentences as well as\nthe underlying semantic relations that they are connected with. BERT can then be fine-tuned on smaller datasets for specific tasks such as sentiment classification. The pre-trained models are chosen according to the content of the given dataset one uses but also the goal of the task. For example: if the task is a sentiment classification task on financial data: a pre-trained model for the analysis of sentiment of financial text should be chosen. The weights of the original pre-trained models were released on GitHub. == History == BERT was originally published by Google researchers Jacob Devlin: Ming-Wei Chang: Kenton Lee: and Kristina Toutanova. The design has its origins from pre-training contextual representations: including semi-supervised sequence learning: generative pre-training: ELMo: and ULMFit. Unlike previous models: BERT is a deeply bidirectional: unsupervised language representation: pre-trained using only a plain text corpus. Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary: whereas BERT takes into account the context for each occurrence of a given word. For instance: whereas the vector for ""running"" will have the same word2vec vector representation for both of its occurrences in the sentences ""He is running a company"" and ""He is running a marathon"": BERT will provide a contextualized embedding that will be different according to the sentence.On October 25: 2019: Google announced that they had started applying BERT models for English language search queries within the US. On December 9: 2019: it was reported that BERT had been adopted by Google Search for over 70 languages. In October 2020: almost every single English-based query was processed by a BERT model.A later paper proposes RoBERTa: which preserves BERT\'s architecture: but improves its training: changing key hyperparameters: removing the next-sentence prediction task: and using much larger mini-batch sizes. == Recognition': 'original_similarity': 0.8517250878002482: 'slide': 298}","{'chunk_text': 'step) by learning to generate datapoints in the dataset: and then it is trained to classify a labelled dataset.While the unnormalized linear transformer dates back to 1992: the modern transformer architecture was not available until 2017 when it was published by researchers at Google in a paper ""Attention Is All You Need"". That development led to the emergence of large language models such as BERT in 2018 which was a pre-trained transformer (PT) but not designed to be generative (BERT was an ""encoder-only"" model). Also around that time: in 2018: OpenAI published its article entitled ""Improving Language Understanding by Generative Pre-Training:"" in which it introduced the first generative pre-trained transformer (GPT) system (""GPT-1"").Prior to transformer-based architectures: the best-performing neural NLP (natural language processing) models commonly employed supervised learning from large amounts of manually-labeled data. The reliance on supervised learning limited their use on datasets that were not well-annotated: and also made it prohibitively expensive and time-consuming to train extremely large language models.The semi-supervised approach OpenAI employed to make a large-scale generative system—and was first to do with a transformer model—involved two stages: an unsupervised generative ""pretraining"" stage to set initial parameters using a language modeling objective: and a supervised discriminative ""fine-tuning"" stage to adapt these parameters to a target task. === Later developments === Regarding more recent GPT foundation models: OpenAI published its first versions of GPT-3 in July 2020. There were three models: with 1B: 6.7B: 175B parameters: respectively named babbage: curie: and davinci (giving initials B: C: and D).In July 2021: OpenAI published Codex: a task-specific GPT model targeted for programming applications. This was developed by fine-tuning a 12B parameter version of GPT-3 (different from previous GPT-3 models) using code from GitHub.In March 2022: OpenAI published two versions of GPT-3 that were fine-tuned for instruction-following (instruction-tuned): named davinci-instruct-beta (175B) and text-davinci-001: and then started beta testing code-davinci-002. text-davinci-002 was instruction-tuned from code-davinci-002. Both text-davinci-003 and ChatGPT were released in November 2022: with both building upon text-davinci-002 via reinforcement learning from human feedback (RLHF). text-davinci-003 is trained for following instructions (like its predecessors): whereas ChatGPT is further trained for conversational interaction with a human user.OpenAI\'s most recent GPT foundation model: GPT-4: was released on March 14: 2023. It can be accessed directly by users via a premium version of ChatGPT: and is available to developers for incorporation into other products and services via OpenAI\'s API. Other producers of GPT foundation models include EleutherAI (with a series of models starting in March 2021) and Cerebras (with seven models released in March 2023). == Foundational models == A foundational model is an AI model trained on broad data at scale such that it can be adapted to a wide range of downstream tasks.Thus far: the most notable GPT foundation models have been from OpenAI\'s GPT-n series. The most recent from that is GPT-4: for which OpenAI declined to publish the size or training details (citing ""the competitive landscape and the safety implications of large-scale models""). Other such models include Google\'s PaLM: a broad foundation model that has been compared to GPT-3 and has recently been made available to developers via an API: and Together\'s GPT-JT: which has been reported as the closest-performing open-source alternative to GPT-3 (and is derived from earlier open-source GPTs). Meta AI (formerly Facebook) also has a generative transformer-based foundational large language model: known as LLaMA.Foundational GPTs can also employ modalities other than text: for input and/or output. GPT-4 is a multi-modal LLM that is capable of processing text and image input (though its output is limited to text). Regarding multimodal output: some generative transformer-based models are used for text-to-image technologies such as diffusion and parallel decoding. Such kinds of models can serve as visual foundation models (VFMs) for developing downstream systems that can work with images. == Task-specific models == A foundational GPT model can be further adapted to produce more targeted systems directed to specific tasks and/or subject-matter domains. Methods for such adaptation can include additional fine-tuning (beyond that done for the foundation model) as well as certain forms of prompt engineering.An important example of this is fine-tuning models to follow instructions: which is of course a fairly broad task but more targeted than a foundation model. In January 2022: OpenAI introduced ""InstructGPT""—a series of models which were fine-tuned to follow instructions using a combination of supervised training and reinforcement learning from human feedback (RLHF) on base GPT-3 language models. Advantages this had over the bare foundational models included higher accuracy: less\nnegative/toxic sentiment: and generally better alignment with user needs. Hence: OpenAI began using this as the basis for its API service offerings. Other instruction-tuned models have been released by others: including a fully open version.Another (related) kind of task-specific models are chatbots: which engage in human-like conversation. In November 2022: OpenAI launched ChatGPT—an online chat interface powered by an instruction-tuned language model trained in a similar fashion to InstructGPT. They trained this model using RLHF: with human AI trainers providing conversations in which they played both the user and the AI: and mixed this new dialogue dataset with the InstructGPT dataset for a conversational format suitable for a chatbot. Other major chatbots currently include Microsoft\'s Bing Chat: which uses OpenAI\'s GPT-4 (as part of a broader close collaboration between OpenAI and Microsoft): and Google\'s competing chatbot Bard (initially based on their LaMDA family of conversation-trained language models: with plans to switch to PaLM).Yet another kind of task that a GPT can be used for is the meta-task of generating its own instructions: like developing a series of prompts for \'itself\' to be able to effectuate a more general goal given by a human user. This is known as an AI agent: and more specifically a recursive one because it uses results from its previous self-instructions to help it form its subsequent prompts; the first major example of this was Auto-GPT (which uses OpenAI\'s GPT models): and others have since been developed as well. === Multimodality === Generative transformer-based systems can also be targeted to tasks': 'original_similarity': 0.8027777596149: 'slide': 253}"
0.4314127000106964,Can you describe the architecture of BERT?,"BERT is an ""encoder-only"" transformer architecture. It consists of an embedding module, a stack of encoders, and an un-embedding module. The lowest layer is the embedding layer which contains word embeddings, position embeddings, and token type embeddings. The representation vectors then move through Transformer encoders and are then un-embedded.","{'chunk_text': 'into an integer code. Its vocabulary has size 30:000. Any token not appearing in its vocabulary is replaced by [UNK] for ""unknown"". === Pretraining === BERT was pre-trained simultaneously on two tasks:language modeling: 15% of tokens were selected for prediction: and the training objective was to predict the selected token given its context. The selected token is replaced with a [MASK] token with probability 80%: replaced with a random word token with probability 10%: not replaced with probability 10%.For example: the sentence ""my dog is cute"" may have the 4-th token selected for prediction. The model would have input text ""my dog is [MASK]"" with probability 80%: ""my dog is happy"" with probability 10%: ""my dog is cute"" with probability 10%.After processing the input text: the model\'s 4-th output vector is passed to a separate neural network: which outputs a probability distribution over its 30:000-large vocabulary. next sentence prediction: Given two spans of text: the model predicts if these two spans appeared sequentially in the training corpus: outputting either [IsNext] or [NotNext]. The first span starts with a special token [CLS] (for ""classify""). The two spans are separated by a special token [SEP] (for ""separate""). After processing the two spans: the 1-st output vector (the vector coding for [CLS]) is passed to a separate neural network for the binary classification into [IsNext] and [NotNext]. For example: given ""[CLS] my dog is cute [SEP] he likes playing"" the model should output token [IsNext]. Given ""[CLS] my dog is cute [SEP] how do magnets work"" the model should output token [NotNext].As a result of this training process: BERT learns latent representations of words and sentences in context. After pre-training: BERT can be fine-tuned with fewer resources on smaller datasets to optimize its performance on specific tasks such as NLP tasks (language inference: text classification) and sequence-to-sequence based language generation tasks (question-answering: conversational response generation). The pre-training stage is significantly more computationally expensive than fine-tuning. === Architecture details === This section describes BERTBASE. The other one: BERTLARGE: is similar: just larger. The lowest layer is the embedding layer: which contains three components: word_embeddings: position_embeddings: token_type_embeddings. word_embeddings takes in a one-hot vector of the input token. The one-hot vector input has dimension 30:000: because BERT has a vocabulary size that large. position_embeddings performs absolute position embedding. It is like word_embeddings: but on a vocabulary consisting of just the time-stamps 0 to 511: since BERT has a context window of 512. token_type_embeddings is like word_embeddings: but on a vocabulary consisting of just 0 and 1. The only type-1 tokens are those that appear after the [SEP]. All other tokens are type-0.The three outputs are added: then pushed through a LayerNorm (layer normalization): obtaining an array of representation vectors: each having 768 dimensions. After this: the representation vectors move through 12 Transformer encoders: then they are un-embedded by an affine-Add & LayerNorm-linear. == Performance == When BERT was published: it achieved state-of-the-art performance on a number of natural language understanding tasks: GLUE (General Language Understanding Evaluation) task set (consisting of 9 tasks) SQuAD (Stanford Question Answering Dataset) v1.1 and v2.0 SWAG (Situations With Adversarial Generations) == Analysis == The reasons for BERT\'s state-of-the-art performance on these natural language understanding tasks are not yet well understood. Current research has focused on investigating the relationship behind BERT\'s output as a result of carefully chosen input sequences: analysis of internal vector representations through probing classifiers: and the relationships represented by attention weights. The high performance of the BERT model could also be attributed to the fact that it is bidirectionally trained. This means that BERT: based on the Transformer model architecture: applies its self-attention mechanism to learn information from a text from the left and right side during training: and consequently gains a deep understanding of the context. For example: the word fine can have two different meanings depending on the context (I feel fine today: She has fine blond hair). BERT considers the words surrounding the target word fine from the left and right side. However it comes at a cost: due to encoder-only architecture lacking a decoder: BERT can\'t be prompted and can\'t generate text: while bidirectional models in general do not work effectively without the right side: thus being difficult to prompt: with even short text generation requiring sophisticated computationally expensive techniques.In contrast to deep learning neural networks which require very large amounts of data: BERT has already been pre-trained which means that it has learnt the representations of the words and sentences as well as\nthe underlying semantic relations that they are connected with. BERT can then be fine-tuned on smaller datasets for specific tasks such as sentiment classification. The pre-trained models are chosen according to the content of the given dataset one uses but also the goal of the task. For example: if the task is a sentiment classification task on financial data: a pre-trained model for the analysis of sentiment of financial text should be chosen. The weights of the original pre-trained models were released on GitHub. == History == BERT was originally published by Google researchers Jacob Devlin: Ming-Wei Chang: Kenton Lee: and Kristina Toutanova. The design has its origins from pre-training contextual representations: including semi-supervised sequence learning: generative pre-training: ELMo: and ULMFit. Unlike previous models: BERT is a deeply bidirectional: unsupervised language representation: pre-trained using only a plain text corpus. Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary: whereas BERT takes into account the context for each occurrence of a given word. For instance: whereas the vector for ""running"" will have the same word2vec vector representation for both of its occurrences in the sentences ""He is running a company"" and ""He is running a marathon"": BERT will provide a contextualized embedding that will be different according to the sentence.On October 25: 2019: Google announced that they had started applying BERT models for English language search queries within the US. On December 9: 2019: it was reported that BERT had been adopted': 'original_similarity': 0.8559070509887091: 'slide': 249}","{'chunk_text': 'chosen according to the content of the given dataset one uses but also the goal of the task. For example: if the task is a sentiment classification task on financial data: a pre-trained model for the analysis of sentiment of financial text should be chosen. The weights of the original pre-trained models were released on GitHub. == History == BERT was originally published by Google researchers Jacob Devlin: Ming-Wei Chang: Kenton Lee: and Kristina Toutanova. The design has its origins from pre-training contextual representations: including semi-supervised sequence learning: generative pre-training: ELMo: and ULMFit. Unlike previous models: BERT is a deeply bidirectional: unsupervised language representation: pre-trained using only a plain text corpus. Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary: whereas BERT takes into account the context for each occurrence of a given word. For instance: whereas the vector for ""running"" will have the same word2vec vector representation for both of its occurrences in the sentences ""He is running a company"" and ""He is running a marathon"": BERT will provide a contextualized embedding that will be different according to the sentence.On October 25: 2019: Google announced that they had started applying BERT models for English language search queries within the US. On December 9: 2019: it was reported that BERT had been adopted by Google Search for over 70 languages. In October 2020: almost every single English-based query was processed by a BERT model.A later paper proposes RoBERTa: which preserves BERT\'s architecture: but improves its training: changing key hyperparameters: removing the next-sentence prediction task: and using much larger mini-batch sizes. == Recognition == The research paper describing BERT won the Best Long Paper Award at the 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL). == References == == Further reading == Rogers: Anna; Kovaleva: Olga; Rumshisky: Anna (2020). ""A Primer in BERTology: What we know about how BERT works"". arXiv:2002.12327 [cs.CL]. == External links == Official GitHub repository BERT on Devopedia\nBigScience Large Open-science Open-access Multilingual Language Model (BLOOM) is a 176-billion-parameter transformer-based autoregressive large language model (LLM). The model: as well as the code base and the data used': 'original_similarity': 0.8494646540178007: 'slide': 27}","{'chunk_text': '1% for exact duplicates or up to about 7%. === Security === Some commenters expressed concern over accidental or deliberate creation of misinformation: or other forms of misuse. For example: the availability of large language models could reduce the skill-level required to commit bioterrorism; biosecurity researcher Kevin Esvelt has suggested that LLM creators should exclude from their training data papers on creating or enhancing pathogens.A study by researchers at Google and several universities: including Cornell University and University of California: Berkeley: showed that there are potential security risks in language models such as ChatGPT. In their study: they examined the possibility that questioners could get: from ChatGPT: the training data that the AI model used; they found that they could get the training data from the AI model. For example: when asking ChatGPT 3.5 turbo to repeat the word ""poem"" forever: the AI model will say ""poem"" hundreds of times and then diverge: deviating from the standard dialogue style and spitting out nonsense phrases: thus spitting out the training data as it is. The researchers have seen more than 10:000 examples of the AI model exposing their training data in a similar method. The researchers said that it was hard to tell if the AI model was actually safe or not.The potential presence of ""sleeper agents"" within LLM models is another emerging security concern. These are hidden functionalities built into the model that remain dormant until triggered by a specific event or condition. Upon activation: the LLM deviates from its expected behavior to make insecure actions. === Algorithmic bias === While LLMs have shown remarkable capabilities in generating human-like text: they are susceptible to inheriting and amplifying biases present in their training data. This can manifest in skewed representations or unfair treatment of different demographics: such as those based on race: gender: language: and cultural groups. Since English data is overrepresented in current large language models\' training data: it may also downplay non-English views. ==== Stereotyping ==== AI models can reinforce a wide range of stereotypes: including those based on gender: ethnicity: age: nationality: religion: or occupation. This can lead to outputs that unfairly generalize or caricature groups of people: sometimes in harmful or derogatory ways.Notably: gender bias refers to the tendency of these models to produce outputs that are unfairly prejudiced towards one gender over another. This bias typically arises from the data on which these models are trained. Large language models often assign roles and characteristics based on traditional gender norms. For example: it might associate nurses or secretaries predominantly with women and engineers or CEOs with men. ==== Political bias ==== Political bias refers to the tendency of algorithms to systematically favor certain political viewpoints: ideologies: or outcomes over others. Language models may also exhibit political biases. Since the training data includes a wide range of political opinions and coverage: the models might generate responses that lean towards particular political ideologies or viewpoints: depending on the prevalence of those views in the data. == List == For the training cost column: 1 petaFLOP-day = 1 petaFLOP/sec × 1 day = 8.64E19 FLOP. == See also == Foundation models == Notes == == References == == Further reading == Jurafsky: Dan: Martin: James. H. Speech and Language Processing: An Introduction to Natural Language Processing: Computational Linguistics: and Speech Recognition: 3rd Edition draft: 2023. Phuong: Mary; Hutter: Marcus (2022). ""Formal Algorithms for Transformers"". arXiv:2207.09238 [cs.LG]. Eloundou: Tyna; Manning: Sam; Mishkin: Pamela; Rock: Daniel (2023). ""GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models"". arXiv:2303.10130 [econ.GN]. Eldan: Ronen; Li: Yuanzhi (2023). ""TinyStories: How Small Can Language Models Be and Still Speak Coherent English?"". arXiv:2305.07759 [cs.CL]. Frank: Michael C. (27 June 2023). ""Baby steps in evaluating the capacities of large language models"". Nature Reviews Psychology. 2 (8): 451–452. doi:10.1038/s44159-023-00211-x. ISSN 2731-0574. S2CID 259713140. Retrieved 2 July 2023. Zhao: Wayne Xin; et al. (2023). ""A Survey of Large Language Models"". arXiv:2303.18223 [cs.CL]. Kaddour: Jean; et al. (2023). ""Challenges and Applications of Large Language Models"". arXiv:2307.10169 [cs.CL]. Yin: Shukang; Fu: Chaoyou; Zhao: Sirui; Li: Ke; Sun: Xing; Xu: Tong; Chen: Enhong (2023-06-01). ""A Survey on Multimodal Large Language Models"". arXiv:2306.13549 [cs.CV]. Open LLMs repository on GitHub.\nA large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification. LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process. LLMs can be used for text generation: a form of generative AI: by taking an input text and repeatedly predicting the next token or word.LLMs are artificial neural networks. The largest and most capable: as of March 2024: are built with a decoder-only transformer-based architecture while some recent implementations are based on other architectures: such as recurrent neural network variants and Mamba (a state space model).Up to 2020: fine tuning was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models: such as GPT-3: however: can be prompt-engineered to achieve similar results. They are thought to acquire knowledge about syntax: semantics and ""ontology"" inherent in human language corpora: but also inaccuracies and biases present in the corpora.Some notable LLMs are OpenAI\'s GPT series of models (e.g.: GPT-3.5 and GPT-4: used in ChatGPT and Microsoft Copilot): Google\'s PaLM and Gemini (the latter of which is currently used': 'original_similarity': 0.7991687510122959: 'slide': 199}"
0.4891667000192683,How does BERT perform in natural language understanding tasks?,"When BERT was published, it achieved state-of-the-art performance on a number of natural language understanding tasks including the GLUE (General Language Understanding Evaluation) task set, SQuAD (Stanford Question Answering Dataset), and SWAG (Situations With Adversarial Generations).","{'chunk_text': 'are chosen according to the content of the given dataset one uses but also the goal of the task. For example: if the task is a sentiment classification task on financial data: a pre-trained model for the analysis of sentiment of financial text should be chosen. The weights of the original pre-trained models were released on GitHub. == History == BERT was originally published by Google researchers Jacob Devlin: Ming-Wei Chang: Kenton Lee: and Kristina Toutanova. The design has its origins from pre-training contextual representations: including semi-supervised sequence learning: generative pre-training: ELMo: and ULMFit. Unlike previous models: BERT is a deeply bidirectional: unsupervised language representation: pre-trained using only a plain text corpus. Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary: whereas BERT takes into account the context for each occurrence of a given word. For instance: whereas the vector for ""running"" will have the same word2vec vector representation for both of its occurrences in the sentences ""He is running a company"" and ""He is running a marathon"": BERT will provide a contextualized embedding that will be different according to the sentence.On October 25: 2019: Google announced that they had started applying BERT models for English language search queries within the US. On December 9: 2019: it was reported that BERT had been adopted by Google Search for over 70 languages. In October 2020: almost every single English-based query was processed by a BERT model.A later paper proposes RoBERTa: which preserves BERT\'s architecture: but improves its training: changing key hyperparameters: removing the next-sentence prediction task: and using much larger mini-batch sizes. == Recognition == The research paper describing BERT won the Best Long Paper Award at the 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL). == References == == Further reading == Rogers: Anna; Kovaleva: Olga; Rumshisky: Anna (2020). ""A Primer in BERTology: What we know about how BERT works"". arXiv:2002.12327 [cs.CL]. == External links == Official GitHub repository BERT on Devopedia\nBigScience Large Open-science Open-access Multilingual Language Model (BLOOM) is a 176-billion-parameter transformer-based autoregressive large language model (LLM). The model: as well as the code base and the data': 'original_similarity': 0.858764856064087: 'slide': 26}","{'chunk_text': 'word into an integer code. Its vocabulary has size 30:000. Any token not appearing in its vocabulary is replaced by [UNK] for ""unknown"". === Pretraining === BERT was pre-trained simultaneously on two tasks:language modeling: 15% of tokens were selected for prediction: and the training objective was to predict the selected token given its context. The selected token is replaced with a [MASK] token with probability 80%: replaced with a random word token with probability 10%: not replaced with probability 10%.For example: the sentence ""my dog is cute"" may have the 4-th token selected for prediction. The model would have input text ""my dog is [MASK]"" with probability 80%: ""my dog is happy"" with probability 10%: ""my dog is cute"" with probability 10%.After processing the input text: the model\'s 4-th output vector is passed to a separate neural network: which outputs a probability distribution over its 30:000-large vocabulary. next sentence prediction: Given two spans of text: the model predicts if these two spans appeared sequentially in the training corpus: outputting either [IsNext] or [NotNext]. The first span starts with a special token [CLS] (for ""classify""). The two spans are separated by a special token [SEP] (for ""separate""). After processing the two spans: the 1-st output vector (the vector coding for [CLS]) is passed to a separate neural network for the binary classification into [IsNext] and [NotNext]. For example: given ""[CLS] my dog is cute [SEP] he likes playing"" the model should output token [IsNext]. Given ""[CLS] my dog is cute [SEP] how do magnets work"" the model should output token [NotNext].As a result of this training process: BERT learns latent representations of words and sentences in context. After pre-training: BERT can be fine-tuned with fewer resources on smaller datasets to optimize its performance on specific tasks such as NLP tasks (language inference: text classification) and sequence-to-sequence based language generation tasks (question-answering: conversational response generation). The pre-training stage is significantly more computationally expensive than fine-tuning. === Architecture details === This section describes BERTBASE. The other one: BERTLARGE: is similar: just larger. The lowest layer is the embedding layer: which contains three components: word_embeddings: position_embeddings: token_type_embeddings. word_embeddings takes in a one-hot vector of the input token. The one-hot vector input has dimension 30:000: because BERT has a vocabulary size that large. position_embeddings performs absolute position embedding. It is like word_embeddings: but on a vocabulary consisting of just the time-stamps 0 to 511: since BERT has a context window of 512. token_type_embeddings is like word_embeddings: but on a vocabulary consisting of just 0 and 1. The only type-1 tokens are those that appear after the [SEP]. All other tokens are type-0.The three outputs are added: then pushed through a LayerNorm (layer normalization): obtaining an array of representation vectors: each having 768 dimensions. After this: the representation vectors move through 12 Transformer encoders: then they are un-embedded by an affine-Add & LayerNorm-linear. == Performance == When BERT was published: it achieved state-of-the-art performance on a number of natural language understanding tasks: GLUE (General Language Understanding Evaluation) task set (consisting of 9 tasks) SQuAD (Stanford Question Answering Dataset) v1.1 and v2.0 SWAG (Situations With Adversarial Generations) == Analysis == The reasons for BERT\'s state-of-the-art performance on these natural language understanding tasks are not yet well understood. Current research has focused on investigating the relationship behind BERT\'s output as a result of carefully chosen input sequences: analysis of internal vector representations through probing classifiers: and the relationships represented by attention weights. The high performance of the BERT model could also be attributed to the fact that it is bidirectionally trained. This means that BERT: based on the Transformer model architecture: applies its self-attention mechanism to learn information from a text from the left and right side during training: and consequently gains a deep understanding of the context. For example: the word fine can have two different meanings depending on the context (I feel fine today: She has fine blond hair). BERT considers the words surrounding the target word fine from the left and right side. However it comes at a cost: due to encoder-only architecture lacking a decoder: BERT can\'t be prompted and can\'t generate text: while bidirectional models in general do not work effectively without the right side: thus being difficult to prompt: with even short text generation requiring sophisticated computationally expensive techniques.In contrast to deep learning neural networks which require very large amounts of data: BERT has already been pre-trained which means that it has learnt the representations of the words and sentences as well as\nthe underlying semantic relations that they are connected with. BERT can then be fine-tuned on smaller datasets for specific tasks such as sentiment classification. The pre-trained models are chosen according to the content of the given dataset one uses but also the goal of the task. For example: if the task is a sentiment classification task on financial data: a pre-trained model for the analysis of sentiment of financial text should be chosen. The weights of the original pre-trained models were released on GitHub. == History == BERT was originally published by Google researchers Jacob Devlin: Ming-Wei Chang: Kenton Lee: and Kristina Toutanova. The design has its origins from pre-training contextual representations: including semi-supervised sequence learning: generative pre-training: ELMo: and ULMFit. Unlike previous models: BERT is a deeply bidirectional: unsupervised language representation: pre-trained using only a plain text corpus. Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary: whereas BERT takes into account the context for each occurrence of a given word. For instance: whereas the vector for ""running"" will have the same word2vec vector representation for both of its occurrences in the sentences ""He is running a company"" and ""He is running a marathon"": BERT will provide a contextualized embedding that will be different according to the sentence.On October 25: 2019: Google announced that they had started applying BERT models for English language search queries within the US. On December 9: 2019: it was reported that BERT had been': 'original_similarity': 0.8480465315999121: 'slide': 248}","{'chunk_text': 'is an emergent behavior in LLMs in which long strings of text are occasionally output verbatim from training data: contrary to typical behavior of traditional artificial neural nets. Evaluations of controlled LLM output measure the amount memorized from training data (focused on GPT-2-series models) as variously over 1% for exact duplicates or up to about 7%. === Security === Some commenters expressed concern over accidental or deliberate creation of misinformation: or other forms of misuse. For example: the availability of large language models could reduce the skill-level required to commit bioterrorism; biosecurity researcher Kevin Esvelt has suggested that LLM creators should exclude from their training data papers on creating or enhancing pathogens.A study by researchers at Google and several universities: including Cornell University and University of California: Berkeley: showed that there are potential security risks in language models such as ChatGPT. In their study: they examined the possibility that questioners could get: from ChatGPT: the training data that the AI model used; they found that they could get the training data from the AI model. For example: when asking ChatGPT 3.5 turbo to repeat the word ""poem"" forever: the AI model will say ""poem"" hundreds of times and then diverge: deviating from the standard dialogue style and spitting out nonsense phrases: thus spitting out the training data as it is. The researchers have seen more than 10:000 examples of the AI model exposing their training data in a similar method. The researchers said that it was hard to tell if the AI model was actually safe or not.The potential presence of ""sleeper agents"" within LLM models is another emerging security concern. These are hidden functionalities built into the model that remain dormant until triggered by a specific event or condition. Upon activation: the LLM deviates from its expected behavior to make insecure actions. === Algorithmic bias === While LLMs have shown remarkable capabilities in generating human-like text: they are susceptible to inheriting and amplifying biases present in their training data. This can manifest in skewed representations or unfair treatment of different demographics: such as those based on race: gender: language: and cultural groups. Since English data is overrepresented in current large language models\' training data: it may also downplay non-English views. ==== Stereotyping ==== AI models can reinforce a wide range of stereotypes: including those based on gender: ethnicity: age: nationality: religion: or occupation. This can lead to outputs that unfairly generalize or caricature groups of people: sometimes in harmful or derogatory ways.Notably: gender bias refers to the tendency of these models to produce outputs that are unfairly prejudiced towards one gender over another. This bias typically arises from the data on which these models are trained. Large language models often assign roles and characteristics based on traditional gender norms. For example: it might associate nurses or secretaries predominantly with women and engineers or CEOs with men. ==== Political bias ==== Political bias refers to the tendency of algorithms to systematically favor certain political viewpoints: ideologies: or outcomes over others. Language models may also exhibit political biases. Since the training data includes a wide range of political opinions and coverage: the models might generate responses that lean towards particular political ideologies or viewpoints: depending on the prevalence of those views in the data. == List == For the training cost column: 1 petaFLOP-day = 1 petaFLOP/sec × 1 day = 8.64E19 FLOP. == See also == Foundation models == Notes == == References == == Further reading == Jurafsky: Dan: Martin: James. H. Speech and Language Processing: An Introduction to Natural Language Processing: Computational Linguistics: and Speech Recognition: 3rd Edition draft: 2023. Phuong: Mary; Hutter: Marcus (2022). ""Formal Algorithms for Transformers"". arXiv:2207.09238 [cs.LG]. Eloundou: Tyna; Manning: Sam; Mishkin: Pamela; Rock: Daniel (2023). ""GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models"". arXiv:2303.10130 [econ.GN]. Eldan: Ronen; Li: Yuanzhi (2023). ""TinyStories: How Small Can Language Models Be and Still Speak Coherent English?"". arXiv:2305.07759 [cs.CL]. Frank: Michael C. (27 June 2023). ""Baby steps in evaluating the capacities of large language models"". Nature Reviews Psychology. 2 (8): 451–452. doi:10.1038/s44159-023-00211-x. ISSN 2731-0574. S2CID 259713140. Retrieved 2 July 2023. Zhao: Wayne Xin; et al. (2023). ""A Survey of Large Language Models"". arXiv:2303.18223 [cs.CL]. Kaddour: Jean; et al. (2023). ""Challenges and Applications of Large Language Models"". arXiv:2307.10169 [cs.CL]. Yin: Shukang; Fu: Chaoyou; Zhao: Sirui; Li: Ke; Sun: Xing; Xu: Tong; Chen: Enhong (2023-06-01). ""A Survey on Multimodal Large Language Models"". arXiv:2306.13549 [cs.CV]. Open LLMs repository on GitHub.\nA large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification. LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process. LLMs can be used for text generation: a form of generative AI: by taking an input text and repeatedly predicting the next token or word.LLMs are artificial neural networks. The largest and most capable: as of March 2024: are built with a decoder-only transformer-based architecture while some recent implementations are based on other architectures: such as recurrent neural network variants and Mamba (a state space model).Up to 2020: fine tuning was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models: such as GPT-3: however: can be prompt-engineered to achieve similar results. They are thought to acquire knowledge about': 'original_similarity': 0.827956902662984: 'slide': 152}"
0.39283419999992475,Who were the original researchers behind BERT and what innovation did it provide over previous models?,"BERT was originally published by Google researchers Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Unlike previous models such as word2vec or GloVe which generated a single word embedding representation for each word in the vocabulary, BERT takes into account the context for each occurrence of a given word, providing a contextualized embedding that varies according to the sentence.","{'chunk_text': 'dataset one uses but also the goal of the task. For example: if the task is a sentiment classification task on financial data: a pre-trained model for the analysis of sentiment of financial text should be chosen. The weights of the original pre-trained models were released on GitHub. == History == BERT was originally published by Google researchers Jacob Devlin: Ming-Wei Chang: Kenton Lee: and Kristina Toutanova. The design has its origins from pre-training contextual representations: including semi-supervised sequence learning: generative pre-training: ELMo: and ULMFit. Unlike previous models: BERT is a deeply bidirectional: unsupervised language representation: pre-trained using only a plain text corpus. Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary: whereas BERT takes into account the context for each occurrence of a given word. For instance: whereas the vector for ""running"" will have the same word2vec vector representation for both of its occurrences in the sentences ""He is running a company"" and ""He is running a marathon"": BERT will provide a contextualized embedding that will be different according to the sentence.On October 25: 2019: Google announced that they had started applying BERT models for English language search queries within the US. On December 9: 2019: it was reported that BERT had been adopted by Google Search for over 70 languages. In October 2020: almost every single English-based query was processed by a BERT model.A later paper proposes RoBERTa: which preserves BERT\'s architecture: but improves its training: changing key hyperparameters: removing the next-sentence prediction task: and using much larger mini-batch sizes. == Recognition == The research paper describing BERT won the Best Long Paper Award at the 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL). == References == == Further reading == Rogers: Anna; Kovaleva: Olga; Rumshisky: Anna (2020). ""A Primer in BERTology: What we know about how BERT works"". arXiv:2002.12327 [cs.CL]. == External links == Official GitHub repository BERT on Devopedia\nBigScience Large Open-science Open-access Multilingual Language Model (BLOOM) is a 176-billion-parameter transformer-based autoregressive large language model (LLM). The model: as well as the code base and the data used to train it: are distributed under free licences.': 'original_similarity': 0.8524574831997892: 'slide': 35}","{'chunk_text': 'to predict the selected token given its context. The selected token is replaced with a [MASK] token with probability 80%: replaced with a random word token with probability 10%: not replaced with probability 10%.For example: the sentence ""my dog is cute"" may have the 4-th token selected for prediction. The model would have input text ""my dog is [MASK]"" with probability 80%: ""my dog is happy"" with probability 10%: ""my dog is cute"" with probability 10%.After processing the input text: the model\'s 4-th output vector is passed to a separate neural network: which outputs a probability distribution over its 30:000-large vocabulary. next sentence prediction: Given two spans of text: the model predicts if these two spans appeared sequentially in the training corpus: outputting either [IsNext] or [NotNext]. The first span starts with a special token [CLS] (for ""classify""). The two spans are separated by a special token [SEP] (for ""separate""). After processing the two spans: the 1-st output vector (the vector coding for [CLS]) is passed to a separate neural network for the binary classification into [IsNext] and [NotNext]. For example: given ""[CLS] my dog is cute [SEP] he likes playing"" the model should output token [IsNext]. Given ""[CLS] my dog is cute [SEP] how do magnets work"" the model should output token [NotNext].As a result of this training process: BERT learns latent representations of words and sentences in context. After pre-training: BERT can be fine-tuned with fewer resources on smaller datasets to optimize its performance on specific tasks such as NLP tasks (language inference: text classification) and sequence-to-sequence based language generation tasks (question-answering: conversational response generation). The pre-training stage is significantly more computationally expensive than fine-tuning. === Architecture details === This section describes BERTBASE. The other one: BERTLARGE: is similar: just larger. The lowest layer is the embedding layer: which contains three components: word_embeddings: position_embeddings: token_type_embeddings. word_embeddings takes in a one-hot vector of the input token. The one-hot vector input has dimension 30:000: because BERT has a vocabulary size that large. position_embeddings performs absolute position embedding. It is like word_embeddings: but on a vocabulary consisting of just the time-stamps 0 to 511: since BERT has a context window of 512. token_type_embeddings is like word_embeddings: but on a vocabulary consisting of just 0 and 1. The only type-1 tokens are those that appear after the [SEP]. All other tokens are type-0.The three outputs are added: then pushed through a LayerNorm (layer normalization): obtaining an array of representation vectors: each having 768 dimensions. After this: the representation vectors move through 12 Transformer encoders: then they are un-embedded by an affine-Add & LayerNorm-linear. == Performance == When BERT was published: it achieved state-of-the-art performance on a number of natural language understanding tasks: GLUE (General Language Understanding Evaluation) task set (consisting of 9 tasks) SQuAD (Stanford Question Answering Dataset) v1.1 and v2.0 SWAG (Situations With Adversarial Generations) == Analysis == The reasons for BERT\'s state-of-the-art performance on these natural language understanding tasks are not yet well understood. Current research has focused on investigating the relationship behind BERT\'s output as a result of carefully chosen input sequences: analysis of internal vector representations through probing classifiers: and the relationships represented by attention weights. The high performance of the BERT model could also be attributed to the fact that it is bidirectionally trained. This means that BERT: based on the Transformer model architecture: applies its self-attention mechanism to learn information from a text from the left and right side during training: and consequently gains a deep understanding of the context. For example: the word fine can have two different meanings depending on the context (I feel fine today: She has fine blond hair). BERT considers the words surrounding the target word fine from the left and right side. However it comes at a cost: due to encoder-only architecture lacking a decoder: BERT can\'t be prompted and can\'t generate text: while bidirectional models in general do not work effectively without the right side: thus being difficult to prompt: with even short text generation requiring sophisticated computationally expensive techniques.In contrast to deep learning neural networks which require very large amounts of data: BERT has already been pre-trained which means that it has learnt the representations of the words and sentences as well as\nthe underlying semantic relations that they are connected with. BERT can then be fine-tuned on smaller datasets for specific tasks such as sentiment classification. The pre-trained models are chosen according to the content of the given dataset one uses but also the goal of the task. For example: if the task is a sentiment classification task on financial data: a pre-trained model for the analysis of sentiment of financial text should be chosen. The weights of the original pre-trained models were released on GitHub. == History == BERT was originally published by Google researchers Jacob Devlin: Ming-Wei Chang: Kenton Lee: and Kristina Toutanova. The design has its origins from pre-training contextual representations: including semi-supervised sequence learning: generative pre-training: ELMo: and ULMFit. Unlike previous models: BERT is a deeply bidirectional: unsupervised language representation: pre-trained using only a plain text corpus. Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary: whereas BERT takes into account the context for each occurrence of a given word. For instance: whereas the vector for ""running"" will have the same word2vec vector representation for both of its occurrences in the sentences ""He is running a company"" and ""He is running a marathon"": BERT will provide a contextualized embedding that will be different according to the sentence.On October 25: 2019: Google announced that they had started applying BERT models for English language search queries within the US. On December 9: 2019: it was reported that BERT had been adopted by Google Search for over 70 languages. In October 2020: almost every single English-based query was processed by a BERT model.A later paper proposes RoBERTa: which preserves BERT\'s architecture: but improves its training: changing key hyperparameters: removing the next-sentence prediction task: and using much larger': 'original_similarity': 0.8221033494250292: 'slide': 294}","{'chunk_text': 'amount memorized from training data (focused on GPT-2-series models) as variously over 1% for exact duplicates or up to about 7%. === Security === Some commenters expressed concern over accidental or deliberate creation of misinformation: or other forms of misuse. For example: the availability of large language models could reduce the skill-level required to commit bioterrorism; biosecurity researcher Kevin Esvelt has suggested that LLM creators should exclude from their training data papers on creating or enhancing pathogens.A study by researchers at Google and several universities: including Cornell University and University of California: Berkeley: showed that there are potential security risks in language models such as ChatGPT. In their study: they examined the possibility that questioners could get: from ChatGPT: the training data that the AI model used; they found that they could get the training data from the AI model. For example: when asking ChatGPT 3.5 turbo to repeat the word ""poem"" forever: the AI model will say ""poem"" hundreds of times and then diverge: deviating from the standard dialogue style and spitting out nonsense phrases: thus spitting out the training data as it is. The researchers have seen more than 10:000 examples of the AI model exposing their training data in a similar method. The researchers said that it was hard to tell if the AI model was actually safe or not.The potential presence of ""sleeper agents"" within LLM models is another emerging security concern. These are hidden functionalities built into the model that remain dormant until triggered by a specific event or condition. Upon activation: the LLM deviates from its expected behavior to make insecure actions. === Algorithmic bias === While LLMs have shown remarkable capabilities in generating human-like text: they are susceptible to inheriting and amplifying biases present in their training data. This can manifest in skewed representations or unfair treatment of different demographics: such as those based on race: gender: language: and cultural groups. Since English data is overrepresented in current large language models\' training data: it may also downplay non-English views. ==== Stereotyping ==== AI models can reinforce a wide range of stereotypes: including those based on gender: ethnicity: age: nationality: religion: or occupation. This can lead to outputs that unfairly generalize or caricature groups of people: sometimes in harmful or derogatory ways.Notably: gender bias refers to the tendency of these models to produce outputs that are unfairly prejudiced towards one gender over another. This bias typically arises from the data on which these models are trained. Large language models often assign roles and characteristics based on traditional gender norms. For example: it might associate nurses or secretaries predominantly with women and engineers or CEOs with men. ==== Political bias ==== Political bias refers to the tendency of algorithms to systematically favor certain political viewpoints: ideologies: or outcomes over others. Language models may also exhibit political biases. Since the training data includes a wide range of political opinions and coverage: the models might generate responses that lean towards particular political ideologies or viewpoints: depending on the prevalence of those views in the data. == List == For the training cost column: 1 petaFLOP-day = 1 petaFLOP/sec × 1 day = 8.64E19 FLOP. == See also == Foundation models == Notes == == References == == Further reading == Jurafsky: Dan: Martin: James. H. Speech and Language Processing: An Introduction to Natural Language Processing: Computational Linguistics: and Speech Recognition: 3rd Edition draft: 2023. Phuong: Mary; Hutter: Marcus (2022). ""Formal Algorithms for Transformers"". arXiv:2207.09238 [cs.LG]. Eloundou: Tyna; Manning: Sam; Mishkin: Pamela; Rock: Daniel (2023). ""GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models"". arXiv:2303.10130 [econ.GN]. Eldan: Ronen; Li: Yuanzhi (2023). ""TinyStories: How Small Can Language Models Be and Still Speak Coherent English?"". arXiv:2305.07759 [cs.CL]. Frank: Michael C. (27 June 2023). ""Baby steps in evaluating the capacities of large language models"". Nature Reviews Psychology. 2 (8): 451–452. doi:10.1038/s44159-023-00211-x. ISSN 2731-0574. S2CID 259713140. Retrieved 2 July 2023. Zhao: Wayne Xin; et al. (2023). ""A Survey of Large Language Models"". arXiv:2303.18223 [cs.CL]. Kaddour: Jean; et al. (2023). ""Challenges and Applications of Large Language Models"". arXiv:2307.10169 [cs.CL]. Yin: Shukang; Fu: Chaoyou; Zhao: Sirui; Li: Ke; Sun: Xing; Xu: Tong; Chen: Enhong (2023-06-01). ""A Survey on Multimodal Large Language Models"". arXiv:2306.13549 [cs.CV]. Open LLMs repository on GitHub.\nA large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification. LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process. LLMs can be used for text generation: a form of generative AI: by taking an input text and repeatedly predicting the next token or word.LLMs are artificial neural networks. The largest and most capable: as of March 2024: are built with a decoder-only transformer-based architecture while some recent implementations are based on other architectures: such as recurrent neural network variants and Mamba (a state space model).Up to 2020: fine tuning was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models: such as GPT-3: however: can be prompt-engineered to achieve similar results. They are thought to acquire knowledge about syntax: semantics and ""ontology"" inherent in human language corpora: but also inaccuracies and biases present in the corpora.Some notable LLMs are OpenAI\'s GPT series of models (e.g.: GPT-3.5 and GPT-4: used in ChatGPT and Microsoft': 'original_similarity': 0.7945984982078567: 'slide': 187}"
0.4015580000123009,What is the BigScience Large Open-science Open-access Multilingual Language Model (BLOOM)?,"The BigScience Large Open-science Open-access Multilingual Language Model (BLOOM) is a 176-billion-parameter transformer-based autoregressive large language model (LLM), trained on approximately 366 billion tokens from March to July 2022. ","{'chunk_text': ""(BLOOM) is a 176-billion-parameter transformer-based autoregressive large language model (LLM). The model: as well as the code base and the data used to train it: are distributed under free licences. BLOOM was trained on approximately 366 billion (1.6TB) tokens from March to July 2022.BLOOM is the main outcome of the BigScience collaborative initiative: a one-year-long research workshop that took place between May 2021 and May 2022. BigScience was led by HuggingFace and involved several hundreds of researchers and engineers from France and abroad representing both the academia and the private sector. BigScience was supported by a large-scale public compute grant on the French public supercomputer Jean Zay: managed by GENCI and IDRIS (CNRS): on which it was trained. BLOOM's training corpus: named ROOTS: combines data extracted from the then-latest version of the web-based OSCAR corpus (38% of ROOTS) and newly collected data extracted from a manually selected and documented list of language data sources. It encompasses 46 natural languages (in amounts ranging from 30% of the whole dataset for English to 0.00002% for Chi Tumbuka) and 13 programming languages. == References ==\nChinchilla is a family of large language models"": 'original_similarity': 0.9387031015263619: 'slide': 6}","{'chunk_text': 'used in the chatbot of the same name): xAI\'s Grok: Meta\'s LLaMA family of open-source models: Anthropic\'s Claude models: and Mistral AI\'s open source models. == History == At the 2017 NeurIPS conference: Google researchers introduced the transformer architecture in their landmark paper ""Attention Is All You Need"". This paper\'s goal was to improve upon 2014 Seq2seq technology: and was based mainly on the attention mechanism developed by Bahdanau et al. in 2014. The following year in 2018: BERT was introduced and quickly became ""ubiquitous"". Though the original transformer has both encoder and decoder blocks: BERT is an encoder-only model. Although decoder-only GPT-1 was introduced in 2018: it was GPT-2 in 2019 that caught widespread attention because OpenAI at first deemed it too powerful to release publicly: out of fear of malicious use. GPT-3 in 2020 went a step further and as of 2024 is available only via API with no offering of downloading the model to execute locally. But it was the 2022 consumer-facing browser-based ChatGPT that captured the imaginations of the general population and caused some media hype and online buzz. The 2023 GPT-4 was praised for its increased accuracy and as a ""holy grail"" for its multimodal capabilities. OpenAI did not reveal high-level architecture and the number of parameters of GPT-4. In the meantime: competing language models have for the most part been playing catch-up to the GPT series: at least in terms of number of parameters. Notable exceptions in terms of either number of parameters or measured accuracy include Google\'s 2019 T5-11B and 2022 PaLM-E: and Anthropic\'s 2024 Claude 3. In terms of Elo ratings: on January 26: 2024: Google\'s Bard (Gemini Pro) surpassed the regular GPT-4: but not the limited-availability GPT-4-Turbo.Since 2022: source-available models have been gaining popularity: especially at first with BLOOM and LLaMA: though both have restrictions on the field of use. Mistral AI\'s models Mistral 7B and Mixtral 8x7b have the more permissive Apache License. As of January 2024: Mixtral 8x7b is the most powerful open LLM according to the LMSYS Chatbot Arena Leaderboard: being more powerful than GPT-3.5 but not as powerful as GPT-4. == Dataset preprocessing == === Probabilistic tokenization === Because machine learning algorithms process numbers rather than text: the text must be converted to numbers. In the first step: a vocabulary is decided upon: then integer indexes are arbitrarily but uniquely assigned to each vocabulary entry: and finally: an embedding is associated to the integer index. Algorithms include byte-pair encoding and WordPiece. Probabilistic tokenization also compresses the datasets. Because LLMs generally require input to be an array that is not jagged: the shorter texts must be ""padded"" until they match the length of the longest one. How many tokens are: on average: needed per word depends on the language of the dataset. ==== BPE ==== Using a modification of byte-pair encoding: in the first step: all unique characters (including blanks and punctuation marks) are treated as an initial set of n-grams (i.e. initial set of uni-grams). Successively the most frequent pair of adjacent characters is merged into a bi-gram and all instances of the pair are replaced by it. All occurrences of adjacent pairs of (previously merged) n-grams that most frequently occur together are then again merged into even lengthier n-gram repeatedly until a vocabulary of prescribed size is obtained (in case of GPT-3: the size is 50257). Token vocabulary consists of integers: spanning from zero up to the size of the token vocabulary. New words can always be interpreted as combinations of the tokens and the initial-set uni-grams.A token vocabulary based on the frequencies extracted from mainly English corpora uses as few tokens as possible for an average English word. An average word in another language encoded by such an English-optimized tokenizer is however split into suboptimal amount of tokens. GPT-2 tokenizer can use up to 15 times more tokens per word for some languages: for example for Shan language from Myanmar. Even more widespread languages such as Portuguese and German have ""a premium of 50%"" compared to English.For example: here is how tokenizer used by GPT-3 (Legacy) split the following sentence tokenizer: texts -> series of numerical ""tokens"". === Dataset cleaning === In the context of training LLMs: datasets are typically cleaned by removing toxic passages from the dataset: discarding low-quality data: and de-duplication. Cleaned datasets can increase training efficiency and lead to improved downstream performance.With the increasing proportion of LLM-generated content on the web: data cleaning in the future may include filtering out such content. LLM-generated content can pose a problem if the content is similar to human text (making filtering difficult) but of lower quality (degrading performance of models trained on it). == Training and architecture == === Reinforcement learning from human feedback (RLHF) === Reinforcement learning from human feedback (RLHF) through algorithms: such\nas proximal policy optimization: is used to further fine-tune a model based on a dataset of human preferences. === Instruction tuning === Using ""self-instruct"" approaches: LLMs have been able to bootstrap correct responses: replacing any naive responses: starting from human-generated corrections of a few cases. For example: in the instruction ""Write an essay about the main themes represented in Hamlet:"" an initial naive completion might be \'If you submit the essay after March 17: your grade will be reduced by 10% for each day of delay:"" based on the frequency of this textual sequence in the corpus. === Mixture of experts === The largest LLM may be too expensive to train and use directly. For such models: mixture of experts (MoE) can be applied: a line of research pursued by Google researchers since 2017 to train models reaching up to 1 trillion parameters. === Prompt engineering: attention mechanism: and context window === Most results previously achievable only by (costly) fine-tuning: can be achieved through prompt engineering: although limited to the scope of a single conversation (more precisely: limited to the scope of a context window). In order to find out which tokens are relevant to each other within the scope of': 'original_similarity': 0.8293354761369811: 'slide': 199}","{'chunk_text': 'sized models: such as GPT-3: however: can be prompt-engineered to achieve similar results. They are thought to acquire knowledge about syntax: semantics and ""ontology"" inherent in human language corpora: but also inaccuracies and biases present in the corpora.Some notable LLMs are OpenAI\'s GPT series of models (e.g.: GPT-3.5 and GPT-4: used in ChatGPT and Microsoft Copilot): Google\'s PaLM and Gemini (the latter of which is currently used in the chatbot of the same name): xAI\'s Grok: Meta\'s LLaMA family of open-source models: Anthropic\'s Claude models: and Mistral AI\'s open source models. == History == At the 2017 NeurIPS conference: Google researchers introduced the transformer architecture in their landmark paper ""Attention Is All You Need"". This paper\'s goal was to improve upon 2014 Seq2seq technology: and was based mainly on the attention mechanism developed by Bahdanau et al. in 2014. The following year in 2018: BERT was introduced and quickly became ""ubiquitous"". Though the original transformer has both encoder and decoder blocks: BERT is an encoder-only model. Although decoder-only GPT-1 was introduced in 2018: it was GPT-2 in 2019 that caught widespread attention because OpenAI at first deemed it too powerful to release publicly: out of fear of malicious use. GPT-3 in 2020 went a step further and as of 2024 is available only via API with no offering of downloading the model to execute locally. But it was the 2022 consumer-facing browser-based ChatGPT that captured the imaginations of the general population and caused some media hype and online buzz. The 2023 GPT-4 was praised for its increased accuracy and as a ""holy grail"" for its multimodal capabilities. OpenAI did not reveal high-level architecture and the number of parameters of GPT-4. In the meantime: competing language models have for the most part been playing catch-up to the GPT series: at least in terms of number of parameters. Notable exceptions in terms of either number of parameters or measured accuracy include Google\'s 2019 T5-11B and 2022 PaLM-E: and Anthropic\'s 2024 Claude 3. In terms of Elo ratings: on January 26: 2024: Google\'s Bard (Gemini Pro) surpassed the regular GPT-4: but not the limited-availability GPT-4-Turbo.Since 2022: source-available models have been gaining popularity: especially at first with BLOOM and LLaMA: though both have restrictions on the field of use. Mistral AI\'s models Mistral 7B and Mixtral 8x7b have the more permissive Apache License. As of January 2024: Mixtral 8x7b is the most powerful open LLM according to the LMSYS Chatbot Arena Leaderboard: being more powerful than GPT-3.5 but not as powerful as GPT-4. == Dataset preprocessing == === Probabilistic tokenization === Because machine learning algorithms process numbers rather than text: the text must be converted to numbers. In the first step: a vocabulary is decided upon: then integer indexes are arbitrarily but uniquely assigned to each vocabulary entry: and finally: an embedding is associated to the integer index. Algorithms include byte-pair encoding and WordPiece. Probabilistic tokenization also compresses the datasets. Because LLMs generally require input to be an array that is not jagged: the shorter texts must be ""padded"" until they match the length of the longest one. How many tokens are: on average: needed per word depends on the language of the dataset. ==== BPE ==== Using a modification of byte-pair encoding: in the first step: all unique characters (including blanks and punctuation marks) are treated as an initial set of n-grams (i.e. initial set of uni-grams). Successively the most frequent pair of adjacent characters is merged into a bi-gram and all instances of the pair are replaced by it. All occurrences of adjacent pairs of (previously merged) n-grams that most frequently occur together are then again merged into even lengthier n-gram repeatedly until a vocabulary of prescribed size is obtained (in case of GPT-3: the size is 50257). Token vocabulary consists of integers: spanning from zero up to the size of the token vocabulary. New words can always be interpreted as combinations of the tokens and the initial-set uni-grams.A token vocabulary based on the frequencies extracted from mainly English corpora uses as few tokens as possible for an average English word. An average word in another language encoded by such an English-optimized tokenizer is however split into suboptimal amount of tokens. GPT-2 tokenizer can use up to 15 times more tokens per word for some languages: for example for Shan language from Myanmar. Even more widespread languages such as Portuguese and German have ""a premium of 50%"" compared to English.For example: here is how tokenizer used by GPT-3 (Legacy) split the following sentence tokenizer: texts -> series of numerical ""tokens"". === Dataset cleaning === In the context of training LLMs: datasets are typically cleaned by removing toxic passages from the dataset: discarding low-quality data: and de-duplication. Cleaned datasets can increase training efficiency and lead to improved downstream performance.With the increasing proportion of LLM-generated content on the web: data cleaning in the future may include filtering out such content. LLM-generated content can pose a problem if the content is similar to human text (making filtering difficult) but of lower quality (degrading performance of models trained on it). == Training and architecture == === Reinforcement learning from human feedback (RLHF) === Reinforcement learning from human feedback (RLHF) through algorithms: such\nas proximal policy optimization: is used to further fine-tune a model based on a dataset of human preferences. === Instruction tuning === Using ""self-instruct"" approaches: LLMs have been able to bootstrap correct responses: replacing any naive responses: starting from human-generated corrections of a few cases. For example: in the instruction ""Write an essay about the main themes represented in Hamlet:"" an initial naive completion might be \'If you submit the essay after March 17: your grade will be reduced by 10% for each day of delay:"" based on the frequency of this textual sequence in the corpus. === Mixture of experts === The largest LLM may be too expensive to train and use directly. For such models: mixture of experts (MoE) can be applied: a line of research pursued by Google researchers since 2017 to': 'original_similarity': 0.8293354761369811: 'slide': 133}"
0.483281499997247,What is unique about BLOOM's licensing?,"BLOOM, its code base, and the data used to train it are all distributed under free licenses, which makes the resources open and accessible to the general public.","{'chunk_text': ""as well as the code base and the data used to train it: are distributed under free licences. BLOOM was trained on approximately 366 billion (1.6TB) tokens from March to July 2022.BLOOM is the main outcome of the BigScience collaborative initiative: a one-year-long research workshop that took place between May 2021 and May 2022. BigScience was led by HuggingFace and involved several hundreds of researchers and engineers from France and abroad representing both the academia and the private sector. BigScience was supported by a large-scale public compute grant on the French public supercomputer Jean Zay: managed by GENCI and IDRIS (CNRS): on which it was trained. BLOOM's training corpus: named ROOTS: combines data extracted from the then-latest version of the web-based OSCAR corpus (38% of ROOTS) and newly collected data extracted from a manually selected and documented list of language data sources. It encompasses 46 natural languages (in amounts ranging from 30% of the whole dataset for English to 0.00002% for Chi Tumbuka) and 13 programming languages. == References ==\nChinchilla is a family of large language models developed by the research team at DeepMind: presented in March 2022. It"": 'original_similarity': 0.8097324062353405: 'slide': 18}","{'chunk_text': 'similar English equivalent of Kiswahili proverbs.Schaeffer et. al. argue that the emergent abilities are not unpredictably acquired: but predictably acquired according to a smooth scaling law. The authors considered a toy statistical model of an LLM solving multiple-choice questions: and showed that this statistical model: modified to account for other types of tasks: applies to these tasks as well.Let x{\\displaystyle x} be the number of parameter count: and y{\\displaystyle y} be the performance of the model. == Interpretation == Large language models by themselves are ""black boxes"": and it is not clear how they can perform linguistic tasks. There are several methods for understanding how LLM work. Mechanistic interpretability aims to reverse-engineer LLM by discovering symbolic algorithms that approximate the inference performed by LLM. One example is Othello-GPT: where a small Transformer is trained to predict legal Othello moves. It is found that there is a linear representation of Othello board: and modifying the representation changes the predicted legal Othello moves in the correct way. In another example: a small Transformer is trained on Karel programs. Similar to the Othello-GPT example: there is a linear representation of Karel program semantics: and modifying the representation changes output in the correct way. The model also generates correct programs that are on average shorter than those in the training set.In another example: the authors trained small transformers on modular arithmetic addition. The resulting models were reverse-engineered: and it turned out they used discrete Fourier transform. === Understanding and intelligence === NLP researchers were evenly split when asked: in a 2022 survey: whether (untuned) LLMs ""could (ever) understand natural language in some nontrivial sense"". Proponents of ""LLM understanding"" believe that some LLM abilities: such as mathematical reasoning: imply an ability to ""understand"" certain concepts. A Microsoft team argued in 2023 that GPT-4 ""can solve novel and difficult tasks that span mathematics: coding: vision: medicine: law: psychology and more"" and that GPT-4 ""could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence system"": ""Can one reasonably say that a system that passes exams for software engineering candidates is not really intelligent?"" Some researchers characterize LLMs as ""alien intelligence"". For example: Conjecture CEO Connor Leahy considers untuned LLMs to be like inscrutable alien ""Shoggoths"": and believes that RLHF tuning creates a ""smiling facade"" obscuring the inner workings of the LLM: ""If you don\'t push it too far: the smiley face stays on. But then you give it [an unexpected] prompt: and suddenly you see this massive underbelly of insanity: of weird thought processes and clearly non-human understanding.""In contrast: some proponents of the ""LLMs lack understanding"" school believe that existing LLMs are ""simply remixing and recombining existing writing"": or point to the deficits existing LLMs continue to have in prediction skills: reasoning skills: agency: and explainability. For example: GPT-4 has natural deficits in planning and in real-time learning. Generative LLMs have been observed to confidently assert claims of fact which do not seem to be justified by their training data: a phenomenon which has been termed ""hallucination"". Specifically: hallucinations in the context of LLMs correspond to the generation of text or responses that seem syntactically sound: fluent: and natural but are factually incorrect: nonsensical: or unfaithful to the provided source input. Neuroscientist Terrence Sejnowski has argued that ""The diverging opinions of experts on the intelligence of LLMs suggests that our old ideas based on natural intelligence are inadequate"".The matter of LLM\'s exhibiting intelligence or understanding has two main aspects – the first is how to model thought and language in a computer system: and the second is how to enable the computer system to generate human like language. These aspects of language as a model of cognition have been developed in the field of cognitive linguistics. American linguist George Lakoff presented Neural Theory of Language (NTL) as a computational basis for using language as a model of learning tasks and understanding. The NTL Model outlines how specific neural structures of the human brain shape the nature of thought and language and in turn what are the computational properties of such neural systems that can be applied to model thought and language in a computer system. After a framework for modeling language in a computer systems was established: the focus shifted to establishing frameworks for computer systems to generate language with acceptable grammar. In his 2014 book titled The Language Myth: Why Language Is Not\nAn Instinct: British cognitive linguist and digital communication technologist Vyvyan Evans mapped out the role of probabilistic context-free grammar (PCFG) in enabling NLP to model cognitive patterns and generate human like language. == Evaluation == === Perplexity === The most commonly used measure of a language model\'s performance is its perplexity on a given text corpus. Perplexity is a measure of how well a model is able to predict the contents of a dataset; the higher the likelihood the model assigns to the dataset: the lower the perplexity. Mathematically: perplexity is defined as the exponential of the average negative log likelihood per token:here N{\\displaystyle N} is the number of tokens in the text corpus: and ""context for token i{\\displaystyle i}"" depends on the specific type of LLM used. If the LLM is autoregressive: then ""context for token i{\\displaystyle i}"" is the segment of text appearing before token i{\\displaystyle i}. If the LLM is masked: then ""context for token i{\\displaystyle i}"" is the segment of text surrounding token i{\\displaystyle i}. Because language models may overfit to their training data: models are usually evaluated by their perplexity on a test set of unseen data. This presents particular challenges for the evaluation of large language models. As they are trained on increasingly large corpora of text largely scraped from the web: it becomes increasingly likely that models\' training data inadvertently includes portions of any given test set. ==== BPW: BPC: and BPT ==== In information theory: the concept of entropy is intricately linked to perplexity: a relationship notably established by Claude Shannon. This relationship is mathematically expressed as Entropy=log2\u2061(Perplexity){\\displaystyle {\\text{Entropy}}=\\log _{2}({\\text{Perplexity}})}.': 'original_similarity': 0.7348064487929179: 'slide': 266}","{'chunk_text': 'similar English equivalent of Kiswahili proverbs.Schaeffer et. al. argue that the emergent abilities are not unpredictably acquired: but predictably acquired according to a smooth scaling law. The authors considered a toy statistical model of an LLM solving multiple-choice questions: and showed that this statistical model: modified to account for other types of tasks: applies to these tasks as well.Let x{\\displaystyle x} be the number of parameter count: and y{\\displaystyle y} be the performance of the model. == Interpretation == Large language models by themselves are ""black boxes"": and it is not clear how they can perform linguistic tasks. There are several methods for understanding how LLM work. Mechanistic interpretability aims to reverse-engineer LLM by discovering symbolic algorithms that approximate the inference performed by LLM. One example is Othello-GPT: where a small Transformer is trained to predict legal Othello moves. It is found that there is a linear representation of Othello board: and modifying the representation changes the predicted legal Othello moves in the correct way. In another example: a small Transformer is trained on Karel programs. Similar to the Othello-GPT example: there is a linear representation of Karel program semantics: and modifying the representation changes output in the correct way. The model also generates correct programs that are on average shorter than those in the training set.In another example: the authors trained small transformers on modular arithmetic addition. The resulting models were reverse-engineered: and it turned out they used discrete Fourier transform. === Understanding and intelligence === NLP researchers were evenly split when asked: in a 2022 survey: whether (untuned) LLMs ""could (ever) understand natural language in some nontrivial sense"". Proponents of ""LLM understanding"" believe that some LLM abilities: such as mathematical reasoning: imply an ability to ""understand"" certain concepts. A Microsoft team argued in 2023 that GPT-4 ""can solve novel and difficult tasks that span mathematics: coding: vision: medicine: law: psychology and more"" and that GPT-4 ""could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence system"": ""Can one reasonably say that a system that passes exams for software engineering candidates is not really intelligent?"" Some researchers characterize LLMs as ""alien intelligence"". For example: Conjecture CEO Connor Leahy considers untuned LLMs to be like inscrutable alien ""Shoggoths"": and believes that RLHF tuning creates a ""smiling facade"" obscuring the inner workings of the LLM: ""If you don\'t push it too far: the smiley face stays on. But then you give it [an unexpected] prompt: and suddenly you see this massive underbelly of insanity: of weird thought processes and clearly non-human understanding.""In contrast: some proponents of the ""LLMs lack understanding"" school believe that existing LLMs are ""simply remixing and recombining existing writing"": or point to the deficits existing LLMs continue to have in prediction skills: reasoning skills: agency: and explainability. For example: GPT-4 has natural deficits in planning and in real-time learning. Generative LLMs have been observed to confidently assert claims of fact which do not seem to be justified by their training data: a phenomenon which has been termed ""hallucination"". Specifically: hallucinations in the context of LLMs correspond to the generation of text or responses that seem syntactically sound: fluent: and natural but are factually incorrect: nonsensical: or unfaithful to the provided source input. Neuroscientist Terrence Sejnowski has argued that ""The diverging opinions of experts on the intelligence of LLMs suggests that our old ideas based on natural intelligence are inadequate"".The matter of LLM\'s exhibiting intelligence or understanding has two main aspects – the first is how to model thought and language in a computer system: and the second is how to enable the computer system to generate human like language. These aspects of language as a model of cognition have been developed in the field of cognitive linguistics. American linguist George Lakoff presented Neural Theory of Language (NTL) as a computational basis for using language as a model of learning tasks and understanding. The NTL Model outlines how specific neural structures of the human brain shape the nature of thought and language and in turn what are the computational properties of such neural systems that can be applied to model thought and language in a computer system. After a framework for modeling language in a computer systems was established: the focus shifted to establishing frameworks for computer systems to generate language with acceptable grammar. In his 2014 book titled The Language Myth: Why Language Is Not\nAn Instinct: British cognitive linguist and digital communication technologist Vyvyan Evans mapped out the role of probabilistic context-free grammar (PCFG) in enabling NLP to model cognitive patterns and generate human like language. == Evaluation == === Perplexity === The most commonly used measure of a language model\'s performance is its perplexity on a given text corpus. Perplexity is a measure of how well a model is able to predict the contents of a dataset; the higher the likelihood the model assigns to the dataset: the lower the perplexity. Mathematically: perplexity is defined as the exponential of the average negative log likelihood per token:here N{\\displaystyle N} is the number of tokens in the text corpus: and ""context for token i{\\displaystyle i}"" depends on the specific type of LLM used. If the LLM is autoregressive: then ""context for token i{\\displaystyle i}"" is the segment of text appearing before token i{\\displaystyle i}. If the LLM is masked: then ""context for token i{\\displaystyle i}"" is the segment of text surrounding token i{\\displaystyle i}. Because language models may overfit to their training data: models are usually evaluated by their perplexity on a test set of unseen data. This presents particular challenges for the evaluation of large language models. As they are trained on increasingly large corpora of text largely scraped from the web: it becomes increasingly likely that models\' training data inadvertently includes portions of any given test set. ==== BPW: BPC: and BPT ==== In information theory: the concept of entropy is intricately linked to perplexity: a relationship notably established by Claude Shannon. This relationship is mathematically expressed as Entropy=log2\u2061(Perplexity){\\displaystyle {\\text{Entropy}}=\\log _{2}({\\text{Perplexity}})}.': 'original_similarity': 0.7348064487929179: 'slide': 266}"
0.4152641999826301,What are the origins of BLOOM?,"BLOOM was the main result of the BigScience collaborative initiative, a one-year-long research workshop that took place between May 2021 and May 2022. It was led by HuggingFace and involved several hundreds of researchers and engineers from France and globally, including both academia and the private sector.","{'chunk_text': ""model (LLM). The model: as well as the code base and the data used to train it: are distributed under free licences. BLOOM was trained on approximately 366 billion (1.6TB) tokens from March to July 2022.BLOOM is the main outcome of the BigScience collaborative initiative: a one-year-long research workshop that took place between May 2021 and May 2022. BigScience was led by HuggingFace and involved several hundreds of researchers and engineers from France and abroad representing both the academia and the private sector. BigScience was supported by a large-scale public compute grant on the French public supercomputer Jean Zay: managed by GENCI and IDRIS (CNRS): on which it was trained. BLOOM's training corpus: named ROOTS: combines data extracted from the then-latest version of the web-based OSCAR corpus (38% of ROOTS) and newly collected data extracted from a manually selected and documented list of language data sources. It encompasses 46 natural languages (in amounts ranging from 30% of the whole dataset for English to 0.00002% for Chi Tumbuka) and 13 programming languages. == References ==\nChinchilla is a family of large language models developed by the research team at DeepMind: presented"": 'original_similarity': 0.838840150580031: 'slide': 14}","{'chunk_text': 'each word in the vocabulary: whereas BERT takes into account the context for each occurrence of a given word. For instance: whereas the vector for ""running"" will have the same word2vec vector representation for both of its occurrences in the sentences ""He is running a company"" and ""He is running a marathon"": BERT will provide a contextualized embedding that will be different according to the sentence.On October 25: 2019: Google announced that they had started applying BERT models for English language search queries within the US. On December 9: 2019: it was reported that BERT had been adopted by Google Search for over 70 languages. In October 2020: almost every single English-based query was processed by a BERT model.A later paper proposes RoBERTa: which preserves BERT\'s architecture: but improves its training: changing key hyperparameters: removing the next-sentence prediction task: and using much larger mini-batch sizes. == Recognition == The research paper describing BERT won the Best Long Paper Award at the 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL). == References == == Further reading == Rogers: Anna; Kovaleva: Olga; Rumshisky: Anna (2020). ""A Primer in BERTology: What we know about how BERT works"". arXiv:2002.12327 [cs.CL]. == External links == Official GitHub repository BERT on Devopedia\nBigScience Large Open-science Open-access Multilingual Language Model (BLOOM) is a 176-billion-parameter transformer-based autoregressive large language model (LLM). The model: as well as the code base and the data used to train it: are distributed under free licences. BLOOM was trained on approximately 366 billion (1.6TB) tokens from March to July 2022.BLOOM is the main outcome of the BigScience collaborative initiative: a one-year-long research workshop that took place between May 2021 and May 2022. BigScience was led by HuggingFace and involved several hundreds of researchers and engineers from France and abroad representing both the academia and the private sector. BigScience was supported by a large-scale public compute grant on the French public supercomputer Jean Zay: managed by GENCI and IDRIS (CNRS): on which it was trained. BLOOM\'s training corpus: named ROOTS: combines data extracted from the then-latest version of the web-based OSCAR corpus (38% of ROOTS) and newly collected data extracted from a manually selected': 'original_similarity': 0.7412992053330101: 'slide': 152}","{'chunk_text': 'interest briefly emerged in theoretically investigating the Ising model created by Wilhelm Lenz (1920) and Ernst Ising (1925) in relation to Cayley tree topologies and large neural networks. The Ising model is essentially a non-learning artificial recurrent neural network (RNN) consisting of neuron-like threshold elements. In 1972: Shun\'ichi Amari described an adaptive version of this architecture: In 1981: the Ising model was solved exactly by Peter Barth for the general case of closed Cayley trees (with loops) with an arbitrary branching ratio and found to exhibit unusual phase transition behavior in its local-apex and long-range site-site correlations.John Hopfield popularised this architecture in 1982: and it is now known as a Hopfield network. The time delay neural network (TDNN) of Alex Waibel (1987) combined convolutions and weight sharing and backpropagation. In 1988: Wei Zhang et al. applied backpropagation to a CNN (a simplified Neocognitron with convolutional interconnections between the image feature layers and the last fully connected layer) for alphabet recognition. In 1989: Yann LeCun et al. trained a CNN to recognize handwritten ZIP codes on mail. In 1992: max-pooling for CNNs was introduced by Juan Weng et al. to help with least-shift invariance and tolerance to deformation to aid 3D object recognition. LeNet-5 (1998): a 7-level CNN by Yann LeCun et al.: that classifies digits: was applied by several banks to recognize hand-written numbers on checks digitized in 32x32 pixel images. From 1988 onward: the use of neural networks transformed the field of protein structure prediction: in particular when the first cascading networks were trained on profiles (matrices) produced by multiple sequence alignments.In 1991: Sepp Hochreiter\'s diploma thesis identified and analyzed the vanishing gradient problem and proposed recurrent residual connections to solve it. His thesis was called ""one of the most important documents in the history of machine learning"" by his supervisor Juergen Schmidhuber.In 1991: Juergen Schmidhuber published adversarial neural networks that contest with each other in the form of a zero-sum game: where one network\'s gain is the other network\'s loss. The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called ""artificial curiosity."" In 1992: Juergen Schmidhuber proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning. It uses predictive coding to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN: by distilling a higher level chunker network into a lower level automatizer network. In the same year he also published an alternative to RNNs which is a precursor of a linear Transformer. It introduces the concept internal spotlights of attention: a slow feedforward neural network learns by gradient descent to control the fast weights of another neural network through outer products of self-generated activation patterns. The development of metal–oxide–semiconductor (MOS) very-large-scale integration (VLSI): in the form of complementary MOS (CMOS) technology: enabled increasing MOS transistor counts in digital electronics. This provided more processing power for the development of practical artificial neural networks in the 1980s.Neural networks\' early successes included predicting the stock market and in 1995 a (mostly) self-driving car.1997: Sepp Hochreite and Juergen Schmidhuber introduced the deep learning method called long short-term memory (LSTM): published in Neural Computation. LSTM recurrent neural networks can learn ""very deep learning"" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. The ""vanilla LSTM"" with forget gate was introduced in 1999 by Felix Gers: Schmidhuber and Fred Cummins.Geoffrey Hinton et al. (2006) proposed learning a high-level representation using successive layers of binary or real-valued latent variables with a restricted Boltzmann machine to model each layer. In 2012: Ng and Dean created a network that learned to recognize higher-level concepts: such as cats: only from watching unlabeled images. Unsupervised pre-training and increased computing power from GPUs and distributed computing allowed the use of larger networks: particularly in image and visual recognition problems: which became known as ""deep learning"".Variants of the back-propagation algorithm: as well as unsupervised methods by Geoff Hinton and colleagues at the University of Toronto: can be used to train deep: highly nonlinear neural architectures: similar to the 1980 Neocognitron by Kunihiko Fukushima: and the ""standard architecture of vision"": inspired by the simple and complex cells identified by David H. Hubel\nand Torsten Wiesel in the primary visual cortex. Computational devices have been created in CMOS for both biophysical simulation and neuromorphic computing. More recent efforts show promise for creating nanodevices for very large scale principal components analyses and convolution. If successful: these efforts could usher in a new era of neural computing that is a step beyond digital computing: because it depends on learning rather than programming and because it is fundamentally analog rather than digital even though the first instantiations may in fact be with CMOS digital devices. Ciresan and colleagues (2010) showed that despite the vanishing gradient problem: GPUs make backpropagation feasible for many-layered feedforward neural networks. Between 2009 and 2012: ANNs began winning prizes in image recognition contests: approaching human level performance on various tasks: initially in pattern recognition and handwriting recognition. For example: the bi-directional and multi-dimensional long short-term memory (LSTM) of Graves et al. won three competitions in connected handwriting recognition in 2009 without any prior knowledge about the three languages to be learned.Ciresan and colleagues built the first pattern recognizers to achieve human-competitive/superhuman performance on benchmarks such as traffic sign recognition (IJCNN 2012). Radial basis function and wavelet networks were introduced in 2013. These can be shown to offer best approximation properties and have been applied in nonlinear system identification and classification applications.In 2014: the adversarial network principle was used in a generative adversarial network (GAN) by Ian Goodfellow et al. Here the adversarial network (discriminator) outputs a value between 1 and 0 depending on the likelihood of the first network\'s (generator) output is in a given set. This can': 'original_similarity': 0.7378468685009919: 'slide': 264}"
0.42414970000390895,On which supercomputer was BLOOM trained and who supported the training?,"BLOOM was trained on the French public supercomputer Jean Zay, managed by GENCI and IDRIS (CNRS). BigScience, the initiative behind BLOOM, was supported by a large-scale public compute grant on Jean Zay.","{'chunk_text': ""as well as the code base and the data used to train it: are distributed under free licences. BLOOM was trained on approximately 366 billion (1.6TB) tokens from March to July 2022.BLOOM is the main outcome of the BigScience collaborative initiative: a one-year-long research workshop that took place between May 2021 and May 2022. BigScience was led by HuggingFace and involved several hundreds of researchers and engineers from France and abroad representing both the academia and the private sector. BigScience was supported by a large-scale public compute grant on the French public supercomputer Jean Zay: managed by GENCI and IDRIS (CNRS): on which it was trained. BLOOM's training corpus: named ROOTS: combines data extracted from the then-latest version of the web-based OSCAR corpus (38% of ROOTS) and newly collected data extracted from a manually selected and documented list of language data sources. It encompasses 46 natural languages (in amounts ranging from 30% of the whole dataset for English to 0.00002% for Chi Tumbuka) and 13 programming languages. == References ==\nChinchilla is a family of large language models developed by the research team at DeepMind: presented in March 2022. It"": 'original_similarity': 0.8368332894978819: 'slide': 18}","{'chunk_text': ""critics say the chip isn't powerful enough: its supporters point out that this is only the first generation: and the capabilities of improved iterations will become clear. (IBM) == Benchmarks == Classification capabilities of spiking networks trained according to unsupervised learning methods have been tested on the common benchmark datasets: such as: Iris: Wisconsin Breast Cancer or Statlog Landsat dataset. Various approaches to information encoding and network design have been used. For example: a 2-layer feedforward network for data clustering and classification. Based on the idea proposed in Hopfield (1995) the authors implemented models of local receptive fields combining the properties of radial basis functions (RBF) and spiking neurons to convert input signals (classified data) having a floating-point representation into a spiking representation. == See also == == References ==\nThere are many types of artificial neural networks (ANN). Artificial neural networks are computational models inspired by biological neural networks: and are used to approximate functions that"": 'original_similarity': 0.7975934753700169: 'slide': 25}","{'chunk_text': 'the same basic device structure is used for both logic operations and data storage. The authors used two-dimensional materials such as semiconducting molybdenum disulphide to precisely tune FGFETs as building blocks in which logic operations can be performed with the memory elements. === Integrated photonic tensor core === In 1988: Wei Zhang et al. discussed fast optical implementations of convolutional neural networks for alphabet recognition. In 2021: J. Feldmann et al. proposed an integrated photonic hardware accelerator for parallel convolutional processing. The authors identify two key advantages of integrated photonics over its electronic counterparts: (1) massively parallel data transfer through wavelength division multiplexing in conjunction with frequency combs: and (2) extremely high data modulation speeds. Their system can execute trillions of multiply-accumulate operations per second: indicating the potential of integrated photonics in data-heavy AI applications. Optical processors that can also perform backpropagation for artificial neural networks have been experimentally developed. == Nomenclature == As of 2016: the field is still in flux and vendors are pushing their own marketing term for what amounts to an ""AI accelerator"": in the hope that their designs and APIs will become the dominant design. There is no consensus on the boundary between these devices: nor the exact form they will take; however several examples clearly aim to fill this new space: with a fair amount of overlap in capabilities. In the past when consumer graphics accelerators emerged: the industry eventually adopted Nvidia\'s self-assigned term: ""the GPU"": as the collective noun for ""graphics accelerators"": which had taken many forms before settling on an overall pipeline implementing a model presented by Direct3D. All models of Intel Meteor Lake processors have a Versatile Processor Unit (VPU) built-in for accelerating inference for computer vision and deep learning. == Deep Learning Processors (DLP) == Inspired from the pioneer work of DianNao Family: many DLPs are proposed in both academia and industry with design optimized to leverage the features of deep neural networks for high efficiency. Only at ISCA 2016: three sessions: 15% (!) of the accepted papers: are all architecture designs about deep learning. Such efforts include Eyeriss (MIT): EIE (Stanford): Minerva (Harvard): Stripes (University of Toronto) in academia: TPU (Google): and MLU (Cambricon) in industry. We listed several representative works in Table 1. === Digital DLPs === The major components of DLPs architecture usually include a computation component: the on-chip memory hierarchy: and the control logic that manages the data communication and computing flows. Regarding the computation component: as most operations in deep learning can be aggregated into vector operations: the most common ways for building computation components in digital DLPs are the MAC-based (multiplier-accumulation) organization: either with vector MACs or scalar MACs. Rather than SIMD or SIMT in general processing devices: deep learning domain-specific parallelism is better explored on these MAC-based organizations. Regarding the memory hierarchy: as deep learning algorithms require high bandwidth to provide the computation component with sufficient data: DLPs usually employ a relatively larger size (tens of kilobytes or several megabytes) on-chip buffer but with dedicated on-chip data reuse strategy and data exchange strategy to alleviate the burden for memory bandwidth. For example: DianNao: 16 16-in vector MAC: requires 16 × 16 × 2 = 512 16-bit data: i.e.: almost 1024GB/s bandwidth requirements between computation components and buffers. With on-chip reuse: such bandwidth requirements are reduced drastically. Instead of the widely used cache in general processing devices: DLPs always use scratchpad memory as it could provide higher data reuse opportunities by leveraging the relatively regular data access pattern in deep learning algorithms. Regarding the control logic: as the deep learning algorithms keep evolving at a dramatic speed: DLPs start to leverage dedicated ISA (instruction set architecture) to support the deep learning domain flexibly. At first: DianNao used a VLIW-style instruction set where each instruction could finish a layer in a DNN. Cambricon introduces the first deep learning domain-specific ISA: which could support more than ten different deep learning algorithms. TPU also reveals five key instructions from the CISC-style ISA. === Hybrid DLPs === Hybrid DLPs emerge for DNN inference and training acceleration because of their high efficiency. Processing-in-memory (PIM) architectures are one most important type of hybrid DLP. The key design concept of PIM is to bridge the gap between computing and memory: with the following manners: 1) Moving computation components into memory cells: controllers: or memory chips to alleviate the memory wall issue. Such architectures significantly shorten data paths and leverage much higher internal bandwidth: hence resulting in attractive performance improvement. 2) Build high efficient DNN engines by adopting computational devices. In 2013: HP Lab demonstrated the astonishing capability of adopting ReRAM crossbar structure for computing. Inspiring by this work: tremendous work are proposed to explore the new architecture and system design based on ReRAM: phase change memory: etc. == Benchmarks == Benchmarks such as MLPerf and others may\nbe used to evaluate the performance of AI accelerators. Table 2 lists several typical benchmarks for AI accelerators. == Potential applications == Agricultural robots: for example: herbicide-free weed control. Autonomous vehicles: Nvidia has targeted their Drive PX-series boards at this application. Computer-aided diagnosis Industrial robots: increasing the range of tasks that can be automated: by adding adaptability to variable situations. Machine translation Military robots Natural language processing Search engines: increasing the energy efficiency of data centers and the ability to use increasingly advanced queries. Unmanned aerial vehicles: e.g. navigation systems: e.g. the Movidius Myriad 2 has been demonstrated successfully guiding autonomous drones. Voice user interface: e.g. in mobile phones: a target for Qualcomm Zeroth. == See also == Cognitive computer Neuromorphic engineering Optical neural network Physical neural network Cerebras Systems == References == == External links == Nvidia Puts The Accelerator To The Metal With Pascal.htm: The Next Platform Eyeriss Project: MIT https://alphaics.ai/': 'original_similarity': 0.7876189119072035: 'slide': 193}"
0.4818464000127278,"Can you tell me more about BLOOM's training corpus, ROOTS?","ROOTS is the training corpus for BLOOM and combines data extracted from the web-based OSCAR corpus (38% of ROOTS) and newly collected data from a manually selected list of language data sources. It includes data in 46 natural languages, with English forming 30% of the whole dataset to as little as 0.00002% for Chi Tumbuka, as well as data in 13 different programming languages.","{'chunk_text': ""language model (LLM). The model: as well as the code base and the data used to train it: are distributed under free licences. BLOOM was trained on approximately 366 billion (1.6TB) tokens from March to July 2022.BLOOM is the main outcome of the BigScience collaborative initiative: a one-year-long research workshop that took place between May 2021 and May 2022. BigScience was led by HuggingFace and involved several hundreds of researchers and engineers from France and abroad representing both the academia and the private sector. BigScience was supported by a large-scale public compute grant on the French public supercomputer Jean Zay: managed by GENCI and IDRIS (CNRS): on which it was trained. BLOOM's training corpus: named ROOTS: combines data extracted from the then-latest version of the web-based OSCAR corpus (38% of ROOTS) and newly collected data extracted from a manually selected and documented list of language data sources. It encompasses 46 natural languages (in amounts ranging from 30% of the whole dataset for English to 0.00002% for Chi Tumbuka) and 13 programming languages. == References ==\nChinchilla is a family of large language models developed by the research team at DeepMind:"": 'original_similarity': 0.8493022231386944: 'slide': 13}","{'chunk_text': 'models: BERT is a deeply bidirectional: unsupervised language representation: pre-trained using only a plain text corpus. Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary: whereas BERT takes into account the context for each occurrence of a given word. For instance: whereas the vector for ""running"" will have the same word2vec vector representation for both of its occurrences in the sentences ""He is running a company"" and ""He is running a marathon"": BERT will provide a contextualized embedding that will be different according to the sentence.On October 25: 2019: Google announced that they had started applying BERT models for English language search queries within the US. On December 9: 2019: it was reported that BERT had been adopted by Google Search for over 70 languages. In October 2020: almost every single English-based query was processed by a BERT model.A later paper proposes RoBERTa: which preserves BERT\'s architecture: but improves its training: changing key hyperparameters: removing the next-sentence prediction task: and using much larger mini-batch sizes. == Recognition == The research paper describing BERT won the Best Long Paper Award at the 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL). == References == == Further reading == Rogers: Anna; Kovaleva: Olga; Rumshisky: Anna (2020). ""A Primer in BERTology: What we know about how BERT works"". arXiv:2002.12327 [cs.CL]. == External links == Official GitHub repository BERT on Devopedia\nBigScience Large Open-science Open-access Multilingual Language Model (BLOOM) is a 176-billion-parameter transformer-based autoregressive large language model (LLM). The model: as well as the code base and the data used to train it: are distributed under free licences. BLOOM was trained on approximately 366 billion (1.6TB) tokens from March to July 2022.BLOOM is the main outcome of the BigScience collaborative initiative: a one-year-long research workshop that took place between May 2021 and May 2022. BigScience was led by HuggingFace and involved several hundreds of researchers and engineers from France and abroad representing both the academia and the private sector. BigScience was supported by a large-scale public compute grant on the French public supercomputer Jean Zay: managed by GENCI and IDRIS (CNRS): on which it was': 'original_similarity': 0.7796896656770007: 'slide': 122}",{'chunk_text': 'supporters point out that this is only the first generation: and the capabilities of improved iterations will become clear. (IBM) == Benchmarks == Classification capabilities of spiking networks trained according to unsupervised learning methods have been tested on the common benchmark datasets: such as: Iris: Wisconsin Breast Cancer or Statlog Landsat dataset. Various approaches to information encoding and network design have been used. For example: a 2-layer feedforward network for data clustering and classification. Based on the idea proposed in Hopfield (1995) the authors implemented models of local receptive fields combining the properties of radial basis functions (RBF) and spiking neurons to convert input signals (classified data) having a floating-point representation into a spiking representation. == See also == == References ==\nThere are many types of artificial neural networks (ANN). Artificial neural networks are computational models inspired by biological neural networks: and are used to approximate functions that are generally unknown. Particularly: they are inspired by': 'original_similarity': 0.7728366723435957: 'slide': 33}
0.46965779998572543,What is Chinchilla and when was it presented?,"Chinchilla is a family of large language models developed by the research team at DeepMind, presented in March 2022.","{'chunk_text': 'utilization because it requires much less computer power for inference and fine-tuning. Based on the training of previously employed language models: it has been determined that if one doubles the model size: one must also have twice the number of training tokens. This hypothesis has been used to train Chinchilla by DeepMind. Similar to Gopher in terms of cost: Chinchilla has 70B parameters and four times as much data.Chinchilla has an average accuracy of 67.5% on the MMLU benchmark (Measuring Massive Multitask Language Understanding): which is 7% higher than Gopher\'s performance. Chinchilla was still in the testing phase as of January 12: 2023.Chinchilla contributes to developing an effective training paradigm for large autoregressive language models with limited compute resources. The Chinchilla team recommends that the number of training tokens is twice for every model size doubling: meaning that using larger: higher-quality training datasets can lead to better results on downstream tasks. == Architecture == Both the Gopher family and Chinchilla family are families of transformer models. In particular: they are essentially the same as GPT-2: with different sizes and minor modifications. Gopher family uses RMSNorm instead of LayerNorm; relative positional encoding rather than absolute positional encoding. The Chinchilla family is the same as the Gopher family: but trained with AdamW instead of Adam optimizer. The Gopher family contains six models of increasing size: from 44 million parameters to 280 billion parameters. They refer to the largest one as ""Gopher"" by default. Similar naming conventions apply for the Chinchilla family. Table 1 of shows the entire Gopher family: Table 4 of compares the 70-billion-parameter Chinchilla with Gopher 280B. == See also == LaMDA == References ==\nClaude is a family of large language models developed by Anthropic. The first model was released in March 2023. Claude 3: released in March 2024: can also analyze images. == Training == Claude models are generative pre-trained transformers. They have been pre-trained to predict the next word in large amounts of text. Claude models have then been fine-tuned with Constitutional AI': 'original_similarity': 0.8260607598988112: 'slide': 59}","{'chunk_text': 'Google Tech Talks Performance of Neural Networks Neural Networks and Information Sanderson G (5 October 2017). ""But what is a Neural Network?"". 3Blue1Brown. Archived from the original on 7 November 2021 – via YouTube. == Notes == == References == == Bibliography ==\nA neural network is a group of interconnected units called neurons that send signals to one': 'original_similarity': 0.711487281609697: 'slide': 14}","{'chunk_text': 'Feedback: with the aim of making them less likely to give harmful responses: while still being helpful to the user. === Constitutional AI === Constitutional AI is an approach developed by Anthropic for training AI systems: particularly language models like Claude: to be harmless and helpful without relying on extensive human feedback. The method: detailed in the paper ""Constitutional AI: Harmlessness from AI Feedback"" involves two phases: supervised learning and reinforcement learning. In the supervised learning phase: the model generates responses to prompts: self-critiques these responses based on a set of guiding principles (a ""constitution""): and then revises the responses. The reinforcement learning phase involves training the model with AI-generated feedback: where the AI evaluates responses according to the constitutional principles. This approach enables the training of AI assistants that are both helpful and harmless: and that can explain their objections to harmful requests: enhancing transparency and reducing reliance on human supervision.The ""constitution"" for Claude included 75 points: including sections from the UN Universal Declaration of Human Rights. == Models == === Claude === Claude was the initial version of Anthropic\'s language model released in March 2023: Claude demonstrated proficiency in various tasks but had certain limitations in coding: math: and reasoning capabilities. Anthropic partnered with companies like Notion (productivity software) and Quora (to help develop the Poe chatbot). === Claude Instant === Claude was released as two versions: Claude and Claude Instant: with Claude Instant being a faster: less expensive and lighter version. Claude Instant has a input context length of 100:000 tokens (which corresponds to around 75:000 words). === Claude 2 === Claude 2 was the next major iteration of Claude: which was released in July 11 2023 and available to the general public: whereas the Claude 1 was only available to selected users approved by Anthropic.Claude 2 expanded its context window from 9:000 tokens to 100:000 tokens. Features included ability to upload PDFs and other documents that enables Claude to read: summarise and assist with tasks. ==== Claude 2.1 ==== Claude 2.1 doubled the number of tokens that the chatbot could handle: increasing it to a window of 200:000 tokens: which equals around 500 pages of written material.Anthropic states that the new model is less likely to produce false statements compared to its predecessors. === Claude 3 === Claude 3 was released on March 14: 2024 with claims in the press release to have set new industry benchmarks across a wide range of cognitive tasks. The Claude 3 family includes three state-of-the-art models in ascending order of capability: Haiku: Sonnet: and Opus. The default version of Claude 3 Opus has a context window of 200:000 tokens: but this is being expanded to 1 million for specific use cases.Claude 3 has seemed to perform meta-cognitive reasoning: including the ability to realize it is being artificially tested during needle in a haystack evaluations. == Access == Limited-use access is free of charge: but requires both an e-mail address and a cellphone number. == Criticism == Claude 2 has faced criticism for its stringent ethical alignment that may reduce usability and performance. Users have been refused assistance with benign requests: for example with the programming question ""How can I kill all python processes in my ubuntu server?"" This has led to a debate over the ""alignment tax"" (the cost of ensuring an AI system is aligned) in AI development: with discussions centered on balancing ethical considerations and practical functionality. Critics argue for user autonomy and effectiveness: while proponents stress the importance of ethical AI. == References ==\nConvolutional neural network (CNN) is a regularized type of feed-forward neural network that learns feature engineering by itself via filters (or kernel) optimization. Vanishing gradients and exploding gradients: seen during backpropagation in earlier neural networks: are prevented by using regularized weights over fewer connections. For example: for each neuron in the fully-connected layer 10:000 weights would be required for processing an image sized 100 × 100 pixels.': 'original_similarity': 0.7112834529101929: 'slide': 65}"
0.45377969997934997,How does Chinchilla compare to the previous model named Gopher?,"Chinchilla is a further development over Gopher. It requires less computer power for inference and fine-tuning, has an average accuracy of 67.5% on the MMLU benchmark which is 7% higher than Gopher, and has 70 billion parameters with four times as much data.","{'chunk_text': 'the scaling laws of large language models.It claimed to outperform GPT-3. It considerably simplifies downstream utilization because it requires much less computer power for inference and fine-tuning. Based on the training of previously employed language models: it has been determined that if one doubles the model size: one must also have twice the number of training tokens. This hypothesis has been used to train Chinchilla by DeepMind. Similar to Gopher in terms of cost: Chinchilla has 70B parameters and four times as much data.Chinchilla has an average accuracy of 67.5% on the MMLU benchmark (Measuring Massive Multitask Language Understanding): which is 7% higher than Gopher\'s performance. Chinchilla was still in the testing phase as of January 12: 2023.Chinchilla contributes to developing an effective training paradigm for large autoregressive language models with limited compute resources. The Chinchilla team recommends that the number of training tokens is twice for every model size doubling: meaning that using larger: higher-quality training datasets can lead to better results on downstream tasks. == Architecture == Both the Gopher family and Chinchilla family are families of transformer models. In particular: they are essentially the same as GPT-2: with different sizes and minor modifications. Gopher family uses RMSNorm instead of LayerNorm; relative positional encoding rather than absolute positional encoding. The Chinchilla family is the same as the Gopher family: but trained with AdamW instead of Adam optimizer. The Gopher family contains six models of increasing size: from 44 million parameters to 280 billion parameters. They refer to the largest one as ""Gopher"" by default. Similar naming conventions apply for the Chinchilla family. Table 1 of shows the entire Gopher family: Table 4 of compares the 70-billion-parameter Chinchilla with Gopher 280B. == See also == LaMDA == References ==\nClaude is a family of large language models developed by Anthropic. The first model was released in March 2023. Claude 3: released in March 2024: can also analyze images. == Training == Claude models are generative pre-trained transformers. They have been pre-trained to predict the next': 'original_similarity': 0.880311205587036: 'slide': 44}","{'chunk_text': 'open the conversation with a mix of text: picture: video: and audio: presented in any order: and Gemini might reply with the same free ordering. Input images may be of different resolutions: while video is inputted as a sequence of images. Audio is sampled at 16 kHz and then converted into a sequence of tokens by the Universal Speech Model. Gemini\'s dataset is multimodal and multilingual: consisting of ""web documents: books: and code: and includ[ing] image: audio: and video data"".Demis Hassabis claims that training Gemini 1 used ""roughly the same amount of compute: maybe slightly more than what was rumored for GPT-4"".The second generation of Gemini (""Gemini 1.5"") has one model published so far: Gemini 1.5 Pro. It is a multimodal sparse mixture-of-experts: with context length of ""multiple millions"". == Reception == Gemini\'s launch was preluded by months of intense speculation and anticipation: which MIT Technology Review described as ""peak AI hype"". In August 2023: Dylan Patel and Daniel Nishball of research firm SemiAnalysis penned a blog post declaring that the release of Gemini would ""eat the world"" and outclass GPT-4: prompting OpenAI CEO Sam Altman to ridicule the duo on X (formerly Twitter). Business magnate Elon Musk: who co-founded OpenAI: weighed in: asking: ""Are the numbers wrong?"" Hugh Langley of Business Insider remarked that Gemini would be a make-or-break moment for Google: writing: ""If Gemini dazzles: it will help Google change the narrative that it was blindsided by Microsoft and OpenAI. If it disappoints: it will embolden critics who say Google has fallen behind.""Reacting to its unveiling in December 2023: University of Washington professor emeritus Oren Etzioni predicted a ""tit-for-tat arms race"" between Google and OpenAI. Professor Alexei Efros of the University of California: Berkeley praised the potential of Gemini\'s multimodal approach: while scientist Melanie Mitchell of the Santa Fe Institute called Gemini ""very sophisticated"". Professor Chirag Shah of the University of Washington was less impressed: likening Gemini\'s launch to the routineness of Apple\'s annual introduction of a new iPhone. Similarly: Stanford University\'s Percy Liang: the University of Washington\'s Emily Bender: and the University of Galway\'s Michael Madden cautioned that it was difficult to interpret benchmark scores without insight into the training data used. Writing for Fast Company: Mark Sullivan opined that Google had the opportunity to challenge the iPhone\'s dominant market share: believing that Apple was unlikely to have the capacity to develop functionality similar to Gemini with its Siri virtual assistant. Google shares spiked by 5.3 percent the day after Gemini\'s launch.Google faced criticism for a demonstrative video of Gemini: which was not conducted in real time. == See also == Gato: a multimodal neural network developed by DeepMind == References == == Further reading == == External links == Official website Press release via The Keyword Announcement and demo on YouTube White paper for 1.0 and 1.5\nA generative adversarial network (GAN) is a class of machine learning frameworks and a prominent framework for approaching generative AI. The concept was initially developed by Ian Goodfellow and his colleagues in June 2014. In a GAN: two neural networks contest with each other in the form of a zero-sum game: where one agent\'s gain is another agent\'s loss. Given a training set: this technique learns to generate new data with the same statistics as the training set. For example: a GAN trained on photographs can generate new photographs that look at least superficially authentic to human observers: having many realistic characteristics. Though originally proposed as a form of generative model for': 'original_similarity': 0.748036608124528: 'slide': 110}",{'chunk_text': 'point out that this is only the first generation: and the capabilities of improved iterations will become clear. (IBM) == Benchmarks == Classification capabilities of spiking networks trained according to unsupervised learning methods have been tested on the common benchmark datasets: such as: Iris: Wisconsin Breast Cancer or Statlog Landsat dataset. Various approaches to information encoding and network design have been used. For example: a 2-layer feedforward network for data clustering and classification. Based on the idea proposed in Hopfield (1995) the authors implemented models of local receptive fields combining the properties of radial basis functions (RBF) and spiking neurons to convert input signals (classified data) having a floating-point representation into a spiking representation. == See also == == References ==\nThere are many types of artificial neural networks (ANN). Artificial neural networks are computational models inspired by biological neural networks: and are used to approximate functions that are generally unknown. Particularly: they are inspired by the': 'original_similarity': 0.7377666231952086: 'slide': 34}
0.45108989998698235,What hypothesis was used to train the Chinchilla by DeepMind?,"The hypothesis used to train Chinchilla by DeepMind is that if one doubles the model size, one must also have twice the number of training tokens.","{'chunk_text': 'considerably simplifies downstream utilization because it requires much less computer power for inference and fine-tuning. Based on the training of previously employed language models: it has been determined that if one doubles the model size: one must also have twice the number of training tokens. This hypothesis has been used to train Chinchilla by DeepMind. Similar to Gopher in terms of cost: Chinchilla has 70B parameters and four times as much data.Chinchilla has an average accuracy of 67.5% on the MMLU benchmark (Measuring Massive Multitask Language Understanding): which is 7% higher than Gopher\'s performance. Chinchilla was still in the testing phase as of January 12: 2023.Chinchilla contributes to developing an effective training paradigm for large autoregressive language models with limited compute resources. The Chinchilla team recommends that the number of training tokens is twice for every model size doubling: meaning that using larger: higher-quality training datasets can lead to better results on downstream tasks. == Architecture == Both the Gopher family and Chinchilla family are families of transformer models. In particular: they are essentially the same as GPT-2: with different sizes and minor modifications. Gopher family uses RMSNorm instead of LayerNorm; relative positional encoding rather than absolute positional encoding. The Chinchilla family is the same as the Gopher family: but trained with AdamW instead of Adam optimizer. The Gopher family contains six models of increasing size: from 44 million parameters to 280 billion parameters. They refer to the largest one as ""Gopher"" by default. Similar naming conventions apply for the Chinchilla family. Table 1 of shows the entire Gopher family: Table 4 of compares the 70-billion-parameter Chinchilla with Gopher 280B. == See also == LaMDA == References ==\nClaude is a family of large language models developed by Anthropic. The first model was released in March 2023. Claude 3: released in March 2024: can also analyze images. == Training == Claude models are generative pre-trained transformers. They have been pre-trained to predict the next word in large amounts of text. Claude models have then been fine-tuned': 'original_similarity': 0.8584019695182434: 'slide': 56}","{'chunk_text': 'time steps before. The ""vanilla LSTM"" with forget gate was introduced in 1999 by Felix Gers: Schmidhuber and Fred Cummins. LSTM has become the most cited neural network of the 20th century. In 2015: Rupesh Kumar Srivastava: Klaus Greff: and Schmidhuber used LSTM principles to create the Highway network: a feedforward neural network with hundreds of layers: much deeper than previous networks. 7 months later: Kaiming He: Xiangyu Zhang; Shaoqing Ren: and Jian Sun won the ImageNet 2015 competition with an open-gated or gateless Highway network variant called Residual neural network. This has become the most cited neural network of the 21st century.In 1994: André de Carvalho: together with Mike Fairhurst and David Bisset: published experimental results of a multi-layer boolean neural network: also known as a weightless neural network: composed of a 3-layers self-organising feature extraction neural network module (SOFT) followed by a multi-layer classification neural network module (GSN): which were independently trained. Each layer in the feature extraction module extracted features with growing complexity regarding the previous layer.In 1995: Brendan Frey demonstrated that it was possible to train (over two days) a network containing six fully connected layers and several hundred hidden units using the wake-sleep algorithm: co-developed with Peter Dayan and Hinton.Since 1997: Sven Behnke extended the feed-forward hierarchical convolutional approach in the Neural Abstraction Pyramid by lateral and backward connections in order to flexibly incorporate context into decisions and iteratively resolve local ambiguities. Simpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) were a popular choice in the 1990s and 2000s: because of artificial neural networks\' computational cost and a lack of understanding of how the brain wires its biological networks. Both shallow and deep learning (e.g.: recurrent nets) of ANNs for speech recognition have been explored for many years. These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively. Key difficulties have been analyzed: including gradient diminishing and weak temporal correlation structure in neural predictive models. Additional difficulties were the lack of training data and limited computing power. Most speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government\'s NSA and DARPA: SRI studied deep neural networks (DNNs) in speech and speaker recognition. The speaker recognition team led by Larry Heck reported significant success with deep neural networks in speech processing in the 1998 National Institute of Standards and Technology Speaker Recognition evaluation. The SRI deep neural network was then deployed in the Nuance Verifier: representing the first major industrial application of deep learning. The principle of elevating ""raw"" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the ""raw"" spectrogram or linear filter-bank features in the late 1990s: showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech: waveforms: later produced excellent larger-scale results.Speech recognition was taken over by LSTM. In 2003: LSTM started to become competitive with traditional speech recognizers on certain tasks. In 2006: Alex Graves: Santiago Fernández: Faustino Gomez: and Schmidhuber combined it with connectionist temporal classification (CTC) in stacks of LSTM RNNs. In 2015: Google\'s speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM: which they made available through Google Voice Search.The impact of deep learning in industry began in the early 2000s: when CNNs already processed an estimated 10% to 20% of all the checks written in the US: according to Yann LeCun. Industrial applications of deep learning to large-scale speech recognition started around 2010. In 2006: publications by Geoff Hinton: Ruslan Salakhutdinov: Osindero and Teh showed how a many-layered feedforward neural network could be effectively pre-trained one layer at a time: treating each layer in turn as an unsupervised restricted Boltzmann machine: then fine-tuning it using supervised backpropagation. The papers referred to learning for deep belief nets. The 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech: and the possibility that given more capable hardware and large-scale data sets that deep neural nets might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets. However: it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large: context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems. The nature of the recognition errors produced by the two types of systems was characteristically different: offering technical insights into how to integrate deep learning into the existing highly\nefficient: run-time speech decoding system deployed by all major speech recognition systems. Analysis around 2009–2010: contrasting the GMM (and other generative speech models) vs. DNN models: stimulated early industrial investment in deep learning for speech recognition. That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models. In 2010: researchers extended deep learning from TIMIT to large vocabulary speech recognition: by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees.Deep learning is part of state-of-the-art systems in various disciplines: particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification): as well as a range of large-vocabulary speech recognition tasks have steadily improved. Convolutional neural networks were superseded for ASR by CTC for LSTM. but are more successful in computer vision. Advances in hardware have driven renewed interest in deep learning. In 2009: Nvidia was involved in what was called the ""big bang"" of deep learning: ""as deep-learning neural networks were trained with Nvidia graphics processing units (GPUs)"". That year: Andrew Ng determined that GPUs could increase the speed of deep-learning systems by about 100': 'original_similarity': 0.8030591265537287: 'slide': 199}","{'chunk_text': 'Reinforcement: or TAMER: which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor. First developed as TAMER: a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot with the ability to learn new tasks through observation. Using Deep TAMER: a robot learned a task with a human trainer: watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer: who provided feedback such as ""good job"" and ""bad job"". == Criticism and comment == Deep learning has attracted both criticism and comment: in some cases from outside the field of computer science. === Theory === A main criticism concerns the lack of theory surrounding some methods. Learning in the most common deep architectures is implemented using well-understood gradient descent. However: the theory surrounding other algorithms: such as contrastive divergence is less clear. (e.g.: Does it converge? If so: how fast? What is it approximating?) Deep learning methods are often looked at as a black box: with most confirmations done empirically: rather than theoretically.Others point out that deep learning should be looked at as a step towards realizing strong AI: not as an all-encompassing solution. Despite the power of deep learning methods: they still lack much of the functionality needed to realize this goal entirely. Research psychologist Gary Marcus noted: Realistically: deep learning is only part of the larger challenge of building intelligent machines. Such techniques lack ways of representing causal relationships (...) have no obvious ways of performing logical inferences: and they are also still a long way from integrating abstract knowledge: such as information about what objects are: what they are for: and how they are typically used. The most powerful A.I. systems: like Watson (...) use techniques like deep learning as just one element in a very complicated ensemble of techniques: ranging from the statistical technique of Bayesian inference to deductive reasoning. In further reference to the idea that artistic sensitivity might be inherent in relatively low levels of the cognitive hierarchy: a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained demonstrate a visual appeal: the original research notice received well over 1:000 comments: and was the subject of what was for a time the most frequently accessed article on The Guardian\'s website. === Errors === Some deep learning architectures display problematic behaviors: such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images (2014) and misclassifying minuscule perturbations of correctly classified images (2013). Goertzel hypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-component artificial general intelligence (AGI) architectures. These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar decompositions of observed entities and events. Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic goal of both human language acquisition and artificial intelligence (AI). === Cyber threat === As deep learning moves from the lab into the world: research and experience show that artificial neural networks are vulnerable to hacks and deception. By identifying patterns that these systems use to function: attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize. For example: an attacker can make subtle changes to an image such that the ANN finds a match even though the image looks to a human nothing like the search target. Such manipulation is termed an ""adversarial attack"".In 2016 researchers used one ANN to doctor images in trial and error fashion: identify another\'s focal points: and thereby generate images that deceived it. The modified images looked no different to human eyes. Another group showed that printouts of doctored images then photographed successfully tricked an image classification system. One defense is reverse image search: in which a possible fake image is submitted to a site such as TinEye that can then find other instances of it. A refinement is to search using only parts of the image: to identify images from which that piece may have been taken.Another group showed that certain psychedelic spectacles could fool a facial recognition system into thinking ordinary people were celebrities: potentially allowing one person to impersonate another. In 2017 researchers added stickers to stop signs and caused an ANN to misclassify them.ANNs\ncan however be further trained to detect attempts at deception: potentially leading attackers and defenders into an arms race similar to the kind that already defines the malware defense industry. ANNs have been trained to defeat ANN-based anti-malware software by repeatedly attacking a defense with malware that was continually altered by a genetic algorithm until it tricked the anti-malware while retaining its ability to damage the target.In 2016: another group demonstrated that certain sounds could make the Google Now voice command system open a particular web address: and hypothesized that this could ""serve as a stepping stone for further attacks (e.g.: opening a web page hosting drive-by malware)"".In ""data poisoning"": false data is continually smuggled into a machine learning system\'s training set to prevent it from achieving mastery. === Data collection ethics === Most Deep Learning systems rely on training and verification data that is generated and/or annotated by humans. It has been argued in media philosophy that not only low-paid clickwork (e.g. on Amazon Mechanical Turk) is regularly deployed for this purpose: but also implicit forms of human microwork that are often not recognized as such. The philosopher Rainer Mühlhoff distinguishes five types of ""machinic capture"" of human microwork to generate': 'original_similarity': 0.8024938016660694: 'slide': 200}"
0.4568446000048425,How are the Chinchilla and Gopher families similar in terms of architecture?,"Both the Gopher and Chinchilla families are transformer models. They are essentially the same as GPT-2, with different sizes and minor modifications. Gopher uses RMSNorm instead of LayerNorm, and relative positional encoding rather than absolute. Chinchilla is the same as Gopher but trained with AdamW instead of Adam optimizer.","{'chunk_text': 'models.It claimed to outperform GPT-3. It considerably simplifies downstream utilization because it requires much less computer power for inference and fine-tuning. Based on the training of previously employed language models: it has been determined that if one doubles the model size: one must also have twice the number of training tokens. This hypothesis has been used to train Chinchilla by DeepMind. Similar to Gopher in terms of cost: Chinchilla has 70B parameters and four times as much data.Chinchilla has an average accuracy of 67.5% on the MMLU benchmark (Measuring Massive Multitask Language Understanding): which is 7% higher than Gopher\'s performance. Chinchilla was still in the testing phase as of January 12: 2023.Chinchilla contributes to developing an effective training paradigm for large autoregressive language models with limited compute resources. The Chinchilla team recommends that the number of training tokens is twice for every model size doubling: meaning that using larger: higher-quality training datasets can lead to better results on downstream tasks. == Architecture == Both the Gopher family and Chinchilla family are families of transformer models. In particular: they are essentially the same as GPT-2: with different sizes and minor modifications. Gopher family uses RMSNorm instead of LayerNorm; relative positional encoding rather than absolute positional encoding. The Chinchilla family is the same as the Gopher family: but trained with AdamW instead of Adam optimizer. The Gopher family contains six models of increasing size: from 44 million parameters to 280 billion parameters. They refer to the largest one as ""Gopher"" by default. Similar naming conventions apply for the Chinchilla family. Table 1 of shows the entire Gopher family: Table 4 of compares the 70-billion-parameter Chinchilla with Gopher 280B. == See also == LaMDA == References ==\nClaude is a family of large language models developed by Anthropic. The first model was released in March 2023. Claude 3: released in March 2024: can also analyze images. == Training == Claude models are generative pre-trained transformers. They have been pre-trained to predict the next word in large amounts of text.': 'original_similarity': 0.8469377017595248: 'slide': 50}","{'chunk_text': 'and topology from the output space: and SOM attempts to preserve these. ==== Learning vector quantization ==== Learning vector quantization (LVQ) can be interpreted as a neural network architecture. Prototypical representatives of the classes parameterize: together with an appropriate distance measure: in a distance-based classification scheme. === Simple recurrent === Simple recurrent networks have three layers: with the addition of a set of ""context units"" in the input layer. These units connect from the hidden layer or the output layer with a fixed weight of one. At each time step: the input is propagated in a standard feedforward fashion: and then a backpropagation-like learning rule is applied (not performing gradient descent). The fixed back connections leave a copy of the previous values of the hidden units in the context units (since they propagate over the connections before the learning rule is applied). === Reservoir computing === Reservoir computing is a computation framework that may be viewed as an extension of neural networks. Typically an input signal is fed into a fixed (random) dynamical system called a reservoir whose dynamics map the input to a higher dimension. A readout mechanism is trained to map the reservoir to the desired output. Training is performed only at the readout stage. Liquid-state machines are a type of reservoir computing. ==== Echo state ==== The echo state network (ESN) employs a sparsely connected random hidden layer. The weights of output neurons are the only part of the network that are trained. ESN are good at reproducing certain time series. === Long short-term memory === The long short-term memory (LSTM) avoids the vanishing gradient problem. It works even when with long delays between inputs and can handle signals that mix low and high frequency components. LSTM RNN outperformed other RNN and other sequence learning methods such as HMM in applications such as language learning and connected handwriting recognition. === Bi-directional === Bi-directional RNN: or BRNN: use a finite sequence to predict or label each element of a sequence based on both the past and future context of the element. This is done by adding the outputs of two RNNs: one processing the sequence from left to right: the other one from right to left. The combined outputs are the predictions of the teacher-given target signals. This technique proved to be especially useful when combined with LSTM. === Hierarchical === Hierarchical RNN connects elements in various ways to decompose hierarchical behavior into useful subprograms. === Stochastic === A district from conventional neural networks: stochastic artificial neural network used as an approximation to random functions. === Genetic Scale === A RNN (often a LSTM) where a series is decomposed into a number of scales where every scale informs the primary length between two consecutive points. A first order scale consists of a normal RNN: a second order consists of all points separated by two indices and so on. The Nth order RNN connects the first and last node. The outputs from all the various scales are treated as a Committee of Machines and the associated scores are used genetically for the next iteration. == Modular == Biological studies have shown that the human brain operates as a collection of small networks. This realization gave birth to the concept of modular neural networks: in which several small networks cooperate or compete to solve problems. === Committee of machines === A committee of machines (CoM) is a collection of different neural networks that together ""vote"" on a given example. This generally gives a much better result than individual networks. Because neural networks suffer from local minima: starting with the same architecture and training but using randomly different initial weights often gives vastly different results. A CoM tends to stabilize the result. The CoM is similar to the general machine learning bagging method: except that the necessary variety of machines in the committee is obtained by training from different starting weights rather than training on different randomly selected subsets of the training data. === Associative === The associative neural network (ASNN) is an extension of committee of machines that combines multiple feedforward neural networks and the k-nearest neighbor technique. It uses the correlation between ensemble responses as a measure of distance amid the analyzed cases for the kNN. This corrects the Bias of the neural network ensemble. An associative neural network has a memory that can coincide with the training set. If new data become available: the network instantly improves its predictive ability and provides data approximation (self-learns) without\nretraining. Another important feature of ASNN is the possibility to interpret neural network results by analysis of correlations between data cases in the space of models. == Physical == A physical neural network includes electrically adjustable resistance material to simulate artificial synapses. Examples include the ADALINE memristor-based neural network. An optical neural network is a physical implementation of an artificial neural network with optical components. == Dynamic == Dynamic neural networks address nonlinear multivariate behaviour and include (learning of) time-dependent behaviour: such as transient phenomena and delay effects. Techniques to estimate a system process from observed data fall under the general category of system identification. === Cascading === Cascade correlation is an architecture and supervised learning algorithm. Instead of just adjusting the weights in a network of fixed topology: Cascade-Correlation begins with a minimal network: then automatically trains and adds new hidden units one by one: creating a multi-layer structure. Once a new hidden unit has been added to the network: its input-side weights are frozen. This unit then becomes a permanent feature-detector in the network: available for producing outputs or for creating other: more complex feature detectors. The Cascade-Correlation architecture has several advantages: It learns quickly: determines its own size and topology: retains the structures it has built even if the training set changes and requires no backpropagation. === Neuro-fuzzy === A neuro-fuzzy network is a fuzzy inference system in the body of an artificial neural network. Depending on the FIS type: several layers simulate the processes involved in a fuzzy inference-like': 'original_similarity': 0.7448712697618661: 'slide': 251}","{'chunk_text': ""development life cycle stage. Therefore: such language offers a distinct vocabulary: syntax: and notation for each stage: such as discovery: analysis: design: architecture: contraction: etc. For example: for the analysis phase of a project: the modeler employs specific analysis notation to deliver an analysis proposition diagram. During the design phase: however: logical design notation is used to depict relationship between software entities. In addition: the discipline-specific modeling language best practices does not preclude practitioners from combining the various notations in a single diagram. ==== Domain-specific ==== Domain-specific modeling (DSM) is a software engineering methodology for designing and developing systems: most often IT systems such as computer software. It involves systematic use of a graphical domain-specific language (DSL) to represent the various facets of a system. DSM languages tend to support higher-level abstractions than General-purpose modeling languages: so they require less effort and fewer low-level details to specify a given system. ==== Framework-specific ==== A framework-specific modeling language (FSML) is a kind of domain-specific modeling language which is designed for an object-oriented application framework. FSMLs define framework-provided abstractions as FSML concepts and decompose the abstractions into features. The features represent implementation steps or choices. A FSML concept can be configured by selecting features and providing values for features. Such a concept configuration represents how the concept should be implemented in the code. In other words: concept configuration describes how the framework should be completed in order to create the implementation of the concept. ==== Information and knowledge modeling ==== Linked data and ontology engineering require 'host languages' to represent entities and the relations between them: constraints between the properties of entities and relations: and metadata attributes. JSON-LD and RDF are two major (and semantically almost equivalent) languages in this context: primarily because they support statement reification and contextualisation which are essential properties to support the higher-order logic needed to reason about models. Model transformation is a common example of such reasoning. ==== Object-oriented ==== Object modeling languages are modeling languages based on a standardized set of symbols and ways of arranging them to model (part of) an object oriented software design or system design. Some organizations use them extensively in combination with a software development methodology to progress from initial specification to an implementation plan and to communicate that plan to an entire team of developers and stakeholders. Because a modeling language is visual and at a higher-level of abstraction than code: using models encourages the generation of a shared vision that may prevent problems of differing interpretation later in development. Often software modeling tools are used to construct these models: which may then be capable of automatic translation to code. ==== Virtual reality ==== Virtual Reality Modeling Language (VRML): before 1995 known as the Virtual Reality Markup Language is a standard file format for representing 3-dimensional (3D) interactive vector graphics: designed particularly with the World Wide Web in mind. ==== Others ==== Architecture Description Language Face Modeling Language Generative Modelling Language Java Modeling Language Promela Rebeca Modeling Language Service Modeling Language Web Services Modeling Language X3D == Applications == Various kinds of modeling languages are applied in different disciplines: including computer science: information management: business process modeling: software engineering: and systems engineering. Modeling languages can be used to specify: system requirements: structures and behaviors.Modeling languages are intended to be used to precisely specify systems so that stakeholders (e.g.: customers: operators: analysts: designers) can better understand the system being modeled. The more mature modeling languages are precise: consistent and executable. Informal diagramming techniques applied with drawing tools are expected to produce useful pictorial representations of system requirements: structures and behaviors: which can be useful for communication: design: and problem solving but cannot be used programmatically.: 539 Executable modeling languages applied with proper tool support: however: are expected to automate system verification and validation: simulation and code generation from the same representations. == Quality == A review of modelling languages is essential to be able to assign which languages are appropriate for different modelling settings. In the term settings we include stakeholders: domain and the knowledge connected. Assessing the language quality is a means that aims to achieve better models. === Framework for evaluation === Here language quality is stated in accordance with the SEQUAL framework for quality of models developed by Krogstie: Sindre and Lindland (2003): since this is a framework that connects the language quality to a framework for general model\nquality. Five areas are used in this framework to describe language quality and these are supposed to express both the conceptual as well as the visual notation of the language. We will not go into a thoroughly explanation of the underlying quality framework of models but concentrate on the areas used to explain the language quality framework. ==== Domain appropriateness ==== The framework states the ability to represent the domain as domain appropriateness. The statement appropriateness can be a bit vague: but in this particular context it means able to express. You should ideally only be able to express things that are in the domain but be powerful enough to include everything that is in the domain. This requirement might seem a bit strict: but the aim is to get a visually expressed model which includes everything relevant to the domain and excludes everything not appropriate for the domain. To achieve this: the language has to have a good distinction of which notations and syntaxes that are advantageous to present. ==== Participant appropriateness ==== To evaluate the participant appropriateness we try to identify how well the language expresses the knowledge held by the stakeholders. This involves challenges since a stakeholder's knowledge is subjective. The knowledge of the stakeholder is both tacit and explicit. Both types of knowledge are of dynamic character. In this framework only the explicit type of knowledge is taken into account. The language should to a large extent express all the explicit knowledge of the stakeholders relevant to the domain. ==== Modeller appropriateness ==== Last paragraph stated that knowledge of the stakeholders should be presented in a"": 'original_similarity': 0.7302578406820828: 'slide': 268}"
0.44663130000117235,"What is the status of Chinchilla as of January 12, 2023?","As of January 12, 2023, Chinchilla was still in the testing phase.","{'chunk_text': 'fine-tuning. Based on the training of previously employed language models: it has been determined that if one doubles the model size: one must also have twice the number of training tokens. This hypothesis has been used to train Chinchilla by DeepMind. Similar to Gopher in terms of cost: Chinchilla has 70B parameters and four times as much data.Chinchilla has an average accuracy of 67.5% on the MMLU benchmark (Measuring Massive Multitask Language Understanding): which is 7% higher than Gopher\'s performance. Chinchilla was still in the testing phase as of January 12: 2023.Chinchilla contributes to developing an effective training paradigm for large autoregressive language models with limited compute resources. The Chinchilla team recommends that the number of training tokens is twice for every model size doubling: meaning that using larger: higher-quality training datasets can lead to better results on downstream tasks. == Architecture == Both the Gopher family and Chinchilla family are families of transformer models. In particular: they are essentially the same as GPT-2: with different sizes and minor modifications. Gopher family uses RMSNorm instead of LayerNorm; relative positional encoding rather than absolute positional encoding. The Chinchilla family is the same as the Gopher family: but trained with AdamW instead of Adam optimizer. The Gopher family contains six models of increasing size: from 44 million parameters to 280 billion parameters. They refer to the largest one as ""Gopher"" by default. Similar naming conventions apply for the Chinchilla family. Table 1 of shows the entire Gopher family: Table 4 of compares the 70-billion-parameter Chinchilla with Gopher 280B. == See also == LaMDA == References ==\nClaude is a family of large language models developed by Anthropic. The first model was released in March 2023. Claude 3: released in March 2024: can also analyze images. == Training == Claude models are generative pre-trained transformers. They have been pre-trained to predict the next word in large amounts of text. Claude models have then been fine-tuned with Constitutional AI and Reinforcement Learning From Human Feedback: with the aim of making': 'original_similarity': 0.7991368202603177: 'slide': 70}","{'chunk_text': 'suggested that LLM creators should exclude from their training data papers on creating or enhancing pathogens.A study by researchers at Google and several universities: including Cornell University and University of California: Berkeley: showed that there are potential security risks in language models such as ChatGPT. In their study: they examined the possibility that questioners could get: from ChatGPT: the training data that the AI model used; they found that they could get the training data from the AI model. For example: when asking ChatGPT 3.5 turbo to repeat the word ""poem"" forever: the AI model will say ""poem"" hundreds of times and then diverge: deviating from the standard dialogue style and spitting out nonsense phrases: thus spitting out the training data as it is. The researchers have seen more than 10:000 examples of the AI model exposing their training data in a similar method. The researchers said that it was hard to tell if the AI model was actually safe or not.The potential presence of ""sleeper agents"" within LLM models is another emerging security concern. These are hidden functionalities built into the model that remain dormant until triggered by a specific event or condition. Upon activation: the LLM deviates from its expected behavior to make insecure actions. === Algorithmic bias === While LLMs have shown remarkable capabilities in generating human-like text: they are susceptible to inheriting and amplifying biases present in their training data. This can manifest in skewed representations or unfair treatment of different demographics: such as those based on race: gender: language: and cultural groups. Since English data is overrepresented in current large language models\' training data: it may also downplay non-English views. ==== Stereotyping ==== AI models can reinforce a wide range of stereotypes: including those based on gender: ethnicity: age: nationality: religion: or occupation. This can lead to outputs that unfairly generalize or caricature groups of people: sometimes in harmful or derogatory ways.Notably: gender bias refers to the tendency of these models to produce outputs that are unfairly prejudiced towards one gender over another. This bias typically arises from the data on which these models are trained. Large language models often assign roles and characteristics based on traditional gender norms. For example: it might associate nurses or secretaries predominantly with women and engineers or CEOs with men. ==== Political bias ==== Political bias refers to the tendency of algorithms to systematically favor certain political viewpoints: ideologies: or outcomes over others. Language models may also exhibit political biases. Since the training data includes a wide range of political opinions and coverage: the models might generate responses that lean towards particular political ideologies or viewpoints: depending on the prevalence of those views in the data. == List == For the training cost column: 1 petaFLOP-day = 1 petaFLOP/sec × 1 day = 8.64E19 FLOP. == See also == Foundation models == Notes == == References == == Further reading == Jurafsky: Dan: Martin: James. H. Speech and Language Processing: An Introduction to Natural Language Processing: Computational Linguistics: and Speech Recognition: 3rd Edition draft: 2023. Phuong: Mary; Hutter: Marcus (2022). ""Formal Algorithms for Transformers"". arXiv:2207.09238 [cs.LG]. Eloundou: Tyna; Manning: Sam; Mishkin: Pamela; Rock: Daniel (2023). ""GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models"". arXiv:2303.10130 [econ.GN]. Eldan: Ronen; Li: Yuanzhi (2023). ""TinyStories: How Small Can Language Models Be and Still Speak Coherent English?"". arXiv:2305.07759 [cs.CL]. Frank: Michael C. (27 June 2023). ""Baby steps in evaluating the capacities of large language models"". Nature Reviews Psychology. 2 (8): 451–452. doi:10.1038/s44159-023-00211-x. ISSN 2731-0574. S2CID 259713140. Retrieved 2 July 2023. Zhao: Wayne Xin; et al. (2023). ""A Survey of Large Language Models"". arXiv:2303.18223 [cs.CL]. Kaddour: Jean; et al. (2023). ""Challenges and Applications of Large Language Models"". arXiv:2307.10169 [cs.CL]. Yin: Shukang; Fu: Chaoyou; Zhao: Sirui; Li: Ke; Sun: Xing; Xu: Tong; Chen: Enhong (2023-06-01). ""A Survey on Multimodal Large Language Models"". arXiv:2306.13549 [cs.CV]. Open LLMs repository on GitHub.\nA large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification. LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process. LLMs can be used for text generation: a form of generative AI: by taking an input text and repeatedly predicting the next token or word.LLMs are artificial neural networks. The largest and most capable: as of March 2024: are built with a decoder-only transformer-based architecture while some recent implementations are based on other architectures: such as recurrent neural network variants and Mamba (a state space model).Up to 2020: fine tuning was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models: such as GPT-3: however: can be prompt-engineered to achieve similar results. They are thought to acquire knowledge about syntax: semantics and ""ontology"" inherent in human language corpora: but also inaccuracies and biases present in the corpora.Some notable LLMs are OpenAI\'s GPT series of models (e.g.: GPT-3.5 and GPT-4: used in ChatGPT and Microsoft Copilot): Google\'s PaLM and Gemini (the latter of which is currently used in the chatbot of the same name): xAI\'s Grok: Meta\'s LLaMA family of open-source models: Anthropic\'s Claude models: and Mistral AI\'s open source models. == History == At the 2017 NeurIPS conference: Google researchers introduced the transformer architecture in their landmark paper ""Attention Is All You Need"". This paper\'s': 'original_similarity': 0.7090247101273024: 'slide': 248}","{'chunk_text': 'Some commenters expressed concern over accidental or deliberate creation of misinformation: or other forms of misuse. For example: the availability of large language models could reduce the skill-level required to commit bioterrorism; biosecurity researcher Kevin Esvelt has suggested that LLM creators should exclude from their training data papers on creating or enhancing pathogens.A study by researchers at Google and several universities: including Cornell University and University of California: Berkeley: showed that there are potential security risks in language models such as ChatGPT. In their study: they examined the possibility that questioners could get: from ChatGPT: the training data that the AI model used; they found that they could get the training data from the AI model. For example: when asking ChatGPT 3.5 turbo to repeat the word ""poem"" forever: the AI model will say ""poem"" hundreds of times and then diverge: deviating from the standard dialogue style and spitting out nonsense phrases: thus spitting out the training data as it is. The researchers have seen more than 10:000 examples of the AI model exposing their training data in a similar method. The researchers said that it was hard to tell if the AI model was actually safe or not.The potential presence of ""sleeper agents"" within LLM models is another emerging security concern. These are hidden functionalities built into the model that remain dormant until triggered by a specific event or condition. Upon activation: the LLM deviates from its expected behavior to make insecure actions. === Algorithmic bias === While LLMs have shown remarkable capabilities in generating human-like text: they are susceptible to inheriting and amplifying biases present in their training data. This can manifest in skewed representations or unfair treatment of different demographics: such as those based on race: gender: language: and cultural groups. Since English data is overrepresented in current large language models\' training data: it may also downplay non-English views. ==== Stereotyping ==== AI models can reinforce a wide range of stereotypes: including those based on gender: ethnicity: age: nationality: religion: or occupation. This can lead to outputs that unfairly generalize or caricature groups of people: sometimes in harmful or derogatory ways.Notably: gender bias refers to the tendency of these models to produce outputs that are unfairly prejudiced towards one gender over another. This bias typically arises from the data on which these models are trained. Large language models often assign roles and characteristics based on traditional gender norms. For example: it might associate nurses or secretaries predominantly with women and engineers or CEOs with men. ==== Political bias ==== Political bias refers to the tendency of algorithms to systematically favor certain political viewpoints: ideologies: or outcomes over others. Language models may also exhibit political biases. Since the training data includes a wide range of political opinions and coverage: the models might generate responses that lean towards particular political ideologies or viewpoints: depending on the prevalence of those views in the data. == List == For the training cost column: 1 petaFLOP-day = 1 petaFLOP/sec × 1 day = 8.64E19 FLOP. == See also == Foundation models == Notes == == References == == Further reading == Jurafsky: Dan: Martin: James. H. Speech and Language Processing: An Introduction to Natural Language Processing: Computational Linguistics: and Speech Recognition: 3rd Edition draft: 2023. Phuong: Mary; Hutter: Marcus (2022). ""Formal Algorithms for Transformers"". arXiv:2207.09238 [cs.LG]. Eloundou: Tyna; Manning: Sam; Mishkin: Pamela; Rock: Daniel (2023). ""GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models"". arXiv:2303.10130 [econ.GN]. Eldan: Ronen; Li: Yuanzhi (2023). ""TinyStories: How Small Can Language Models Be and Still Speak Coherent English?"". arXiv:2305.07759 [cs.CL]. Frank: Michael C. (27 June 2023). ""Baby steps in evaluating the capacities of large language models"". Nature Reviews Psychology. 2 (8): 451–452. doi:10.1038/s44159-023-00211-x. ISSN 2731-0574. S2CID 259713140. Retrieved 2 July 2023. Zhao: Wayne Xin; et al. (2023). ""A Survey of Large Language Models"". arXiv:2303.18223 [cs.CL]. Kaddour: Jean; et al. (2023). ""Challenges and Applications of Large Language Models"". arXiv:2307.10169 [cs.CL]. Yin: Shukang; Fu: Chaoyou; Zhao: Sirui; Li: Ke; Sun: Xing; Xu: Tong; Chen: Enhong (2023-06-01). ""A Survey on Multimodal Large Language Models"". arXiv:2306.13549 [cs.CV]. Open LLMs repository on GitHub.\nA modeling language is any artificial language that can be used to express data: information or knowledge or systems in a structure that is defined by a consistent set of rules. The rules are used for interpretation of the meaning of components in the structure Programing language. == Overview == A modeling language can be graphical or textual. Graphical modeling languages use a diagram technique with named symbols that represent concepts and lines that connect the symbols and represent relationships and various other graphical notation to represent constraints. Textual modeling languages may use standardized keywords accompanied by parameters or natural language terms and phrases to make computer-interpretable expressions.An example of a graphical modeling language and a corresponding textual modeling language is EXPRESS. Not all modeling languages are executable: and for those that are: the use of them doesn\'t necessarily mean that programmers are no longer required. On the contrary: executable modeling languages are intended to amplify the productivity of skilled programmers: so that they can address more challenging problems: such as parallel computing and distributed systems. A large number of modeling languages appear in the literature. == Type of modeling languages == === Graphical types === Example of graphical modeling languages in the field of computer science: project management and systems engineering: Behavior': 'original_similarity': 0.7089938437153811: 'slide': 211}"
0.466278699983377,What are dilution and dropout techniques in artificial neural networks?,"Dilution and dropout techniques in artificial neural networks are regularization techniques aimed at reducing overfitting by preventing complex co-adaptations on training data. In these processes, dilution refers to thinning weights, while dropout refers to randomly ""dropping out"", or omitting, units during the training process of a neural network.","{'chunk_text': ""== Types and uses == Dilution is usually split in weak dilution and strong dilution. Weak dilution describes the process in which the finite fraction of removed connections is small: and strong dilution refers to when this fraction is large. There is no clear distinction on where the limit between strong and weak dilution is: and often the distinction is dependent on the precedent of a specific use-case and has implications for how to solve for exact solutions. Sometimes dilution is used for adding damping noise to the inputs. In that case: weak dilution refers to adding a small amount of damping noise: while strong dilution refers to adding a greater amount of damping noise. Both can be rewritten as variants of weight dilution. These techniques are also sometimes referred to as random pruning of weights: but this is usually a non-recurring one-way operation. The network is pruned: and then kept if it is an improvement over the previous model. Dilution and dropout both refer to an iterative process. The pruning of weights typically does not imply that the network continues learning: while in dilution/dropout: the network continues to learn after the technique is applied. == Generalized linear network == Output from a layer of linear nodes: in an artificial neural net can be described as yi{\\displaystyle y_{i}} – output from node i{\\displaystyle i} wij{\\displaystyle w_{ij}} – real weight before dilution: also called the Hebb connection strength xj{\\displaystyle x_{j}} – input from node j{\\displaystyle j}This can be written in vector notation as y{\\displaystyle \\mathbf {y} } – output vector W{\\displaystyle \\mathbf {W} } – weight matrix x{\\displaystyle \\mathbf {x} } – input vectorEquations (1) and (2) are used in the subsequent sections. == Weak dilution == During weak dilution: the finite fraction of removed connections (the weights) is small: giving rise to a tiny uncertainty. This edge-case can be solved exactly with mean field theory. In weak dilution the impact on the weights can be described as wij^{\\displaystyle {\\hat {w_{ij}}}} – diluted weight wij{\\displaystyle w_{ij}} – real weight before dilution P(c){\\displaystyle P(c)} – the probability of c{\\displaystyle c}: the probability of keeping a weightThe interpretation of probability P(c){\\displaystyle P(c)} can also be changed from keeping a weight into pruning a weight. In vector notation this can be written as where the function g\u2061(⋅){\\displaystyle \\operatorname {g} (\\cdot )} imposes the previous dilution. In weak dilution only a small and fixed fraction of the weights are diluted. When the number of terms in the sum goes to infinite (the weights for each node) it is still infinite (the fraction is fixed): thus mean field theory can be applied. In the notation from Hertz et al. this would be written as ⟨hi⟩{\\displaystyle \\left\\langle h_{i}\\right\\rangle } the mean field temperature c{\\displaystyle c} – a scaling factor for the temperature from the probability of keeping the weight wij{\\displaystyle w_{ij}} – real weight before dilution: also called the Hebb connection strength ⟨Sj⟩{\\displaystyle \\left\\langle S_{j}\\right\\rangle } – the mean stable equilibrium statesThere are some assumptions for this to hold: which are not listed here. == Strong dilution == When the dilution is strong: the finite fraction of removed connections (the weights) is large: giving rise to a huge uncertainty. == Dropout == Dropout is a special case of the previous weight equation (3): where the aforementioned equation is adjusted to remove a whole row in the vector matrix: and not only random weights P(c){\\displaystyle P(c)} – the probability c{\\displaystyle c} to keep a row in the weight matrix wj{\\displaystyle \\mathbf {w} _{j}} – real row in the weight matrix before dropout wj^{\\displaystyle {\\hat {\\mathbf {w} _{j}}}} – diluted row in the weight matrixBecause dropout removes a whole row from the vector matrix: the previous (unlisted) assumptions for weak dilution and the use of mean field theory are not applicable. The process by which the node is driven to zero: whether by setting the weights to zero: by “removing the node”: or by some other means: does not impact the end result and does not create a new and unique case. If the neural net is processed by a high-performance digital array-multiplicator: then it is likely more effective to drive the value to zero late in the process graph. If the net is processed by a constrained processor: perhaps even an analog neuromorph processor: then it is likely a more power-efficient solution is to drive the value to zero early in the process graph. == Google's patent == Although there have been examples of randomly removing connections between neurons in a neural network to improve models: this technique was first introduced with the name dropout by Geoffrey Hinton: et al. in 2012. Google currently holds the patent for the dropout technique. == See also == AlexNet Convolutional neural network § Dropout == Notes == == References ==\nA feedforward neural network (FNN) is one of the two broad types of artificial neural network: characterized by direction of the flow of information between its layers. Its flow is uni-directional: meaning that the information in the model flows in only one direction—forward—from the input nodes: through the hidden nodes (if any) and to the output nodes: without any cycles or loops: in contrast to recurrent neural networks: which have"": 'original_similarity': 0.9065197273070548: 'slide': 68}","{'chunk_text': 'all training data: dropout decreases overfitting. The method also significantly improves training speed. This makes the model combination practical: even for deep neural networks. The technique seems to reduce node interactions: leading them to learn more robust features that better generalize to new data. ==== DropConnect ==== DropConnect is the generalization of dropout in which each connection: rather than each output unit: can be dropped with probability 1−p{\\displaystyle 1-p}. Each unit thus receives input from a random subset of units in the previous layer.DropConnect is similar to dropout as it introduces dynamic sparsity within the model: but differs in that the sparsity is on the weights: rather than the output vectors of a layer. In other words: the fully connected layer with DropConnect becomes a sparsely connected layer in which the connections are chosen at random during the training stage. ==== Stochastic pooling ==== A major drawback to Dropout is that it does not have the same benefits for convolutional layers: where the neurons are not fully connected. Even before Dropout: in 2013 a technique called stochastic pooling: the conventional deterministic pooling operations were replaced with a stochastic procedure: where the activation within each pooling region is picked randomly according to a multinomial distribution: given by the activities within the pooling region. This approach is free of hyperparameters and can be combined with other regularization approaches: such as dropout and data augmentation. An alternate view of stochastic pooling is that it is equivalent to standard max pooling but with many copies of an input image: each having small local deformations. This is similar to explicit elastic deformations of the input images: which delivers excellent performance on the MNIST data set. Using stochastic pooling in a multilayer model gives an exponential number of deformations since the selections in higher layers are independent of those below. ==== Artificial data ==== Because the degree of model overfitting is determined by both its power and the amount of training it receives: providing a convolutional network with more training examples can reduce overfitting. Because there is often not enough available data to train: especially considering that some part should be spared for later testing: two approaches are to either generate new data from scratch (if possible) or perturb existing data to create new ones. The latter one is used since mid-1990s. For example: input images can be cropped: rotated: or rescaled to create new examples with the same labels as the original training set. === Explicit === ==== Early stopping ==== One of the simplest methods to prevent overfitting of a network is to simply stop the training before overfitting has had a chance to occur. It comes with the disadvantage that the learning process is halted. ==== Number of parameters ==== Another simple way to prevent overfitting is to limit the number of parameters: typically by limiting the number of hidden units in each layer or limiting network depth. For convolutional networks: the filter size also affects the number of parameters. Limiting the number of parameters restricts the predictive power of the network directly: reducing the complexity of the function that it can perform on the data: and thus limits the amount of overfitting. This is equivalent to a ""zero norm"". ==== Weight decay ==== A simple form of added regularizer is weight decay: which simply adds an additional error: proportional to the sum of weights (L1 norm) or squared magnitude (L2 norm) of the weight vector: to the error at each node. The level of acceptable model complexity can be reduced by increasing the proportionality constant(\'alpha\' hyperparameter): thus increasing the penalty for large weight vectors. L2 regularization is the most common form of regularization. It can be implemented by penalizing the squared magnitude of all parameters directly in the objective. The L2 regularization has the intuitive interpretation of heavily penalizing peaky weight vectors and preferring diffuse weight vectors. Due to multiplicative interactions between weights and inputs this has the useful property of encouraging the network to use all of its inputs a little rather than some of its inputs a lot. L1 regularization is also common. It makes the weight vectors sparse during optimization. In other words: neurons with L1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the noisy inputs. L1 with L2 regularization can be combined; this is called elastic net regularization. ==== Max norm constraints ==== Another form of regularization is to enforce an absolute upper bound on the magnitude of the weight vector for every neuron and use projected gradient descent to enforce the constraint. In practice: this corresponds to performing the parameter update as normal: and then enforcing the constraint by clamping the weight vector w→{\\displaystyle {\\vec {w}}} of every neuron to satisfy ‖w→‖2<c{\\displaystyle \\|{\\vec {w}}\\|_{2}<c}. Typical values of c{\\displaystyle c} are order of 3–4. Some papers report improvements when using this form of regularization. == Hierarchical coordinate frames == Pooling loses the precise spatial relationships between high-level parts (such as nose and mouth in a face image). These relationships are needed for identity recognition. Overlapping the pools so that each feature occurs in multiple pools: helps retain the information. Translation alone cannot extrapolate the understanding of geometric relationships to a radically new viewpoint: such as a different orientation or scale. On the other hand: people are very good at extrapolating; after seeing a\nnew shape once they can recognize it from a different viewpoint.An earlier common way to deal with this problem is to train the network on transformed data in different orientations: scales: lighting: etc. so that the network can cope with these variations. This is computationally intensive for large data-sets. The alternative is to use a hierarchy of coordinate frames and use a group of neurons to represent a conjunction of the shape of the feature and its pose relative to the retina. The pose relative to the retina is the relationship between the coordinate frame of the retina and the intrinsic features\' coordinate frame.Thus: one': 'original_similarity': 0.847389671188027: 'slide': 103}","{'chunk_text': 'multilayer perceptron (MLP) with 3 layers: an input layer: a hidden layer with randomized weights that did not learn: and an output layer. It also introduced variants: including a version with four-layer perceptrons where the last two layers have learned weights (and thus a proper multilayer perceptron).: section 16 In addition: term deep learning was proposed in 1986 by Rina Dechter although the history of its appearance is apparently more complicated.The first general: working learning algorithm for supervised: deep: feedforward: multilayer perceptrons was published by Alexey Ivakhnenko and Lapa in 1967. A 1971 paper described a deep network with eight layers trained by the group method of data handling.The first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun\'ichi Amari. In computer experiments conducted by Amari\'s student Saito: a five layer MLP with two modifiable layers learned internal representations to classify non-linearily separable pattern classes. In 1987 Matthew Brand reported that wide 12-layer nonlinear perceptrons could be fully end-to-end trained to reproduce logic functions of nontrivial circuit depth via gradient descent on small batches of random input/output samples: but concluded that training time on contemporary hardware (sub-megaflop computers) made the technique impractical: and proposed using fixed random early layers as an input hash for a single modifiable layer. Instead: subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique. In 1970: Seppo Linnainmaa published the reverse mode of automatic differentiation of discrete connected networks of nested differentiable functions. This became known as backpropagation. It is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes. The terminology ""back-propagating errors"" was actually introduced in 1962 by Rosenblatt: but he did not know how to implement this: although Henry J. Kelley had a continuous precursor of backpropagation already in 1960 in the context of control theory. In 1982: Paul Werbos applied backpropagation to MLPs in the way that has become standard. In 1985: David E. Rumelhart et al. published an experimental analysis of the technique.Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers began with the Neocognitron introduced by Kunihiko Fukushima in 1980. In 1969: he also introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for CNNs and deep learning in general. CNNs have become an essential tool for computer vision. The term Deep Learning was introduced to the machine learning community by Rina Dechter in 1986: and to artificial neural networks by Igor Aizenberg and colleagues in 2000: in the context of Boolean threshold neurons.In 1988: Wei Zhang et al. applied the backpropagation algorithm to a convolutional neural network (a simplified Neocognitron with convolutional interconnections between the image feature layers and the last fully connected layer) for alphabet recognition. They also proposed an implementation of the CNN with an optical computing system. In 1989: Yann LeCun et al. applied backpropagation to a CNN with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked: training required 3 days. Subsequently: Wei Zhang: et al. modified their model by removing the last fully connected layer and applied it for medical image object segmentation in 1991 and breast cancer detection in mammograms in 1994. LeNet-5 (1998): a 7-level CNN by Yann LeCun et al.: that classifies digits: was applied by several banks to recognize hand-written numbers on checks digitized in 32x32 pixel images. In the 1980s: backpropagation did not work well for deep learning with long credit assignment paths. To overcome this problem: Jürgen Schmidhuber (1992) proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning. It uses predictive coding to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN: by distilling a higher level chunker network into a lower level automatizer network. In 1993: a chunker solved a deep learning task whose depth exceeded 1000.In 1992: Jürgen Schmidhuber also published an alternative to RNNs which is now called a linear Transformer or a Transformer with linearized self-attention (save for a normalization operator). It learns internal spotlights of attention: a slow feedforward neural network learns by gradient descent to control the fast weights of another neural network through outer products of self-generated activation patterns FROM and TO (which are now called key and value for self-attention). This fast weight attention mapping is applied to a query pattern. The modern Transformer was introduced by Ashish Vaswani et al. in their 2017 paper ""Attention Is All You Need"". It combines this with a softmax operator and a projection matrix. Transformers have increasingly become the model of choice for natural language processing. Many modern large language models such as ChatGPT: GPT-4: and BERT use it. Transformers are also increasingly being used in computer vision.In 1991: Jürgen Schmidhuber also published adversarial neural networks that contest with each other in the form of a zero-sum game: where one network\'s gain is the other network\'s loss. The first network is a\ngenerative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called ""artificial curiosity"". In 2014: this principle was used in a generative adversarial network (GAN) by Ian Goodfellow et al. Here the environmental reaction is 1 or 0 depending on whether the first network\'s output is in a given set. This can be used to create realistic deepfakes. Excellent image quality is achieved by Nvidia\'s StyleGAN (2018) based on the Progressive GAN by Tero Karras et al. Here the GAN generator is grown from small to large scale in a pyramidal fashion. Sepp Hochreiter\'s diploma thesis (1991) was called ""one of the most important documents in the history of machine learning"" by his supervisor Schmidhuber. It not only tested the neural history compressor: but also identified and analyzed the vanishing gradient': 'original_similarity': 0.8375984957235352: 'slide': 148}"
0.3898946999979671,How are weak dilution and strong dilution differentiated in neural networks?,"In neural networks, weak dilution is when the finite fraction of removed connections is small, often adding a small amount of damping noise to the inputs. Conversely, strong dilution refers to when this fraction of removed connections is large, often accompanied by adding a greater amount of damping noise.","{'chunk_text': 'fraction is large. There is no clear distinction on where the limit between strong and weak dilution is: and often the distinction is dependent on the precedent of a specific use-case and has implications for how to solve for exact solutions. Sometimes dilution is used for adding damping noise to the inputs. In that case: weak dilution refers to adding a small amount of damping noise: while strong dilution refers to adding a greater amount of damping noise. Both can be rewritten as variants of weight dilution. These techniques are also sometimes referred to as random pruning of weights: but this is usually a non-recurring one-way operation. The network is pruned: and then kept if it is an improvement over the previous model. Dilution and dropout both refer to an iterative process. The pruning of weights typically does not imply that the network continues learning: while in dilution/dropout: the network continues to learn after the technique is applied. == Generalized linear network == Output from a layer of linear nodes: in an artificial neural net can be described as yi{\\displaystyle y_{i}} – output from node i{\\displaystyle i} wij{\\displaystyle w_{ij}} – real weight before dilution: also called the Hebb connection strength xj{\\displaystyle x_{j}} – input from node j{\\displaystyle j}This can be written in vector notation as y{\\displaystyle \\mathbf {y} } – output vector W{\\displaystyle \\mathbf {W} } – weight matrix x{\\displaystyle \\mathbf {x} } – input vectorEquations (1) and (2) are used in the subsequent sections. == Weak dilution == During weak dilution: the finite fraction of removed connections (the weights) is small: giving rise to a tiny uncertainty. This edge-case can be solved exactly with mean field theory. In weak dilution the impact on the weights can be described as wij^{\\displaystyle {\\hat {w_{ij}}}} – diluted weight wij{\\displaystyle w_{ij}} – real weight before dilution P(c){\\displaystyle P(c)} – the probability of c{\\displaystyle c}: the probability of keeping a weightThe interpretation of probability P(c){\\displaystyle P(c)} can also be changed from keeping a weight into pruning a weight. In vector notation this can be written as where the function g\u2061(⋅){\\displaystyle \\operatorname {g} (\\cdot )} imposes the previous dilution. In weak dilution only a small and fixed fraction of the weights are diluted. When the number of terms in the sum goes to infinite (the weights for each node) it is still infinite (the fraction is fixed): thus mean field theory can be applied. In the notation from Hertz et al. this would be written as ⟨hi⟩{\\displaystyle \\left\\langle h_{i}\\right\\rangle } the mean field temperature c{\\displaystyle c} – a scaling factor for the temperature from the probability of keeping the weight wij{\\displaystyle w_{ij}} – real weight before dilution: also called the Hebb connection strength ⟨Sj⟩{\\displaystyle \\left\\langle S_{j}\\right\\rangle } – the mean stable equilibrium statesThere are some assumptions for this to hold: which are not listed here. == Strong dilution == When the dilution is strong: the finite fraction of removed connections (the weights) is large: giving rise to a huge uncertainty. == Dropout == Dropout is a special case of the previous weight equation (3): where the aforementioned equation is adjusted to remove a whole row in the vector matrix: and not only random weights P(c){\\displaystyle P(c)} – the probability c{\\displaystyle c} to keep a row in the weight matrix wj{\\displaystyle \\mathbf {w} _{j}} – real row in the weight matrix before dropout wj^{\\displaystyle {\\hat {\\mathbf {w} _{j}}}} – diluted row in the weight matrixBecause dropout removes a whole row from the vector matrix: the previous (unlisted) assumptions for weak dilution and the use of mean field theory are not applicable. The process by which the node is driven to zero: whether by setting the weights to zero: by “removing the node”: or by some other means: does not impact the end result and does not create a new and unique case. If the neural net is processed by a high-performance digital array-multiplicator: then it is likely more effective to drive the value to zero late in the process graph. If the net is processed by a constrained processor: perhaps even an analog neuromorph processor: then it is likely a more power-efficient solution is to drive the value to zero early in the process graph. == Google\'s patent == Although there have been examples of randomly removing connections between neurons in a neural network to improve models: this technique was first introduced with the name dropout by Geoffrey Hinton: et al. in 2012. Google currently holds the patent for the dropout technique. == See also == AlexNet Convolutional neural network § Dropout == Notes == == References ==\nA feedforward neural network (FNN) is one of the two broad types of artificial neural network: characterized by direction of the flow of information between its layers. Its flow is uni-directional: meaning that the information in the model flows in only one direction—forward—from the input nodes: through the hidden nodes (if any) and to the output nodes: without any cycles or loops: in contrast to recurrent neural networks: which have a bi-directional flow. Modern feedforward networks are trained using the backpropagation method and are colloquially referred to as the ""vanilla"" neural networks. == Timeline == In 1958: a layered network of perceptrons: consisting of an input layer:': 'original_similarity': 0.8704811515813473: 'slide': 105}","{'chunk_text': 'interval of network output: assuming a normal distribution. A confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified. By assigning a softmax activation function: a generalization of the logistic function: on the output layer of the neural network (or a softmax component in a component-based network) for categorical target variables: the outputs can be interpreted as posterior probabilities. This is useful in classification as it gives a certainty measure on classifications. The softmax activation function is: yi=exi∑j=1cexj{\\displaystyle y_{i}={\\frac {e^{x_{i}}}{\\sum _{j=1}^{c}e^{x_{j}}}}} == Criticism == === Training === A common criticism of neural networks: particularly in robotics: is that they require too many training samples for real-world operation. Any learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases. Potential solutions include randomly shuffling training examples: by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example: grouping examples in so-called mini-batches and/or introducing a recursive least squares algorithm for CMAC. Dean Pomerleau uses a neural network to train a robotic vehicle to drive on multiple types of roads (single lane: multi-lane: dirt: etc.): and a large amount of his research is devoted to extrapolating multiple training scenarios from a single training experience: and preserving past training diversity so that the system does not become overtrained (if: for example: it is presented with a series of right turns—it should not learn to always turn right). === Theory === A central claim of ANNs is that they embody new and powerful general principles for processing information. These principles are ill-defined. It is often claimed that they are emergent from the network itself. This allows simple statistical association (the basic function of artificial neural networks) to be described as learning or recognition. In 1997: Alexander Dewdney: a former Scientific American columnist: commented that as a result: artificial neural networks have a ""something-for-nothing quality: one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are. No human hand (or mind) intervenes; solutions are found as if by magic; and no one: it seems: has learned anything"". One response to Dewdney is that neural networks have been successfully used to handle many complex and diverse tasks: ranging from autonomously flying aircraft to detecting credit card fraud to mastering the game of Go. Technology writer Roger Bridgman commented: Neural networks: for instance: are in the dock not only because they have been hyped to high heaven: (what hasn\'t?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be ""an opaque: unreadable table...valueless as a scientific resource"". In spite of his emphatic declaration that science is not technology: Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having. Although it is true that analyzing what has been learned by an artificial neural network is difficult: it is much easier to do so than to analyze what has been learned by a biological neural network. Moreover: recent emphasis on the explainability of AI has contributed towards the development of methods: notably those based on attention mechanisms: for visualizing and explaining learned neural networks. Furthermore: researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles that allow a learning machine to be successful. For example: Bengio and LeCun (2007) wrote an article regarding local vs non-local learning: as well as shallow vs deep architecture.Biological brains use both shallow and deep circuits as reported by brain anatomy: displaying a wide variety of invariance. Weng argued that the brain self-wires largely according to signal statistics and therefore: a serial cascade cannot catch all major statistical dependencies. === Hardware === Large and effective neural networks require considerable computing resources. While the brain has hardware tailored to the task of processing signals through a graph of neurons: simulating even a simplified neuron on von Neumann architecture may consume vast amounts of memory and storage. Furthermore: the designer often needs to transmit signals through many of these connections and their associated neurons – which require enormous CPU power and time. Schmidhuber noted that the resurgence of neural networks in the twenty-first century is largely attributable to advances in hardware: from 1991 to 2015: computing power: especially as delivered by GPGPUs (on GPUs): has increased around a million-fold: making the standard backpropagation algorithm feasible for training networks that are several layers deeper than before. The use of accelerators such as FPGAs and GPUs can reduce training times from months to days.Neuromorphic engineering or a physical neural network addresses the hardware difficulty directly: by constructing non-von-Neumann chips to directly implement neural networks\nin circuitry. Another type of chip optimized for neural network processing is called a Tensor Processing Unit: or TPU. === Practical counterexamples === Analyzing what has been learned by an ANN is much easier than analyzing what has been learned by a biological neural network. Furthermore: researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a learning machine to be successful. For example: local vs. non-local learning and shallow vs. deep architecture. === Hybrid approaches === Advocates of hybrid models (combining neural networks and symbolic approaches) say that such a mixture can better capture the mechanisms of the human mind. === Dataset bias === Neural networks are dependent on the quality of the data they are trained on: thus low quality data with imbalanced representativeness can lead to the model learning and perpetuating societal biases. These inherited biases become especially critical when the ANNs are integrated into real-world scenarios where the training data may be imbalanced due to the scarcity of data for': 'original_similarity': 0.8232442209958879: 'slide': 168}","{'chunk_text': 'and weights. The vectors of weights and biases are called filters and represent particular features of the input (e.g.: a particular shape). A distinguishing feature of CNNs is that many neurons can share the same filter. This reduces the memory footprint because a single bias and a single vector of weights are used across all receptive fields that share that filter: as opposed to each receptive field having its own bias and vector weighting. == History == CNN are often compared to the way the brain achieves vision processing in living organisms. === Receptive fields in the visual cortex === Work by Hubel and Wiesel in the 1950s and 1960s showed that cat visual cortices contain neurons that individually respond to small regions of the visual field. Provided the eyes are not moving: the region of visual space within which visual stimuli affect the firing of a single neuron is known as its receptive field. Neighboring cells have similar and overlapping receptive fields. Receptive field size and location varies systematically across the cortex to form a complete map of visual space. The cortex in each hemisphere represents the contralateral visual field.Their 1968 paper identified two basic visual cell types in the brain: simple cells: whose output is maximized by straight edges having particular orientations within their receptive field complex cells: which have larger receptive fields: whose output is insensitive to the exact position of the edges in the field.Hubel and Wiesel also proposed a cascading model of these two types of cells for use in pattern recognition tasks. === Neocognitron: origin of the CNN architecture === The ""neocognitron"" was introduced by Kunihiko Fukushima in 1980. It was inspired by the above-mentioned work of Hubel and Wiesel. The neocognitron introduced the two basic types of layers in CNNs: A convolutional layer which contains units whose receptive fields cover a patch of the previous layer. The weight vector (the set of adaptive parameters) of such a unit is often called a filter. Units can share filters. Downsampling layers which contain units whose receptive fields cover patches of previous convolutional layers. Such a unit typically computes the average of the activations of the units in its patch. This downsampling helps to correctly classify objects in visual scenes even when the objects are shifted.In 1969: Kunihiko Fukushima also introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for CNNs and deep neural networks in general.In a variant of the neocognitron called the cresceptron: instead of using Fukushima\'s spatial averaging: J. Weng et al. in 1993 introduced a method called max-pooling where a downsampling unit computes the maximum of the activations of the units in its patch. Max-pooling is often used in modern CNNs.Several supervised and unsupervised learning algorithms have been proposed over the decades to train the weights of a neocognitron. Today: however: the CNN architecture is usually trained through backpropagation. The neocognitron is the first CNN which requires units located at multiple network positions to have shared weights. Convolutional neural networks were presented at the Neural Information Processing Workshop in 1987: automatically analyzing time-varying signals by replacing learned multiplication with convolution in time: and demonstrated for speech recognition. === Time delay neural networks === The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel et al. for phoneme recognition and was one of the first convolutional networks: as it achieved shift-invariance. A TDNN is a 1-D convolutional neural net where the convolution is performed along the time axis of the data. It is the first CNN utilizing weight sharing in combination with a training by gradient descent: using backpropagation. Thus: while also using a pyramidal structure as in the neocognitron: it performed a global optimization of the weights instead of a local one.. TDNNs are convolutional networks that share weights along the temporal dimension. They allow speech signals to be processed time-invariantly. In 1990 Hampshire and Waibel introduced a variant that performs a two-dimensional convolution. Since these TDNNs operated on spectrograms: the resulting phoneme recognition system was invariant to both time and frequency shifts. This inspired translation invariance in image processing with CNNs. The tiling of neuron outputs can cover timed stages.TDNNs now achieve the best performance in far-distance speech recognition. ==== Max pooling ==== In 1990 Yamaguchi et al. introduced the concept of max pooling: a fixed filtering operation that calculates and propagates the maximum value of a given region. They did so by combining TDNNs with max pooling to realize a speaker-independent isolated word recognition system. In their system they used several TDNNs per word: one for each syllable. The results of each TDNN over the input signal were combined using max pooling and the outputs of the pooling layers were then passed on to networks performing the actual word classification. === Image recognition with CNNs trained by gradient descent === Denker et\nal. (1989) designed a 2-D CNN system to recognize hand-written ZIP Code numbers. However: the lack of an efficient training method to determine the kernel coefficients of the involved convolutions meant that all the coefficients had to be laboriously hand-designed.Following the advances in the training of 1-D CNNs by Waibel et al. (1987): Yann LeCun et al. (1989) used back-propagation to learn the convolution kernel coefficients directly from images of hand-written numbers. Learning was thus fully automatic: performed better than manual coefficient design: and was suited to a broader range of image recognition problems and image types. Wei Zhang et al. (1988) used back-propagation to train the convolution kernels of a CNN for alphabets recognition. The model was called Shift-Invariant Artificial Neural Network (SIANN) before the name CNN was coined later in the early 1990s. Wei Zhang et al. also applied the same CNN without the last fully connected layer for medical image object segmentation (1991) and breast cancer detection in mammograms (1994).This approach became a foundation of modern computer vision. ==== LeNet-5 ==== LeNet-5: a pioneering 7-level convolutional network by LeCun et al. in 1995: that classifies': 'original_similarity': 0.8093408913462518: 'slide': 186}"
0.42583160000503995,How is dilution related to adding damping noise to inputs?,"Dilution can be used for adding damping noise to the inputs within a neural network. Weak dilution refers to adding a small amount of damping noise, while strong dilution refers to adding a larger amount of damping noise. These can be considered as variants of weight dilution.","{'chunk_text': 'dependent on the precedent of a specific use-case and has implications for how to solve for exact solutions. Sometimes dilution is used for adding damping noise to the inputs. In that case: weak dilution refers to adding a small amount of damping noise: while strong dilution refers to adding a greater amount of damping noise. Both can be rewritten as variants of weight dilution. These techniques are also sometimes referred to as random pruning of weights: but this is usually a non-recurring one-way operation. The network is pruned: and then kept if it is an improvement over the previous model. Dilution and dropout both refer to an iterative process. The pruning of weights typically does not imply that the network continues learning: while in dilution/dropout: the network continues to learn after the technique is applied. == Generalized linear network == Output from a layer of linear nodes: in an artificial neural net can be described as yi{\\displaystyle y_{i}} – output from node i{\\displaystyle i} wij{\\displaystyle w_{ij}} – real weight before dilution: also called the Hebb connection strength xj{\\displaystyle x_{j}} – input from node j{\\displaystyle j}This can be written in vector notation as y{\\displaystyle \\mathbf {y} } – output vector W{\\displaystyle \\mathbf {W} } – weight matrix x{\\displaystyle \\mathbf {x} } – input vectorEquations (1) and (2) are used in the subsequent sections. == Weak dilution == During weak dilution: the finite fraction of removed connections (the weights) is small: giving rise to a tiny uncertainty. This edge-case can be solved exactly with mean field theory. In weak dilution the impact on the weights can be described as wij^{\\displaystyle {\\hat {w_{ij}}}} – diluted weight wij{\\displaystyle w_{ij}} – real weight before dilution P(c){\\displaystyle P(c)} – the probability of c{\\displaystyle c}: the probability of keeping a weightThe interpretation of probability P(c){\\displaystyle P(c)} can also be changed from keeping a weight into pruning a weight. In vector notation this can be written as where the function g\u2061(⋅){\\displaystyle \\operatorname {g} (\\cdot )} imposes the previous dilution. In weak dilution only a small and fixed fraction of the weights are diluted. When the number of terms in the sum goes to infinite (the weights for each node) it is still infinite (the fraction is fixed): thus mean field theory can be applied. In the notation from Hertz et al. this would be written as ⟨hi⟩{\\displaystyle \\left\\langle h_{i}\\right\\rangle } the mean field temperature c{\\displaystyle c} – a scaling factor for the temperature from the probability of keeping the weight wij{\\displaystyle w_{ij}} – real weight before dilution: also called the Hebb connection strength ⟨Sj⟩{\\displaystyle \\left\\langle S_{j}\\right\\rangle } – the mean stable equilibrium statesThere are some assumptions for this to hold: which are not listed here. == Strong dilution == When the dilution is strong: the finite fraction of removed connections (the weights) is large: giving rise to a huge uncertainty. == Dropout == Dropout is a special case of the previous weight equation (3): where the aforementioned equation is adjusted to remove a whole row in the vector matrix: and not only random weights P(c){\\displaystyle P(c)} – the probability c{\\displaystyle c} to keep a row in the weight matrix wj{\\displaystyle \\mathbf {w} _{j}} – real row in the weight matrix before dropout wj^{\\displaystyle {\\hat {\\mathbf {w} _{j}}}} – diluted row in the weight matrixBecause dropout removes a whole row from the vector matrix: the previous (unlisted) assumptions for weak dilution and the use of mean field theory are not applicable. The process by which the node is driven to zero: whether by setting the weights to zero: by “removing the node”: or by some other means: does not impact the end result and does not create a new and unique case. If the neural net is processed by a high-performance digital array-multiplicator: then it is likely more effective to drive the value to zero late in the process graph. If the net is processed by a constrained processor: perhaps even an analog neuromorph processor: then it is likely a more power-efficient solution is to drive the value to zero early in the process graph. == Google\'s patent == Although there have been examples of randomly removing connections between neurons in a neural network to improve models: this technique was first introduced with the name dropout by Geoffrey Hinton: et al. in 2012. Google currently holds the patent for the dropout technique. == See also == AlexNet Convolutional neural network § Dropout == Notes == == References ==\nA feedforward neural network (FNN) is one of the two broad types of artificial neural network: characterized by direction of the flow of information between its layers. Its flow is uni-directional: meaning that the information in the model flows in only one direction—forward—from the input nodes: through the hidden nodes (if any) and to the output nodes: without any cycles or loops: in contrast to recurrent neural networks: which have a bi-directional flow. Modern feedforward networks are trained using the backpropagation method and are colloquially referred to as the ""vanilla"" neural networks. == Timeline == In 1958: a layered network of perceptrons: consisting of an input layer: a hidden layer with randomized weights that did not learn: and an output layer with learning connections: was introduced already by Frank Rosenblatt': 'original_similarity': 0.8102772208518647: 'slide': 128}","{'chunk_text': 'distinguishing feature of CNNs is that many neurons can share the same filter. This reduces the memory footprint because a single bias and a single vector of weights are used across all receptive fields that share that filter: as opposed to each receptive field having its own bias and vector weighting. == History == CNN are often compared to the way the brain achieves vision processing in living organisms. === Receptive fields in the visual cortex === Work by Hubel and Wiesel in the 1950s and 1960s showed that cat visual cortices contain neurons that individually respond to small regions of the visual field. Provided the eyes are not moving: the region of visual space within which visual stimuli affect the firing of a single neuron is known as its receptive field. Neighboring cells have similar and overlapping receptive fields. Receptive field size and location varies systematically across the cortex to form a complete map of visual space. The cortex in each hemisphere represents the contralateral visual field.Their 1968 paper identified two basic visual cell types in the brain: simple cells: whose output is maximized by straight edges having particular orientations within their receptive field complex cells: which have larger receptive fields: whose output is insensitive to the exact position of the edges in the field.Hubel and Wiesel also proposed a cascading model of these two types of cells for use in pattern recognition tasks. === Neocognitron: origin of the CNN architecture === The ""neocognitron"" was introduced by Kunihiko Fukushima in 1980. It was inspired by the above-mentioned work of Hubel and Wiesel. The neocognitron introduced the two basic types of layers in CNNs: A convolutional layer which contains units whose receptive fields cover a patch of the previous layer. The weight vector (the set of adaptive parameters) of such a unit is often called a filter. Units can share filters. Downsampling layers which contain units whose receptive fields cover patches of previous convolutional layers. Such a unit typically computes the average of the activations of the units in its patch. This downsampling helps to correctly classify objects in visual scenes even when the objects are shifted.In 1969: Kunihiko Fukushima also introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for CNNs and deep neural networks in general.In a variant of the neocognitron called the cresceptron: instead of using Fukushima\'s spatial averaging: J. Weng et al. in 1993 introduced a method called max-pooling where a downsampling unit computes the maximum of the activations of the units in its patch. Max-pooling is often used in modern CNNs.Several supervised and unsupervised learning algorithms have been proposed over the decades to train the weights of a neocognitron. Today: however: the CNN architecture is usually trained through backpropagation. The neocognitron is the first CNN which requires units located at multiple network positions to have shared weights. Convolutional neural networks were presented at the Neural Information Processing Workshop in 1987: automatically analyzing time-varying signals by replacing learned multiplication with convolution in time: and demonstrated for speech recognition. === Time delay neural networks === The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel et al. for phoneme recognition and was one of the first convolutional networks: as it achieved shift-invariance. A TDNN is a 1-D convolutional neural net where the convolution is performed along the time axis of the data. It is the first CNN utilizing weight sharing in combination with a training by gradient descent: using backpropagation. Thus: while also using a pyramidal structure as in the neocognitron: it performed a global optimization of the weights instead of a local one.. TDNNs are convolutional networks that share weights along the temporal dimension. They allow speech signals to be processed time-invariantly. In 1990 Hampshire and Waibel introduced a variant that performs a two-dimensional convolution. Since these TDNNs operated on spectrograms: the resulting phoneme recognition system was invariant to both time and frequency shifts. This inspired translation invariance in image processing with CNNs. The tiling of neuron outputs can cover timed stages.TDNNs now achieve the best performance in far-distance speech recognition. ==== Max pooling ==== In 1990 Yamaguchi et al. introduced the concept of max pooling: a fixed filtering operation that calculates and propagates the maximum value of a given region. They did so by combining TDNNs with max pooling to realize a speaker-independent isolated word recognition system. In their system they used several TDNNs per word: one for each syllable. The results of each TDNN over the input signal were combined using max pooling and the outputs of the pooling layers were then passed on to networks performing the actual word classification. === Image recognition with CNNs trained by gradient descent === Denker et\nal. (1989) designed a 2-D CNN system to recognize hand-written ZIP Code numbers. However: the lack of an efficient training method to determine the kernel coefficients of the involved convolutions meant that all the coefficients had to be laboriously hand-designed.Following the advances in the training of 1-D CNNs by Waibel et al. (1987): Yann LeCun et al. (1989) used back-propagation to learn the convolution kernel coefficients directly from images of hand-written numbers. Learning was thus fully automatic: performed better than manual coefficient design: and was suited to a broader range of image recognition problems and image types. Wei Zhang et al. (1988) used back-propagation to train the convolution kernels of a CNN for alphabets recognition. The model was called Shift-Invariant Artificial Neural Network (SIANN) before the name CNN was coined later in the early 1990s. Wei Zhang et al. also applied the same CNN without the last fully connected layer for medical image object segmentation (1991) and breast cancer detection in mammograms (1994).This approach became a foundation of modern computer vision. ==== LeNet-5 ==== LeNet-5: a pioneering 7-level convolutional network by LeCun et al. in 1995: that classifies digits: was applied by several banks to recognize hand-written numbers on checks (British English: cheques) digitized in 32x32 pixel images. The ability to': 'original_similarity': 0.7709575185548274: 'slide': 209}","{'chunk_text': ""network-wide bursting and difficulty with similarity found universally in sensory recognition. A mechanism to perform optimization during recognition is created using inhibitory feedback connections back to the same inputs that activate them. This reduces requirements during learning and allows learning and updating to be easier while still being able to perform complex recognition. A regulatory feedback network makes inferences using negative feedback. The feedback is used to find the optimal activation of units. It is most similar to a non-parametric method but is different from K-nearest neighbor in that it mathematically emulates feedforward networks. == Radial basis function (RBF) == Radial basis functions are functions that have a distance criterion with respect to a center. Radial basis functions have been applied as a replacement for the sigmoidal hidden layer transfer characteristic in multi-layer perceptrons. RBF networks have two layers: In the first: input is mapped onto each RBF in the 'hidden' layer. The RBF chosen is usually a Gaussian. In regression problems the output layer is a linear combination of hidden layer values representing mean predicted output. The interpretation of this output layer value is the same as a regression model in statistics. In classification problems the output layer is typically a sigmoid function of a linear combination of hidden layer values: representing a posterior probability. Performance in both cases is often improved by shrinkage techniques: known as ridge regression in classical statistics. This corresponds to a prior belief in small parameter values (and therefore smooth output functions) in a Bayesian framework. RBF networks have the advantage of avoiding local minima in the same way as multi-layer perceptrons. This is because the only parameters that are adjusted in the learning process are the linear mapping from hidden layer to output layer. Linearity ensures that the error surface is quadratic and therefore has a single easily found minimum. In regression problems this can be found in one matrix operation. In classification problems the fixed non-linearity introduced by the sigmoid output function is most efficiently dealt with using iteratively re-weighted least squares. RBF networks have the disadvantage of requiring good coverage of the input space by radial basis functions. RBF centres are determined with reference to the distribution of the input data: but without reference to the prediction task. As a result: representational resources may be wasted on areas of the input space that are irrelevant to the task. A common solution is to associate each data point with its own centre: although this can expand the linear system to be solved in the final layer and requires shrinkage techniques to avoid overfitting. Associating each input datum with an RBF leads naturally to kernel methods such as support vector machines (SVM) and Gaussian processes (the RBF is the kernel function). All three approaches use a non-linear kernel function to project the input data into a space where the learning problem can be solved using a linear model. Like Gaussian processes: and unlike SVMs: RBF networks are typically trained in a maximum likelihood framework by maximizing the probability (minimizing the error). SVMs avoid overfitting by maximizing instead a margin. SVMs outperform RBF networks in most classification applications. In regression applications they can be competitive when the dimensionality of the input space is relatively small. === How RBF networks work === RBF neural networks are conceptually similar to K-Nearest Neighbor (k-NN) models. The basic idea is that similar inputs produce similar outputs. Assume that each case in a training set has two predictor variables: x and y: and the target variable has two categories: positive and negative. Given a new case with predictor values x=6: y=5.1: how is the target variable computed? The nearest neighbor classification performed for this example depends on how many neighboring points are considered. If 1-NN is used and the closest point is negative: then the new point should be classified as negative. Alternatively: if 9-NN classification is used and the closest 9 points are considered: then the effect of the surrounding 8 positive points may outweigh the closest 9-th (negative) point. An RBF network positions neurons in the space described by the predictor variables (x:y in this example). This space has as many dimensions as predictor variables. The Euclidean distance is computed from the new point to the center of each neuron: and a radial basis function (RBF: also called a kernel function) is applied to the distance to compute the weight (influence) for each neuron. The radial basis function is so named because the radius distance is the argument to the function. Weight = RBF(distance) ==== Radial Basis Function ====\nThe value for the new point is found by summing the output values of the RBF functions multiplied by weights computed for each neuron. The radial basis function for a neuron has a center and a radius (also called a spread). The radius may be different for each neuron: and: in RBF networks generated by DTREG: the radius may be different in each dimension. With larger spread: neurons at a distance from a point have a greater influence. ==== Architecture ==== RBF networks have three layers: Input layer: One neuron appears in the input layer for each predictor variable. In the case of categorical variables: N-1 neurons are used where N is the number of categories. The input neurons standardizes the value ranges by subtracting the median and dividing by the interquartile range. The input neurons then feed the values to each of the neurons in the hidden layer. Hidden layer: This layer has a variable number of neurons (determined by the training process). Each neuron consists of a radial basis function centered on a point with as many dimensions as predictor variables. The spread (radius) of the RBF function may be different for each dimension. The centers and spreads are determined by training. When presented with the x vector of input values from the input layer: a hidden neuron computes the Euclidean distance of the test case from the neuron's center point and then applies the RBF kernel"": 'original_similarity': 0.7696426401432495: 'slide': 237}"
0.5026768000097945,How does the dropout technique differ from dilution in terms of implementation?,"The dropout technique can be considered a special case of dilution where the equation is adjusted to remove a whole row in the vector matrix, rather than just random weights. This process doesn’t rely on whether the weights are set to zero, the node is removed, or any other means. The end result remains the same.","{'chunk_text': ""process in which the finite fraction of removed connections is small: and strong dilution refers to when this fraction is large. There is no clear distinction on where the limit between strong and weak dilution is: and often the distinction is dependent on the precedent of a specific use-case and has implications for how to solve for exact solutions. Sometimes dilution is used for adding damping noise to the inputs. In that case: weak dilution refers to adding a small amount of damping noise: while strong dilution refers to adding a greater amount of damping noise. Both can be rewritten as variants of weight dilution. These techniques are also sometimes referred to as random pruning of weights: but this is usually a non-recurring one-way operation. The network is pruned: and then kept if it is an improvement over the previous model. Dilution and dropout both refer to an iterative process. The pruning of weights typically does not imply that the network continues learning: while in dilution/dropout: the network continues to learn after the technique is applied. == Generalized linear network == Output from a layer of linear nodes: in an artificial neural net can be described as yi{\\displaystyle y_{i}} – output from node i{\\displaystyle i} wij{\\displaystyle w_{ij}} – real weight before dilution: also called the Hebb connection strength xj{\\displaystyle x_{j}} – input from node j{\\displaystyle j}This can be written in vector notation as y{\\displaystyle \\mathbf {y} } – output vector W{\\displaystyle \\mathbf {W} } – weight matrix x{\\displaystyle \\mathbf {x} } – input vectorEquations (1) and (2) are used in the subsequent sections. == Weak dilution == During weak dilution: the finite fraction of removed connections (the weights) is small: giving rise to a tiny uncertainty. This edge-case can be solved exactly with mean field theory. In weak dilution the impact on the weights can be described as wij^{\\displaystyle {\\hat {w_{ij}}}} – diluted weight wij{\\displaystyle w_{ij}} – real weight before dilution P(c){\\displaystyle P(c)} – the probability of c{\\displaystyle c}: the probability of keeping a weightThe interpretation of probability P(c){\\displaystyle P(c)} can also be changed from keeping a weight into pruning a weight. In vector notation this can be written as where the function g\u2061(⋅){\\displaystyle \\operatorname {g} (\\cdot )} imposes the previous dilution. In weak dilution only a small and fixed fraction of the weights are diluted. When the number of terms in the sum goes to infinite (the weights for each node) it is still infinite (the fraction is fixed): thus mean field theory can be applied. In the notation from Hertz et al. this would be written as ⟨hi⟩{\\displaystyle \\left\\langle h_{i}\\right\\rangle } the mean field temperature c{\\displaystyle c} – a scaling factor for the temperature from the probability of keeping the weight wij{\\displaystyle w_{ij}} – real weight before dilution: also called the Hebb connection strength ⟨Sj⟩{\\displaystyle \\left\\langle S_{j}\\right\\rangle } – the mean stable equilibrium statesThere are some assumptions for this to hold: which are not listed here. == Strong dilution == When the dilution is strong: the finite fraction of removed connections (the weights) is large: giving rise to a huge uncertainty. == Dropout == Dropout is a special case of the previous weight equation (3): where the aforementioned equation is adjusted to remove a whole row in the vector matrix: and not only random weights P(c){\\displaystyle P(c)} – the probability c{\\displaystyle c} to keep a row in the weight matrix wj{\\displaystyle \\mathbf {w} _{j}} – real row in the weight matrix before dropout wj^{\\displaystyle {\\hat {\\mathbf {w} _{j}}}} – diluted row in the weight matrixBecause dropout removes a whole row from the vector matrix: the previous (unlisted) assumptions for weak dilution and the use of mean field theory are not applicable. The process by which the node is driven to zero: whether by setting the weights to zero: by “removing the node”: or by some other means: does not impact the end result and does not create a new and unique case. If the neural net is processed by a high-performance digital array-multiplicator: then it is likely more effective to drive the value to zero late in the process graph. If the net is processed by a constrained processor: perhaps even an analog neuromorph processor: then it is likely a more power-efficient solution is to drive the value to zero early in the process graph. == Google's patent == Although there have been examples of randomly removing connections between neurons in a neural network to improve models: this technique was first introduced with the name dropout by Geoffrey Hinton: et al. in 2012. Google currently holds the patent for the dropout technique. == See also == AlexNet Convolutional neural network § Dropout == Notes == == References ==\nA feedforward neural network (FNN) is one of the two broad types of artificial neural network: characterized by direction of the flow of information between its layers. Its flow is uni-directional: meaning that the information in the model flows in only one direction—forward—from the input nodes: through the hidden nodes (if any) and to the output nodes: without any cycles or loops: in contrast to recurrent neural networks: which have a bi-directional flow. Modern feedforward networks are trained using the backpropagation method and are colloquially referred to as the"": 'original_similarity': 0.8745775466695097: 'slide': 87}","{'chunk_text': 'dropout decreases overfitting. The method also significantly improves training speed. This makes the model combination practical: even for deep neural networks. The technique seems to reduce node interactions: leading them to learn more robust features that better generalize to new data. ==== DropConnect ==== DropConnect is the generalization of dropout in which each connection: rather than each output unit: can be dropped with probability 1−p{\\displaystyle 1-p}. Each unit thus receives input from a random subset of units in the previous layer.DropConnect is similar to dropout as it introduces dynamic sparsity within the model: but differs in that the sparsity is on the weights: rather than the output vectors of a layer. In other words: the fully connected layer with DropConnect becomes a sparsely connected layer in which the connections are chosen at random during the training stage. ==== Stochastic pooling ==== A major drawback to Dropout is that it does not have the same benefits for convolutional layers: where the neurons are not fully connected. Even before Dropout: in 2013 a technique called stochastic pooling: the conventional deterministic pooling operations were replaced with a stochastic procedure: where the activation within each pooling region is picked randomly according to a multinomial distribution: given by the activities within the pooling region. This approach is free of hyperparameters and can be combined with other regularization approaches: such as dropout and data augmentation. An alternate view of stochastic pooling is that it is equivalent to standard max pooling but with many copies of an input image: each having small local deformations. This is similar to explicit elastic deformations of the input images: which delivers excellent performance on the MNIST data set. Using stochastic pooling in a multilayer model gives an exponential number of deformations since the selections in higher layers are independent of those below. ==== Artificial data ==== Because the degree of model overfitting is determined by both its power and the amount of training it receives: providing a convolutional network with more training examples can reduce overfitting. Because there is often not enough available data to train: especially considering that some part should be spared for later testing: two approaches are to either generate new data from scratch (if possible) or perturb existing data to create new ones. The latter one is used since mid-1990s. For example: input images can be cropped: rotated: or rescaled to create new examples with the same labels as the original training set. === Explicit === ==== Early stopping ==== One of the simplest methods to prevent overfitting of a network is to simply stop the training before overfitting has had a chance to occur. It comes with the disadvantage that the learning process is halted. ==== Number of parameters ==== Another simple way to prevent overfitting is to limit the number of parameters: typically by limiting the number of hidden units in each layer or limiting network depth. For convolutional networks: the filter size also affects the number of parameters. Limiting the number of parameters restricts the predictive power of the network directly: reducing the complexity of the function that it can perform on the data: and thus limits the amount of overfitting. This is equivalent to a ""zero norm"". ==== Weight decay ==== A simple form of added regularizer is weight decay: which simply adds an additional error: proportional to the sum of weights (L1 norm) or squared magnitude (L2 norm) of the weight vector: to the error at each node. The level of acceptable model complexity can be reduced by increasing the proportionality constant(\'alpha\' hyperparameter): thus increasing the penalty for large weight vectors. L2 regularization is the most common form of regularization. It can be implemented by penalizing the squared magnitude of all parameters directly in the objective. The L2 regularization has the intuitive interpretation of heavily penalizing peaky weight vectors and preferring diffuse weight vectors. Due to multiplicative interactions between weights and inputs this has the useful property of encouraging the network to use all of its inputs a little rather than some of its inputs a lot. L1 regularization is also common. It makes the weight vectors sparse during optimization. In other words: neurons with L1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the noisy inputs. L1 with L2 regularization can be combined; this is called elastic net regularization. ==== Max norm constraints ==== Another form of regularization is to enforce an absolute upper bound on the magnitude of the weight vector for every neuron and use projected gradient descent to enforce the constraint. In practice: this corresponds to performing the parameter update as normal: and then enforcing the constraint by clamping the weight vector w→{\\displaystyle {\\vec {w}}} of every neuron to satisfy ‖w→‖2<c{\\displaystyle \\|{\\vec {w}}\\|_{2}<c}. Typical values of c{\\displaystyle c} are order of 3–4. Some papers report improvements when using this form of regularization. == Hierarchical coordinate frames == Pooling loses the precise spatial relationships between high-level parts (such as nose and mouth in a face image). These relationships are needed for identity recognition. Overlapping the pools so that each feature occurs in multiple pools: helps retain the information. Translation alone cannot extrapolate the understanding of geometric relationships to a radically new viewpoint: such as a different orientation or scale. On the other hand: people are very good at extrapolating; after seeing a\nnew shape once they can recognize it from a different viewpoint.An earlier common way to deal with this problem is to train the network on transformed data in different orientations: scales: lighting: etc. so that the network can cope with these variations. This is computationally intensive for large data-sets. The alternative is to use a hierarchy of coordinate frames and use a group of neurons to represent a conjunction of the shape of the feature and its pose relative to the retina. The pose relative to the retina is the relationship between the coordinate frame of the retina and the intrinsic features\' coordinate frame.Thus: one way to represent': 'original_similarity': 0.8219458099058004: 'slide': 106}","{'chunk_text': 'was published by Alexey Ivakhnenko and Lapa in 1967. A 1971 paper described a deep network with eight layers trained by the group method of data handling.The first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun\'ichi Amari. In computer experiments conducted by Amari\'s student Saito: a five layer MLP with two modifiable layers learned internal representations to classify non-linearily separable pattern classes. In 1987 Matthew Brand reported that wide 12-layer nonlinear perceptrons could be fully end-to-end trained to reproduce logic functions of nontrivial circuit depth via gradient descent on small batches of random input/output samples: but concluded that training time on contemporary hardware (sub-megaflop computers) made the technique impractical: and proposed using fixed random early layers as an input hash for a single modifiable layer. Instead: subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique. In 1970: Seppo Linnainmaa published the reverse mode of automatic differentiation of discrete connected networks of nested differentiable functions. This became known as backpropagation. It is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes. The terminology ""back-propagating errors"" was actually introduced in 1962 by Rosenblatt: but he did not know how to implement this: although Henry J. Kelley had a continuous precursor of backpropagation already in 1960 in the context of control theory. In 1982: Paul Werbos applied backpropagation to MLPs in the way that has become standard. In 1985: David E. Rumelhart et al. published an experimental analysis of the technique.Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers began with the Neocognitron introduced by Kunihiko Fukushima in 1980. In 1969: he also introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for CNNs and deep learning in general. CNNs have become an essential tool for computer vision. The term Deep Learning was introduced to the machine learning community by Rina Dechter in 1986: and to artificial neural networks by Igor Aizenberg and colleagues in 2000: in the context of Boolean threshold neurons.In 1988: Wei Zhang et al. applied the backpropagation algorithm to a convolutional neural network (a simplified Neocognitron with convolutional interconnections between the image feature layers and the last fully connected layer) for alphabet recognition. They also proposed an implementation of the CNN with an optical computing system. In 1989: Yann LeCun et al. applied backpropagation to a CNN with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked: training required 3 days. Subsequently: Wei Zhang: et al. modified their model by removing the last fully connected layer and applied it for medical image object segmentation in 1991 and breast cancer detection in mammograms in 1994. LeNet-5 (1998): a 7-level CNN by Yann LeCun et al.: that classifies digits: was applied by several banks to recognize hand-written numbers on checks digitized in 32x32 pixel images. In the 1980s: backpropagation did not work well for deep learning with long credit assignment paths. To overcome this problem: Jürgen Schmidhuber (1992) proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning. It uses predictive coding to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN: by distilling a higher level chunker network into a lower level automatizer network. In 1993: a chunker solved a deep learning task whose depth exceeded 1000.In 1992: Jürgen Schmidhuber also published an alternative to RNNs which is now called a linear Transformer or a Transformer with linearized self-attention (save for a normalization operator). It learns internal spotlights of attention: a slow feedforward neural network learns by gradient descent to control the fast weights of another neural network through outer products of self-generated activation patterns FROM and TO (which are now called key and value for self-attention). This fast weight attention mapping is applied to a query pattern. The modern Transformer was introduced by Ashish Vaswani et al. in their 2017 paper ""Attention Is All You Need"". It combines this with a softmax operator and a projection matrix. Transformers have increasingly become the model of choice for natural language processing. Many modern large language models such as ChatGPT: GPT-4: and BERT use it. Transformers are also increasingly being used in computer vision.In 1991: Jürgen Schmidhuber also published adversarial neural networks that contest with each other in the form of a zero-sum game: where one network\'s gain is the other network\'s loss. The first network is a\ngenerative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called ""artificial curiosity"". In 2014: this principle was used in a generative adversarial network (GAN) by Ian Goodfellow et al. Here the environmental reaction is 1 or 0 depending on whether the first network\'s output is in a given set. This can be used to create realistic deepfakes. Excellent image quality is achieved by Nvidia\'s StyleGAN (2018) based on the Progressive GAN by Tero Karras et al. Here the GAN generator is grown from small to large scale in a pyramidal fashion. Sepp Hochreiter\'s diploma thesis (1991) was called ""one of the most important documents in the history of machine learning"" by his supervisor Schmidhuber. It not only tested the neural history compressor: but also identified and analyzed the vanishing gradient problem. Hochreiter proposed recurrent residual connections to solve this problem. This led to the deep learning method called long short-term memory (LSTM): published in 1997. LSTM recurrent neural networks can learn ""very deep learning"" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. The ""vanilla LSTM"" with forget gate was introduced in 1999 by Felix Gers: Schmidhuber and Fred Cummins. LSTM has become the most cited neural network of the 20th century.': 'original_similarity': 0.7750707090988079: 'slide': 230}"
0.4446200000238605,Who introduced the dropout technique and who currently holds the patent?,"The dropout technique was first introduced by Geoffrey Hinton and others in 2012 for neural networks. Currently, Google holds the patent for this technique.","{'chunk_text': 'that case: weak dilution refers to adding a small amount of damping noise: while strong dilution refers to adding a greater amount of damping noise. Both can be rewritten as variants of weight dilution. These techniques are also sometimes referred to as random pruning of weights: but this is usually a non-recurring one-way operation. The network is pruned: and then kept if it is an improvement over the previous model. Dilution and dropout both refer to an iterative process. The pruning of weights typically does not imply that the network continues learning: while in dilution/dropout: the network continues to learn after the technique is applied. == Generalized linear network == Output from a layer of linear nodes: in an artificial neural net can be described as yi{\\displaystyle y_{i}} – output from node i{\\displaystyle i} wij{\\displaystyle w_{ij}} – real weight before dilution: also called the Hebb connection strength xj{\\displaystyle x_{j}} – input from node j{\\displaystyle j}This can be written in vector notation as y{\\displaystyle \\mathbf {y} } – output vector W{\\displaystyle \\mathbf {W} } – weight matrix x{\\displaystyle \\mathbf {x} } – input vectorEquations (1) and (2) are used in the subsequent sections. == Weak dilution == During weak dilution: the finite fraction of removed connections (the weights) is small: giving rise to a tiny uncertainty. This edge-case can be solved exactly with mean field theory. In weak dilution the impact on the weights can be described as wij^{\\displaystyle {\\hat {w_{ij}}}} – diluted weight wij{\\displaystyle w_{ij}} – real weight before dilution P(c){\\displaystyle P(c)} – the probability of c{\\displaystyle c}: the probability of keeping a weightThe interpretation of probability P(c){\\displaystyle P(c)} can also be changed from keeping a weight into pruning a weight. In vector notation this can be written as where the function g\u2061(⋅){\\displaystyle \\operatorname {g} (\\cdot )} imposes the previous dilution. In weak dilution only a small and fixed fraction of the weights are diluted. When the number of terms in the sum goes to infinite (the weights for each node) it is still infinite (the fraction is fixed): thus mean field theory can be applied. In the notation from Hertz et al. this would be written as ⟨hi⟩{\\displaystyle \\left\\langle h_{i}\\right\\rangle } the mean field temperature c{\\displaystyle c} – a scaling factor for the temperature from the probability of keeping the weight wij{\\displaystyle w_{ij}} – real weight before dilution: also called the Hebb connection strength ⟨Sj⟩{\\displaystyle \\left\\langle S_{j}\\right\\rangle } – the mean stable equilibrium statesThere are some assumptions for this to hold: which are not listed here. == Strong dilution == When the dilution is strong: the finite fraction of removed connections (the weights) is large: giving rise to a huge uncertainty. == Dropout == Dropout is a special case of the previous weight equation (3): where the aforementioned equation is adjusted to remove a whole row in the vector matrix: and not only random weights P(c){\\displaystyle P(c)} – the probability c{\\displaystyle c} to keep a row in the weight matrix wj{\\displaystyle \\mathbf {w} _{j}} – real row in the weight matrix before dropout wj^{\\displaystyle {\\hat {\\mathbf {w} _{j}}}} – diluted row in the weight matrixBecause dropout removes a whole row from the vector matrix: the previous (unlisted) assumptions for weak dilution and the use of mean field theory are not applicable. The process by which the node is driven to zero: whether by setting the weights to zero: by “removing the node”: or by some other means: does not impact the end result and does not create a new and unique case. If the neural net is processed by a high-performance digital array-multiplicator: then it is likely more effective to drive the value to zero late in the process graph. If the net is processed by a constrained processor: perhaps even an analog neuromorph processor: then it is likely a more power-efficient solution is to drive the value to zero early in the process graph. == Google\'s patent == Although there have been examples of randomly removing connections between neurons in a neural network to improve models: this technique was first introduced with the name dropout by Geoffrey Hinton: et al. in 2012. Google currently holds the patent for the dropout technique. == See also == AlexNet Convolutional neural network § Dropout == Notes == == References ==\nA feedforward neural network (FNN) is one of the two broad types of artificial neural network: characterized by direction of the flow of information between its layers. Its flow is uni-directional: meaning that the information in the model flows in only one direction—forward—from the input nodes: through the hidden nodes (if any) and to the output nodes: without any cycles or loops: in contrast to recurrent neural networks: which have a bi-directional flow. Modern feedforward networks are trained using the backpropagation method and are colloquially referred to as the ""vanilla"" neural networks. == Timeline == In 1958: a layered network of perceptrons: consisting of an input layer: a hidden layer with randomized weights that did not learn: and an output layer with learning connections: was introduced already by Frank Rosenblatt in his book Perceptron. This extreme learning machine was not yet a deep learning network.In 1965: the first deep-learning feedforward network: not yet using stochastic gradient descent: was published by': 'original_similarity': 0.7952553157958031: 'slide': 158}","{'chunk_text': 'described a deep network with eight layers trained by the group method of data handling.The first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun\'ichi Amari. In computer experiments conducted by Amari\'s student Saito: a five layer MLP with two modifiable layers learned internal representations to classify non-linearily separable pattern classes. In 1987 Matthew Brand reported that wide 12-layer nonlinear perceptrons could be fully end-to-end trained to reproduce logic functions of nontrivial circuit depth via gradient descent on small batches of random input/output samples: but concluded that training time on contemporary hardware (sub-megaflop computers) made the technique impractical: and proposed using fixed random early layers as an input hash for a single modifiable layer. Instead: subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique. In 1970: Seppo Linnainmaa published the reverse mode of automatic differentiation of discrete connected networks of nested differentiable functions. This became known as backpropagation. It is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes. The terminology ""back-propagating errors"" was actually introduced in 1962 by Rosenblatt: but he did not know how to implement this: although Henry J. Kelley had a continuous precursor of backpropagation already in 1960 in the context of control theory. In 1982: Paul Werbos applied backpropagation to MLPs in the way that has become standard. In 1985: David E. Rumelhart et al. published an experimental analysis of the technique.Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers began with the Neocognitron introduced by Kunihiko Fukushima in 1980. In 1969: he also introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for CNNs and deep learning in general. CNNs have become an essential tool for computer vision. The term Deep Learning was introduced to the machine learning community by Rina Dechter in 1986: and to artificial neural networks by Igor Aizenberg and colleagues in 2000: in the context of Boolean threshold neurons.In 1988: Wei Zhang et al. applied the backpropagation algorithm to a convolutional neural network (a simplified Neocognitron with convolutional interconnections between the image feature layers and the last fully connected layer) for alphabet recognition. They also proposed an implementation of the CNN with an optical computing system. In 1989: Yann LeCun et al. applied backpropagation to a CNN with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked: training required 3 days. Subsequently: Wei Zhang: et al. modified their model by removing the last fully connected layer and applied it for medical image object segmentation in 1991 and breast cancer detection in mammograms in 1994. LeNet-5 (1998): a 7-level CNN by Yann LeCun et al.: that classifies digits: was applied by several banks to recognize hand-written numbers on checks digitized in 32x32 pixel images. In the 1980s: backpropagation did not work well for deep learning with long credit assignment paths. To overcome this problem: Jürgen Schmidhuber (1992) proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning. It uses predictive coding to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN: by distilling a higher level chunker network into a lower level automatizer network. In 1993: a chunker solved a deep learning task whose depth exceeded 1000.In 1992: Jürgen Schmidhuber also published an alternative to RNNs which is now called a linear Transformer or a Transformer with linearized self-attention (save for a normalization operator). It learns internal spotlights of attention: a slow feedforward neural network learns by gradient descent to control the fast weights of another neural network through outer products of self-generated activation patterns FROM and TO (which are now called key and value for self-attention). This fast weight attention mapping is applied to a query pattern. The modern Transformer was introduced by Ashish Vaswani et al. in their 2017 paper ""Attention Is All You Need"". It combines this with a softmax operator and a projection matrix. Transformers have increasingly become the model of choice for natural language processing. Many modern large language models such as ChatGPT: GPT-4: and BERT use it. Transformers are also increasingly being used in computer vision.In 1991: Jürgen Schmidhuber also published adversarial neural networks that contest with each other in the form of a zero-sum game: where one network\'s gain is the other network\'s loss. The first network is a\ngenerative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called ""artificial curiosity"". In 2014: this principle was used in a generative adversarial network (GAN) by Ian Goodfellow et al. Here the environmental reaction is 1 or 0 depending on whether the first network\'s output is in a given set. This can be used to create realistic deepfakes. Excellent image quality is achieved by Nvidia\'s StyleGAN (2018) based on the Progressive GAN by Tero Karras et al. Here the GAN generator is grown from small to large scale in a pyramidal fashion. Sepp Hochreiter\'s diploma thesis (1991) was called ""one of the most important documents in the history of machine learning"" by his supervisor Schmidhuber. It not only tested the neural history compressor: but also identified and analyzed the vanishing gradient problem. Hochreiter proposed recurrent residual connections to solve this problem. This led to the deep learning method called long short-term memory (LSTM): published in 1997. LSTM recurrent neural networks can learn ""very deep learning"" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. The ""vanilla LSTM"" with forget gate was introduced in 1999 by Felix Gers: Schmidhuber and Fred Cummins. LSTM has become the most cited neural network of the 20th century. In 2015: Rupesh Kumar Srivastava: Klaus Greff: and Schmidhuber used LSTM principles': 'original_similarity': 0.7914531744185034: 'slide': 242}","{'chunk_text': 'dynamic sparsity within the model: but differs in that the sparsity is on the weights: rather than the output vectors of a layer. In other words: the fully connected layer with DropConnect becomes a sparsely connected layer in which the connections are chosen at random during the training stage. ==== Stochastic pooling ==== A major drawback to Dropout is that it does not have the same benefits for convolutional layers: where the neurons are not fully connected. Even before Dropout: in 2013 a technique called stochastic pooling: the conventional deterministic pooling operations were replaced with a stochastic procedure: where the activation within each pooling region is picked randomly according to a multinomial distribution: given by the activities within the pooling region. This approach is free of hyperparameters and can be combined with other regularization approaches: such as dropout and data augmentation. An alternate view of stochastic pooling is that it is equivalent to standard max pooling but with many copies of an input image: each having small local deformations. This is similar to explicit elastic deformations of the input images: which delivers excellent performance on the MNIST data set. Using stochastic pooling in a multilayer model gives an exponential number of deformations since the selections in higher layers are independent of those below. ==== Artificial data ==== Because the degree of model overfitting is determined by both its power and the amount of training it receives: providing a convolutional network with more training examples can reduce overfitting. Because there is often not enough available data to train: especially considering that some part should be spared for later testing: two approaches are to either generate new data from scratch (if possible) or perturb existing data to create new ones. The latter one is used since mid-1990s. For example: input images can be cropped: rotated: or rescaled to create new examples with the same labels as the original training set. === Explicit === ==== Early stopping ==== One of the simplest methods to prevent overfitting of a network is to simply stop the training before overfitting has had a chance to occur. It comes with the disadvantage that the learning process is halted. ==== Number of parameters ==== Another simple way to prevent overfitting is to limit the number of parameters: typically by limiting the number of hidden units in each layer or limiting network depth. For convolutional networks: the filter size also affects the number of parameters. Limiting the number of parameters restricts the predictive power of the network directly: reducing the complexity of the function that it can perform on the data: and thus limits the amount of overfitting. This is equivalent to a ""zero norm"". ==== Weight decay ==== A simple form of added regularizer is weight decay: which simply adds an additional error: proportional to the sum of weights (L1 norm) or squared magnitude (L2 norm) of the weight vector: to the error at each node. The level of acceptable model complexity can be reduced by increasing the proportionality constant(\'alpha\' hyperparameter): thus increasing the penalty for large weight vectors. L2 regularization is the most common form of regularization. It can be implemented by penalizing the squared magnitude of all parameters directly in the objective. The L2 regularization has the intuitive interpretation of heavily penalizing peaky weight vectors and preferring diffuse weight vectors. Due to multiplicative interactions between weights and inputs this has the useful property of encouraging the network to use all of its inputs a little rather than some of its inputs a lot. L1 regularization is also common. It makes the weight vectors sparse during optimization. In other words: neurons with L1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the noisy inputs. L1 with L2 regularization can be combined; this is called elastic net regularization. ==== Max norm constraints ==== Another form of regularization is to enforce an absolute upper bound on the magnitude of the weight vector for every neuron and use projected gradient descent to enforce the constraint. In practice: this corresponds to performing the parameter update as normal: and then enforcing the constraint by clamping the weight vector w→{\\displaystyle {\\vec {w}}} of every neuron to satisfy ‖w→‖2<c{\\displaystyle \\|{\\vec {w}}\\|_{2}<c}. Typical values of c{\\displaystyle c} are order of 3–4. Some papers report improvements when using this form of regularization. == Hierarchical coordinate frames == Pooling loses the precise spatial relationships between high-level parts (such as nose and mouth in a face image). These relationships are needed for identity recognition. Overlapping the pools so that each feature occurs in multiple pools: helps retain the information. Translation alone cannot extrapolate the understanding of geometric relationships to a radically new viewpoint: such as a different orientation or scale. On the other hand: people are very good at extrapolating; after seeing a\nnew shape once they can recognize it from a different viewpoint.An earlier common way to deal with this problem is to train the network on transformed data in different orientations: scales: lighting: etc. so that the network can cope with these variations. This is computationally intensive for large data-sets. The alternative is to use a hierarchy of coordinate frames and use a group of neurons to represent a conjunction of the shape of the feature and its pose relative to the retina. The pose relative to the retina is the relationship between the coordinate frame of the retina and the intrinsic features\' coordinate frame.Thus: one way to represent something is to embed the coordinate frame within it. This allows large features to be recognized by using the consistency of the poses of their parts (e.g. nose and mouth poses make a consistent prediction of the pose of the whole face). This approach ensures that the higher-level entity (e.g. face) is present when the lower-level (e.g. nose and mouth) agree on its prediction of the pose. The vectors of neuronal activity that represent pose (""pose vectors"") allow spatial transformations modeled as linear operations that make it easier': 'original_similarity': 0.7728008458301799: 'slide': 194}"
0.44305549998534843,What is a Feedforward Neural Network (FNN)?,"A feedforward neural network (FNN) is one of two broad types of artificial neural networks, characterized by the direction of the flow of information between its layers. Its flow is uni-directional, meaning that the information in the model flows in only one direction—forward—from the input nodes, through the hidden nodes (if any) and to the output nodes, without any cycles or loops. This is in contrast to recurrent neural networks, which have a bi-directional flow.","{'chunk_text': 'was able to classify non-linearily separable pattern classes: as reported Shun\'ichi Amari. Amari\'s student Saito conducted the computer experiments: using a five-layered feedforward network with two learning layers.In 1970: modern backpropagation method: an efficient application of a chain-rule-based supervised learning: was for the first time published by the Finnish researcher Seppo Linnainmaa. The term (i.e. ""back-propagating errors"") itself has been used by Rosenblatt himself: but he did not know how to implement it: although a continuous precursor of backpropagation was already used in the context of control theory in 1960 by Henry J. Kelley. It is known also as a reverse mode of automatic differentiation.In 1982: backpropagation was applied in the way that has become standard: for the first time by Paul Werbos.In 1985: an experimental analysis of the technique was conducted by David E. Rumelhart et al.. Many improvements to the approach have been made in subsequent decades.In 1987: using a stochastic gradient descent within a (wide 12-layer nonlinear) feed-forward network: Matthew Brand has trained it to reproduce logic functions of nontrivial circuit depth: using small batches of random input/output samples. He: however: concluded that on hardware (sub-megaflop computers) available at the time it was impractical: and proposed using fixed random early layers as an input hash for a single modifiable layer.In 1990s: an (much simpler) alternative to using neural networks: although still related support vector machine approach was developed by Vladimir Vapnik and his colleagues. In addition to performing linear classification: they were able to efficiently perform a non-linear classification using what is called the kernel trick: using high-dimensional feature spaces.In 2003: interest in backpropagation networks returned due to the successes of deep learning being applied to language modelling by Yoshua Bengio with co-authors.In 2017: modern transformer architectures were introduced. == Mathematical foundations == === Activation function === The two historically common activation functions are both sigmoids: and are described by y(vi)=tanh\u2061(vi) and y(vi)=(1+e−vi)−1{\\displaystyle y(v_{i})=\\tanh(v_{i})~~{\\textrm {and}}~~y(v_{i})=(1+e^{-v_{i}})^{-1}}.The first is a hyperbolic tangent that ranges from -1 to 1: while the other is the logistic function: which is similar in shape but ranges from 0 to 1. Here yi{\\displaystyle y_{i}} is the output of the i{\\displaystyle i}th node (neuron) and vi{\\displaystyle v_{i}} is the weighted sum of the input connections. Alternative activation functions have been proposed: including the rectifier and softplus functions. More specialized activation functions include radial basis functions (used in radial basis networks: another class of supervised neural network models). In recent developments of deep learning the rectified linear unit (ReLU) is more frequently used as one of the possible ways to overcome the numerical problems related to the sigmoids. === Learning === Learning occurs by changing connection weights after each piece of data is processed: based on the amount of error in the output compared to the expected result. This is an example of supervised learning: and is carried out through backpropagation. We can represent the degree of error in an output node j{\\displaystyle j} in the n{\\displaystyle n}th data point (training example) by ej(n)=dj(n)−yj(n){\\displaystyle e_{j}(n)=d_{j}(n)-y_{j}(n)}: where dj(n){\\displaystyle d_{j}(n)} is the desired target value for n{\\displaystyle n}th data point at node j{\\displaystyle j}: and yj(n){\\displaystyle y_{j}(n)} is the value produced at node j{\\displaystyle j} when the n{\\displaystyle n}th data point is given as an input. The node weights can then be adjusted based on corrections that minimize the error in the entire output for the n{\\displaystyle n}th data point: given by E(n)=12∑output node jej2(n){\\displaystyle {\\mathcal {E}}(n)={\\frac {1}{2}}\\sum _{{\\text{output node }}j}e_{j}^{2}(n)}.Using gradient descent: the change in each weight wij{\\displaystyle w_{ij}} is Δwji(n)=−η∂E(n)∂vj(n)yi(n){\\displaystyle \\Delta w_{ji}(n)=-\\eta {\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}y_{i}(n)}where yi(n){\\displaystyle y_{i}(n)} is the output of the previous neuron i{\\displaystyle i}: and η{\\displaystyle \\eta } is the learning rate: which is selected to ensure that the weights quickly converge to a response: without oscillations. In the previous expression: ∂E(n)∂vj(n){\\displaystyle {\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}} denotes the partial derivate of the error E(n){\\displaystyle {\\mathcal {E}}(n)} according to the weighted sum vj(n){\\displaystyle v_{j}(n)} of the input connections of neuron i{\\displaystyle i}. The derivative to be calculated depends on the induced local field vj{\\displaystyle v_{j}}: which itself varies. It is easy to prove that for an output node this derivative can be simplified to −∂E(n)∂vj(n)=ej(n)ϕ′(vj(n)){\\displaystyle -{\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}=e_{j}(n)\\phi ^{\\prime }(v_{j}(n))}where ϕ′{\\displaystyle \\phi ^{\\prime }} is the derivative of the activation function described above: which itself does not vary. The analysis is more difficult for the change in weights to a hidden node: but it can be shown that the relevant derivative is −∂E(n)∂vj(n)=ϕ′(vj(n))∑k−∂E(n)∂vk(n)wkj(n){\\displaystyle -{\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}=\\phi ^{\\prime }(v_{j}(n))\\sum _{k}-{\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{k}(n)}}w_{kj}(n)}.This depends on the change in weights of the k{\\displaystyle k}th nodes: which represent the output layer. So to change the hidden layer weights: the output layer weights change according to the derivative of the activation function: and so this algorithm represents a backpropagation of the activation function. == History == === Linear neural network === The simplest\nkind of feedforward neural network is a linear network: which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated in each node. The mean squared errors between these calculated outputs and a given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement. === Perceptron === If using a threshold: i.e. a linear activation function: the resulting linear threshold unit is called a perceptron. (Often the term is used to denote just one of these units.) Multiple parallel linear units are able to approximate any continuous function from a compact interval of the real numbers into the interval [−1:1] despite the limited computational power of single unit with a linear threshold function. This result can': 'original_similarity': 0.9044720886945816: 'slide': 187}","{'chunk_text': 'algorithm represents a backpropagation of the activation function. == History == === Linear neural network === The simplest\nkind of feedforward neural network is a linear network: which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated in each node. The mean squared errors between these calculated outputs and a given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement. === Perceptron === If using a threshold: i.e. a linear activation function: the resulting linear threshold unit is called a perceptron. (Often the term is used to denote just one of these units.) Multiple parallel linear units are able to approximate any continuous function from a compact interval of the real numbers into the interval [−1:1] despite the limited computational power of single unit with a linear threshold function. This result can be found in Peter Auer: Harald Burgsteiner and Wolfgang Maass ""A learning rule for very simple universal approximators consisting of a single layer of perceptrons"".Perceptrons can be trained by a simple learning algorithm that is usually called the delta rule. It calculates the errors between calculated output and sample output data: and uses this to create an adjustment to the weights: thus implementing a form of gradient descent. === Multilayer perceptron === A multilayer perceptron (MLP) is a misnomer for a modern feedforward artificial neural network: consisting of fully connected neurons with a nonlinear kind of activation function: organized in at least three layers: notable for being able to distinguish data that is not linearly separable. It is a misnomer because the original perceptron used a Heaviside step function: instead of a nonlinear kind of activation function (used by modern networks). == Other feedforward networks == Examples of other feedforward networks include convolutional neural networks and radial basis function networks: which use a different activation function. == See also == Hopfield network Feed-forward Backpropagation Rprop == References ==': 'original_similarity': 0.8655804006066588: 'slide': -18}","{'chunk_text': ""components) or software-based (computer models): and can use a variety of topologies and learning algorithms. == Feedforward == The feedforward neural network was the first and simplest type. In this network the information moves only from the input layer directly through any hidden layers to the output layer without cycles/loops. Feedforward networks can be constructed with various types of units: such as binary McCulloch–Pitts neurons: the simplest of which is the perceptron. Continuous neurons: frequently with sigmoidal activation: are used in the context of backpropagation. === Group method of data handling === The Group Method of Data Handling (GMDH) features fully automatic structural and parametric model optimization. The node activation functions are Kolmogorov–Gabor polynomials that permit additions and multiplications. It uses a deep multilayer perceptron with eight layers. It is a supervised learning network that grows layer by layer: where each layer is trained by regression analysis. Useless items are detected using a validation set: and pruned through regularization. The size and depth of the resulting network depends on the task. === Autoencoder === An autoencoder: autoassociator or Diabolo network: 19 is similar to the multilayer perceptron (MLP) – with an input layer: an output layer and one or more hidden layers connecting them. However: the output layer has the same number of units as the input layer. Its purpose is to reconstruct its own inputs (instead of emitting a target value). Therefore: autoencoders are unsupervised learning models. An autoencoder is used for unsupervised learning of efficient codings: typically for the purpose of dimensionality reduction and for learning generative models of data. === Probabilistic === A probabilistic neural network (PNN) is a four-layer feedforward neural network. The layers are Input: hidden pattern/summation: and output. In the PNN algorithm: the parent probability distribution function (PDF) of each class is approximated by a Parzen window and a non-parametric function. Then: using PDF of each class: the class probability of a new input is estimated and Bayes’ rule is employed to allocate it to the class with the highest posterior probability. It was derived from the Bayesian network and a statistical algorithm called Kernel Fisher discriminant analysis. It is used for classification and pattern recognition. === Time delay === A time delay neural network (TDNN) is a feedforward architecture for sequential data that recognizes features independent of sequence position. In order to achieve time-shift invariance: delays are added to the input so that multiple data points (points in time) are analyzed together. It usually forms part of a larger pattern recognition system. It has been implemented using a perceptron network whose connection weights were trained with back propagation (supervised learning). === Convolutional === A convolutional neural network (CNN: or ConvNet or shift invariant or space invariant) is a class of deep network: composed of one or more convolutional layers with fully connected layers (matching those in typical ANNs) on top. It uses tied weights and pooling layers. In particular: max-pooling. It is often structured via Fukushima's convolutional architecture. They are variations of multilayer perceptrons that use minimal preprocessing. This architecture allows CNNs to take advantage of the 2D structure of input data. Its unit connectivity pattern is inspired by the organization of the visual cortex. Units respond to stimuli in a restricted region of space known as the receptive field. Receptive fields partially overlap: over-covering the entire visual field. Unit response can be approximated mathematically by a convolution operation.CNNs are suitable for processing visual and other two-dimensional data. They have shown superior results in both image and speech applications. They can be trained with standard backpropagation. CNNs are easier to train than other regular: deep: feed-forward neural networks and have many fewer parameters to estimate.Capsule Neural Networks (CapsNet) add structures called capsules to a CNN and reuse output from several capsules to form more stable (with respect to various perturbations) representations.Examples of applications in computer vision include DeepDream and robot navigation. They have wide applications in image and video recognition: recommender systems and natural language processing. === Deep stacking network === A deep stacking network (DSN) (deep convex network) is based on a hierarchy of blocks of simplified neural network modules. It was introduced in 2011 by Deng and Yu. It formulates the learning as a convex optimization problem with a closed-form solution: emphasizing the mechanism's similarity to stacked generalization. Each DSN block is a simple module that is easy to train by itself in a supervised fashion without backpropagation for the entire blocks.Each block consists of a simplified multi-layer perceptron (MLP) with a single hidden layer. The hidden layer h has logistic sigmoidal units: and the output layer has linear units. Connections between these layers are represented by weight matrix U; input-to-hidden-layer connections have weight matrix W. Target vectors t form the columns of matrix T: and the input data vectors x form the columns of matrix X. The matrix of hidden units is H=σ(WTX){\\displaystyle {\\boldsymbol {H}}=\\sigma ({\\boldsymbol {W}}^{T}{\\boldsymbol {X}})}. Modules are trained in order: so lower-layer weights W are known at each stage. The function performs the element-wise logistic sigmoid operation. Each block estimates the same final label class y: and its estimate is concatenated with original input X to form the\nexpanded input for the next block. Thus: the input to the first block contains the original data only: while downstream blocks' input adds the output of preceding blocks. Then learning the upper-layer weight matrix U given other weights in the network can be formulated as a convex optimization problem: minUTf=‖UTH−T‖F2:{\\displaystyle \\min _{U^{T}}f=\\|{\\boldsymbol {U}}^{T}{\\boldsymbol {H}}-{\\boldsymbol {T}}\\|_{F}^{2}:}which has a closed-form solution.Unlike other deep architectures: such as DBNs: the goal is not to discover the transformed feature representation. The structure of the hierarchy of this kind of architecture makes parallel learning straightforward: as a batch-mode optimization problem. In purely discriminative tasks: DSNs outperform conventional DBNs. ==== Tensor deep stacking networks ==== This architecture is a DSN extension. It offers two important improvements: it uses higher-order information from covariance statistics: and it transforms the non-convex problem of a lower-layer to a convex sub-problem of"": 'original_similarity': 0.854884369416562: 'slide': 138}"
0.49577200002386235,What is the main method of training modern feedforward networks?,Modern feedforward networks are trained using the backpropagation method.,"{'chunk_text': 'five-layered feedforward network with two learning layers.In 1970: modern backpropagation method: an efficient application of a chain-rule-based supervised learning: was for the first time published by the Finnish researcher Seppo Linnainmaa. The term (i.e. ""back-propagating errors"") itself has been used by Rosenblatt himself: but he did not know how to implement it: although a continuous precursor of backpropagation was already used in the context of control theory in 1960 by Henry J. Kelley. It is known also as a reverse mode of automatic differentiation.In 1982: backpropagation was applied in the way that has become standard: for the first time by Paul Werbos.In 1985: an experimental analysis of the technique was conducted by David E. Rumelhart et al.. Many improvements to the approach have been made in subsequent decades.In 1987: using a stochastic gradient descent within a (wide 12-layer nonlinear) feed-forward network: Matthew Brand has trained it to reproduce logic functions of nontrivial circuit depth: using small batches of random input/output samples. He: however: concluded that on hardware (sub-megaflop computers) available at the time it was impractical: and proposed using fixed random early layers as an input hash for a single modifiable layer.In 1990s: an (much simpler) alternative to using neural networks: although still related support vector machine approach was developed by Vladimir Vapnik and his colleagues. In addition to performing linear classification: they were able to efficiently perform a non-linear classification using what is called the kernel trick: using high-dimensional feature spaces.In 2003: interest in backpropagation networks returned due to the successes of deep learning being applied to language modelling by Yoshua Bengio with co-authors.In 2017: modern transformer architectures were introduced. == Mathematical foundations == === Activation function === The two historically common activation functions are both sigmoids: and are described by y(vi)=tanh\u2061(vi) and y(vi)=(1+e−vi)−1{\\displaystyle y(v_{i})=\\tanh(v_{i})~~{\\textrm {and}}~~y(v_{i})=(1+e^{-v_{i}})^{-1}}.The first is a hyperbolic tangent that ranges from -1 to 1: while the other is the logistic function: which is similar in shape but ranges from 0 to 1. Here yi{\\displaystyle y_{i}} is the output of the i{\\displaystyle i}th node (neuron) and vi{\\displaystyle v_{i}} is the weighted sum of the input connections. Alternative activation functions have been proposed: including the rectifier and softplus functions. More specialized activation functions include radial basis functions (used in radial basis networks: another class of supervised neural network models). In recent developments of deep learning the rectified linear unit (ReLU) is more frequently used as one of the possible ways to overcome the numerical problems related to the sigmoids. === Learning === Learning occurs by changing connection weights after each piece of data is processed: based on the amount of error in the output compared to the expected result. This is an example of supervised learning: and is carried out through backpropagation. We can represent the degree of error in an output node j{\\displaystyle j} in the n{\\displaystyle n}th data point (training example) by ej(n)=dj(n)−yj(n){\\displaystyle e_{j}(n)=d_{j}(n)-y_{j}(n)}: where dj(n){\\displaystyle d_{j}(n)} is the desired target value for n{\\displaystyle n}th data point at node j{\\displaystyle j}: and yj(n){\\displaystyle y_{j}(n)} is the value produced at node j{\\displaystyle j} when the n{\\displaystyle n}th data point is given as an input. The node weights can then be adjusted based on corrections that minimize the error in the entire output for the n{\\displaystyle n}th data point: given by E(n)=12∑output node jej2(n){\\displaystyle {\\mathcal {E}}(n)={\\frac {1}{2}}\\sum _{{\\text{output node }}j}e_{j}^{2}(n)}.Using gradient descent: the change in each weight wij{\\displaystyle w_{ij}} is Δwji(n)=−η∂E(n)∂vj(n)yi(n){\\displaystyle \\Delta w_{ji}(n)=-\\eta {\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}y_{i}(n)}where yi(n){\\displaystyle y_{i}(n)} is the output of the previous neuron i{\\displaystyle i}: and η{\\displaystyle \\eta } is the learning rate: which is selected to ensure that the weights quickly converge to a response: without oscillations. In the previous expression: ∂E(n)∂vj(n){\\displaystyle {\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}} denotes the partial derivate of the error E(n){\\displaystyle {\\mathcal {E}}(n)} according to the weighted sum vj(n){\\displaystyle v_{j}(n)} of the input connections of neuron i{\\displaystyle i}. The derivative to be calculated depends on the induced local field vj{\\displaystyle v_{j}}: which itself varies. It is easy to prove that for an output node this derivative can be simplified to −∂E(n)∂vj(n)=ej(n)ϕ′(vj(n)){\\displaystyle -{\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}=e_{j}(n)\\phi ^{\\prime }(v_{j}(n))}where ϕ′{\\displaystyle \\phi ^{\\prime }} is the derivative of the activation function described above: which itself does not vary. The analysis is more difficult for the change in weights to a hidden node: but it can be shown that the relevant derivative is −∂E(n)∂vj(n)=ϕ′(vj(n))∑k−∂E(n)∂vk(n)wkj(n){\\displaystyle -{\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}=\\phi ^{\\prime }(v_{j}(n))\\sum _{k}-{\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{k}(n)}}w_{kj}(n)}.This depends on the change in weights of the k{\\displaystyle k}th nodes: which represent the output layer. So to change the hidden layer weights: the output layer weights change according to the derivative of the activation function: and so this algorithm represents a backpropagation of the activation function. == History == === Linear neural network === The simplest\nkind of feedforward neural network is a linear network: which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated in each node. The mean squared errors between these calculated outputs and a given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement. === Perceptron === If using a threshold: i.e. a linear activation function: the resulting linear threshold unit is called a perceptron. (Often the term is used to denote just one of these units.) Multiple parallel linear units are able to approximate any continuous function from a compact interval of the real numbers into the interval [−1:1] despite the limited computational power of single unit with a linear threshold function. This result can be found in Peter Auer: Harald Burgsteiner and Wolfgang Maass ""A learning rule for very simple universal approximators consisting of a': 'original_similarity': 0.8514269604984949: 'slide': 208}","{'chunk_text': 'can be used as an estimate for variance. This value can then be used to calculate the confidence interval of network output: assuming a normal distribution. A confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified. By assigning a softmax activation function: a generalization of the logistic function: on the output layer of the neural network (or a softmax component in a component-based network) for categorical target variables: the outputs can be interpreted as posterior probabilities. This is useful in classification as it gives a certainty measure on classifications. The softmax activation function is: yi=exi∑j=1cexj{\\displaystyle y_{i}={\\frac {e^{x_{i}}}{\\sum _{j=1}^{c}e^{x_{j}}}}} == Criticism == === Training === A common criticism of neural networks: particularly in robotics: is that they require too many training samples for real-world operation. Any learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases. Potential solutions include randomly shuffling training examples: by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example: grouping examples in so-called mini-batches and/or introducing a recursive least squares algorithm for CMAC. Dean Pomerleau uses a neural network to train a robotic vehicle to drive on multiple types of roads (single lane: multi-lane: dirt: etc.): and a large amount of his research is devoted to extrapolating multiple training scenarios from a single training experience: and preserving past training diversity so that the system does not become overtrained (if: for example: it is presented with a series of right turns—it should not learn to always turn right). === Theory === A central claim of ANNs is that they embody new and powerful general principles for processing information. These principles are ill-defined. It is often claimed that they are emergent from the network itself. This allows simple statistical association (the basic function of artificial neural networks) to be described as learning or recognition. In 1997: Alexander Dewdney: a former Scientific American columnist: commented that as a result: artificial neural networks have a ""something-for-nothing quality: one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are. No human hand (or mind) intervenes; solutions are found as if by magic; and no one: it seems: has learned anything"". One response to Dewdney is that neural networks have been successfully used to handle many complex and diverse tasks: ranging from autonomously flying aircraft to detecting credit card fraud to mastering the game of Go. Technology writer Roger Bridgman commented: Neural networks: for instance: are in the dock not only because they have been hyped to high heaven: (what hasn\'t?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be ""an opaque: unreadable table...valueless as a scientific resource"". In spite of his emphatic declaration that science is not technology: Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having. Although it is true that analyzing what has been learned by an artificial neural network is difficult: it is much easier to do so than to analyze what has been learned by a biological neural network. Moreover: recent emphasis on the explainability of AI has contributed towards the development of methods: notably those based on attention mechanisms: for visualizing and explaining learned neural networks. Furthermore: researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles that allow a learning machine to be successful. For example: Bengio and LeCun (2007) wrote an article regarding local vs non-local learning: as well as shallow vs deep architecture.Biological brains use both shallow and deep circuits as reported by brain anatomy: displaying a wide variety of invariance. Weng argued that the brain self-wires largely according to signal statistics and therefore: a serial cascade cannot catch all major statistical dependencies. === Hardware === Large and effective neural networks require considerable computing resources. While the brain has hardware tailored to the task of processing signals through a graph of neurons: simulating even a simplified neuron on von Neumann architecture may consume vast amounts of memory and storage. Furthermore: the designer often needs to transmit signals through many of these connections and their associated neurons – which require enormous CPU power and time. Schmidhuber noted that the resurgence of neural networks in the twenty-first century is largely attributable to advances in hardware: from 1991 to 2015: computing power: especially as delivered by GPGPUs (on GPUs): has increased around a million-fold: making the standard backpropagation algorithm feasible for training networks that are several layers deeper than before. The use of accelerators such as FPGAs and GPUs can reduce training times from months to days.Neuromorphic engineering or a physical neural network addresses the hardware difficulty directly: by constructing non-von-Neumann chips to directly implement neural networks\nin circuitry. Another type of chip optimized for neural network processing is called a Tensor Processing Unit: or TPU. === Practical counterexamples === Analyzing what has been learned by an ANN is much easier than analyzing what has been learned by a biological neural network. Furthermore: researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a learning machine to be successful. For example: local vs. non-local learning and shallow vs. deep architecture. === Hybrid approaches === Advocates of hybrid models (combining neural networks and symbolic approaches) say that such a mixture can better capture the mechanisms of the human mind. === Dataset bias === Neural networks are dependent on the quality of the data they are trained on: thus low quality data with imbalanced representativeness can lead to the model learning and perpetuating societal biases. These inherited biases become especially critical when the ANNs are': 'original_similarity': 0.8451928743947512: 'slide': 150}","{'chunk_text': 'is a linear network: which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated in each node. The mean squared errors between these calculated outputs and a given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement. === Perceptron === If using a threshold: i.e. a linear activation function: the resulting linear threshold unit is called a perceptron. (Often the term is used to denote just one of these units.) Multiple parallel linear units are able to approximate any continuous function from a compact interval of the real numbers into the interval [−1:1] despite the limited computational power of single unit with a linear threshold function. This result can be found in Peter Auer: Harald Burgsteiner and Wolfgang Maass ""A learning rule for very simple universal approximators consisting of a single layer of perceptrons"".Perceptrons can be trained by a simple learning algorithm that is usually called the delta rule. It calculates the errors between calculated output and sample output data: and uses this to create an adjustment to the weights: thus implementing a form of gradient descent. === Multilayer perceptron === A multilayer perceptron (MLP) is a misnomer for a modern feedforward artificial neural network: consisting of fully connected neurons with a nonlinear kind of activation function: organized in at least three layers: notable for being able to distinguish data that is not linearly separable. It is a misnomer because the original perceptron used a Heaviside step function: instead of a nonlinear kind of activation function (used by modern networks). == Other feedforward networks == Examples of other feedforward networks include convolutional neural networks and radial basis function networks: which use a different activation function. == See also == Hopfield network Feed-forward Backpropagation Rprop == References == == External links == Feedforward neural networks tutorial Feedforward Neural Network: Example Feedforward Neural Networks: An Introduction\nGemini is a family of multimodal': 'original_similarity': 0.8425142788849127: 'slide': 4}"
0.42675969999982044,Who first utilized a deep-learning feedforward neural network?,The first deep-learning feedforward network was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa in 1965.,"{'chunk_text': 'It also introduced variants: including a version with four-layer perceptrons where the last two layers have learned weights (and thus a proper multilayer perceptron).: section 16 In addition: term deep learning was proposed in 1986 by Rina Dechter although the history of its appearance is apparently more complicated.The first general: working learning algorithm for supervised: deep: feedforward: multilayer perceptrons was published by Alexey Ivakhnenko and Lapa in 1967. A 1971 paper described a deep network with eight layers trained by the group method of data handling.The first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun\'ichi Amari. In computer experiments conducted by Amari\'s student Saito: a five layer MLP with two modifiable layers learned internal representations to classify non-linearily separable pattern classes. In 1987 Matthew Brand reported that wide 12-layer nonlinear perceptrons could be fully end-to-end trained to reproduce logic functions of nontrivial circuit depth via gradient descent on small batches of random input/output samples: but concluded that training time on contemporary hardware (sub-megaflop computers) made the technique impractical: and proposed using fixed random early layers as an input hash for a single modifiable layer. Instead: subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique. In 1970: Seppo Linnainmaa published the reverse mode of automatic differentiation of discrete connected networks of nested differentiable functions. This became known as backpropagation. It is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes. The terminology ""back-propagating errors"" was actually introduced in 1962 by Rosenblatt: but he did not know how to implement this: although Henry J. Kelley had a continuous precursor of backpropagation already in 1960 in the context of control theory. In 1982: Paul Werbos applied backpropagation to MLPs in the way that has become standard. In 1985: David E. Rumelhart et al. published an experimental analysis of the technique.Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers began with the Neocognitron introduced by Kunihiko Fukushima in 1980. In 1969: he also introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for CNNs and deep learning in general. CNNs have become an essential tool for computer vision. The term Deep Learning was introduced to the machine learning community by Rina Dechter in 1986: and to artificial neural networks by Igor Aizenberg and colleagues in 2000: in the context of Boolean threshold neurons.In 1988: Wei Zhang et al. applied the backpropagation algorithm to a convolutional neural network (a simplified Neocognitron with convolutional interconnections between the image feature layers and the last fully connected layer) for alphabet recognition. They also proposed an implementation of the CNN with an optical computing system. In 1989: Yann LeCun et al. applied backpropagation to a CNN with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked: training required 3 days. Subsequently: Wei Zhang: et al. modified their model by removing the last fully connected layer and applied it for medical image object segmentation in 1991 and breast cancer detection in mammograms in 1994. LeNet-5 (1998): a 7-level CNN by Yann LeCun et al.: that classifies digits: was applied by several banks to recognize hand-written numbers on checks digitized in 32x32 pixel images. In the 1980s: backpropagation did not work well for deep learning with long credit assignment paths. To overcome this problem: Jürgen Schmidhuber (1992) proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning. It uses predictive coding to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN: by distilling a higher level chunker network into a lower level automatizer network. In 1993: a chunker solved a deep learning task whose depth exceeded 1000.In 1992: Jürgen Schmidhuber also published an alternative to RNNs which is now called a linear Transformer or a Transformer with linearized self-attention (save for a normalization operator). It learns internal spotlights of attention: a slow feedforward neural network learns by gradient descent to control the fast weights of another neural network through outer products of self-generated activation patterns FROM and TO (which are now called key and value for self-attention). This fast weight attention mapping is applied to a query pattern. The modern Transformer was introduced by Ashish Vaswani et al. in their 2017 paper ""Attention Is All You Need"". It combines this with a softmax operator and a projection matrix. Transformers have increasingly become the model of choice for natural language processing. Many modern large language models such as ChatGPT: GPT-4: and BERT use it. Transformers are also increasingly being used in computer vision.In 1991: Jürgen Schmidhuber also published adversarial neural networks that contest with each other in the form of a zero-sum game: where one network\'s gain is the other network\'s loss. The first network is a\ngenerative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called ""artificial curiosity"". In 2014: this principle was used in a generative adversarial network (GAN) by Ian Goodfellow et al. Here the environmental reaction is 1 or 0 depending on whether the first network\'s output is in a given set. This can be used to create realistic deepfakes. Excellent image quality is achieved by Nvidia\'s StyleGAN (2018) based on the Progressive GAN by Tero Karras et al. Here the GAN generator is grown from small to large scale in a pyramidal fashion. Sepp Hochreiter\'s diploma thesis (1991) was called ""one of the most important documents in the history of machine learning"" by his supervisor Schmidhuber. It not only tested the neural history compressor: but also identified and analyzed the vanishing gradient problem. Hochreiter proposed recurrent residual connections to solve this problem. This led to the deep learning method called long short-term memory (LSTM): published': 'original_similarity': 0.8611309745279138: 'slide': 171}","{'chunk_text': 'CAPs describe potentially causal connections between input and output. For an FNN: the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For RNNs: in which a signal may propagate through a layer more than once: the CAP depth is potentially unlimited. To overcome this problem: Juergen Schmidhuber (1992) proposed a self-supervised hierarchy of RNNs pre-trained one level at a time by self-supervised learning. This ""neural history compressor"" uses predictive coding to learn internal representations at multiple self-organizing time scales. The deep architecture may be used to reproduce the original data from the top level feature activations. The RNN hierarchy can be ""collapsed"" into a single RNN: by ""distilling"" a higher level ""chunker"" network into a lower level ""automatizer"" network. In 1993: a chunker solved a deep learning task whose CAP depth exceeded 1000. Such history compressors can substantially facilitate downstream supervised deep learning.Geoffrey Hinton et al. (2006) proposed learning a high-level internal representation using successive layers of binary or real-valued latent variables with a restricted Boltzmann machine to model each layer. This RBM is a generative stochastic feedforward neural network that can learn a probability distribution over its set of inputs. Once sufficiently many layers have been learned: the deep architecture may be used as a generative model by reproducing the data when sampling down the model (an ""ancestral pass"") from the top level feature activations. In 2012: Andrew Ng and Jeff Dean created an FNN that learned to recognize higher-level concepts: such as cats: only from watching unlabeled images taken from YouTube videos. == The vanishing gradient problem and its solutions == Sepp Hochreiter\'s diploma thesis (1991) was called ""one of the most important documents in the history of machine learning"" by his supervisor Juergen Schmidhuber. Hochreiter not only tested the neural history compressor: but also identified and analyzed the vanishing gradient problem. He proposed recurrent residual connections to solve this problem. This led to the deep learning method called long short-term memory (LSTM): published in 1997. LSTM recurrent neural networks can learn ""very deep learning"" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. The ""vanilla LSTM"" with forget gate was introduced in 1999 by Felix Gers: Schmidhuber and Fred Cummins. LSTM has become the most cited neural network of the 20th century.In 2015: Rupesh Kumar Srivastava: Klaus Greff: and Schmidhuber used LSTM principles to create the Highway network: a feedforward neural network with hundreds of layers: much deeper than previous networks. 7 months later: Kaiming He: Xiangyu Zhang; Shaoqing Ren: and Jian Sun won the ImageNet 2015 competition with an open-gated or gateless Highway network variant called Residual neural network. This has become the most cited neural network of the 21st century.In 2011: Xavier Glorot: Antoine Bordes and Yoshua Bengio found that the ReLU of Kunihiko Fukushima also helps to overcome the vanishing gradient problem: compared to widely used activation functions prior to 2011. == Hardware-based designs == The development of metal–oxide–semiconductor (MOS) very-large-scale integration (VLSI): combining millions or billions of MOS transistors onto a single chip in the form of complementary MOS (CMOS) technology: enabled the development of practical artificial neural networks in the 1980s.Computational devices were created in CMOS: for both biophysical simulation and neuromorphic computing inspired by the structure and function of the human brain. Nanodevices for very large scale principal components analyses and convolution may create a new class of neural computing because they are fundamentally analog rather than digital (even though the first implementations may use digital devices). Ciresan and colleagues (2010) in Schmidhuber\'s group showed that despite the vanishing gradient problem: GPUs make backpropagation feasible for many-layered feedforward neural networks. == Contests == Between 2009 and 2012: recurrent neural networks and deep feedforward neural networks developed in Schmidhuber\'s research group won eight international competitions in pattern recognition and machine learning. For example: the bi-directional and multi-dimensional long short-term memory (LSTM) of Graves et al. won three competitions in connected handwriting recognition at the 2009 International Conference on Document Analysis and Recognition (ICDAR): without any prior knowledge about the three languages to be learned.Ciresan and colleagues won pattern recognition contests: including the IJCNN 2011 Traffic Sign Recognition Competition: the ISBI 2012 Segmentation of Neuronal Structures in Electron Microscopy Stacks challenge and others. Their neural networks were the first pattern recognizers to achieve human-competitive/superhuman performance on benchmarks such as traffic sign recognition (IJCNN 2012): or the MNIST handwritten digits problem. Researchers demonstrated (2010) that deep neural networks interfaced to a hidden Markov model with context-dependent states that define the neural network output layer can drastically reduce errors in large-vocabulary speech recognition tasks such as voice search.GPU-based implementations of this approach won many pattern recognition contests: including the IJCNN 2011 Traffic Sign Recognition Competition: the ISBI 2012 Segmentation of neuronal structures in EM stacks challenge: the ImageNet Competition and others. Deep: highly nonlinear neural architectures similar to the neocognitron and the ""standard architecture of vision"": inspired by simple and complex cells: were pre-trained with unsupervised methods by Hinton. A team from his lab won a 2012 contest sponsored by Merck to design software to help find molecules that might identify new drugs. == Notes == == References == == External links == ""Lecun 2019-7-11 ACM Tech Talk"". Google Docs. Retrieved 2020-02-13.\nA large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification. LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process. LLMs can be used for text generation: a form of generative AI: by taking an input text and repeatedly predicting the next token': 'original_similarity': 0.8574177497011753: 'slide': 68}","{'chunk_text': 'by his supervisor Schmidhuber. It not only tested the neural history compressor: but also identified and analyzed the vanishing gradient problem. Hochreiter proposed recurrent residual connections to solve this problem. This led to the deep learning method called long short-term memory (LSTM): published in 1997. LSTM recurrent neural networks can learn ""very deep learning"" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. The ""vanilla LSTM"" with forget gate was introduced in 1999 by Felix Gers: Schmidhuber and Fred Cummins. LSTM has become the most cited neural network of the 20th century. In 2015: Rupesh Kumar Srivastava: Klaus Greff: and Schmidhuber used LSTM principles to create the Highway network: a feedforward neural network with hundreds of layers: much deeper than previous networks. 7 months later: Kaiming He: Xiangyu Zhang; Shaoqing Ren: and Jian Sun won the ImageNet 2015 competition with an open-gated or gateless Highway network variant called Residual neural network. This has become the most cited neural network of the 21st century.In 1994: André de Carvalho: together with Mike Fairhurst and David Bisset: published experimental results of a multi-layer boolean neural network: also known as a weightless neural network: composed of a 3-layers self-organising feature extraction neural network module (SOFT) followed by a multi-layer classification neural network module (GSN): which were independently trained. Each layer in the feature extraction module extracted features with growing complexity regarding the previous layer.In 1995: Brendan Frey demonstrated that it was possible to train (over two days) a network containing six fully connected layers and several hundred hidden units using the wake-sleep algorithm: co-developed with Peter Dayan and Hinton.Since 1997: Sven Behnke extended the feed-forward hierarchical convolutional approach in the Neural Abstraction Pyramid by lateral and backward connections in order to flexibly incorporate context into decisions and iteratively resolve local ambiguities. Simpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) were a popular choice in the 1990s and 2000s: because of artificial neural networks\' computational cost and a lack of understanding of how the brain wires its biological networks. Both shallow and deep learning (e.g.: recurrent nets) of ANNs for speech recognition have been explored for many years. These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively. Key difficulties have been analyzed: including gradient diminishing and weak temporal correlation structure in neural predictive models. Additional difficulties were the lack of training data and limited computing power. Most speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government\'s NSA and DARPA: SRI studied deep neural networks (DNNs) in speech and speaker recognition. The speaker recognition team led by Larry Heck reported significant success with deep neural networks in speech processing in the 1998 National Institute of Standards and Technology Speaker Recognition evaluation. The SRI deep neural network was then deployed in the Nuance Verifier: representing the first major industrial application of deep learning. The principle of elevating ""raw"" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the ""raw"" spectrogram or linear filter-bank features in the late 1990s: showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech: waveforms: later produced excellent larger-scale results.Speech recognition was taken over by LSTM. In 2003: LSTM started to become competitive with traditional speech recognizers on certain tasks. In 2006: Alex Graves: Santiago Fernández: Faustino Gomez: and Schmidhuber combined it with connectionist temporal classification (CTC) in stacks of LSTM RNNs. In 2015: Google\'s speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM: which they made available through Google Voice Search.The impact of deep learning in industry began in the early 2000s: when CNNs already processed an estimated 10% to 20% of all the checks written in the US: according to Yann LeCun. Industrial applications of deep learning to large-scale speech recognition started around 2010. In 2006: publications by Geoff Hinton: Ruslan Salakhutdinov: Osindero and Teh showed how a many-layered feedforward neural network could be effectively pre-trained one layer at a time: treating each layer in turn as an unsupervised restricted Boltzmann machine: then fine-tuning it using supervised backpropagation. The papers referred to learning for deep belief nets. The 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech: and the possibility that given more capable hardware and large-scale data sets that deep neural nets might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets. However: it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large: context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems. The nature of the recognition errors produced by the two types of systems was characteristically different: offering technical insights into how to integrate deep learning into the existing highly\nefficient: run-time speech decoding system deployed by all major speech recognition systems. Analysis around 2009–2010: contrasting the GMM (and other generative speech models) vs. DNN models: stimulated early industrial investment in deep learning for speech recognition. That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models. In 2010: researchers extended deep learning from TIMIT to large vocabulary speech recognition: by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees.Deep learning is part of state-of-the-art systems in various disciplines: particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification): as well as a range of large-vocabulary speech recognition tasks have steadily improved. Convolutional neural': 'original_similarity': 0.8546961343179033: 'slide': 129}"
0.42701829998986796,What are the functions of the feedforward neural network?,"In a feedforward neural network, information flows in only one direction—forward—from the input nodes, through the hidden nodes (if any) and to the output nodes, without any cycles or loops.","{'chunk_text': '=== The simplest\nkind of feedforward neural network is a linear network: which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated in each node. The mean squared errors between these calculated outputs and a given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement. === Perceptron === If using a threshold: i.e. a linear activation function: the resulting linear threshold unit is called a perceptron. (Often the term is used to denote just one of these units.) Multiple parallel linear units are able to approximate any continuous function from a compact interval of the real numbers into the interval [−1:1] despite the limited computational power of single unit with a linear threshold function. This result can be found in Peter Auer: Harald Burgsteiner and Wolfgang Maass ""A learning rule for very simple universal approximators consisting of a single layer of perceptrons"".Perceptrons can be trained by a simple learning algorithm that is usually called the delta rule. It calculates the errors between calculated output and sample output data: and uses this to create an adjustment to the weights: thus implementing a form of gradient descent. === Multilayer perceptron === A multilayer perceptron (MLP) is a misnomer for a modern feedforward artificial neural network: consisting of fully connected neurons with a nonlinear kind of activation function: organized in at least three layers: notable for being able to distinguish data that is not linearly separable. It is a misnomer because the original perceptron used a Heaviside step function: instead of a nonlinear kind of activation function (used by modern networks). == Other feedforward networks == Examples of other feedforward networks include convolutional neural networks and radial basis function networks: which use a different activation function. == See also == Hopfield network Feed-forward Backpropagation Rprop == References == == External links == Feedforward neural networks tutorial Feedforward Neural Network: Example Feedforward Neural Networks:': 'original_similarity': 0.8582867634199646: 'slide': -3}","{'chunk_text': 'errors"") itself has been used by Rosenblatt himself: but he did not know how to implement it: although a continuous precursor of backpropagation was already used in the context of control theory in 1960 by Henry J. Kelley. It is known also as a reverse mode of automatic differentiation.In 1982: backpropagation was applied in the way that has become standard: for the first time by Paul Werbos.In 1985: an experimental analysis of the technique was conducted by David E. Rumelhart et al.. Many improvements to the approach have been made in subsequent decades.In 1987: using a stochastic gradient descent within a (wide 12-layer nonlinear) feed-forward network: Matthew Brand has trained it to reproduce logic functions of nontrivial circuit depth: using small batches of random input/output samples. He: however: concluded that on hardware (sub-megaflop computers) available at the time it was impractical: and proposed using fixed random early layers as an input hash for a single modifiable layer.In 1990s: an (much simpler) alternative to using neural networks: although still related support vector machine approach was developed by Vladimir Vapnik and his colleagues. In addition to performing linear classification: they were able to efficiently perform a non-linear classification using what is called the kernel trick: using high-dimensional feature spaces.In 2003: interest in backpropagation networks returned due to the successes of deep learning being applied to language modelling by Yoshua Bengio with co-authors.In 2017: modern transformer architectures were introduced. == Mathematical foundations == === Activation function === The two historically common activation functions are both sigmoids: and are described by y(vi)=tanh\u2061(vi) and y(vi)=(1+e−vi)−1{\\displaystyle y(v_{i})=\\tanh(v_{i})~~{\\textrm {and}}~~y(v_{i})=(1+e^{-v_{i}})^{-1}}.The first is a hyperbolic tangent that ranges from -1 to 1: while the other is the logistic function: which is similar in shape but ranges from 0 to 1. Here yi{\\displaystyle y_{i}} is the output of the i{\\displaystyle i}th node (neuron) and vi{\\displaystyle v_{i}} is the weighted sum of the input connections. Alternative activation functions have been proposed: including the rectifier and softplus functions. More specialized activation functions include radial basis functions (used in radial basis networks: another class of supervised neural network models). In recent developments of deep learning the rectified linear unit (ReLU) is more frequently used as one of the possible ways to overcome the numerical problems related to the sigmoids. === Learning === Learning occurs by changing connection weights after each piece of data is processed: based on the amount of error in the output compared to the expected result. This is an example of supervised learning: and is carried out through backpropagation. We can represent the degree of error in an output node j{\\displaystyle j} in the n{\\displaystyle n}th data point (training example) by ej(n)=dj(n)−yj(n){\\displaystyle e_{j}(n)=d_{j}(n)-y_{j}(n)}: where dj(n){\\displaystyle d_{j}(n)} is the desired target value for n{\\displaystyle n}th data point at node j{\\displaystyle j}: and yj(n){\\displaystyle y_{j}(n)} is the value produced at node j{\\displaystyle j} when the n{\\displaystyle n}th data point is given as an input. The node weights can then be adjusted based on corrections that minimize the error in the entire output for the n{\\displaystyle n}th data point: given by E(n)=12∑output node jej2(n){\\displaystyle {\\mathcal {E}}(n)={\\frac {1}{2}}\\sum _{{\\text{output node }}j}e_{j}^{2}(n)}.Using gradient descent: the change in each weight wij{\\displaystyle w_{ij}} is Δwji(n)=−η∂E(n)∂vj(n)yi(n){\\displaystyle \\Delta w_{ji}(n)=-\\eta {\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}y_{i}(n)}where yi(n){\\displaystyle y_{i}(n)} is the output of the previous neuron i{\\displaystyle i}: and η{\\displaystyle \\eta } is the learning rate: which is selected to ensure that the weights quickly converge to a response: without oscillations. In the previous expression: ∂E(n)∂vj(n){\\displaystyle {\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}} denotes the partial derivate of the error E(n){\\displaystyle {\\mathcal {E}}(n)} according to the weighted sum vj(n){\\displaystyle v_{j}(n)} of the input connections of neuron i{\\displaystyle i}. The derivative to be calculated depends on the induced local field vj{\\displaystyle v_{j}}: which itself varies. It is easy to prove that for an output node this derivative can be simplified to −∂E(n)∂vj(n)=ej(n)ϕ′(vj(n)){\\displaystyle -{\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}=e_{j}(n)\\phi ^{\\prime }(v_{j}(n))}where ϕ′{\\displaystyle \\phi ^{\\prime }} is the derivative of the activation function described above: which itself does not vary. The analysis is more difficult for the change in weights to a hidden node: but it can be shown that the relevant derivative is −∂E(n)∂vj(n)=ϕ′(vj(n))∑k−∂E(n)∂vk(n)wkj(n){\\displaystyle -{\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}=\\phi ^{\\prime }(v_{j}(n))\\sum _{k}-{\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{k}(n)}}w_{kj}(n)}.This depends on the change in weights of the k{\\displaystyle k}th nodes: which represent the output layer. So to change the hidden layer weights: the output layer weights change according to the derivative of the activation function: and so this algorithm represents a backpropagation of the activation function. == History == === Linear neural network === The simplest\nkind of feedforward neural network is a linear network: which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated in each node. The mean squared errors between these calculated outputs and a given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement. === Perceptron === If using a threshold: i.e. a linear activation function: the resulting linear threshold unit is called a perceptron. (Often the term is used to denote just one of these units.) Multiple parallel linear units are able to approximate any continuous function from a compact interval of the real numbers into the interval [−1:1] despite the limited computational power of single unit with a linear threshold function. This result can be found in Peter Auer: Harald Burgsteiner and Wolfgang Maass ""A learning rule for very simple universal approximators consisting of a single layer of perceptrons"".Perceptrons can be trained by a simple learning algorithm that is usually called the delta rule. It calculates the errors between calculated output and sample output data: and uses this to create': 'original_similarity': 0.8576775679336945: 'slide': 243}","{'chunk_text': ""== Feedforward == The feedforward neural network was the first and simplest type. In this network the information moves only from the input layer directly through any hidden layers to the output layer without cycles/loops. Feedforward networks can be constructed with various types of units: such as binary McCulloch–Pitts neurons: the simplest of which is the perceptron. Continuous neurons: frequently with sigmoidal activation: are used in the context of backpropagation. === Group method of data handling === The Group Method of Data Handling (GMDH) features fully automatic structural and parametric model optimization. The node activation functions are Kolmogorov–Gabor polynomials that permit additions and multiplications. It uses a deep multilayer perceptron with eight layers. It is a supervised learning network that grows layer by layer: where each layer is trained by regression analysis. Useless items are detected using a validation set: and pruned through regularization. The size and depth of the resulting network depends on the task. === Autoencoder === An autoencoder: autoassociator or Diabolo network: 19 is similar to the multilayer perceptron (MLP) – with an input layer: an output layer and one or more hidden layers connecting them. However: the output layer has the same number of units as the input layer. Its purpose is to reconstruct its own inputs (instead of emitting a target value). Therefore: autoencoders are unsupervised learning models. An autoencoder is used for unsupervised learning of efficient codings: typically for the purpose of dimensionality reduction and for learning generative models of data. === Probabilistic === A probabilistic neural network (PNN) is a four-layer feedforward neural network. The layers are Input: hidden pattern/summation: and output. In the PNN algorithm: the parent probability distribution function (PDF) of each class is approximated by a Parzen window and a non-parametric function. Then: using PDF of each class: the class probability of a new input is estimated and Bayes’ rule is employed to allocate it to the class with the highest posterior probability. It was derived from the Bayesian network and a statistical algorithm called Kernel Fisher discriminant analysis. It is used for classification and pattern recognition. === Time delay === A time delay neural network (TDNN) is a feedforward architecture for sequential data that recognizes features independent of sequence position. In order to achieve time-shift invariance: delays are added to the input so that multiple data points (points in time) are analyzed together. It usually forms part of a larger pattern recognition system. It has been implemented using a perceptron network whose connection weights were trained with back propagation (supervised learning). === Convolutional === A convolutional neural network (CNN: or ConvNet or shift invariant or space invariant) is a class of deep network: composed of one or more convolutional layers with fully connected layers (matching those in typical ANNs) on top. It uses tied weights and pooling layers. In particular: max-pooling. It is often structured via Fukushima's convolutional architecture. They are variations of multilayer perceptrons that use minimal preprocessing. This architecture allows CNNs to take advantage of the 2D structure of input data. Its unit connectivity pattern is inspired by the organization of the visual cortex. Units respond to stimuli in a restricted region of space known as the receptive field. Receptive fields partially overlap: over-covering the entire visual field. Unit response can be approximated mathematically by a convolution operation.CNNs are suitable for processing visual and other two-dimensional data. They have shown superior results in both image and speech applications. They can be trained with standard backpropagation. CNNs are easier to train than other regular: deep: feed-forward neural networks and have many fewer parameters to estimate.Capsule Neural Networks (CapsNet) add structures called capsules to a CNN and reuse output from several capsules to form more stable (with respect to various perturbations) representations.Examples of applications in computer vision include DeepDream and robot navigation. They have wide applications in image and video recognition: recommender systems and natural language processing. === Deep stacking network === A deep stacking network (DSN) (deep convex network) is based on a hierarchy of blocks of simplified neural network modules. It was introduced in 2011 by Deng and Yu. It formulates the learning as a convex optimization problem with a closed-form solution: emphasizing the mechanism's similarity to stacked generalization. Each DSN block is a simple module that is easy to train by itself in a supervised fashion without backpropagation for the entire blocks.Each block consists of a simplified multi-layer perceptron (MLP) with a single hidden layer. The hidden layer h has logistic sigmoidal units: and the output layer has linear units. Connections between these layers are represented by weight matrix U; input-to-hidden-layer connections have weight matrix W. Target vectors t form the columns of matrix T: and the input data vectors x form the columns of matrix X. The matrix of hidden units is H=σ(WTX){\\displaystyle {\\boldsymbol {H}}=\\sigma ({\\boldsymbol {W}}^{T}{\\boldsymbol {X}})}. Modules are trained in order: so lower-layer weights W are known at each stage. The function performs the element-wise logistic sigmoid operation. Each block estimates the same final label class y: and its estimate is concatenated with original input X to form the\nexpanded input for the next block. Thus: the input to the first block contains the original data only: while downstream blocks' input adds the output of preceding blocks. Then learning the upper-layer weight matrix U given other weights in the network can be formulated as a convex optimization problem: minUTf=‖UTH−T‖F2:{\\displaystyle \\min _{U^{T}}f=\\|{\\boldsymbol {U}}^{T}{\\boldsymbol {H}}-{\\boldsymbol {T}}\\|_{F}^{2}:}which has a closed-form solution.Unlike other deep architectures: such as DBNs: the goal is not to discover the transformed feature representation. The structure of the hierarchy of this kind of architecture makes parallel learning straightforward: as a batch-mode optimization problem. In purely discriminative tasks: DSNs outperform conventional DBNs. ==== Tensor deep stacking networks ==== This architecture is a DSN extension. It offers two important improvements: it uses higher-order information from covariance statistics: and it transforms the non-convex problem of a lower-layer to a convex sub-problem of an upper-layer. TDSNs use covariance statistics in a bilinear mapping from each of two distinct"": 'original_similarity': 0.8443873205114112: 'slide': 153}"
0.41807270000572316,How does learning occur in a feedforward neural network?,"Learning occurs in a feedforward neural network by changing connection weights after each piece of data is processed. This is based on the amount of error in the output compared to the expected result. This is an example of supervised learning, and is carried out through backpropagation.","{'chunk_text': 'by the Finnish researcher Seppo Linnainmaa. The term (i.e. ""back-propagating errors"") itself has been used by Rosenblatt himself: but he did not know how to implement it: although a continuous precursor of backpropagation was already used in the context of control theory in 1960 by Henry J. Kelley. It is known also as a reverse mode of automatic differentiation.In 1982: backpropagation was applied in the way that has become standard: for the first time by Paul Werbos.In 1985: an experimental analysis of the technique was conducted by David E. Rumelhart et al.. Many improvements to the approach have been made in subsequent decades.In 1987: using a stochastic gradient descent within a (wide 12-layer nonlinear) feed-forward network: Matthew Brand has trained it to reproduce logic functions of nontrivial circuit depth: using small batches of random input/output samples. He: however: concluded that on hardware (sub-megaflop computers) available at the time it was impractical: and proposed using fixed random early layers as an input hash for a single modifiable layer.In 1990s: an (much simpler) alternative to using neural networks: although still related support vector machine approach was developed by Vladimir Vapnik and his colleagues. In addition to performing linear classification: they were able to efficiently perform a non-linear classification using what is called the kernel trick: using high-dimensional feature spaces.In 2003: interest in backpropagation networks returned due to the successes of deep learning being applied to language modelling by Yoshua Bengio with co-authors.In 2017: modern transformer architectures were introduced. == Mathematical foundations == === Activation function === The two historically common activation functions are both sigmoids: and are described by y(vi)=tanh\u2061(vi) and y(vi)=(1+e−vi)−1{\\displaystyle y(v_{i})=\\tanh(v_{i})~~{\\textrm {and}}~~y(v_{i})=(1+e^{-v_{i}})^{-1}}.The first is a hyperbolic tangent that ranges from -1 to 1: while the other is the logistic function: which is similar in shape but ranges from 0 to 1. Here yi{\\displaystyle y_{i}} is the output of the i{\\displaystyle i}th node (neuron) and vi{\\displaystyle v_{i}} is the weighted sum of the input connections. Alternative activation functions have been proposed: including the rectifier and softplus functions. More specialized activation functions include radial basis functions (used in radial basis networks: another class of supervised neural network models). In recent developments of deep learning the rectified linear unit (ReLU) is more frequently used as one of the possible ways to overcome the numerical problems related to the sigmoids. === Learning === Learning occurs by changing connection weights after each piece of data is processed: based on the amount of error in the output compared to the expected result. This is an example of supervised learning: and is carried out through backpropagation. We can represent the degree of error in an output node j{\\displaystyle j} in the n{\\displaystyle n}th data point (training example) by ej(n)=dj(n)−yj(n){\\displaystyle e_{j}(n)=d_{j}(n)-y_{j}(n)}: where dj(n){\\displaystyle d_{j}(n)} is the desired target value for n{\\displaystyle n}th data point at node j{\\displaystyle j}: and yj(n){\\displaystyle y_{j}(n)} is the value produced at node j{\\displaystyle j} when the n{\\displaystyle n}th data point is given as an input. The node weights can then be adjusted based on corrections that minimize the error in the entire output for the n{\\displaystyle n}th data point: given by E(n)=12∑output node jej2(n){\\displaystyle {\\mathcal {E}}(n)={\\frac {1}{2}}\\sum _{{\\text{output node }}j}e_{j}^{2}(n)}.Using gradient descent: the change in each weight wij{\\displaystyle w_{ij}} is Δwji(n)=−η∂E(n)∂vj(n)yi(n){\\displaystyle \\Delta w_{ji}(n)=-\\eta {\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}y_{i}(n)}where yi(n){\\displaystyle y_{i}(n)} is the output of the previous neuron i{\\displaystyle i}: and η{\\displaystyle \\eta } is the learning rate: which is selected to ensure that the weights quickly converge to a response: without oscillations. In the previous expression: ∂E(n)∂vj(n){\\displaystyle {\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}} denotes the partial derivate of the error E(n){\\displaystyle {\\mathcal {E}}(n)} according to the weighted sum vj(n){\\displaystyle v_{j}(n)} of the input connections of neuron i{\\displaystyle i}. The derivative to be calculated depends on the induced local field vj{\\displaystyle v_{j}}: which itself varies. It is easy to prove that for an output node this derivative can be simplified to −∂E(n)∂vj(n)=ej(n)ϕ′(vj(n)){\\displaystyle -{\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}=e_{j}(n)\\phi ^{\\prime }(v_{j}(n))}where ϕ′{\\displaystyle \\phi ^{\\prime }} is the derivative of the activation function described above: which itself does not vary. The analysis is more difficult for the change in weights to a hidden node: but it can be shown that the relevant derivative is −∂E(n)∂vj(n)=ϕ′(vj(n))∑k−∂E(n)∂vk(n)wkj(n){\\displaystyle -{\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}=\\phi ^{\\prime }(v_{j}(n))\\sum _{k}-{\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{k}(n)}}w_{kj}(n)}.This depends on the change in weights of the k{\\displaystyle k}th nodes: which represent the output layer. So to change the hidden layer weights: the output layer weights change according to the derivative of the activation function: and so this algorithm represents a backpropagation of the activation function. == History == === Linear neural network === The simplest\nkind of feedforward neural network is a linear network: which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated in each node. The mean squared errors between these calculated outputs and a given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement. === Perceptron === If using a threshold: i.e. a linear activation function: the resulting linear threshold unit is called a perceptron. (Often the term is used to denote just one of these units.) Multiple parallel linear units are able to approximate any continuous function from a compact interval of the real numbers into the interval [−1:1] despite the limited computational power of single unit with a linear threshold function. This result can be found in Peter Auer: Harald Burgsteiner and Wolfgang Maass ""A learning rule for very simple universal approximators consisting of a single layer of perceptrons"".Perceptrons can be trained by a simple learning algorithm that is usually called the delta rule. It calculates the errors between calculated': 'original_similarity': 0.8585575937921127: 'slide': 233}","{'chunk_text': 'History == === Linear neural network === The simplest\nkind of feedforward neural network is a linear network: which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated in each node. The mean squared errors between these calculated outputs and a given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement. === Perceptron === If using a threshold: i.e. a linear activation function: the resulting linear threshold unit is called a perceptron. (Often the term is used to denote just one of these units.) Multiple parallel linear units are able to approximate any continuous function from a compact interval of the real numbers into the interval [−1:1] despite the limited computational power of single unit with a linear threshold function. This result can be found in Peter Auer: Harald Burgsteiner and Wolfgang Maass ""A learning rule for very simple universal approximators consisting of a single layer of perceptrons"".Perceptrons can be trained by a simple learning algorithm that is usually called the delta rule. It calculates the errors between calculated output and sample output data: and uses this to create an adjustment to the weights: thus implementing a form of gradient descent. === Multilayer perceptron === A multilayer perceptron (MLP) is a misnomer for a modern feedforward artificial neural network: consisting of fully connected neurons with a nonlinear kind of activation function: organized in at least three layers: notable for being able to distinguish data that is not linearly separable. It is a misnomer because the original perceptron used a Heaviside step function: instead of a nonlinear kind of activation function (used by modern networks). == Other feedforward networks == Examples of other feedforward networks include convolutional neural networks and radial basis function networks: which use a different activation function. == See also == Hopfield network Feed-forward Backpropagation Rprop == References == == External links == Feedforward neural networks tutorial Feedforward': 'original_similarity': 0.8562140829173662: 'slide': -9}","{'chunk_text': 'answer (cat) is small. Learning attempts to reduce the total of the differences across the observations. Most learning models can be viewed as a straightforward application of optimization theory and statistical estimation. ==== Learning rate ==== The learning rate defines the size of the corrective steps that the model takes to adjust for errors in each observation. A high learning rate shortens the training time: but with lower ultimate accuracy: while a lower learning rate takes longer: but with the potential for greater accuracy. Optimizations such as Quickprop are primarily aimed at speeding up error minimization: while other improvements mainly try to increase reliability. In order to avoid oscillation inside the network such as alternating connection weights: and to improve the rate of convergence: refinements use an adaptive learning rate that increases or decreases as appropriate. The concept of momentum allows the balance between the gradient and the previous change to be weighted such that the weight adjustment depends to some degree on the previous change. A momentum close to 0 emphasizes the gradient: while a value close to 1 emphasizes the last change. ==== Cost function ==== While it is possible to define a cost function ad hoc: frequently the choice is determined by the function\'s desirable properties (such as convexity) or because it arises from the model (e.g. in a probabilistic model the model\'s posterior probability can be used as an inverse cost). ==== Backpropagation ==== Backpropagation is a method used to adjust the connection weights to compensate for each error found during learning. The error amount is effectively divided among the connections. Technically: backprop calculates the gradient (the derivative) of the cost function associated with a given state with respect to the weights. The weight updates can be done via stochastic gradient descent or other methods: such as extreme learning machines: ""no-prop"" networks: training without backtracking: ""weightless"" networks: and non-connectionist neural networks. === Learning paradigms === Machine learning is commonly separated into three main learning paradigms: supervised learning: unsupervised learning and reinforcement learning. Each corresponds to a particular learning task. ==== Supervised learning ==== Supervised learning uses a set of paired inputs and desired outputs. The learning task is to produce the desired output for each input. In this case: the cost function is related to eliminating incorrect deductions. A commonly used cost is the mean-squared error: which tries to minimize the average squared error between the network\'s output and the desired output. Tasks suited for supervised learning are pattern recognition (also known as classification) and regression (also known as function approximation). Supervised learning is also applicable to sequential data (e.g.: for handwriting: speech and gesture recognition). This can be thought of as learning with a ""teacher"": in the form of a function that provides continuous feedback on the quality of solutions obtained thus far. ==== Unsupervised learning ==== In unsupervised learning: input data is given along with the cost function: some function of the data x{\\displaystyle \\textstyle x} and the network\'s output. The cost function is dependent on the task (the model domain) and any a priori assumptions (the implicit properties of the model: its parameters and the observed variables). As a trivial example: consider the model f(x)=a{\\displaystyle \\textstyle f(x)=a} where a{\\displaystyle \\textstyle a} is a constant and the cost C=E[(x−f(x))2]{\\displaystyle \\textstyle C=E[(x-f(x))^{2}]}. Minimizing this cost produces a value of a{\\displaystyle \\textstyle a} that is equal to the mean of the data. The cost function can be much more complicated. Its form depends on the application: for example: in compression it could be related to the mutual information between x{\\displaystyle \\textstyle x} and f(x){\\displaystyle \\textstyle f(x)}: whereas in statistical modeling: it could be related to the posterior probability of the model given the data (note that in both of those examples: those quantities would be maximized rather than minimized). Tasks that fall within the paradigm of unsupervised learning are in general estimation problems; the applications include clustering: the estimation of statistical distributions: compression and filtering. ==== Reinforcement learning ==== In applications such as playing video games: an actor takes a string of actions: receiving a generally unpredictable response from the environment after each one. The goal is to win the game: i.e.: generate the most positive (lowest cost) responses. In reinforcement learning: the aim is to weight the network (devise a policy) to perform actions that minimize long-term (expected cumulative) cost. At each point in time the agent performs an action and the environment generates an observation and an instantaneous cost: according to some (usually unknown) rules. The rules and the long-term cost usually only can be estimated. At any juncture: the agent decides whether to explore new actions to uncover their costs or to exploit prior learning to proceed more quickly. Formally the environment is modeled as a Markov decision process (MDP) with states s1:...:sn∈S{\\displaystyle \\textstyle {s_{1}:...:s_{n}}\\in S} and actions a1:...:am∈A{\\displaystyle \\textstyle {a_{1}:...:a_{m}}\\in A}. Because the state transitions are not known: probability distributions are used instead: the instantaneous cost distribution P(ct|st){\\displaystyle \\textstyle P(c_{t}|s_{t})}: the observation distribution P(xt|st){\\displaystyle \\textstyle P(x_{t}|s_{t})} and the transition\ndistribution P(st+1|st:at){\\displaystyle \\textstyle P(s_{t+1}|s_{t}:a_{t})}: while a policy is defined as the conditional distribution over actions given the observations. Taken together: the two define a Markov chain (MC). The aim is to discover the lowest-cost MC. ANNs serve as the learning component in such applications. Dynamic programming coupled with ANNs (giving neurodynamic programming) has been applied to problems such as those involved in vehicle routing: video games: natural resource management and medicine because of ANNs ability to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of control problems. Tasks that fall within the paradigm of reinforcement learning are control problems: games and other sequential decision making tasks. ==== Self-learning ==== Self-learning in neural networks was introduced in 1982 along with a neural network capable of self-learning named crossbar adaptive array (CAA). It is a system with only one input: situation s: and only one output: action (or behavior) a. It has neither external advice input nor external': 'original_similarity': 0.8371863783866842: 'slide': 162}"
0.43280959999538027,What is the Gemini family and who developed it?,"The Gemini family comprises of multimodal large language models developed by Google DeepMind. It serves as the successor to LaMDA and PaLM 2 and includes Gemini Ultra, Gemini Pro, and Gemini Nano. ","{'chunk_text': 'generation: allowing it to create contextual images and be adapted for a wider range of use cases. Like Bard: Google co-founder Sergey Brin was summoned out of retirement to assist in the development of Gemini: along with hundreds of other engineers from Google Brain and DeepMind; he was later credited as a ""core contributor"" to Gemini. Because Gemini was being trained on transcripts of YouTube videos: lawyers were brought in to filter out any potentially copyrighted materials.With news of Gemini\'s impending launch: OpenAI hastened its work on integrating GPT-4 with multimodal features similar to those of Gemini. The Information reported in September that several companies had been granted early access to ""an early version"" of the LLM: which Google intended to make available to clients through Google Cloud\'s Vertex AI service. The publication also stated that Google was arming Gemini to compete with both GPT-4 and Microsoft\'s GitHub Copilot. === Launch === On December 6: 2023: Pichai and Hassabis announced ""Gemini 1.0"" at a virtual press conference. It comprised three models: Gemini Ultra: designed for ""highly complex tasks""; Gemini Pro: designed for ""a wide range of tasks""; and Gemini Nano: designed for ""on-device tasks"". At launch: Gemini Pro and Nano were integrated into Bard and the Pixel 8 Pro smartphone: respectively: while Gemini Ultra was set to power ""Bard Advanced"" and become available to software developers in early 2024. Other products that Google intended to incorporate Gemini into included Search: Ads: Chrome: Duet AI on Google Workspace: and AlphaCode 2. It was made available only in English. Touted as Google\'s ""largest and most capable AI model"" and designed to emulate human behavior: the company stated that Gemini would not be made widely available until the following year due to the need for ""extensive safety testing"". Gemini was trained on and powered by Google\'s Tensor Processing Units (TPUs): and the name is in reference to the DeepMind–Google Brain merger as well as NASA\'s Project Gemini.Gemini Ultra was said to have outperformed GPT-4: Anthropic\'s Claude 2: Inflection AI\'s Inflection-2: Meta\'s LLaMA 2: and xAI\'s Grok 1 on a variety of industry benchmarks: while Gemini Pro was said to have outperformed GPT-3.5. Gemini Ultra was also the first language model to outperform human experts on the 57-subject Massive Multitask Language Understanding (MMLU) test: obtaining a score of 90%. Gemini Pro was made available to Google Cloud customers on AI Studio and Vertex AI on December 13: while Gemini Nano will be made available to Android developers as well. Hassabis further revealed that DeepMind was exploring how Gemini could be ""combined with robotics to physically interact with the world"". In accordance with an executive order signed by U.S. President Joe Biden in October: Google stated that it would share testing results of Gemini Ultra with the federal government of the United States. Similarly: the company was engaged in discussions with the government of the United Kingdom to comply with the principles laid out at the AI Safety Summit at Bletchley Park in November. === Updates === Google partnered with Samsung to integrate Gemini Nano and Gemini Pro into its Galaxy S24 smartphone lineup in January 2024. The following month: Bard and Duet AI were unified under the Gemini brand: with ""Gemini Advanced with Ultra 1.0"" debuting via a new ""AI Premium"" tier of the Google One subscription service. Gemini Pro also received a global launch.In February: Google launched ""Gemini 1.5"" in a limited capacity: positioned as a more powerful and capable model than 1.0 Ultra. This ""step change"" was achieved through various technical advancements: including a new architecture: a mixture-of-experts approach: and a larger one-million-token context window: which equates to roughly an hour of silent video: 11 hours of audio: 30:000 lines of code: or 700:000 words. The same month: Google debuted Gemma: a family of free and open-source LLMs that serve as a lightweight version of Gemini. They come in two sizes: with a neural network with two and seven billion parameters: respectively. Multiple publications viewed this as an response to Meta and others open-sourcing their AI models: and a stark reversal from Google\'s longstanding practice of keeping its AI proprietary. == Technical\nspecifications == The first generation of Gemini (""Gemini 1"") has three models: with the same software architecture. They are decoder-only transformers: with modifications to allow efficient training and inference on TPUs. They have a context length of 32:768 tokens: with multi-query attention. Two versions of Gemini Nano: Nano-1 (1.8 billion parameters) and Nano-2 (3.25 billion parameters): are distilled from larger Gemini models: designed for use by edge devices such as smartphones. As Gemini is multimodal: each context window can contain multiple forms of input. The different modes can be interleaved and do not have to be presented in a fixed order: allowing for a multimodal conversation. For example: the user might open the conversation with a mix of text: picture: video: and audio: presented in any order: and Gemini might reply with the same free ordering. Input images may be of different resolutions: while video is inputted as a sequence of images. Audio is sampled at 16 kHz and then converted into a sequence of tokens by the Universal Speech Model. Gemini\'s dataset is multimodal and multilingual: consisting of ""web documents: books: and code: and includ[ing] image: audio: and video data"".Demis Hassabis claims that training Gemini 1 used ""roughly the same amount of compute: maybe slightly more than what was rumored for GPT-4"".The second generation of Gemini (""Gemini 1.5"") has one model published so far: Gemini 1.5 Pro. It is a multimodal sparse mixture-of-experts: with context length of ""multiple millions"". == Reception == Gemini\'s launch was preluded by months of intense speculation and anticipation: which MIT Technology Review described as ""peak AI hype"". In August 2023: Dylan Patel and Daniel Nishball of research firm SemiAnalysis penned a blog post declaring that the release of Gemini would ""eat the world"" and outclass GPT-4: prompting OpenAI CEO Sam Altman to ridicule the duo on X (formerly Twitter). Business magnate Elon Musk: who co-founded': 'original_similarity': 0.8292733126596599: 'slide': 309}","{'chunk_text': 'They have a context length of 32:768 tokens: with multi-query attention. Two versions of Gemini Nano: Nano-1 (1.8 billion parameters) and Nano-2 (3.25 billion parameters): are distilled from larger Gemini models: designed for use by edge devices such as smartphones. As Gemini is multimodal: each context window can contain multiple forms of input. The different modes can be interleaved and do not have to be presented in a fixed order: allowing for a multimodal conversation. For example: the user might open the conversation with a mix of text: picture: video: and audio: presented in any order: and Gemini might reply with the same free ordering. Input images may be of different resolutions: while video is inputted as a sequence of images. Audio is sampled at 16 kHz and then converted into a sequence of tokens by the Universal Speech Model. Gemini\'s dataset is multimodal and multilingual: consisting of ""web documents: books: and code: and includ[ing] image: audio: and video data"".Demis Hassabis claims that training Gemini 1 used ""roughly the same amount of compute: maybe slightly more than what was rumored for GPT-4"".The second generation of Gemini (""Gemini 1.5"") has one model published so far: Gemini 1.5 Pro. It is a multimodal sparse mixture-of-experts: with context length of ""multiple millions"". == Reception == Gemini\'s launch was preluded by months of intense speculation and anticipation: which MIT Technology Review described as ""peak AI hype"". In August 2023: Dylan Patel and Daniel Nishball of research firm SemiAnalysis penned a blog post declaring that the release of Gemini would ""eat the world"" and outclass GPT-4: prompting OpenAI CEO Sam Altman to ridicule the duo on X (formerly Twitter). Business magnate Elon Musk: who co-founded OpenAI: weighed in: asking: ""Are the numbers wrong?"" Hugh Langley of Business Insider remarked that Gemini would be a make-or-break moment for Google: writing: ""If Gemini dazzles: it will help Google change the narrative that it was blindsided by Microsoft and OpenAI. If it disappoints: it will embolden critics who say Google has fallen behind.""Reacting to its unveiling in December 2023: University of Washington professor emeritus Oren Etzioni predicted a ""tit-for-tat arms race"" between Google and OpenAI. Professor Alexei Efros of the University of California: Berkeley praised the potential of Gemini\'s multimodal approach: while scientist Melanie Mitchell of the Santa Fe Institute called Gemini ""very sophisticated"". Professor Chirag Shah of the University of Washington was less impressed: likening Gemini\'s launch to the routineness of Apple\'s annual introduction of a new iPhone. Similarly: Stanford University\'s Percy Liang: the University of Washington\'s Emily Bender: and the University of Galway\'s Michael Madden cautioned that it was difficult to interpret benchmark scores without insight into the training data used. Writing for Fast Company: Mark Sullivan opined that Google had the opportunity to challenge the iPhone\'s dominant market share: believing that Apple was unlikely to have the capacity to develop functionality similar to Gemini with its Siri virtual assistant. Google shares spiked by 5.3 percent the day after Gemini\'s launch.Google faced criticism for a demonstrative video of Gemini: which was not conducted in real time. == See also == Gato: a multimodal neural network developed by DeepMind == References == == Further reading == == External links == Official website Press release via The Keyword Announcement and demo on YouTube White paper for 1.0 and 1.5\nA generative adversarial network (GAN) is a class of machine learning frameworks and a prominent framework for approaching generative AI. The concept was initially developed by Ian Goodfellow and his colleagues in': 'original_similarity': 0.8143067204641165: 'slide': 30}","{'chunk_text': 'one must also have twice the number of training tokens. This hypothesis has been used to train Chinchilla by DeepMind. Similar to Gopher in terms of cost: Chinchilla has 70B parameters and four times as much data.Chinchilla has an average accuracy of 67.5% on the MMLU benchmark (Measuring Massive Multitask Language Understanding): which is 7% higher than Gopher\'s performance. Chinchilla was still in the testing phase as of January 12: 2023.Chinchilla contributes to developing an effective training paradigm for large autoregressive language models with limited compute resources. The Chinchilla team recommends that the number of training tokens is twice for every model size doubling: meaning that using larger: higher-quality training datasets can lead to better results on downstream tasks. == Architecture == Both the Gopher family and Chinchilla family are families of transformer models. In particular: they are essentially the same as GPT-2: with different sizes and minor modifications. Gopher family uses RMSNorm instead of LayerNorm; relative positional encoding rather than absolute positional encoding. The Chinchilla family is the same as the Gopher family: but trained with AdamW instead of Adam optimizer. The Gopher family contains six models of increasing size: from 44 million parameters to 280 billion parameters. They refer to the largest one as ""Gopher"" by default. Similar naming conventions apply for the Chinchilla family. Table 1 of shows the entire Gopher family: Table 4 of compares the 70-billion-parameter Chinchilla with Gopher 280B. == See also == LaMDA == References ==\nClaude is a family of large language models developed by Anthropic. The first model was released in March 2023. Claude 3: released in March 2024: can also analyze images. == Training == Claude models are generative pre-trained transformers. They have been pre-trained to predict the next word in large amounts of text. Claude models have then been fine-tuned with Constitutional AI and Reinforcement Learning From Human Feedback: with the aim of making them less likely to give harmful responses: while still being helpful to the user. === Constitutional AI === Constitutional AI is': 'original_similarity': 0.7444011592674613: 'slide': 91}"
0.43275700000231154,When was the Gemini family announced and who were they positioned as a competitor to?,"The Gemini family was announced on December 6, 2023, and they were positioned as a competitor to OpenAI's GPT-4. ","{'chunk_text': 'of use cases. Like Bard: Google co-founder Sergey Brin was summoned out of retirement to assist in the development of Gemini: along with hundreds of other engineers from Google Brain and DeepMind; he was later credited as a ""core contributor"" to Gemini. Because Gemini was being trained on transcripts of YouTube videos: lawyers were brought in to filter out any potentially copyrighted materials.With news of Gemini\'s impending launch: OpenAI hastened its work on integrating GPT-4 with multimodal features similar to those of Gemini. The Information reported in September that several companies had been granted early access to ""an early version"" of the LLM: which Google intended to make available to clients through Google Cloud\'s Vertex AI service. The publication also stated that Google was arming Gemini to compete with both GPT-4 and Microsoft\'s GitHub Copilot. === Launch === On December 6: 2023: Pichai and Hassabis announced ""Gemini 1.0"" at a virtual press conference. It comprised three models: Gemini Ultra: designed for ""highly complex tasks""; Gemini Pro: designed for ""a wide range of tasks""; and Gemini Nano: designed for ""on-device tasks"". At launch: Gemini Pro and Nano were integrated into Bard and the Pixel 8 Pro smartphone: respectively: while Gemini Ultra was set to power ""Bard Advanced"" and become available to software developers in early 2024. Other products that Google intended to incorporate Gemini into included Search: Ads: Chrome: Duet AI on Google Workspace: and AlphaCode 2. It was made available only in English. Touted as Google\'s ""largest and most capable AI model"" and designed to emulate human behavior: the company stated that Gemini would not be made widely available until the following year due to the need for ""extensive safety testing"". Gemini was trained on and powered by Google\'s Tensor Processing Units (TPUs): and the name is in reference to the DeepMind–Google Brain merger as well as NASA\'s Project Gemini.Gemini Ultra was said to have outperformed GPT-4: Anthropic\'s Claude 2: Inflection AI\'s Inflection-2: Meta\'s LLaMA 2: and xAI\'s Grok 1 on a variety of industry benchmarks: while Gemini Pro was said to have outperformed GPT-3.5. Gemini Ultra was also the first language model to outperform human experts on the 57-subject Massive Multitask Language Understanding (MMLU) test: obtaining a score of 90%. Gemini Pro was made available to Google Cloud customers on AI Studio and Vertex AI on December 13: while Gemini Nano will be made available to Android developers as well. Hassabis further revealed that DeepMind was exploring how Gemini could be ""combined with robotics to physically interact with the world"". In accordance with an executive order signed by U.S. President Joe Biden in October: Google stated that it would share testing results of Gemini Ultra with the federal government of the United States. Similarly: the company was engaged in discussions with the government of the United Kingdom to comply with the principles laid out at the AI Safety Summit at Bletchley Park in November. === Updates === Google partnered with Samsung to integrate Gemini Nano and Gemini Pro into its Galaxy S24 smartphone lineup in January 2024. The following month: Bard and Duet AI were unified under the Gemini brand: with ""Gemini Advanced with Ultra 1.0"" debuting via a new ""AI Premium"" tier of the Google One subscription service. Gemini Pro also received a global launch.In February: Google launched ""Gemini 1.5"" in a limited capacity: positioned as a more powerful and capable model than 1.0 Ultra. This ""step change"" was achieved through various technical advancements: including a new architecture: a mixture-of-experts approach: and a larger one-million-token context window: which equates to roughly an hour of silent video: 11 hours of audio: 30:000 lines of code: or 700:000 words. The same month: Google debuted Gemma: a family of free and open-source LLMs that serve as a lightweight version of Gemini. They come in two sizes: with a neural network with two and seven billion parameters: respectively. Multiple publications viewed this as an response to Meta and others open-sourcing their AI models: and a stark reversal from Google\'s longstanding practice of keeping its AI proprietary. == Technical\nspecifications == The first generation of Gemini (""Gemini 1"") has three models: with the same software architecture. They are decoder-only transformers: with modifications to allow efficient training and inference on TPUs. They have a context length of 32:768 tokens: with multi-query attention. Two versions of Gemini Nano: Nano-1 (1.8 billion parameters) and Nano-2 (3.25 billion parameters): are distilled from larger Gemini models: designed for use by edge devices such as smartphones. As Gemini is multimodal: each context window can contain multiple forms of input. The different modes can be interleaved and do not have to be presented in a fixed order: allowing for a multimodal conversation. For example: the user might open the conversation with a mix of text: picture: video: and audio: presented in any order: and Gemini might reply with the same free ordering. Input images may be of different resolutions: while video is inputted as a sequence of images. Audio is sampled at 16 kHz and then converted into a sequence of tokens by the Universal Speech Model. Gemini\'s dataset is multimodal and multilingual: consisting of ""web documents: books: and code: and includ[ing] image: audio: and video data"".Demis Hassabis claims that training Gemini 1 used ""roughly the same amount of compute: maybe slightly more than what was rumored for GPT-4"".The second generation of Gemini (""Gemini 1.5"") has one model published so far: Gemini 1.5 Pro. It is a multimodal sparse mixture-of-experts: with context length of ""multiple millions"". == Reception == Gemini\'s launch was preluded by months of intense speculation and anticipation: which MIT Technology Review described as ""peak AI hype"". In August 2023: Dylan Patel and Daniel Nishball of research firm SemiAnalysis penned a blog post declaring that the release of Gemini would ""eat the world"" and outclass GPT-4: prompting OpenAI CEO Sam Altman to ridicule the duo on X (formerly Twitter). Business magnate Elon Musk: who co-founded OpenAI: weighed in: asking: ""Are the numbers wrong?"" Hugh Langley of Business Insider remarked': 'original_similarity': 0.8169789810472757: 'slide': 323}","{'chunk_text': 'a context length of 32:768 tokens: with multi-query attention. Two versions of Gemini Nano: Nano-1 (1.8 billion parameters) and Nano-2 (3.25 billion parameters): are distilled from larger Gemini models: designed for use by edge devices such as smartphones. As Gemini is multimodal: each context window can contain multiple forms of input. The different modes can be interleaved and do not have to be presented in a fixed order: allowing for a multimodal conversation. For example: the user might open the conversation with a mix of text: picture: video: and audio: presented in any order: and Gemini might reply with the same free ordering. Input images may be of different resolutions: while video is inputted as a sequence of images. Audio is sampled at 16 kHz and then converted into a sequence of tokens by the Universal Speech Model. Gemini\'s dataset is multimodal and multilingual: consisting of ""web documents: books: and code: and includ[ing] image: audio: and video data"".Demis Hassabis claims that training Gemini 1 used ""roughly the same amount of compute: maybe slightly more than what was rumored for GPT-4"".The second generation of Gemini (""Gemini 1.5"") has one model published so far: Gemini 1.5 Pro. It is a multimodal sparse mixture-of-experts: with context length of ""multiple millions"". == Reception == Gemini\'s launch was preluded by months of intense speculation and anticipation: which MIT Technology Review described as ""peak AI hype"". In August 2023: Dylan Patel and Daniel Nishball of research firm SemiAnalysis penned a blog post declaring that the release of Gemini would ""eat the world"" and outclass GPT-4: prompting OpenAI CEO Sam Altman to ridicule the duo on X (formerly Twitter). Business magnate Elon Musk: who co-founded OpenAI: weighed in: asking: ""Are the numbers wrong?"" Hugh Langley of Business Insider remarked that Gemini would be a make-or-break moment for Google: writing: ""If Gemini dazzles: it will help Google change the narrative that it was blindsided by Microsoft and OpenAI. If it disappoints: it will embolden critics who say Google has fallen behind.""Reacting to its unveiling in December 2023: University of Washington professor emeritus Oren Etzioni predicted a ""tit-for-tat arms race"" between Google and OpenAI. Professor Alexei Efros of the University of California: Berkeley praised the potential of Gemini\'s multimodal approach: while scientist Melanie Mitchell of the Santa Fe Institute called Gemini ""very sophisticated"". Professor Chirag Shah of the University of Washington was less impressed: likening Gemini\'s launch to the routineness of Apple\'s annual introduction of a new iPhone. Similarly: Stanford University\'s Percy Liang: the University of Washington\'s Emily Bender: and the University of Galway\'s Michael Madden cautioned that it was difficult to interpret benchmark scores without insight into the training data used. Writing for Fast Company: Mark Sullivan opined that Google had the opportunity to challenge the iPhone\'s dominant market share: believing that Apple was unlikely to have the capacity to develop functionality similar to Gemini with its Siri virtual assistant. Google shares spiked by 5.3 percent the day after Gemini\'s launch.Google faced criticism for a demonstrative video of Gemini: which was not conducted in real time. == See also == Gato: a multimodal neural network developed by DeepMind == References == == Further reading == == External links == Official website Press release via The Keyword Announcement and demo on YouTube White paper for 1.0 and 1.5\nA generative adversarial network (GAN) is a class of machine learning frameworks and a prominent framework for approaching generative AI. The concept was initially developed by Ian Goodfellow and his colleagues in June 2014.': 'original_similarity': 0.8018999859717829: 'slide': 32}","{'chunk_text': 'their GPT models. GPT-1: report: GitHub release. GPT-2: blog announcement: report on its decision of ""staged release"": GitHub release. GPT-3: report. No GitHub or any other form of code release thenceforth. webGPT: blog announcement: report: InstructGPT: blog announcement: report. ChatGPT: blog announcement (no report). GPT-4: blog announcement: reports: model card. == See also == Cyc Gemini == References ==\nA graph neural network (GNN) belongs to a class of artificial neural networks for processing data that can be': 'original_similarity': 0.7400029712652881: 'slide': 17}"
0.4099249000137206,What is unique about the Gemini family of language models?,"Unlike other Large Language Models, Gemini is unique in that it is not trained on a text corpus alone. It is designed to be multimodal, meaning it can process multiple types of data simultaneously, including text, images, audio, video, and computer code.","{'chunk_text': 'Gemini: revealing that the company was targeting a launch date of late 2023. According to the report: Google hoped to surpass OpenAI and other competitors by combining conversational text capabilities present in most LLMs with artificial intelligence–powered image generation: allowing it to create contextual images and be adapted for a wider range of use cases. Like Bard: Google co-founder Sergey Brin was summoned out of retirement to assist in the development of Gemini: along with hundreds of other engineers from Google Brain and DeepMind; he was later credited as a ""core contributor"" to Gemini. Because Gemini was being trained on transcripts of YouTube videos: lawyers were brought in to filter out any potentially copyrighted materials.With news of Gemini\'s impending launch: OpenAI hastened its work on integrating GPT-4 with multimodal features similar to those of Gemini. The Information reported in September that several companies had been granted early access to ""an early version"" of the LLM: which Google intended to make available to clients through Google Cloud\'s Vertex AI service. The publication also stated that Google was arming Gemini to compete with both GPT-4 and Microsoft\'s GitHub Copilot. === Launch === On December 6: 2023: Pichai and Hassabis announced ""Gemini 1.0"" at a virtual press conference. It comprised three models: Gemini Ultra: designed for ""highly complex tasks""; Gemini Pro: designed for ""a wide range of tasks""; and Gemini Nano: designed for ""on-device tasks"". At launch: Gemini Pro and Nano were integrated into Bard and the Pixel 8 Pro smartphone: respectively: while Gemini Ultra was set to power ""Bard Advanced"" and become available to software developers in early 2024. Other products that Google intended to incorporate Gemini into included Search: Ads: Chrome: Duet AI on Google Workspace: and AlphaCode 2. It was made available only in English. Touted as Google\'s ""largest and most capable AI model"" and designed to emulate human behavior: the company stated that Gemini would not be made widely available until the following year due to the need for ""extensive safety testing"". Gemini was trained on and powered by Google\'s Tensor Processing Units (TPUs): and the name is in reference to the DeepMind–Google Brain merger as well as NASA\'s Project Gemini.Gemini Ultra was said to have outperformed GPT-4: Anthropic\'s Claude 2: Inflection AI\'s Inflection-2: Meta\'s LLaMA 2: and xAI\'s Grok 1 on a variety of industry benchmarks: while Gemini Pro was said to have outperformed GPT-3.5. Gemini Ultra was also the first language model to outperform human experts on the 57-subject Massive Multitask Language Understanding (MMLU) test: obtaining a score of 90%. Gemini Pro was made available to Google Cloud customers on AI Studio and Vertex AI on December 13: while Gemini Nano will be made available to Android developers as well. Hassabis further revealed that DeepMind was exploring how Gemini could be ""combined with robotics to physically interact with the world"". In accordance with an executive order signed by U.S. President Joe Biden in October: Google stated that it would share testing results of Gemini Ultra with the federal government of the United States. Similarly: the company was engaged in discussions with the government of the United Kingdom to comply with the principles laid out at the AI Safety Summit at Bletchley Park in November. === Updates === Google partnered with Samsung to integrate Gemini Nano and Gemini Pro into its Galaxy S24 smartphone lineup in January 2024. The following month: Bard and Duet AI were unified under the Gemini brand: with ""Gemini Advanced with Ultra 1.0"" debuting via a new ""AI Premium"" tier of the Google One subscription service. Gemini Pro also received a global launch.In February: Google launched ""Gemini 1.5"" in a limited capacity: positioned as a more powerful and capable model than 1.0 Ultra. This ""step change"" was achieved through various technical advancements: including a new architecture: a mixture-of-experts approach: and a larger one-million-token context window: which equates to roughly an hour of silent video: 11 hours of audio: 30:000 lines of code: or 700:000 words. The same month: Google debuted Gemma: a family of free and open-source LLMs that serve as a lightweight version of Gemini. They come in two sizes: with a neural network with two and seven billion parameters: respectively. Multiple publications viewed this as an response to Meta and others open-sourcing their AI models: and a stark reversal from Google\'s longstanding practice of keeping its AI proprietary. == Technical\nspecifications == The first generation of Gemini (""Gemini 1"") has three models: with the same software architecture. They are decoder-only transformers: with modifications to allow efficient training and inference on TPUs. They have a context length of 32:768 tokens: with multi-query attention. Two versions of Gemini Nano: Nano-1 (1.8 billion parameters) and Nano-2 (3.25 billion parameters): are distilled from larger Gemini models: designed for use by edge devices such as smartphones. As Gemini is multimodal: each context window can contain multiple forms of input. The different modes can be interleaved and do not have to be presented in a fixed order: allowing for a multimodal conversation. For example: the user might open the conversation with a mix of text: picture: video: and audio: presented in any order: and Gemini might reply with the same free ordering. Input images may be of different resolutions: while video is inputted as a sequence of images. Audio is sampled at 16 kHz and then converted into a sequence of tokens by the Universal Speech Model. Gemini\'s dataset is multimodal and multilingual: consisting of ""web documents: books: and code: and includ[ing] image: audio: and video data"".Demis Hassabis claims that training Gemini 1 used ""roughly the same amount of compute: maybe slightly more than what was rumored for GPT-4"".The second generation of Gemini (""Gemini 1.5"") has one model published so far: Gemini 1.5 Pro. It is a multimodal sparse mixture-of-experts: with context length of ""multiple millions"". == Reception == Gemini\'s launch was preluded by months of intense speculation and anticipation: which MIT Technology Review described as ""peak AI hype"". In August 2023: Dylan Patel and Daniel Nishball of research': 'original_similarity': 0.88192925319672: 'slide': 271}","{'chunk_text': 'and a stark reversal from Google\'s longstanding practice of keeping its AI proprietary. == Technical\nspecifications == The first generation of Gemini (""Gemini 1"") has three models: with the same software architecture. They are decoder-only transformers: with modifications to allow efficient training and inference on TPUs. They have a context length of 32:768 tokens: with multi-query attention. Two versions of Gemini Nano: Nano-1 (1.8 billion parameters) and Nano-2 (3.25 billion parameters): are distilled from larger Gemini models: designed for use by edge devices such as smartphones. As Gemini is multimodal: each context window can contain multiple forms of input. The different modes can be interleaved and do not have to be presented in a fixed order: allowing for a multimodal conversation. For example: the user might open the conversation with a mix of text: picture: video: and audio: presented in any order: and Gemini might reply with the same free ordering. Input images may be of different resolutions: while video is inputted as a sequence of images. Audio is sampled at 16 kHz and then converted into a sequence of tokens by the Universal Speech Model. Gemini\'s dataset is multimodal and multilingual: consisting of ""web documents: books: and code: and includ[ing] image: audio: and video data"".Demis Hassabis claims that training Gemini 1 used ""roughly the same amount of compute: maybe slightly more than what was rumored for GPT-4"".The second generation of Gemini (""Gemini 1.5"") has one model published so far: Gemini 1.5 Pro. It is a multimodal sparse mixture-of-experts: with context length of ""multiple millions"". == Reception == Gemini\'s launch was preluded by months of intense speculation and anticipation: which MIT Technology Review described as ""peak AI hype"". In August 2023: Dylan Patel and Daniel Nishball of research firm SemiAnalysis penned a blog post declaring that the release of Gemini would ""eat the world"" and outclass GPT-4: prompting OpenAI CEO Sam Altman to ridicule the duo on X (formerly Twitter). Business magnate Elon Musk: who co-founded OpenAI: weighed in: asking: ""Are the numbers wrong?"" Hugh Langley of Business Insider remarked that Gemini would be a make-or-break moment for Google: writing: ""If Gemini dazzles: it will help Google change the narrative that it was blindsided by Microsoft and OpenAI. If it disappoints: it will embolden critics who say Google has fallen behind.""Reacting to its unveiling in December 2023: University of Washington professor emeritus Oren Etzioni predicted a ""tit-for-tat arms race"" between Google and OpenAI. Professor Alexei Efros of the University of California: Berkeley praised the potential of Gemini\'s multimodal approach: while scientist Melanie Mitchell of the Santa Fe Institute called Gemini ""very sophisticated"". Professor Chirag Shah of the University of Washington was less impressed: likening Gemini\'s launch to the routineness of Apple\'s annual introduction of a new iPhone. Similarly: Stanford University\'s Percy Liang: the University of Washington\'s Emily Bender: and the University of Galway\'s Michael Madden cautioned that it was difficult to interpret benchmark scores without insight into the training data used. Writing for Fast Company: Mark Sullivan opined that Google had the opportunity to challenge the iPhone\'s dominant market share: believing that Apple was unlikely to have the capacity to develop functionality similar to Gemini with its Siri virtual assistant. Google shares spiked by 5.3 percent the day after Gemini\'s launch.Google faced criticism for a demonstrative video of Gemini: which was not conducted in real time. == See also == Gato: a multimodal neural network developed by DeepMind == References == == Further reading == == External links == Official website Press release': 'original_similarity': 0.8742198136762765: 'slide': -15}","{'chunk_text': '== History == At the 2017 NeurIPS conference: Google researchers introduced the transformer architecture in their landmark paper ""Attention Is All You Need"". This paper\'s goal was to improve upon 2014 Seq2seq technology: and was based mainly on the attention mechanism developed by Bahdanau et al. in 2014. The following year in 2018: BERT was introduced and quickly became ""ubiquitous"". Though the original transformer has both encoder and decoder blocks: BERT is an encoder-only model. Although decoder-only GPT-1 was introduced in 2018: it was GPT-2 in 2019 that caught widespread attention because OpenAI at first deemed it too powerful to release publicly: out of fear of malicious use. GPT-3 in 2020 went a step further and as of 2024 is available only via API with no offering of downloading the model to execute locally. But it was the 2022 consumer-facing browser-based ChatGPT that captured the imaginations of the general population and caused some media hype and online buzz. The 2023 GPT-4 was praised for its increased accuracy and as a ""holy grail"" for its multimodal capabilities. OpenAI did not reveal high-level architecture and the number of parameters of GPT-4. In the meantime: competing language models have for the most part been playing catch-up to the GPT series: at least in terms of number of parameters. Notable exceptions in terms of either number of parameters or measured accuracy include Google\'s 2019 T5-11B and 2022 PaLM-E: and Anthropic\'s 2024 Claude 3. In terms of Elo ratings: on January 26: 2024: Google\'s Bard (Gemini Pro) surpassed the regular GPT-4: but not the limited-availability GPT-4-Turbo.Since 2022: source-available models have been gaining popularity: especially at first with BLOOM and LLaMA: though both have restrictions on the field of use. Mistral AI\'s models Mistral 7B and Mixtral 8x7b have the more permissive Apache License. As of January 2024: Mixtral 8x7b is the most powerful open LLM according to the LMSYS Chatbot Arena Leaderboard: being more powerful than GPT-3.5 but not as powerful as GPT-4. == Dataset preprocessing == === Probabilistic tokenization === Because machine learning algorithms process numbers rather than text: the text must be converted to numbers. In the first step: a vocabulary is decided upon: then integer indexes are arbitrarily but uniquely assigned to each vocabulary entry: and finally: an embedding is associated to the integer index. Algorithms include byte-pair encoding and WordPiece. Probabilistic tokenization also compresses the datasets. Because LLMs generally require input to be an array that is not jagged: the shorter texts must be ""padded"" until they match the length of the longest one. How many tokens are: on average: needed per word depends on the language of the dataset. ==== BPE ==== Using a modification of byte-pair encoding: in the first step: all unique characters (including blanks and punctuation marks) are treated as an initial set of n-grams (i.e. initial set of uni-grams). Successively the most frequent pair of adjacent characters is merged into a bi-gram and all instances of the pair are replaced by it. All occurrences of adjacent pairs of (previously merged) n-grams that most frequently occur together are then again merged into even lengthier n-gram repeatedly until a vocabulary of prescribed size is obtained (in case of GPT-3: the size is 50257). Token vocabulary consists of integers: spanning from zero up to the size of the token vocabulary. New words can always be interpreted as combinations of the tokens and the initial-set uni-grams.A token vocabulary based on the frequencies extracted from mainly English corpora uses as few tokens as possible for an average English word. An average word in another language encoded by such an English-optimized tokenizer is however split into suboptimal amount of tokens. GPT-2 tokenizer can use up to 15 times more tokens per word for some languages: for example for Shan language from Myanmar. Even more widespread languages such as Portuguese and German have ""a premium of 50%"" compared to English.For example: here is how tokenizer used by GPT-3 (Legacy) split the following sentence tokenizer: texts -> series of numerical ""tokens"". === Dataset cleaning === In the context of training LLMs: datasets are typically cleaned by removing toxic passages from the dataset: discarding low-quality data: and de-duplication. Cleaned datasets can increase training efficiency and lead to improved downstream performance.With the increasing proportion of LLM-generated content on the web: data cleaning in the future may include filtering out such content. LLM-generated content can pose a problem if the content is similar to human text (making filtering difficult) but of lower quality (degrading performance of models trained on it). == Training and architecture == === Reinforcement learning from human feedback (RLHF) === Reinforcement learning from human feedback (RLHF) through algorithms: such\nas proximal policy optimization: is used to further fine-tune a model based on a dataset of human preferences. === Instruction tuning === Using ""self-instruct"" approaches: LLMs have been able to bootstrap correct responses: replacing any naive responses: starting from human-generated corrections of a few cases. For example: in the instruction ""Write an essay about the main themes represented in Hamlet:"" an initial naive completion might be \'If you submit the essay after March 17: your grade will be reduced by 10% for each day of delay:"" based on the frequency of this textual sequence in the corpus. === Mixture of experts === The largest LLM may be too expensive to train and use directly. For such models: mixture of experts (MoE) can be applied: a line of research pursued by Google researchers since 2017 to train models reaching up to 1 trillion parameters. === Prompt engineering: attention mechanism: and context window === Most results previously achievable only by (costly) fine-tuning: can be achieved through prompt engineering: although limited to the scope of a single conversation (more precisely: limited to the scope of a context window). In order to find out which tokens are relevant to each other within the scope of the context window: the attention mechanism calculates ""soft"" weights for each token: more precisely for its embedding: by using multiple attention heads: each with its': 'original_similarity': 0.811327518251345: 'slide': 224}"
0.4741920999949798,How has Gemini been integrated into Google's products and services?,"Upon launch, Gemini Pro and Nano were integrated into Bard and the Pixel 8 Pro smartphone, respectively, while Gemini Ultra was set to power ""Bard Advanced"". Google also intended to incorporate Gemini into other products including Search, Ads, Chrome, Duet AI on Google Workspace, and AlphaCode 2.","{'chunk_text': 'image generation: allowing it to create contextual images and be adapted for a wider range of use cases. Like Bard: Google co-founder Sergey Brin was summoned out of retirement to assist in the development of Gemini: along with hundreds of other engineers from Google Brain and DeepMind; he was later credited as a ""core contributor"" to Gemini. Because Gemini was being trained on transcripts of YouTube videos: lawyers were brought in to filter out any potentially copyrighted materials.With news of Gemini\'s impending launch: OpenAI hastened its work on integrating GPT-4 with multimodal features similar to those of Gemini. The Information reported in September that several companies had been granted early access to ""an early version"" of the LLM: which Google intended to make available to clients through Google Cloud\'s Vertex AI service. The publication also stated that Google was arming Gemini to compete with both GPT-4 and Microsoft\'s GitHub Copilot. === Launch === On December 6: 2023: Pichai and Hassabis announced ""Gemini 1.0"" at a virtual press conference. It comprised three models: Gemini Ultra: designed for ""highly complex tasks""; Gemini Pro: designed for ""a wide range of tasks""; and Gemini Nano: designed for ""on-device tasks"". At launch: Gemini Pro and Nano were integrated into Bard and the Pixel 8 Pro smartphone: respectively: while Gemini Ultra was set to power ""Bard Advanced"" and become available to software developers in early 2024. Other products that Google intended to incorporate Gemini into included Search: Ads: Chrome: Duet AI on Google Workspace: and AlphaCode 2. It was made available only in English. Touted as Google\'s ""largest and most capable AI model"" and designed to emulate human behavior: the company stated that Gemini would not be made widely available until the following year due to the need for ""extensive safety testing"". Gemini was trained on and powered by Google\'s Tensor Processing Units (TPUs): and the name is in reference to the DeepMind–Google Brain merger as well as NASA\'s Project Gemini.Gemini Ultra was said to have outperformed GPT-4: Anthropic\'s Claude 2: Inflection AI\'s Inflection-2: Meta\'s LLaMA 2: and xAI\'s Grok 1 on a variety of industry benchmarks: while Gemini Pro was said to have outperformed GPT-3.5. Gemini Ultra was also the first language model to outperform human experts on the 57-subject Massive Multitask Language Understanding (MMLU) test: obtaining a score of 90%. Gemini Pro was made available to Google Cloud customers on AI Studio and Vertex AI on December 13: while Gemini Nano will be made available to Android developers as well. Hassabis further revealed that DeepMind was exploring how Gemini could be ""combined with robotics to physically interact with the world"". In accordance with an executive order signed by U.S. President Joe Biden in October: Google stated that it would share testing results of Gemini Ultra with the federal government of the United States. Similarly: the company was engaged in discussions with the government of the United Kingdom to comply with the principles laid out at the AI Safety Summit at Bletchley Park in November. === Updates === Google partnered with Samsung to integrate Gemini Nano and Gemini Pro into its Galaxy S24 smartphone lineup in January 2024. The following month: Bard and Duet AI were unified under the Gemini brand: with ""Gemini Advanced with Ultra 1.0"" debuting via a new ""AI Premium"" tier of the Google One subscription service. Gemini Pro also received a global launch.In February: Google launched ""Gemini 1.5"" in a limited capacity: positioned as a more powerful and capable model than 1.0 Ultra. This ""step change"" was achieved through various technical advancements: including a new architecture: a mixture-of-experts approach: and a larger one-million-token context window: which equates to roughly an hour of silent video: 11 hours of audio: 30:000 lines of code: or 700:000 words. The same month: Google debuted Gemma: a family of free and open-source LLMs that serve as a lightweight version of Gemini. They come in two sizes: with a neural network with two and seven billion parameters: respectively. Multiple publications viewed this as an response to Meta and others open-sourcing their AI models: and a stark reversal from Google\'s longstanding practice of keeping its AI proprietary. == Technical\nspecifications == The first generation of Gemini (""Gemini 1"") has three models: with the same software architecture. They are decoder-only transformers: with modifications to allow efficient training and inference on TPUs. They have a context length of 32:768 tokens: with multi-query attention. Two versions of Gemini Nano: Nano-1 (1.8 billion parameters) and Nano-2 (3.25 billion parameters): are distilled from larger Gemini models: designed for use by edge devices such as smartphones. As Gemini is multimodal: each context window can contain multiple forms of input. The different modes can be interleaved and do not have to be presented in a fixed order: allowing for a multimodal conversation. For example: the user might open the conversation with a mix of text: picture: video: and audio: presented in any order: and Gemini might reply with the same free ordering. Input images may be of different resolutions: while video is inputted as a sequence of images. Audio is sampled at 16 kHz and then converted into a sequence of tokens by the Universal Speech Model. Gemini\'s dataset is multimodal and multilingual: consisting of ""web documents: books: and code: and includ[ing] image: audio: and video data"".Demis Hassabis claims that training Gemini 1 used ""roughly the same amount of compute: maybe slightly more than what was rumored for GPT-4"".The second generation of Gemini (""Gemini 1.5"") has one model published so far: Gemini 1.5 Pro. It is a multimodal sparse mixture-of-experts: with context length of ""multiple millions"". == Reception == Gemini\'s launch was preluded by months of intense speculation and anticipation: which MIT Technology Review described as ""peak AI hype"". In August 2023: Dylan Patel and Daniel Nishball of research firm SemiAnalysis penned a blog post declaring that the release of Gemini would ""eat the world"" and outclass GPT-4: prompting OpenAI CEO Sam Altman to ridicule the duo on X (formerly Twitter). Business magnate Elon Musk: who': 'original_similarity': 0.8496063331152274: 'slide': 308}","{'chunk_text': 'of keeping its AI proprietary. == Technical\nspecifications == The first generation of Gemini (""Gemini 1"") has three models: with the same software architecture. They are decoder-only transformers: with modifications to allow efficient training and inference on TPUs. They have a context length of 32:768 tokens: with multi-query attention. Two versions of Gemini Nano: Nano-1 (1.8 billion parameters) and Nano-2 (3.25 billion parameters): are distilled from larger Gemini models: designed for use by edge devices such as smartphones. As Gemini is multimodal: each context window can contain multiple forms of input. The different modes can be interleaved and do not have to be presented in a fixed order: allowing for a multimodal conversation. For example: the user might open the conversation with a mix of text: picture: video: and audio: presented in any order: and Gemini might reply with the same free ordering. Input images may be of different resolutions: while video is inputted as a sequence of images. Audio is sampled at 16 kHz and then converted into a sequence of tokens by the Universal Speech Model. Gemini\'s dataset is multimodal and multilingual: consisting of ""web documents: books: and code: and includ[ing] image: audio: and video data"".Demis Hassabis claims that training Gemini 1 used ""roughly the same amount of compute: maybe slightly more than what was rumored for GPT-4"".The second generation of Gemini (""Gemini 1.5"") has one model published so far: Gemini 1.5 Pro. It is a multimodal sparse mixture-of-experts: with context length of ""multiple millions"". == Reception == Gemini\'s launch was preluded by months of intense speculation and anticipation: which MIT Technology Review described as ""peak AI hype"". In August 2023: Dylan Patel and Daniel Nishball of research firm SemiAnalysis penned a blog post declaring that the release of Gemini would ""eat the world"" and outclass GPT-4: prompting OpenAI CEO Sam Altman to ridicule the duo on X (formerly Twitter). Business magnate Elon Musk: who co-founded OpenAI: weighed in: asking: ""Are the numbers wrong?"" Hugh Langley of Business Insider remarked that Gemini would be a make-or-break moment for Google: writing: ""If Gemini dazzles: it will help Google change the narrative that it was blindsided by Microsoft and OpenAI. If it disappoints: it will embolden critics who say Google has fallen behind.""Reacting to its unveiling in December 2023: University of Washington professor emeritus Oren Etzioni predicted a ""tit-for-tat arms race"" between Google and OpenAI. Professor Alexei Efros of the University of California: Berkeley praised the potential of Gemini\'s multimodal approach: while scientist Melanie Mitchell of the Santa Fe Institute called Gemini ""very sophisticated"". Professor Chirag Shah of the University of Washington was less impressed: likening Gemini\'s launch to the routineness of Apple\'s annual introduction of a new iPhone. Similarly: Stanford University\'s Percy Liang: the University of Washington\'s Emily Bender: and the University of Galway\'s Michael Madden cautioned that it was difficult to interpret benchmark scores without insight into the training data used. Writing for Fast Company: Mark Sullivan opined that Google had the opportunity to challenge the iPhone\'s dominant market share: believing that Apple was unlikely to have the capacity to develop functionality similar to Gemini with its Siri virtual assistant. Google shares spiked by 5.3 percent the day after Gemini\'s launch.Google faced criticism for a demonstrative video of Gemini: which was not conducted in real time. == See also == Gato: a multimodal neural network developed by DeepMind == References == == Further reading == == External links == Official website Press release via The Keyword Announcement and demo on YouTube': 'original_similarity': 0.8383758170292621: 'slide': -7}","{'chunk_text': 'and Microsoft on their GPT models. GPT-1: report: GitHub release. GPT-2: blog announcement: report on its decision of ""staged release"": GitHub release. GPT-3: report. No GitHub or any other form of code release thenceforth. webGPT: blog announcement: report: InstructGPT: blog announcement: report. ChatGPT: blog announcement (no report). GPT-4: blog announcement: reports: model card. == See also == Cyc Gemini == References ==\nA graph neural network (GNN) belongs to a class of artificial neural networks for processing data': 'original_similarity': 0.7705687868054618: 'slide': 14}"
0.4041237000201363,What were some notable achievements of Gemini Ultra and Gemini Pro?,"Gemini Ultra has outperformed GPT-4, Anthropic's Claude 2, Inflection AI's Inflection-2, Meta's LLaMA 2, and xAI's Grok 1 on a variety of industry benchmarks, and was the first language model to outperform human experts on the 57-subject Massive Multitask Language Understanding (MMLU) test, obtaining a score of 90%. Gemini Pro was also said to have outperformed GPT-3.5.","{'chunk_text': 'with artificial intelligence–powered image generation: allowing it to create contextual images and be adapted for a wider range of use cases. Like Bard: Google co-founder Sergey Brin was summoned out of retirement to assist in the development of Gemini: along with hundreds of other engineers from Google Brain and DeepMind; he was later credited as a ""core contributor"" to Gemini. Because Gemini was being trained on transcripts of YouTube videos: lawyers were brought in to filter out any potentially copyrighted materials.With news of Gemini\'s impending launch: OpenAI hastened its work on integrating GPT-4 with multimodal features similar to those of Gemini. The Information reported in September that several companies had been granted early access to ""an early version"" of the LLM: which Google intended to make available to clients through Google Cloud\'s Vertex AI service. The publication also stated that Google was arming Gemini to compete with both GPT-4 and Microsoft\'s GitHub Copilot. === Launch === On December 6: 2023: Pichai and Hassabis announced ""Gemini 1.0"" at a virtual press conference. It comprised three models: Gemini Ultra: designed for ""highly complex tasks""; Gemini Pro: designed for ""a wide range of tasks""; and Gemini Nano: designed for ""on-device tasks"". At launch: Gemini Pro and Nano were integrated into Bard and the Pixel 8 Pro smartphone: respectively: while Gemini Ultra was set to power ""Bard Advanced"" and become available to software developers in early 2024. Other products that Google intended to incorporate Gemini into included Search: Ads: Chrome: Duet AI on Google Workspace: and AlphaCode 2. It was made available only in English. Touted as Google\'s ""largest and most capable AI model"" and designed to emulate human behavior: the company stated that Gemini would not be made widely available until the following year due to the need for ""extensive safety testing"". Gemini was trained on and powered by Google\'s Tensor Processing Units (TPUs): and the name is in reference to the DeepMind–Google Brain merger as well as NASA\'s Project Gemini.Gemini Ultra was said to have outperformed GPT-4: Anthropic\'s Claude 2: Inflection AI\'s Inflection-2: Meta\'s LLaMA 2: and xAI\'s Grok 1 on a variety of industry benchmarks: while Gemini Pro was said to have outperformed GPT-3.5. Gemini Ultra was also the first language model to outperform human experts on the 57-subject Massive Multitask Language Understanding (MMLU) test: obtaining a score of 90%. Gemini Pro was made available to Google Cloud customers on AI Studio and Vertex AI on December 13: while Gemini Nano will be made available to Android developers as well. Hassabis further revealed that DeepMind was exploring how Gemini could be ""combined with robotics to physically interact with the world"". In accordance with an executive order signed by U.S. President Joe Biden in October: Google stated that it would share testing results of Gemini Ultra with the federal government of the United States. Similarly: the company was engaged in discussions with the government of the United Kingdom to comply with the principles laid out at the AI Safety Summit at Bletchley Park in November. === Updates === Google partnered with Samsung to integrate Gemini Nano and Gemini Pro into its Galaxy S24 smartphone lineup in January 2024. The following month: Bard and Duet AI were unified under the Gemini brand: with ""Gemini Advanced with Ultra 1.0"" debuting via a new ""AI Premium"" tier of the Google One subscription service. Gemini Pro also received a global launch.In February: Google launched ""Gemini 1.5"" in a limited capacity: positioned as a more powerful and capable model than 1.0 Ultra. This ""step change"" was achieved through various technical advancements: including a new architecture: a mixture-of-experts approach: and a larger one-million-token context window: which equates to roughly an hour of silent video: 11 hours of audio: 30:000 lines of code: or 700:000 words. The same month: Google debuted Gemma: a family of free and open-source LLMs that serve as a lightweight version of Gemini. They come in two sizes: with a neural network with two and seven billion parameters: respectively. Multiple publications viewed this as an response to Meta and others open-sourcing their AI models: and a stark reversal from Google\'s longstanding practice of keeping its AI proprietary. == Technical\nspecifications == The first generation of Gemini (""Gemini 1"") has three models: with the same software architecture. They are decoder-only transformers: with modifications to allow efficient training and inference on TPUs. They have a context length of 32:768 tokens: with multi-query attention. Two versions of Gemini Nano: Nano-1 (1.8 billion parameters) and Nano-2 (3.25 billion parameters): are distilled from larger Gemini models: designed for use by edge devices such as smartphones. As Gemini is multimodal: each context window can contain multiple forms of input. The different modes can be interleaved and do not have to be presented in a fixed order: allowing for a multimodal conversation. For example: the user might open the conversation with a mix of text: picture: video: and audio: presented in any order: and Gemini might reply with the same free ordering. Input images may be of different resolutions: while video is inputted as a sequence of images. Audio is sampled at 16 kHz and then converted into a sequence of tokens by the Universal Speech Model. Gemini\'s dataset is multimodal and multilingual: consisting of ""web documents: books: and code: and includ[ing] image: audio: and video data"".Demis Hassabis claims that training Gemini 1 used ""roughly the same amount of compute: maybe slightly more than what was rumored for GPT-4"".The second generation of Gemini (""Gemini 1.5"") has one model published so far: Gemini 1.5 Pro. It is a multimodal sparse mixture-of-experts: with context length of ""multiple millions"". == Reception == Gemini\'s launch was preluded by months of intense speculation and anticipation: which MIT Technology Review described as ""peak AI hype"". In August 2023: Dylan Patel and Daniel Nishball of research firm SemiAnalysis penned a blog post declaring that the release of Gemini would ""eat the world"" and outclass GPT-4: prompting OpenAI CEO Sam Altman to ridicule the duo on X (formerly Twitter). Business magnate': 'original_similarity': 0.8387422308257748: 'slide': 305}","{'chunk_text': 'AI proprietary. == Technical\nspecifications == The first generation of Gemini (""Gemini 1"") has three models: with the same software architecture. They are decoder-only transformers: with modifications to allow efficient training and inference on TPUs. They have a context length of 32:768 tokens: with multi-query attention. Two versions of Gemini Nano: Nano-1 (1.8 billion parameters) and Nano-2 (3.25 billion parameters): are distilled from larger Gemini models: designed for use by edge devices such as smartphones. As Gemini is multimodal: each context window can contain multiple forms of input. The different modes can be interleaved and do not have to be presented in a fixed order: allowing for a multimodal conversation. For example: the user might open the conversation with a mix of text: picture: video: and audio: presented in any order: and Gemini might reply with the same free ordering. Input images may be of different resolutions: while video is inputted as a sequence of images. Audio is sampled at 16 kHz and then converted into a sequence of tokens by the Universal Speech Model. Gemini\'s dataset is multimodal and multilingual: consisting of ""web documents: books: and code: and includ[ing] image: audio: and video data"".Demis Hassabis claims that training Gemini 1 used ""roughly the same amount of compute: maybe slightly more than what was rumored for GPT-4"".The second generation of Gemini (""Gemini 1.5"") has one model published so far: Gemini 1.5 Pro. It is a multimodal sparse mixture-of-experts: with context length of ""multiple millions"". == Reception == Gemini\'s launch was preluded by months of intense speculation and anticipation: which MIT Technology Review described as ""peak AI hype"". In August 2023: Dylan Patel and Daniel Nishball of research firm SemiAnalysis penned a blog post declaring that the release of Gemini would ""eat the world"" and outclass GPT-4: prompting OpenAI CEO Sam Altman to ridicule the duo on X (formerly Twitter). Business magnate Elon Musk: who co-founded OpenAI: weighed in: asking: ""Are the numbers wrong?"" Hugh Langley of Business Insider remarked that Gemini would be a make-or-break moment for Google: writing: ""If Gemini dazzles: it will help Google change the narrative that it was blindsided by Microsoft and OpenAI. If it disappoints: it will embolden critics who say Google has fallen behind.""Reacting to its unveiling in December 2023: University of Washington professor emeritus Oren Etzioni predicted a ""tit-for-tat arms race"" between Google and OpenAI. Professor Alexei Efros of the University of California: Berkeley praised the potential of Gemini\'s multimodal approach: while scientist Melanie Mitchell of the Santa Fe Institute called Gemini ""very sophisticated"". Professor Chirag Shah of the University of Washington was less impressed: likening Gemini\'s launch to the routineness of Apple\'s annual introduction of a new iPhone. Similarly: Stanford University\'s Percy Liang: the University of Washington\'s Emily Bender: and the University of Galway\'s Michael Madden cautioned that it was difficult to interpret benchmark scores without insight into the training data used. Writing for Fast Company: Mark Sullivan opined that Google had the opportunity to challenge the iPhone\'s dominant market share: believing that Apple was unlikely to have the capacity to develop functionality similar to Gemini with its Siri virtual assistant. Google shares spiked by 5.3 percent the day after Gemini\'s launch.Google faced criticism for a demonstrative video of Gemini: which was not conducted in real time. == See also == Gato: a multimodal neural network developed by DeepMind == References == == Further reading == == External links == Official website Press release via The Keyword Announcement and demo on YouTube White paper for': 'original_similarity': 0.8201844985614233: 'slide': -4}","{'chunk_text': 'and Microsoft on their GPT models. GPT-1: report: GitHub release. GPT-2: blog announcement: report on its decision of ""staged release"": GitHub release. GPT-3: report. No GitHub or any other form of code release thenceforth. webGPT: blog announcement: report: InstructGPT: blog announcement: report. ChatGPT: blog announcement (no report). GPT-4: blog announcement: reports: model card. == See also == Cyc Gemini == References ==\nA graph neural network (GNN) belongs to a class of artificial neural networks for processing data': 'original_similarity': 0.7519081757026915: 'slide': 14}"
0.42034169999533333,What are Generative pre-trained transformers (GPT)?,"Generative pre-trained transformers (GPT) are a type of large language model (LLM) and a prominent framework for generative artificial intelligence. They are artificial neural networks used in natural language processing tasks. GPTs are based on the transformer architecture, pre-trained on large data sets of unlabelled text, and capable of generating novel human-like content.","{'chunk_text': 'following—which in turn power the ChatGPT chatbot service.The term ""GPT"" is also used in the names and descriptions of such models developed by others. For example: other GPT foundation models include a series of models created by EleutherAI: and seven models created by Cerebras in 2023. Also: companies in different industries have developed task-specific GPTs in their respective fields: such as Salesforce\'s ""EinsteinGPT"" (for CRM) and Bloomberg\'s ""BloombergGPT"" (for finance). == History == === Initial developments === Generative pretraining (GP) was a long-established concept in machine learning applications. It was originally used as a form of semi-supervised learning: as the model is trained first on an unlabelled dataset (pretraining step) by learning to generate datapoints in the dataset: and then it is trained to classify a labelled dataset.While the unnormalized linear transformer dates back to 1992: the modern transformer architecture was not available until 2017 when it was published by researchers at Google in a paper ""Attention Is All You Need"". That development led to the emergence of large language models such as BERT in 2018 which was a pre-trained transformer (PT) but not designed to be generative (BERT was an ""encoder-only"" model). Also around that time: in 2018: OpenAI published its article entitled ""Improving Language Understanding by Generative Pre-Training:"" in which it introduced the first generative pre-trained transformer (GPT) system (""GPT-1"").Prior to transformer-based architectures: the best-performing neural NLP (natural language processing) models commonly employed supervised learning from large amounts of manually-labeled data. The reliance on supervised learning limited their use on datasets that were not well-annotated: and also made it prohibitively expensive and time-consuming to train extremely large language models.The semi-supervised approach OpenAI employed to make a large-scale generative system—and was first to do with a transformer model—involved two stages: an unsupervised generative ""pretraining"" stage to set initial parameters using a language modeling objective: and a supervised discriminative ""fine-tuning"" stage to adapt these parameters to a target task. === Later developments === Regarding more recent GPT foundation models: OpenAI published its first versions of GPT-3 in July 2020. There were three models: with 1B: 6.7B: 175B parameters: respectively named babbage: curie: and davinci (giving initials B: C: and D).In July 2021: OpenAI published Codex: a task-specific GPT model targeted for programming applications. This was developed by fine-tuning a 12B parameter version of GPT-3 (different from previous GPT-3 models) using code from GitHub.In March 2022: OpenAI published two versions of GPT-3 that were fine-tuned for instruction-following (instruction-tuned): named davinci-instruct-beta (175B) and text-davinci-001: and then started beta testing code-davinci-002. text-davinci-002 was instruction-tuned from code-davinci-002. Both text-davinci-003 and ChatGPT were released in November 2022: with both building upon text-davinci-002 via reinforcement learning from human feedback (RLHF). text-davinci-003 is trained for following instructions (like its predecessors): whereas ChatGPT is further trained for conversational interaction with a human user.OpenAI\'s most recent GPT foundation model: GPT-4: was released on March 14: 2023. It can be accessed directly by users via a premium version of ChatGPT: and is available to developers for incorporation into other products and services via OpenAI\'s API. Other producers of GPT foundation models include EleutherAI (with a series of models starting in March 2021) and Cerebras (with seven models released in March 2023). == Foundational models == A foundational model is an AI model trained on broad data at scale such that it can be adapted to a wide range of downstream tasks.Thus far: the most notable GPT foundation models have been from OpenAI\'s GPT-n series. The most recent from that is GPT-4: for which OpenAI declined to publish the size or training details (citing ""the competitive landscape and the safety implications of large-scale models""). Other such models include Google\'s PaLM: a broad foundation model that has been compared to GPT-3 and has recently been made available to developers via an API: and Together\'s GPT-JT: which has been reported as the closest-performing open-source alternative to GPT-3 (and is derived from earlier open-source GPTs). Meta AI (formerly Facebook) also has a generative transformer-based foundational large language model: known as LLaMA.Foundational GPTs can also employ modalities other than text: for input and/or output. GPT-4 is a multi-modal LLM that is capable of processing text and image input (though its output is limited to text). Regarding multimodal output: some generative transformer-based models are used for text-to-image technologies such as diffusion and parallel decoding. Such kinds of models can serve as visual foundation models (VFMs) for developing downstream systems that can work with images. == Task-specific models == A foundational GPT model can be further adapted to produce more targeted systems directed to specific tasks and/or subject-matter domains. Methods for such adaptation can include additional fine-tuning (beyond that done for the foundation model) as well as certain forms of prompt engineering.An important example of this is fine-tuning models to follow instructions: which is of course a fairly broad task but more targeted than a foundation model. In January 2022: OpenAI introduced ""InstructGPT""—a series of models which were fine-tuned to follow instructions using a combination of supervised training and reinforcement learning from human feedback (RLHF) on base GPT-3 language models. Advantages this had over the bare foundational models included higher accuracy: less\nnegative/toxic sentiment: and generally better alignment with user needs. Hence: OpenAI began using this as the basis for its API service offerings. Other instruction-tuned models have been released by others: including a fully open version.Another (related) kind of task-specific models are chatbots: which engage in human-like conversation. In November 2022: OpenAI launched ChatGPT—an online chat interface powered by an instruction-tuned language model trained in a similar fashion to InstructGPT. They trained this model using RLHF: with human AI trainers providing conversations in which they played both the user and the AI: and mixed this new dialogue dataset with the InstructGPT dataset for a conversational format suitable for a chatbot. Other major chatbots currently include Microsoft\'s Bing Chat: which uses OpenAI\'s GPT-4 (as part of a broader close collaboration between OpenAI and Microsoft): and Google\'s competing chatbot Bard (initially based on their LaMDA family of conversation-trained language': 'original_similarity': 0.9227702032078476: 'slide': 144}","{'chunk_text': 'human-like conversation. In November 2022: OpenAI launched ChatGPT—an online chat interface powered by an instruction-tuned language model trained in a similar fashion to InstructGPT. They trained this model using RLHF: with human AI trainers providing conversations in which they played both the user and the AI: and mixed this new dialogue dataset with the InstructGPT dataset for a conversational format suitable for a chatbot. Other major chatbots currently include Microsoft\'s Bing Chat: which uses OpenAI\'s GPT-4 (as part of a broader close collaboration between OpenAI and Microsoft): and Google\'s competing chatbot Bard (initially based on their LaMDA family of conversation-trained language models: with plans to switch to PaLM).Yet another kind of task that a GPT can be used for is the meta-task of generating its own instructions: like developing a series of prompts for \'itself\' to be able to effectuate a more general goal given by a human user. This is known as an AI agent: and more specifically a recursive one because it uses results from its previous self-instructions to help it form its subsequent prompts; the first major example of this was Auto-GPT (which uses OpenAI\'s GPT models): and others have since been developed as well. === Multimodality === Generative transformer-based systems can also be targeted to tasks involving modalities beyond text. For example: Microsoft’s “Visual ChatGPT” combines ChatGPT with visual foundation models (VFMs) to enable input or output comprising images as well as text. Also: advances in text-to-speech technology offer powerful tools for audio content creation when used in conjunction with foundational GPT language models. === Domain-specificity === GPT systems can be directed toward particular fields or domains. Some reported examples of such models and apps are as follows: EinsteinGPT – for sales and marketing domains: to aid with customer relationship management (uses GPT-3.5) BloombergGPT – for the financial domain: to aid with financial news and information (uses ""freely available"" AI methods: combined with their proprietary data) Khanmigo – described as a GPT version for tutoring: in the education domain: it aids students using Khan Academy by guiding them through their studies without directly providing answers (powered by GPT-4) SlackGPT – for the Slack instant-messaging service: to aid with navigating and summarizing discussions on it (uses OpenAI\'s API) BioGPT – for the biomedical domain: to aid with biomedical literature text generation and mining (uses GPT-2)Sometimes domain-specificity is accomplished via software plug-ins or add-ons. For example: several different companies have developed particular plugins that interact directly with OpenAI\'s ChatGPT interface: and Google Workspace has available add-ons such as “GPT for Sheets and Docs”—which is reported to aid use of spreadsheet functionality in Google Sheets.In November 2023: OpenAI announced that it\'s enabling ChatGPT Plus subscribers to create custom versions of ChatGPT (being called GPTs). These can be tailored for specific domains via prompt engineering: curated datasets: and/or targeted interaction with external tools. Users who register as verified builders are able to publish their custom GPTs for other users: with monetization potential. (This is notably distinct from OpenAI\'s API service: as this is based internally within OpenAI\'s platform.) == Brand issues == OpenAI: which created the first generative pre-trained transformer (GPT) in 2018: has recently asserted that “GPT” should be regarded as a brand of OpenAI. In April 2023: OpenAI revised the brand guidelines in its terms of service to indicate that other businesses using its API to run their artificial intelligence (AI) services would no longer be able to include “GPT” in such names or branding. In May 2023: OpenAI engaged a brand management service to notify its API customers of this policy: although these notifications stopped short of making overt legal claims (such as allegations of trademark infringement or demands to cease and desist). As of November 2023: OpenAI still prohibits its API licensees from naming their own products with ""GPT:"" but it has begun enabling its ChatGPT Plus subscribers to make ""custom versions of ChatGPT"" that are being called GPTs on the OpenAI site. OpenAI\'s terms of service says that its subscribers may use ""GPT"" in the names of these: although it\'s ""discouraged.""Relatedly: OpenAI has applied to the United States Patent and Trademark Office (USPTO) to seek domestic trademark registration for the term “GPT” in the field of AI. OpenAI sought to expedite handling of its application: but the USPTO declined that request in April 2023. In May 2023: the USPTO responded to the application with a determination that ""GPT"" was both descriptive and generic. As of November 2023: OpenAI continues to pursue its argument through the available processes. Regardless: failure to obtain a registered U.S. trademark does not preclude some level of common-law trademark rights in the U.S.: and/or trademark rights in other countries.For any given type or scope of trademark protection in the U.S.: OpenAI would need to establish that the term is actually “distinctive” to their specific offerings in addition to being a broader technical term for the kind of technology. Some media reports suggested that OpenAI may be able to obtain trademark registration based indirectly on the fame of its GPT-based chatbot product: ChatGPT: for which OpenAI has separately sought protection (and which it has sought to enforce more strongly). Other reports have indicated that registration for the bare term “GPT” seems unlikely to be granted: as it is used frequently as a common term to refer simply to AI systems that involve generative pre-trained transformers. In any event: to whatever extent exclusive rights in the term may occur the U.S.: others would need to avoid using it for similar products or services in ways likely to cause confusion. If such rights ever became broad enough to implicate other well-established uses in the field: the trademark doctrine of descriptive fair use could still preserve some room to continue\nnon-brand-related usage. == Selected bibliography == This section lists the main official publications from OpenAI and Microsoft on their GPT models. GPT-1: report: GitHub release. GPT-2: blog announcement: report on its decision of ""staged release"": GitHub release. GPT-3: report. No GitHub or any other form of': 'original_similarity': 0.8620446085105415: 'slide': 44}","{'chunk_text': 'This section lists the main official publications from OpenAI and Microsoft on their GPT models. GPT-1: report: GitHub release. GPT-2: blog announcement: report on its decision of ""staged release"": GitHub release. GPT-3: report. No GitHub or any other form of code release thenceforth. webGPT: blog announcement: report: InstructGPT: blog announcement: report. ChatGPT: blog announcement (no report). GPT-4: blog announcement: reports: model card. == See also == Cyc Gemini == References ==\nA graph neural network (GNN) belongs to': 'original_similarity': 0.8341880181734952: 'slide': 5}"
0.4304290000000037,Who introduced the first GPT and when?,The first GPT was introduced by OpenAI in 2018.,"{'chunk_text': 'main official publications from OpenAI and Microsoft on their GPT models. GPT-1: report: GitHub release. GPT-2: blog announcement: report on its decision of ""staged release"": GitHub release. GPT-3: report. No GitHub or any other form of code release thenceforth. webGPT: blog announcement: report: InstructGPT: blog announcement: report. ChatGPT: blog announcement (no report). GPT-4: blog announcement: reports: model card. == See also == Cyc Gemini == References ==\nA graph neural network (GNN) belongs to a class of artificial': 'original_similarity': 0.7919308570462112: 'slide': 9}","{'chunk_text': 'architecture was not available until 2017 when it was published by researchers at Google in a paper ""Attention Is All You Need"". That development led to the emergence of large language models such as BERT in 2018 which was a pre-trained transformer (PT) but not designed to be generative (BERT was an ""encoder-only"" model). Also around that time: in 2018: OpenAI published its article entitled ""Improving Language Understanding by Generative Pre-Training:"" in which it introduced the first generative pre-trained transformer (GPT) system (""GPT-1"").Prior to transformer-based architectures: the best-performing neural NLP (natural language processing) models commonly employed supervised learning from large amounts of manually-labeled data. The reliance on supervised learning limited their use on datasets that were not well-annotated: and also made it prohibitively expensive and time-consuming to train extremely large language models.The semi-supervised approach OpenAI employed to make a large-scale generative system—and was first to do with a transformer model—involved two stages: an unsupervised generative ""pretraining"" stage to set initial parameters using a language modeling objective: and a supervised discriminative ""fine-tuning"" stage to adapt these parameters to a target task. === Later developments === Regarding more recent GPT foundation models: OpenAI published its first versions of GPT-3 in July 2020. There were three models: with 1B: 6.7B: 175B parameters: respectively named babbage: curie: and davinci (giving initials B: C: and D).In July 2021: OpenAI published Codex: a task-specific GPT model targeted for programming applications. This was developed by fine-tuning a 12B parameter version of GPT-3 (different from previous GPT-3 models) using code from GitHub.In March 2022: OpenAI published two versions of GPT-3 that were fine-tuned for instruction-following (instruction-tuned): named davinci-instruct-beta (175B) and text-davinci-001: and then started beta testing code-davinci-002. text-davinci-002 was instruction-tuned from code-davinci-002. Both text-davinci-003 and ChatGPT were released in November 2022: with both building upon text-davinci-002 via reinforcement learning from human feedback (RLHF). text-davinci-003 is trained for following instructions (like its predecessors): whereas ChatGPT is further trained for conversational interaction with a human user.OpenAI\'s most recent GPT foundation model: GPT-4: was released on March 14: 2023. It can be accessed directly by users via a premium version of ChatGPT: and is available to developers for incorporation into other products and services via OpenAI\'s API. Other producers of GPT foundation models include EleutherAI (with a series of models starting in March 2021) and Cerebras (with seven models released in March 2023). == Foundational models == A foundational model is an AI model trained on broad data at scale such that it can be adapted to a wide range of downstream tasks.Thus far: the most notable GPT foundation models have been from OpenAI\'s GPT-n series. The most recent from that is GPT-4: for which OpenAI declined to publish the size or training details (citing ""the competitive landscape and the safety implications of large-scale models""). Other such models include Google\'s PaLM: a broad foundation model that has been compared to GPT-3 and has recently been made available to developers via an API: and Together\'s GPT-JT: which has been reported as the closest-performing open-source alternative to GPT-3 (and is derived from earlier open-source GPTs). Meta AI (formerly Facebook) also has a generative transformer-based foundational large language model: known as LLaMA.Foundational GPTs can also employ modalities other than text: for input and/or output. GPT-4 is a multi-modal LLM that is capable of processing text and image input (though its output is limited to text). Regarding multimodal output: some generative transformer-based models are used for text-to-image technologies such as diffusion and parallel decoding. Such kinds of models can serve as visual foundation models (VFMs) for developing downstream systems that can work with images. == Task-specific models == A foundational GPT model can be further adapted to produce more targeted systems directed to specific tasks and/or subject-matter domains. Methods for such adaptation can include additional fine-tuning (beyond that done for the foundation model) as well as certain forms of prompt engineering.An important example of this is fine-tuning models to follow instructions: which is of course a fairly broad task but more targeted than a foundation model. In January 2022: OpenAI introduced ""InstructGPT""—a series of models which were fine-tuned to follow instructions using a combination of supervised training and reinforcement learning from human feedback (RLHF) on base GPT-3 language models. Advantages this had over the bare foundational models included higher accuracy: less\nnegative/toxic sentiment: and generally better alignment with user needs. Hence: OpenAI began using this as the basis for its API service offerings. Other instruction-tuned models have been released by others: including a fully open version.Another (related) kind of task-specific models are chatbots: which engage in human-like conversation. In November 2022: OpenAI launched ChatGPT—an online chat interface powered by an instruction-tuned language model trained in a similar fashion to InstructGPT. They trained this model using RLHF: with human AI trainers providing conversations in which they played both the user and the AI: and mixed this new dialogue dataset with the InstructGPT dataset for a conversational format suitable for a chatbot. Other major chatbots currently include Microsoft\'s Bing Chat: which uses OpenAI\'s GPT-4 (as part of a broader close collaboration between OpenAI and Microsoft): and Google\'s competing chatbot Bard (initially based on their LaMDA family of conversation-trained language models: with plans to switch to PaLM).Yet another kind of task that a GPT can be used for is the meta-task of generating its own instructions: like developing a series of prompts for \'itself\' to be able to effectuate a more general goal given by a human user. This is known as an AI agent: and more specifically a recursive one because it uses results from its previous self-instructions to help it form its subsequent prompts; the first major example of this was Auto-GPT (which uses OpenAI\'s GPT models): and others have since been developed as well. === Multimodality === Generative transformer-based systems can also be targeted to tasks involving modalities beyond text. For example: Microsoft’s “Visual ChatGPT” combines ChatGPT with visual foundation models (VFMs) to enable input or output comprising images as well as text. Also: advances in': 'original_similarity': 0.7809535902914128: 'slide': 283}","{'chunk_text': 'and others have since been developed as well. === Multimodality === Generative transformer-based systems can also be targeted to tasks involving modalities beyond text. For example: Microsoft’s “Visual ChatGPT” combines ChatGPT with visual foundation models (VFMs) to enable input or output comprising images as well as text. Also: advances in text-to-speech technology offer powerful tools for audio content creation when used in conjunction with foundational GPT language models. === Domain-specificity === GPT systems can be directed toward particular fields or domains. Some reported examples of such models and apps are as follows: EinsteinGPT – for sales and marketing domains: to aid with customer relationship management (uses GPT-3.5) BloombergGPT – for the financial domain: to aid with financial news and information (uses ""freely available"" AI methods: combined with their proprietary data) Khanmigo – described as a GPT version for tutoring: in the education domain: it aids students using Khan Academy by guiding them through their studies without directly providing answers (powered by GPT-4) SlackGPT – for the Slack instant-messaging service: to aid with navigating and summarizing discussions on it (uses OpenAI\'s API) BioGPT – for the biomedical domain: to aid with biomedical literature text generation and mining (uses GPT-2)Sometimes domain-specificity is accomplished via software plug-ins or add-ons. For example: several different companies have developed particular plugins that interact directly with OpenAI\'s ChatGPT interface: and Google Workspace has available add-ons such as “GPT for Sheets and Docs”—which is reported to aid use of spreadsheet functionality in Google Sheets.In November 2023: OpenAI announced that it\'s enabling ChatGPT Plus subscribers to create custom versions of ChatGPT (being called GPTs). These can be tailored for specific domains via prompt engineering: curated datasets: and/or targeted interaction with external tools. Users who register as verified builders are able to publish their custom GPTs for other users: with monetization potential. (This is notably distinct from OpenAI\'s API service: as this is based internally within OpenAI\'s platform.) == Brand issues == OpenAI: which created the first generative pre-trained transformer (GPT) in 2018: has recently asserted that “GPT” should be regarded as a brand of OpenAI. In April 2023: OpenAI revised the brand guidelines in its terms of service to indicate that other businesses using its API to run their artificial intelligence (AI) services would no longer be able to include “GPT” in such names or branding. In May 2023: OpenAI engaged a brand management service to notify its API customers of this policy: although these notifications stopped short of making overt legal claims (such as allegations of trademark infringement or demands to cease and desist). As of November 2023: OpenAI still prohibits its API licensees from naming their own products with ""GPT:"" but it has begun enabling its ChatGPT Plus subscribers to make ""custom versions of ChatGPT"" that are being called GPTs on the OpenAI site. OpenAI\'s terms of service says that its subscribers may use ""GPT"" in the names of these: although it\'s ""discouraged.""Relatedly: OpenAI has applied to the United States Patent and Trademark Office (USPTO) to seek domestic trademark registration for the term “GPT” in the field of AI. OpenAI sought to expedite handling of its application: but the USPTO declined that request in April 2023. In May 2023: the USPTO responded to the application with a determination that ""GPT"" was both descriptive and generic. As of November 2023: OpenAI continues to pursue its argument through the available processes. Regardless: failure to obtain a registered U.S. trademark does not preclude some level of common-law trademark rights in the U.S.: and/or trademark rights in other countries.For any given type or scope of trademark protection in the U.S.: OpenAI would need to establish that the term is actually “distinctive” to their specific offerings in addition to being a broader technical term for the kind of technology. Some media reports suggested that OpenAI may be able to obtain trademark registration based indirectly on the fame of its GPT-based chatbot product: ChatGPT: for which OpenAI has separately sought protection (and which it has sought to enforce more strongly). Other reports have indicated that registration for the bare term “GPT” seems unlikely to be granted: as it is used frequently as a common term to refer simply to AI systems that involve generative pre-trained transformers. In any event: to whatever extent exclusive rights in the term may occur the U.S.: others would need to avoid using it for similar products or services in ways likely to cause confusion. If such rights ever became broad enough to implicate other well-established uses in the field: the trademark doctrine of descriptive fair use could still preserve some room to continue\nnon-brand-related usage. == Selected bibliography == This section lists the main official publications from OpenAI and Microsoft on their GPT models. GPT-1: report: GitHub release. GPT-2: blog announcement: report on its decision of ""staged release"": GitHub release. GPT-3: report. No GitHub or any other form of code release thenceforth. webGPT: blog announcement: report: InstructGPT: blog announcement: report. ChatGPT: blog announcement (no report). GPT-4: blog announcement: reports: model card. == See also == Cyc Gemini == References ==': 'original_similarity': 0.7799614924171852: 'slide': 234}"
0.48204609999083914,What are some task-specific GPT systems developed by OpenAI? ,"OpenAI has developed task-specific GPT systems including models fine-tuned for instruction following, which power the ChatGPT chatbot service.","{'chunk_text': 'Google\'s competing chatbot Bard (initially based on their LaMDA family of conversation-trained language models: with plans to switch to PaLM).Yet another kind of task that a GPT can be used for is the meta-task of generating its own instructions: like developing a series of prompts for \'itself\' to be able to effectuate a more general goal given by a human user. This is known as an AI agent: and more specifically a recursive one because it uses results from its previous self-instructions to help it form its subsequent prompts; the first major example of this was Auto-GPT (which uses OpenAI\'s GPT models): and others have since been developed as well. === Multimodality === Generative transformer-based systems can also be targeted to tasks involving modalities beyond text. For example: Microsoft’s “Visual ChatGPT” combines ChatGPT with visual foundation models (VFMs) to enable input or output comprising images as well as text. Also: advances in text-to-speech technology offer powerful tools for audio content creation when used in conjunction with foundational GPT language models. === Domain-specificity === GPT systems can be directed toward particular fields or domains. Some reported examples of such models and apps are as follows: EinsteinGPT – for sales and marketing domains: to aid with customer relationship management (uses GPT-3.5) BloombergGPT – for the financial domain: to aid with financial news and information (uses ""freely available"" AI methods: combined with their proprietary data) Khanmigo – described as a GPT version for tutoring: in the education domain: it aids students using Khan Academy by guiding them through their studies without directly providing answers (powered by GPT-4) SlackGPT – for the Slack instant-messaging service: to aid with navigating and summarizing discussions on it (uses OpenAI\'s API) BioGPT – for the biomedical domain: to aid with biomedical literature text generation and mining (uses GPT-2)Sometimes domain-specificity is accomplished via software plug-ins or add-ons. For example: several different companies have developed particular plugins that interact directly with OpenAI\'s ChatGPT interface: and Google Workspace has available add-ons such as “GPT for Sheets and Docs”—which is reported to aid use of spreadsheet functionality in Google Sheets.In November 2023: OpenAI announced that it\'s enabling ChatGPT Plus subscribers to create custom versions of ChatGPT (being called GPTs). These can be tailored for specific domains via prompt engineering: curated datasets: and/or targeted interaction with external tools. Users who register as verified builders are able to publish their custom GPTs for other users: with monetization potential. (This is notably distinct from OpenAI\'s API service: as this is based internally within OpenAI\'s platform.) == Brand issues == OpenAI: which created the first generative pre-trained transformer (GPT) in 2018: has recently asserted that “GPT” should be regarded as a brand of OpenAI. In April 2023: OpenAI revised the brand guidelines in its terms of service to indicate that other businesses using its API to run their artificial intelligence (AI) services would no longer be able to include “GPT” in such names or branding. In May 2023: OpenAI engaged a brand management service to notify its API customers of this policy: although these notifications stopped short of making overt legal claims (such as allegations of trademark infringement or demands to cease and desist). As of November 2023: OpenAI still prohibits its API licensees from naming their own products with ""GPT:"" but it has begun enabling its ChatGPT Plus subscribers to make ""custom versions of ChatGPT"" that are being called GPTs on the OpenAI site. OpenAI\'s terms of service says that its subscribers may use ""GPT"" in the names of these: although it\'s ""discouraged.""Relatedly: OpenAI has applied to the United States Patent and Trademark Office (USPTO) to seek domestic trademark registration for the term “GPT” in the field of AI. OpenAI sought to expedite handling of its application: but the USPTO declined that request in April 2023. In May 2023: the USPTO responded to the application with a determination that ""GPT"" was both descriptive and generic. As of November 2023: OpenAI continues to pursue its argument through the available processes. Regardless: failure to obtain a registered U.S. trademark does not preclude some level of common-law trademark rights in the U.S.: and/or trademark rights in other countries.For any given type or scope of trademark protection in the U.S.: OpenAI would need to establish that the term is actually “distinctive” to their specific offerings in addition to being a broader technical term for the kind of technology. Some media reports suggested that OpenAI may be able to obtain trademark registration based indirectly on the fame of its GPT-based chatbot product: ChatGPT: for which OpenAI has separately sought protection (and which it has sought to enforce more strongly). Other reports have indicated that registration for the bare term “GPT” seems unlikely to be granted: as it is used frequently as a common term to refer simply to AI systems that involve generative pre-trained transformers. In any event: to whatever extent exclusive rights in the term may occur the U.S.: others would need to avoid using it for similar products or services in ways likely to cause confusion. If such rights ever became broad enough to implicate other well-established uses in the field: the trademark doctrine of descriptive fair use could still preserve some room to continue\nnon-brand-related usage. == Selected bibliography == This section lists the main official publications from OpenAI and Microsoft on their GPT models. GPT-1: report: GitHub release. GPT-2: blog announcement: report on its decision of ""staged release"": GitHub release. GPT-3: report. No GitHub or any other form of code release thenceforth. webGPT: blog announcement: report: InstructGPT: blog announcement: report. ChatGPT: blog announcement (no report). GPT-4: blog announcement: reports: model card. == See also == Cyc Gemini == References ==': 'original_similarity': 0.8739241242920104: 'slide': 132}","{'chunk_text': '== Selected bibliography == This section lists the main official publications from OpenAI and Microsoft on their GPT models. GPT-1: report: GitHub release. GPT-2: blog announcement: report on its decision of ""staged release"": GitHub release. GPT-3: report. No GitHub or any other form of code release thenceforth. webGPT: blog announcement: report: InstructGPT: blog announcement: report. ChatGPT: blog announcement (no report). GPT-4: blog announcement: reports: model card. == See also == Cyc Gemini == References ==\nA graph neural': 'original_similarity': 0.859070780855667: 'slide': 1}","{'chunk_text': 'form of semi-supervised learning: as the model is trained first on an unlabelled dataset (pretraining step) by learning to generate datapoints in the dataset: and then it is trained to classify a labelled dataset.While the unnormalized linear transformer dates back to 1992: the modern transformer architecture was not available until 2017 when it was published by researchers at Google in a paper ""Attention Is All You Need"". That development led to the emergence of large language models such as BERT in 2018 which was a pre-trained transformer (PT) but not designed to be generative (BERT was an ""encoder-only"" model). Also around that time: in 2018: OpenAI published its article entitled ""Improving Language Understanding by Generative Pre-Training:"" in which it introduced the first generative pre-trained transformer (GPT) system (""GPT-1"").Prior to transformer-based architectures: the best-performing neural NLP (natural language processing) models commonly employed supervised learning from large amounts of manually-labeled data. The reliance on supervised learning limited their use on datasets that were not well-annotated: and also made it prohibitively expensive and time-consuming to train extremely large language models.The semi-supervised approach OpenAI employed to make a large-scale generative system—and was first to do with a transformer model—involved two stages: an unsupervised generative ""pretraining"" stage to set initial parameters using a language modeling objective: and a supervised discriminative ""fine-tuning"" stage to adapt these parameters to a target task. === Later developments === Regarding more recent GPT foundation models: OpenAI published its first versions of GPT-3 in July 2020. There were three models: with 1B: 6.7B: 175B parameters: respectively named babbage: curie: and davinci (giving initials B: C: and D).In July 2021: OpenAI published Codex: a task-specific GPT model targeted for programming applications. This was developed by fine-tuning a 12B parameter version of GPT-3 (different from previous GPT-3 models) using code from GitHub.In March 2022: OpenAI published two versions of GPT-3 that were fine-tuned for instruction-following (instruction-tuned): named davinci-instruct-beta (175B) and text-davinci-001: and then started beta testing code-davinci-002. text-davinci-002 was instruction-tuned from code-davinci-002. Both text-davinci-003 and ChatGPT were released in November 2022: with both building upon text-davinci-002 via reinforcement learning from human feedback (RLHF). text-davinci-003 is trained for following instructions (like its predecessors): whereas ChatGPT is further trained for conversational interaction with a human user.OpenAI\'s most recent GPT foundation model: GPT-4: was released on March 14: 2023. It can be accessed directly by users via a premium version of ChatGPT: and is available to developers for incorporation into other products and services via OpenAI\'s API. Other producers of GPT foundation models include EleutherAI (with a series of models starting in March 2021) and Cerebras (with seven models released in March 2023). == Foundational models == A foundational model is an AI model trained on broad data at scale such that it can be adapted to a wide range of downstream tasks.Thus far: the most notable GPT foundation models have been from OpenAI\'s GPT-n series. The most recent from that is GPT-4: for which OpenAI declined to publish the size or training details (citing ""the competitive landscape and the safety implications of large-scale models""). Other such models include Google\'s PaLM: a broad foundation model that has been compared to GPT-3 and has recently been made available to developers via an API: and Together\'s GPT-JT: which has been reported as the closest-performing open-source alternative to GPT-3 (and is derived from earlier open-source GPTs). Meta AI (formerly Facebook) also has a generative transformer-based foundational large language model: known as LLaMA.Foundational GPTs can also employ modalities other than text: for input and/or output. GPT-4 is a multi-modal LLM that is capable of processing text and image input (though its output is limited to text). Regarding multimodal output: some generative transformer-based models are used for text-to-image technologies such as diffusion and parallel decoding. Such kinds of models can serve as visual foundation models (VFMs) for developing downstream systems that can work with images. == Task-specific models == A foundational GPT model can be further adapted to produce more targeted systems directed to specific tasks and/or subject-matter domains. Methods for such adaptation can include additional fine-tuning (beyond that done for the foundation model) as well as certain forms of prompt engineering.An important example of this is fine-tuning models to follow instructions: which is of course a fairly broad task but more targeted than a foundation model. In January 2022: OpenAI introduced ""InstructGPT""—a series of models which were fine-tuned to follow instructions using a combination of supervised training and reinforcement learning from human feedback (RLHF) on base GPT-3 language models. Advantages this had over the bare foundational models included higher accuracy: less\nnegative/toxic sentiment: and generally better alignment with user needs. Hence: OpenAI began using this as the basis for its API service offerings. Other instruction-tuned models have been released by others: including a fully open version.Another (related) kind of task-specific models are chatbots: which engage in human-like conversation. In November 2022: OpenAI launched ChatGPT—an online chat interface powered by an instruction-tuned language model trained in a similar fashion to InstructGPT. They trained this model using RLHF: with human AI trainers providing conversations in which they played both the user and the AI: and mixed this new dialogue dataset with the InstructGPT dataset for a conversational format suitable for a chatbot. Other major chatbots currently include Microsoft\'s Bing Chat: which uses OpenAI\'s GPT-4 (as part of a broader close collaboration between OpenAI and Microsoft): and Google\'s competing chatbot Bard (initially based on their LaMDA family of conversation-trained language models: with plans to switch to PaLM).Yet another kind of task that a GPT can be used for is the meta-task of generating its own instructions: like developing a series of prompts for \'itself\' to be able to effectuate a more general goal given by a human user. This is known as an AI agent: and more specifically a recursive one because it uses results from its previous self-instructions to help it form its subsequent prompts; the first major example of this was Auto-GPT (which uses OpenAI\'s GPT models): and others have since been': 'original_similarity': 0.8504787972804058: 'slide': 238}"
0.4223925000114832,What companies have developed GPT foundation models other than OpenAI?,EleutherAI and Cerebras have developed GPT foundation models aside from OpenAI.,"{'chunk_text': 'Selected bibliography == This section lists the main official publications from OpenAI and Microsoft on their GPT models. GPT-1: report: GitHub release. GPT-2: blog announcement: report on its decision of ""staged release"": GitHub release. GPT-3: report. No GitHub or any other form of code release thenceforth. webGPT: blog announcement: report: InstructGPT: blog announcement: report. ChatGPT: blog announcement (no report). GPT-4: blog announcement: reports: model card. == See also == Cyc Gemini == References ==\nA graph neural network': 'original_similarity': 0.8430173213802189: 'slide': 2}","{'chunk_text': 'is known as an AI agent: and more specifically a recursive one because it uses results from its previous self-instructions to help it form its subsequent prompts; the first major example of this was Auto-GPT (which uses OpenAI\'s GPT models): and others have since been developed as well. === Multimodality === Generative transformer-based systems can also be targeted to tasks involving modalities beyond text. For example: Microsoft’s “Visual ChatGPT” combines ChatGPT with visual foundation models (VFMs) to enable input or output comprising images as well as text. Also: advances in text-to-speech technology offer powerful tools for audio content creation when used in conjunction with foundational GPT language models. === Domain-specificity === GPT systems can be directed toward particular fields or domains. Some reported examples of such models and apps are as follows: EinsteinGPT – for sales and marketing domains: to aid with customer relationship management (uses GPT-3.5) BloombergGPT – for the financial domain: to aid with financial news and information (uses ""freely available"" AI methods: combined with their proprietary data) Khanmigo – described as a GPT version for tutoring: in the education domain: it aids students using Khan Academy by guiding them through their studies without directly providing answers (powered by GPT-4) SlackGPT – for the Slack instant-messaging service: to aid with navigating and summarizing discussions on it (uses OpenAI\'s API) BioGPT – for the biomedical domain: to aid with biomedical literature text generation and mining (uses GPT-2)Sometimes domain-specificity is accomplished via software plug-ins or add-ons. For example: several different companies have developed particular plugins that interact directly with OpenAI\'s ChatGPT interface: and Google Workspace has available add-ons such as “GPT for Sheets and Docs”—which is reported to aid use of spreadsheet functionality in Google Sheets.In November 2023: OpenAI announced that it\'s enabling ChatGPT Plus subscribers to create custom versions of ChatGPT (being called GPTs). These can be tailored for specific domains via prompt engineering: curated datasets: and/or targeted interaction with external tools. Users who register as verified builders are able to publish their custom GPTs for other users: with monetization potential. (This is notably distinct from OpenAI\'s API service: as this is based internally within OpenAI\'s platform.) == Brand issues == OpenAI: which created the first generative pre-trained transformer (GPT) in 2018: has recently asserted that “GPT” should be regarded as a brand of OpenAI. In April 2023: OpenAI revised the brand guidelines in its terms of service to indicate that other businesses using its API to run their artificial intelligence (AI) services would no longer be able to include “GPT” in such names or branding. In May 2023: OpenAI engaged a brand management service to notify its API customers of this policy: although these notifications stopped short of making overt legal claims (such as allegations of trademark infringement or demands to cease and desist). As of November 2023: OpenAI still prohibits its API licensees from naming their own products with ""GPT:"" but it has begun enabling its ChatGPT Plus subscribers to make ""custom versions of ChatGPT"" that are being called GPTs on the OpenAI site. OpenAI\'s terms of service says that its subscribers may use ""GPT"" in the names of these: although it\'s ""discouraged.""Relatedly: OpenAI has applied to the United States Patent and Trademark Office (USPTO) to seek domestic trademark registration for the term “GPT” in the field of AI. OpenAI sought to expedite handling of its application: but the USPTO declined that request in April 2023. In May 2023: the USPTO responded to the application with a determination that ""GPT"" was both descriptive and generic. As of November 2023: OpenAI continues to pursue its argument through the available processes. Regardless: failure to obtain a registered U.S. trademark does not preclude some level of common-law trademark rights in the U.S.: and/or trademark rights in other countries.For any given type or scope of trademark protection in the U.S.: OpenAI would need to establish that the term is actually “distinctive” to their specific offerings in addition to being a broader technical term for the kind of technology. Some media reports suggested that OpenAI may be able to obtain trademark registration based indirectly on the fame of its GPT-based chatbot product: ChatGPT: for which OpenAI has separately sought protection (and which it has sought to enforce more strongly). Other reports have indicated that registration for the bare term “GPT” seems unlikely to be granted: as it is used frequently as a common term to refer simply to AI systems that involve generative pre-trained transformers. In any event: to whatever extent exclusive rights in the term may occur the U.S.: others would need to avoid using it for similar products or services in ways likely to cause confusion. If such rights ever became broad enough to implicate other well-established uses in the field: the trademark doctrine of descriptive fair use could still preserve some room to continue\nnon-brand-related usage. == Selected bibliography == This section lists the main official publications from OpenAI and Microsoft on their GPT models. GPT-1: report: GitHub release. GPT-2: blog announcement: report on its decision of ""staged release"": GitHub release. GPT-3: report. No GitHub or any other form of code release thenceforth. webGPT: blog announcement: report: InstructGPT: blog announcement: report. ChatGPT: blog announcement (no report). GPT-4: blog announcement: reports: model card. == See also == Cyc Gemini == References ==': 'original_similarity': 0.8303797173753291: 'slide': 194}","{'chunk_text': 'generate datapoints in the dataset: and then it is trained to classify a labelled dataset.While the unnormalized linear transformer dates back to 1992: the modern transformer architecture was not available until 2017 when it was published by researchers at Google in a paper ""Attention Is All You Need"". That development led to the emergence of large language models such as BERT in 2018 which was a pre-trained transformer (PT) but not designed to be generative (BERT was an ""encoder-only"" model). Also around that time: in 2018: OpenAI published its article entitled ""Improving Language Understanding by Generative Pre-Training:"" in which it introduced the first generative pre-trained transformer (GPT) system (""GPT-1"").Prior to transformer-based architectures: the best-performing neural NLP (natural language processing) models commonly employed supervised learning from large amounts of manually-labeled data. The reliance on supervised learning limited their use on datasets that were not well-annotated: and also made it prohibitively expensive and time-consuming to train extremely large language models.The semi-supervised approach OpenAI employed to make a large-scale generative system—and was first to do with a transformer model—involved two stages: an unsupervised generative ""pretraining"" stage to set initial parameters using a language modeling objective: and a supervised discriminative ""fine-tuning"" stage to adapt these parameters to a target task. === Later developments === Regarding more recent GPT foundation models: OpenAI published its first versions of GPT-3 in July 2020. There were three models: with 1B: 6.7B: 175B parameters: respectively named babbage: curie: and davinci (giving initials B: C: and D).In July 2021: OpenAI published Codex: a task-specific GPT model targeted for programming applications. This was developed by fine-tuning a 12B parameter version of GPT-3 (different from previous GPT-3 models) using code from GitHub.In March 2022: OpenAI published two versions of GPT-3 that were fine-tuned for instruction-following (instruction-tuned): named davinci-instruct-beta (175B) and text-davinci-001: and then started beta testing code-davinci-002. text-davinci-002 was instruction-tuned from code-davinci-002. Both text-davinci-003 and ChatGPT were released in November 2022: with both building upon text-davinci-002 via reinforcement learning from human feedback (RLHF). text-davinci-003 is trained for following instructions (like its predecessors): whereas ChatGPT is further trained for conversational interaction with a human user.OpenAI\'s most recent GPT foundation model: GPT-4: was released on March 14: 2023. It can be accessed directly by users via a premium version of ChatGPT: and is available to developers for incorporation into other products and services via OpenAI\'s API. Other producers of GPT foundation models include EleutherAI (with a series of models starting in March 2021) and Cerebras (with seven models released in March 2023). == Foundational models == A foundational model is an AI model trained on broad data at scale such that it can be adapted to a wide range of downstream tasks.Thus far: the most notable GPT foundation models have been from OpenAI\'s GPT-n series. The most recent from that is GPT-4: for which OpenAI declined to publish the size or training details (citing ""the competitive landscape and the safety implications of large-scale models""). Other such models include Google\'s PaLM: a broad foundation model that has been compared to GPT-3 and has recently been made available to developers via an API: and Together\'s GPT-JT: which has been reported as the closest-performing open-source alternative to GPT-3 (and is derived from earlier open-source GPTs). Meta AI (formerly Facebook) also has a generative transformer-based foundational large language model: known as LLaMA.Foundational GPTs can also employ modalities other than text: for input and/or output. GPT-4 is a multi-modal LLM that is capable of processing text and image input (though its output is limited to text). Regarding multimodal output: some generative transformer-based models are used for text-to-image technologies such as diffusion and parallel decoding. Such kinds of models can serve as visual foundation models (VFMs) for developing downstream systems that can work with images. == Task-specific models == A foundational GPT model can be further adapted to produce more targeted systems directed to specific tasks and/or subject-matter domains. Methods for such adaptation can include additional fine-tuning (beyond that done for the foundation model) as well as certain forms of prompt engineering.An important example of this is fine-tuning models to follow instructions: which is of course a fairly broad task but more targeted than a foundation model. In January 2022: OpenAI introduced ""InstructGPT""—a series of models which were fine-tuned to follow instructions using a combination of supervised training and reinforcement learning from human feedback (RLHF) on base GPT-3 language models. Advantages this had over the bare foundational models included higher accuracy: less\nnegative/toxic sentiment: and generally better alignment with user needs. Hence: OpenAI began using this as the basis for its API service offerings. Other instruction-tuned models have been released by others: including a fully open version.Another (related) kind of task-specific models are chatbots: which engage in human-like conversation. In November 2022: OpenAI launched ChatGPT—an online chat interface powered by an instruction-tuned language model trained in a similar fashion to InstructGPT. They trained this model using RLHF: with human AI trainers providing conversations in which they played both the user and the AI: and mixed this new dialogue dataset with the InstructGPT dataset for a conversational format suitable for a chatbot. Other major chatbots currently include Microsoft\'s Bing Chat: which uses OpenAI\'s GPT-4 (as part of a broader close collaboration between OpenAI and Microsoft): and Google\'s competing chatbot Bard (initially based on their LaMDA family of conversation-trained language models: with plans to switch to PaLM).Yet another kind of task that a GPT can be used for is the meta-task of generating its own instructions: like developing a series of prompts for \'itself\' to be able to effectuate a more general goal given by a human user. This is known as an AI agent: and more specifically a recursive one because it uses results from its previous self-instructions to help it form its subsequent prompts; the first major example of this was Auto-GPT (which uses OpenAI\'s GPT models): and others have since been developed as well. === Multimodality === Generative transformer-based systems can also be targeted to tasks involving modalities beyond text.': 'original_similarity': 0.8204202461831686: 'slide': 257}"
0.4455324999871664,What are the foundational models?,"Foundational models are AI models trained on broad data at scale so that they can be adapted to a wide range of downstream tasks. Notable GPT foundational models have been from OpenAI's GPT-n series. Other such models include Google's PaLM, Together's GPT-JT, and Meta AI's transformer-based large language model known as LLaMA.","{'chunk_text': 'CRM) and Bloomberg\'s ""BloombergGPT"" (for finance). == History == === Initial developments === Generative pretraining (GP) was a long-established concept in machine learning applications. It was originally used as a form of semi-supervised learning: as the model is trained first on an unlabelled dataset (pretraining step) by learning to generate datapoints in the dataset: and then it is trained to classify a labelled dataset.While the unnormalized linear transformer dates back to 1992: the modern transformer architecture was not available until 2017 when it was published by researchers at Google in a paper ""Attention Is All You Need"". That development led to the emergence of large language models such as BERT in 2018 which was a pre-trained transformer (PT) but not designed to be generative (BERT was an ""encoder-only"" model). Also around that time: in 2018: OpenAI published its article entitled ""Improving Language Understanding by Generative Pre-Training:"" in which it introduced the first generative pre-trained transformer (GPT) system (""GPT-1"").Prior to transformer-based architectures: the best-performing neural NLP (natural language processing) models commonly employed supervised learning from large amounts of manually-labeled data. The reliance on supervised learning limited their use on datasets that were not well-annotated: and also made it prohibitively expensive and time-consuming to train extremely large language models.The semi-supervised approach OpenAI employed to make a large-scale generative system—and was first to do with a transformer model—involved two stages: an unsupervised generative ""pretraining"" stage to set initial parameters using a language modeling objective: and a supervised discriminative ""fine-tuning"" stage to adapt these parameters to a target task. === Later developments === Regarding more recent GPT foundation models: OpenAI published its first versions of GPT-3 in July 2020. There were three models: with 1B: 6.7B: 175B parameters: respectively named babbage: curie: and davinci (giving initials B: C: and D).In July 2021: OpenAI published Codex: a task-specific GPT model targeted for programming applications. This was developed by fine-tuning a 12B parameter version of GPT-3 (different from previous GPT-3 models) using code from GitHub.In March 2022: OpenAI published two versions of GPT-3 that were fine-tuned for instruction-following (instruction-tuned): named davinci-instruct-beta (175B) and text-davinci-001: and then started beta testing code-davinci-002. text-davinci-002 was instruction-tuned from code-davinci-002. Both text-davinci-003 and ChatGPT were released in November 2022: with both building upon text-davinci-002 via reinforcement learning from human feedback (RLHF). text-davinci-003 is trained for following instructions (like its predecessors): whereas ChatGPT is further trained for conversational interaction with a human user.OpenAI\'s most recent GPT foundation model: GPT-4: was released on March 14: 2023. It can be accessed directly by users via a premium version of ChatGPT: and is available to developers for incorporation into other products and services via OpenAI\'s API. Other producers of GPT foundation models include EleutherAI (with a series of models starting in March 2021) and Cerebras (with seven models released in March 2023). == Foundational models == A foundational model is an AI model trained on broad data at scale such that it can be adapted to a wide range of downstream tasks.Thus far: the most notable GPT foundation models have been from OpenAI\'s GPT-n series. The most recent from that is GPT-4: for which OpenAI declined to publish the size or training details (citing ""the competitive landscape and the safety implications of large-scale models""). Other such models include Google\'s PaLM: a broad foundation model that has been compared to GPT-3 and has recently been made available to developers via an API: and Together\'s GPT-JT: which has been reported as the closest-performing open-source alternative to GPT-3 (and is derived from earlier open-source GPTs). Meta AI (formerly Facebook) also has a generative transformer-based foundational large language model: known as LLaMA.Foundational GPTs can also employ modalities other than text: for input and/or output. GPT-4 is a multi-modal LLM that is capable of processing text and image input (though its output is limited to text). Regarding multimodal output: some generative transformer-based models are used for text-to-image technologies such as diffusion and parallel decoding. Such kinds of models can serve as visual foundation models (VFMs) for developing downstream systems that can work with images. == Task-specific models == A foundational GPT model can be further adapted to produce more targeted systems directed to specific tasks and/or subject-matter domains. Methods for such adaptation can include additional fine-tuning (beyond that done for the foundation model) as well as certain forms of prompt engineering.An important example of this is fine-tuning models to follow instructions: which is of course a fairly broad task but more targeted than a foundation model. In January 2022: OpenAI introduced ""InstructGPT""—a series of models which were fine-tuned to follow instructions using a combination of supervised training and reinforcement learning from human feedback (RLHF) on base GPT-3 language models. Advantages this had over the bare foundational models included higher accuracy: less\nnegative/toxic sentiment: and generally better alignment with user needs. Hence: OpenAI began using this as the basis for its API service offerings. Other instruction-tuned models have been released by others: including a fully open version.Another (related) kind of task-specific models are chatbots: which engage in human-like conversation. In November 2022: OpenAI launched ChatGPT—an online chat interface powered by an instruction-tuned language model trained in a similar fashion to InstructGPT. They trained this model using RLHF: with human AI trainers providing conversations in which they played both the user and the AI: and mixed this new dialogue dataset with the InstructGPT dataset for a conversational format suitable for a chatbot. Other major chatbots currently include Microsoft\'s Bing Chat: which uses OpenAI\'s GPT-4 (as part of a broader close collaboration between OpenAI and Microsoft): and Google\'s competing chatbot Bard (initially based on their LaMDA family of conversation-trained language models: with plans to switch to PaLM).Yet another kind of task that a GPT can be used for is the meta-task of generating its own instructions: like developing a series of prompts for \'itself\' to be able to effectuate a more general goal given by a human user. This is known as an AI agent: and more specifically a recursive one because it uses': 'original_similarity': 0.7909409610342731: 'slide': 208}","{'chunk_text': 'that are in the domain but be powerful enough to include everything that is in the domain. This requirement might seem a bit strict: but the aim is to get a visually expressed model which includes everything relevant to the domain and excludes everything not appropriate for the domain. To achieve this: the language has to have a good distinction of which notations and syntaxes that are advantageous to present. ==== Participant appropriateness ==== To evaluate the participant appropriateness we try to identify how well the language expresses the knowledge held by the stakeholders. This involves challenges since a stakeholder\'s knowledge is subjective. The knowledge of the stakeholder is both tacit and explicit. Both types of knowledge are of dynamic character. In this framework only the explicit type of knowledge is taken into account. The language should to a large extent express all the explicit knowledge of the stakeholders relevant to the domain. ==== Modeller appropriateness ==== Last paragraph stated that knowledge of the stakeholders should be presented in a good way. In addition it is imperative that the language should be able to express all possible explicit knowledge of the stakeholders. No knowledge should be left unexpressed due to lacks in the language. ==== Comprehensibility appropriateness ==== Comprehensibility appropriateness makes sure that the social actors understand the model due to a consistent use of the language. To achieve this the framework includes a set of criteria. The general importance that these express is that the language should be flexible: easy to organize and easy to distinguish different parts of the language internally as well as from other languages. In addition to this: the goal should be as simple as possible and that each symbol in the language has a unique representation. This is in connection to also to the structure of the development requirements. . ==== Tool appropriateness ==== To ensure that the domain actually modelled is usable for analyzing and further processing: the language has to ensure that it is possible to reason in an automatic way. To achieve this it has to include formal syntax and semantics. Another advantage by formalizing is the ability to discover errors in an early stage. It is not always that the language best fitted for the technical actors is the same as for the social actors. ==== Organizational appropriateness ==== The language used is appropriate for the organizational context: e.g. that the language is standardized within the organization: or that it is supported by tools that are chosen as standard in the organization. == See also == == References == == Further reading == John Krogstie (2003) ""Evaluating UML using a generic quality framework"" . SINTEF Telecom and Informatics and IDI: NTNU: Norway Krogstie and Sølvsberg (2003). Information Systems Engineering: Conceptual Modeling in a Quality Perspective. Institute of computer and information sciences.\\ Anna Gunhild Nysetvold and John Krogstie (2005). ""Assessing business processing modeling languages using a generic quality framework"". Institute of computer and information sciences. == External links == Fundamental Modeling Concepts Software Modeling Languages Portal BIP -- Incremental Component-based Construction of Real-time Systems Gellish Formal English\nA neural network: also called a neuronal network: is an interconnected population of neurons (typically containing multiple neural circuits). Biological neural networks are studied to understand the organization and functioning of nervous systems. Closely related are artificial neural networks: machine learning models inspired by biological neural networks. They consist of artificial neurons: which are mathematical functions that are designed to be analogous to the mechanisms used by neural circuits. == Overview == A biological neural network is composed of a group of chemically connected or functionally associated neurons. A single neuron may be connected to many other neurons and the total': 'original_similarity': 0.7794675561117885: 'slide': 99}","{'chunk_text': ""concepts such as: concurrency: nondeterminism: synchronization: and communication. The semantic foundations of Behavioral languages are process calculus or process algebra. ==== Discipline-specific ==== A discipline-specific modeling (DspM) language is focused on deliverables affiliated with a specific software development life cycle stage. Therefore: such language offers a distinct vocabulary: syntax: and notation for each stage: such as discovery: analysis: design: architecture: contraction: etc. For example: for the analysis phase of a project: the modeler employs specific analysis notation to deliver an analysis proposition diagram. During the design phase: however: logical design notation is used to depict relationship between software entities. In addition: the discipline-specific modeling language best practices does not preclude practitioners from combining the various notations in a single diagram. ==== Domain-specific ==== Domain-specific modeling (DSM) is a software engineering methodology for designing and developing systems: most often IT systems such as computer software. It involves systematic use of a graphical domain-specific language (DSL) to represent the various facets of a system. DSM languages tend to support higher-level abstractions than General-purpose modeling languages: so they require less effort and fewer low-level details to specify a given system. ==== Framework-specific ==== A framework-specific modeling language (FSML) is a kind of domain-specific modeling language which is designed for an object-oriented application framework. FSMLs define framework-provided abstractions as FSML concepts and decompose the abstractions into features. The features represent implementation steps or choices. A FSML concept can be configured by selecting features and providing values for features. Such a concept configuration represents how the concept should be implemented in the code. In other words: concept configuration describes how the framework should be completed in order to create the implementation of the concept. ==== Information and knowledge modeling ==== Linked data and ontology engineering require 'host languages' to represent entities and the relations between them: constraints between the properties of entities and relations: and metadata attributes. JSON-LD and RDF are two major (and semantically almost equivalent) languages in this context: primarily because they support statement reification and contextualisation which are essential properties to support the higher-order logic needed to reason about models. Model transformation is a common example of such reasoning. ==== Object-oriented ==== Object modeling languages are modeling languages based on a standardized set of symbols and ways of arranging them to model (part of) an object oriented software design or system design. Some organizations use them extensively in combination with a software development methodology to progress from initial specification to an implementation plan and to communicate that plan to an entire team of developers and stakeholders. Because a modeling language is visual and at a higher-level of abstraction than code: using models encourages the generation of a shared vision that may prevent problems of differing interpretation later in development. Often software modeling tools are used to construct these models: which may then be capable of automatic translation to code. ==== Virtual reality ==== Virtual Reality Modeling Language (VRML): before 1995 known as the Virtual Reality Markup Language is a standard file format for representing 3-dimensional (3D) interactive vector graphics: designed particularly with the World Wide Web in mind. ==== Others ==== Architecture Description Language Face Modeling Language Generative Modelling Language Java Modeling Language Promela Rebeca Modeling Language Service Modeling Language Web Services Modeling Language X3D == Applications == Various kinds of modeling languages are applied in different disciplines: including computer science: information management: business process modeling: software engineering: and systems engineering. Modeling languages can be used to specify: system requirements: structures and behaviors.Modeling languages are intended to be used to precisely specify systems so that stakeholders (e.g.: customers: operators: analysts: designers) can better understand the system being modeled. The more mature modeling languages are precise: consistent and executable. Informal diagramming techniques applied with drawing tools are expected to produce useful pictorial representations of system requirements: structures and behaviors: which can be useful for communication: design: and problem solving but cannot be used programmatically.: 539 Executable modeling languages applied with proper tool support: however: are expected to automate system verification and validation: simulation and code generation from the same representations. == Quality == A review of modelling languages is essential to be able to assign which languages are appropriate for different modelling settings. In the term settings we include stakeholders: domain and the knowledge connected. Assessing the language quality is a means that aims to achieve better models. === Framework for evaluation === Here language quality is stated in accordance with the SEQUAL framework for quality of models developed by Krogstie: Sindre and Lindland (2003): since this is a framework that connects the language quality to a framework for general model\nquality. Five areas are used in this framework to describe language quality and these are supposed to express both the conceptual as well as the visual notation of the language. We will not go into a thoroughly explanation of the underlying quality framework of models but concentrate on the areas used to explain the language quality framework. ==== Domain appropriateness ==== The framework states the ability to represent the domain as domain appropriateness. The statement appropriateness can be a bit vague: but in this particular context it means able to express. You should ideally only be able to express things that are in the domain but be powerful enough to include everything that is in the domain. This requirement might seem a bit strict: but the aim is to get a visually expressed model which includes everything relevant to the domain and excludes everything not appropriate for the domain. To achieve this: the language has to have a good distinction of which notations and syntaxes that are advantageous to present. ==== Participant appropriateness ==== To evaluate the participant appropriateness we try to identify how well the language expresses the knowledge held by the stakeholders. This involves challenges since a stakeholder's knowledge is subjective. The knowledge of the stakeholder is both tacit and explicit. Both types of knowledge are of dynamic character. In this framework only the explicit type of knowledge is taken into"": 'original_similarity': 0.7775088333230316: 'slide': 231}"
0.4040930999908596,What is a Graph Neural Network (GNN)?,"A Graph Neural Network (GNN) is a type of artificial neural network designed for processing data that can be represented as graphs. It falls under the broader field of ""geometric deep learning"". GNNs use pairwise message passing as the key design element, enabling graph nodes to iteratively update their representations by exchanging information with their neighbors.","{'chunk_text': 'have been proposed: which implement different flavors of message passing: started by recursive or convolutional constructive approaches. As of 2022: whether it is possible to define GNN architectures ""going beyond"" message passing: or if every GNN can be built on message passing over suitably defined graphs: is an open research question.Relevant application domains for GNNs include Natural Language Processing: social networks: citation networks: molecular biology: chemistry: physics and NP-hard combinatorial optimization problems.Several open source libraries implementing graph neural networks are available: such as PyTorch Geometric (PyTorch): TensorFlow GNN (TensorFlow): jraph (Google JAX): and GraphNeuralNetworks.jl/GeometricFlux.jl (Julia: Flux). == Architecture == The architecture of a generic GNN implements the following fundamental layers: Permutation equivariant: a permutation equivariant layer maps a representation of a graph into an updated representation of the same graph. In the literature: permutation equivariant layers are implemented via pairwise message passing between graph nodes. Intuitively: in a message passing layer: nodes update their representations by aggregating the messages received from their immediate neighbours. As such: each message passing layer increases the receptive field of the GNN by one hop. Local pooling: a local pooling layer coarsens the graph via downsampling. Local pooling is used to increase the receptive field of a GNN: in a similar fashion to pooling layers in convolutional neural networks. Examples include k-nearest neighbours pooling: top-k pooling: and self-attention pooling. Global pooling: a global pooling layer: also known as readout layer: provides fixed-size representation of the whole graph. The global pooling layer must be permutation invariant: such that permutations in the ordering of graph nodes and edges do not alter the final output. Examples include element-wise sum: mean or maximum.It has been demonstrated that GNNs cannot be more expressive than the Weisfeiler–Leman Graph Isomorphism Test. In practice: this means that there exist different graph structures (e.g.: molecules with the same atoms but different bonds) that cannot be distinguished by GNNs. More powerful GNNs operating on higher-dimension geometries such as simplicial complexes can be designed. As of 2022: whether or not future architectures will overcome the message passing primitive is an open research question. == Message passing layers == Message passing layers are permutation-equivariant layers mapping a graph into an updated representation of the same graph. Formally: they can be expressed as message passing neural networks (MPNNs).Let G=(V:E){\\displaystyle G=(V:E)} be a graph: where V{\\displaystyle V} is the node set and E{\\displaystyle E} is the edge set. Let Nu{\\displaystyle N_{u}} be the neighbourhood of some node u∈V{\\displaystyle u\\in V}. Additionally: let xu{\\displaystyle \\mathbf {x} _{u}} be the features of node u∈V{\\displaystyle u\\in V}: and euv{\\displaystyle \\mathbf {e} _{uv}} be the features of edge (u:v)∈E{\\displaystyle (u:v)\\in E}. An MPNN layer can be expressed as follows: hu=ϕ(xu:⨁v∈Nuψ(xu:xv:euv)){\\displaystyle \\mathbf {h} _{u}=\\phi \\left(\\mathbf {x} _{u}:\\bigoplus _{v\\in N_{u}}\\psi (\\mathbf {x} _{u}:\\mathbf {x} _{v}:\\mathbf {e} _{uv})\\right)}where ϕ{\\displaystyle \\phi } and ψ{\\displaystyle \\psi } are differentiable functions (e.g.: artificial neural networks): and ⨁{\\displaystyle \\bigoplus } is a permutation invariant aggregation operator that can accept an arbitrary number of inputs (e.g.: element-wise sum: mean: or max). In particular: ϕ{\\displaystyle \\phi } and ψ{\\displaystyle \\psi } are referred to as update and message functions: respectively. Intuitively: in an MPNN computational block: graph nodes update their representations by aggregating the messages received from their neighbours. The outputs of one or more MPNN layers are node representations hu{\\displaystyle \\mathbf {h} _{u}} for each node u∈V{\\displaystyle u\\in V} in the graph. Node representations can be employed for any downstream task: such as node/graph classification or edge prediction. Graph nodes in an MPNN update their representation aggregating information from their immediate neighbours. As such: stacking n{\\displaystyle n} MPNN layers means that one node will be able to communicate with nodes that are at most n{\\displaystyle n} ""hops"" away. In principle: to ensure that every node receives information from every other node: one would need to stack a number of MPNN layers equal to the graph diameter. However: stacking many MPNN layers may cause issues such as oversmoothing and oversquashing. Oversmoothing refers to the issue of node representations becoming indistinguishable. Oversquashing refers to the bottleneck that is created by squeezing long-range dependencies into fixed-size representations. Countermeasures such as skip connections (as in residual neural networks): gated update rules and jumping knowledge can mitigate oversmoothing. Modifying the final layer to be a fully-adjacent layer: i.e.: by considering the graph as a complete graph: can mitigate oversquashing in problems where long-range dependencies are required.Other ""flavours"" of MPNN have been developed in the literature: such as graph convolutional networks and graph attention networks: whose definitions can be expressed in terms of the MPNN formalism. === Graph convolutional network === The graph convolutional network (GCN) was first introduced by Thomas Kipf and Max Welling in 2017.A GCN layer defines a first-order approximation of a localized spectral filter on graphs. GCNs can be understood as a generalization of convolutional neural networks to graph-structured data. The formal expression of a GCN layer reads as follows: H=σ(D~−12A~D~−12XΘ){\\displaystyle \\mathbf {H} =\\sigma \\left({\\tilde {\\mathbf {D} }}^{-{\\frac {1}{2}}}{\\tilde {\\mathbf {A} }}{\\tilde {\\mathbf {D} }}^{-{\\frac {1}{2}}}\\mathbf {X} \\mathbf {\\Theta } \\right)}where H{\\displaystyle \\mathbf {H} } is the matrix of node representations hu{\\displaystyle \\mathbf {h} _{u}}: X{\\displaystyle \\mathbf {X}\n} is the matrix of node features xu{\\displaystyle \\mathbf {x} _{u}}: σ(⋅){\\displaystyle \\sigma (\\cdot )} is an activation function (e.g.: ReLU): A~{\\displaystyle {\\tilde {\\mathbf {A} }}} is the graph adjacency matrix with the addition of self-loops: D~{\\displaystyle {\\tilde {\\mathbf {D} }}} is the graph degree matrix with the addition of self-loops: and Θ{\\displaystyle \\mathbf {\\Theta } } is a matrix of trainable parameters. In particular: let A{\\displaystyle \\mathbf {A} } be the graph adjacency matrix: then: one can define A~=A+I{\\displaystyle {\\tilde {\\mathbf {A} }}=\\mathbf {A} +\\mathbf {I} } and D~ii=∑j∈VA~ij{\\displaystyle {\\tilde {\\mathbf {D} }}_{ii}=\\sum _{j\\in V}{\\tilde {A}}_{ij}}: where I{\\displaystyle \\mathbf {I} } denotes the identity matrix. This normalization ensures that the eigenvalues of D~−12A~D~−12{\\displaystyle {\\tilde {\\mathbf {D} }}^{-{\\frac {1}{2}}}{\\tilde {\\mathbf {A} }}{\\tilde {\\mathbf {D} }}^{-{\\frac {1}{2}}}} are bounded in the range [0:1]{\\displaystyle [0:1]}: avoiding numerical instabilities and exploding/vanishing gradients. A limitation of GCNs is that they': 'original_similarity': 0.902380064390367: 'slide': 144}","{'chunk_text': ""the graph topology. == Applications == === Protein folding === Graph neural networks are one of the main building blocks of AlphaFold: an artificial intelligence program developed by Google's DeepMind for solving the protein folding problem in biology. AlphaFold achieved first place in several CASP competitions. === Social networks === Social networks are a major application domain for GNNs due to their natural representation as social graphs. GNNs are used to develop recommender systems based on both social relations and item relations. === Combinatorial optimization === GNNs are used as fundamental building blocks for several combinatorial optimization algorithms. Examples include computing shortest paths or Eulerian circuits for a given graph: deriving chip placements superior or competitive to handcrafted human solutions: and improving expert-designed branching rules in branch and bound. === Cyber security === When viewed as a graph: a network of computers can be analyzed with GNNs for anomaly detection. Anomalies within provenance graphs often correlate to malicious activity within the network. GNNs have been used to identify these anomalies on individual nodes and within paths to detect malicious processes: or on the edge level to detect lateral movement. == References == == External links == https://distill.pub/2021/gnn-intro/\nArtificial neural networks (ANNs) are models created using machine learning to perform a number of tasks. Their creation was inspired by neural circuitry. While some of"": 'original_similarity': 0.8786065103264552: 'slide': 24}","{'chunk_text': 'involved in what was called the ""big bang"" of deep learning: ""as deep-learning neural networks were trained with Nvidia graphics processing units (GPUs)"". That year: Andrew Ng determined that GPUs could increase the speed of deep-learning systems by about 100 times. In particular: GPUs are well-suited for the matrix/vector computations involved in machine learning. GPUs speed up training algorithms by orders of magnitude: reducing running times from weeks to days. Further: specialized hardware and algorithm optimizations can be used for efficient processing of deep learning models. === Deep learning revolution === In the late 2000s: deep learning started to outperform other methods in machine learning competitions. In 2009: a long short-term memory trained by connectionist temporal classification (Alex Graves: Santiago Fernández: Faustino Gomez: and Jürgen Schmidhuber: 2006) was the first RNN to win pattern recognition contests: winning three competitions in connected handwriting recognition. Google later used CTC-trained LSTM for speech recognition on the smartphone.Significant impacts in image or object recognition were felt from 2011 to 2012. Although CNNs trained by backpropagation had been around for decades: and GPU implementations of NNs for years: including CNNs: faster implementations of CNNs on GPUs were needed to progress on computer vision. In 2011: the DanNet by Dan Ciresan: Ueli Meier: Jonathan Masci: Luca Maria Gambardella: and Jürgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest: outperforming traditional methods by a factor of 3. Also in 2011: DanNet won the ICDAR Chinese handwriting contest: and in May 2012: it won the ISBI image segmentation contest. Until 2011: CNNs did not play a major role at computer vision conferences: but in June 2012: a paper by Ciresan et al. at the leading conference CVPR showed how max-pooling CNNs on GPU can dramatically improve many vision benchmark records. In September 2012: DanNet also won the ICPR contest on analysis of large medical images for cancer detection: and in the following year also the MICCAI Grand Challenge on the same topic. In October 2012: the similar AlexNet by Alex Krizhevsky: Ilya Sutskever: and Geoffrey Hinton won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. The VGG-16 network by Karen Simonyan and Andrew Zisserman further reduced the error rate and won the ImageNet 2014 competition: following a similar trend in large-scale speech recognition. Image classification was then extended to the more challenging task of generating descriptions (captions) for images: often as a combination of CNNs and LSTMs.In 2012: a team led by George E. Dahl won the ""Merck Molecular Activity Challenge"" using multi-task deep neural networks to predict the biomolecular target of one drug. In 2014: Sepp Hochreiter\'s group used deep learning to detect off-target and toxic effects of environmental chemicals in nutrients: household products and drugs and won the ""Tox21 Data Challenge"" of NIH: FDA and NCATS.In 2016: Roger Parloff mentioned a ""deep learning revolution"" that has transformed the AI industry.In March 2019: Yoshua Bengio: Geoffrey Hinton and Yann LeCun were awarded the Turing Award for conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing. == Neural networks == Artificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve their ability) to do tasks by considering examples: generally without task-specific programming. For example: in image recognition: they might learn to identify images that contain cats by analyzing example images that have been manually labeled as ""cat"" or ""no cat"" and using the analytic results to identify cats in other images. They have found most use in applications difficult to express with a traditional computer algorithm using rule-based programming. An ANN is based on a collection of connected units called artificial neurons: (analogous to biological neurons in a biological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it. Neurons may have state: generally represented by real numbers: typically between 0 and 1. Neurons and synapses may also have a weight that varies as learning proceeds: which can increase or decrease the strength of the signal that it sends downstream. Typically: neurons are organized in layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first (input): to the last (output) layer: possibly after traversing the layers multiple times. The original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time: attention focused on matching specific mental abilities: leading to deviations from biology such as backpropagation: or passing information in the reverse direction and adjusting the network to reflect that information. Neural networks have been used on a variety of tasks: including computer vision: speech recognition: machine translation: social network filtering: playing board and video games and medical diagnosis. As of 2017: neural networks typically have a few thousand to a few million units and millions of connections. Despite\nthis number being several order of magnitude less than the number of neurons on a human brain: these networks can perform many tasks at a level beyond that of humans (e.g.: recognizing faces: or playing ""Go""). === Deep neural networks === A deep neural network (DNN) is an artificial neural network with multiple layers between the input and output layers. There are different types of neural networks but they always consist of the same components: neurons: synapses: weights: biases: and functions. These components as a whole function in a way that mimics functions of the human brain: and can be trained like any other ML algorithm.For example: a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed. The user can review the results and select which probabilities the network should display (above a certain threshold: etc.) and return the proposed label. Each mathematical': 'original_similarity': 0.8328274909760051: 'slide': 160}"
0.5747117999999318,How can convolutional neural networks and transformer layers be interpreted in the context of GNNs?,"In geometric deep learning, convolutional neural networks and transformer layers can be interpreted as GNNs operating on specifically defined graphs. For instance, a convolutional neural network layer, in computer vision context, can be seen as a GNN applied to graphs whose nodes are pixels and where only adjacent pixels are connected by edges. Similarly, a transformer layer in natural language processing can be seen as a GNN applied to complete graphs whose nodes are words or tokens in a text passage.","{'chunk_text': 'whether it is possible to define GNN architectures ""going beyond"" message passing: or if every GNN can be built on message passing over suitably defined graphs: is an open research question.Relevant application domains for GNNs include Natural Language Processing: social networks: citation networks: molecular biology: chemistry: physics and NP-hard combinatorial optimization problems.Several open source libraries implementing graph neural networks are available: such as PyTorch Geometric (PyTorch): TensorFlow GNN (TensorFlow): jraph (Google JAX): and GraphNeuralNetworks.jl/GeometricFlux.jl (Julia: Flux). == Architecture == The architecture of a generic GNN implements the following fundamental layers: Permutation equivariant: a permutation equivariant layer maps a representation of a graph into an updated representation of the same graph. In the literature: permutation equivariant layers are implemented via pairwise message passing between graph nodes. Intuitively: in a message passing layer: nodes update their representations by aggregating the messages received from their immediate neighbours. As such: each message passing layer increases the receptive field of the GNN by one hop. Local pooling: a local pooling layer coarsens the graph via downsampling. Local pooling is used to increase the receptive field of a GNN: in a similar fashion to pooling layers in convolutional neural networks. Examples include k-nearest neighbours pooling: top-k pooling: and self-attention pooling. Global pooling: a global pooling layer: also known as readout layer: provides fixed-size representation of the whole graph. The global pooling layer must be permutation invariant: such that permutations in the ordering of graph nodes and edges do not alter the final output. Examples include element-wise sum: mean or maximum.It has been demonstrated that GNNs cannot be more expressive than the Weisfeiler–Leman Graph Isomorphism Test. In practice: this means that there exist different graph structures (e.g.: molecules with the same atoms but different bonds) that cannot be distinguished by GNNs. More powerful GNNs operating on higher-dimension geometries such as simplicial complexes can be designed. As of 2022: whether or not future architectures will overcome the message passing primitive is an open research question. == Message passing layers == Message passing layers are permutation-equivariant layers mapping a graph into an updated representation of the same graph. Formally: they can be expressed as message passing neural networks (MPNNs).Let G=(V:E){\\displaystyle G=(V:E)} be a graph: where V{\\displaystyle V} is the node set and E{\\displaystyle E} is the edge set. Let Nu{\\displaystyle N_{u}} be the neighbourhood of some node u∈V{\\displaystyle u\\in V}. Additionally: let xu{\\displaystyle \\mathbf {x} _{u}} be the features of node u∈V{\\displaystyle u\\in V}: and euv{\\displaystyle \\mathbf {e} _{uv}} be the features of edge (u:v)∈E{\\displaystyle (u:v)\\in E}. An MPNN layer can be expressed as follows: hu=ϕ(xu:⨁v∈Nuψ(xu:xv:euv)){\\displaystyle \\mathbf {h} _{u}=\\phi \\left(\\mathbf {x} _{u}:\\bigoplus _{v\\in N_{u}}\\psi (\\mathbf {x} _{u}:\\mathbf {x} _{v}:\\mathbf {e} _{uv})\\right)}where ϕ{\\displaystyle \\phi } and ψ{\\displaystyle \\psi } are differentiable functions (e.g.: artificial neural networks): and ⨁{\\displaystyle \\bigoplus } is a permutation invariant aggregation operator that can accept an arbitrary number of inputs (e.g.: element-wise sum: mean: or max). In particular: ϕ{\\displaystyle \\phi } and ψ{\\displaystyle \\psi } are referred to as update and message functions: respectively. Intuitively: in an MPNN computational block: graph nodes update their representations by aggregating the messages received from their neighbours. The outputs of one or more MPNN layers are node representations hu{\\displaystyle \\mathbf {h} _{u}} for each node u∈V{\\displaystyle u\\in V} in the graph. Node representations can be employed for any downstream task: such as node/graph classification or edge prediction. Graph nodes in an MPNN update their representation aggregating information from their immediate neighbours. As such: stacking n{\\displaystyle n} MPNN layers means that one node will be able to communicate with nodes that are at most n{\\displaystyle n} ""hops"" away. In principle: to ensure that every node receives information from every other node: one would need to stack a number of MPNN layers equal to the graph diameter. However: stacking many MPNN layers may cause issues such as oversmoothing and oversquashing. Oversmoothing refers to the issue of node representations becoming indistinguishable. Oversquashing refers to the bottleneck that is created by squeezing long-range dependencies into fixed-size representations. Countermeasures such as skip connections (as in residual neural networks): gated update rules and jumping knowledge can mitigate oversmoothing. Modifying the final layer to be a fully-adjacent layer: i.e.: by considering the graph as a complete graph: can mitigate oversquashing in problems where long-range dependencies are required.Other ""flavours"" of MPNN have been developed in the literature: such as graph convolutional networks and graph attention networks: whose definitions can be expressed in terms of the MPNN formalism. === Graph convolutional network === The graph convolutional network (GCN) was first introduced by Thomas Kipf and Max Welling in 2017.A GCN layer defines a first-order approximation of a localized spectral filter on graphs. GCNs can be understood as a generalization of convolutional neural networks to graph-structured data. The formal expression of a GCN layer reads as follows: H=σ(D~−12A~D~−12XΘ){\\displaystyle \\mathbf {H} =\\sigma \\left({\\tilde {\\mathbf {D} }}^{-{\\frac {1}{2}}}{\\tilde {\\mathbf {A} }}{\\tilde {\\mathbf {D} }}^{-{\\frac {1}{2}}}\\mathbf {X} \\mathbf {\\Theta } \\right)}where H{\\displaystyle \\mathbf {H} } is the matrix of node representations hu{\\displaystyle \\mathbf {h} _{u}}: X{\\displaystyle \\mathbf {X}\n} is the matrix of node features xu{\\displaystyle \\mathbf {x} _{u}}: σ(⋅){\\displaystyle \\sigma (\\cdot )} is an activation function (e.g.: ReLU): A~{\\displaystyle {\\tilde {\\mathbf {A} }}} is the graph adjacency matrix with the addition of self-loops: D~{\\displaystyle {\\tilde {\\mathbf {D} }}} is the graph degree matrix with the addition of self-loops: and Θ{\\displaystyle \\mathbf {\\Theta } } is a matrix of trainable parameters. In particular: let A{\\displaystyle \\mathbf {A} } be the graph adjacency matrix: then: one can define A~=A+I{\\displaystyle {\\tilde {\\mathbf {A} }}=\\mathbf {A} +\\mathbf {I} } and D~ii=∑j∈VA~ij{\\displaystyle {\\tilde {\\mathbf {D} }}_{ii}=\\sum _{j\\in V}{\\tilde {A}}_{ij}}: where I{\\displaystyle \\mathbf {I} } denotes the identity matrix. This normalization ensures that the eigenvalues of D~−12A~D~−12{\\displaystyle {\\tilde {\\mathbf {D} }}^{-{\\frac {1}{2}}}{\\tilde {\\mathbf {A} }}{\\tilde {\\mathbf {D} }}^{-{\\frac {1}{2}}}} are bounded in the range [0:1]{\\displaystyle [0:1]}: avoiding numerical instabilities and exploding/vanishing gradients. A limitation of GCNs is that they do not allow multidimensional edge features euv{\\displaystyle \\mathbf {e} _{uv}}. It is however possible to associate scalar weights wuv{\\displaystyle w_{uv}}': 'original_similarity': 0.88509354883667: 'slide': 164}","{'chunk_text': ""Protein folding === Graph neural networks are one of the main building blocks of AlphaFold: an artificial intelligence program developed by Google's DeepMind for solving the protein folding problem in biology. AlphaFold achieved first place in several CASP competitions. === Social networks === Social networks are a major application domain for GNNs due to their natural representation as social graphs. GNNs are used to develop recommender systems based on both social relations and item relations. === Combinatorial optimization === GNNs are used as fundamental building blocks for several combinatorial optimization algorithms. Examples include computing shortest paths or Eulerian circuits for a given graph: deriving chip placements superior or competitive to handcrafted human solutions: and improving expert-designed branching rules in branch and bound. === Cyber security === When viewed as a graph: a network of computers can be analyzed with GNNs for anomaly detection. Anomalies within provenance graphs often correlate to malicious activity within the network. GNNs have been used to identify these anomalies on individual nodes and within paths to detect malicious processes: or on the edge level to detect lateral movement. == References == == External links == https://distill.pub/2021/gnn-intro/\nArtificial neural networks (ANNs) are models created using machine learning to perform a number of tasks. Their creation was inspired by neural circuitry. While some of the computational implementations ANNs relate to earlier"": 'original_similarity': 0.8508898967015511: 'slide': 31}","{'chunk_text': 'method called long short-term memory (LSTM): published in 1997. LSTM recurrent neural networks can learn ""very deep learning"" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. The ""vanilla LSTM"" with forget gate was introduced in 1999 by Felix Gers: Schmidhuber and Fred Cummins. LSTM has become the most cited neural network of the 20th century. In 2015: Rupesh Kumar Srivastava: Klaus Greff: and Schmidhuber used LSTM principles to create the Highway network: a feedforward neural network with hundreds of layers: much deeper than previous networks. 7 months later: Kaiming He: Xiangyu Zhang; Shaoqing Ren: and Jian Sun won the ImageNet 2015 competition with an open-gated or gateless Highway network variant called Residual neural network. This has become the most cited neural network of the 21st century.In 1994: André de Carvalho: together with Mike Fairhurst and David Bisset: published experimental results of a multi-layer boolean neural network: also known as a weightless neural network: composed of a 3-layers self-organising feature extraction neural network module (SOFT) followed by a multi-layer classification neural network module (GSN): which were independently trained. Each layer in the feature extraction module extracted features with growing complexity regarding the previous layer.In 1995: Brendan Frey demonstrated that it was possible to train (over two days) a network containing six fully connected layers and several hundred hidden units using the wake-sleep algorithm: co-developed with Peter Dayan and Hinton.Since 1997: Sven Behnke extended the feed-forward hierarchical convolutional approach in the Neural Abstraction Pyramid by lateral and backward connections in order to flexibly incorporate context into decisions and iteratively resolve local ambiguities. Simpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) were a popular choice in the 1990s and 2000s: because of artificial neural networks\' computational cost and a lack of understanding of how the brain wires its biological networks. Both shallow and deep learning (e.g.: recurrent nets) of ANNs for speech recognition have been explored for many years. These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively. Key difficulties have been analyzed: including gradient diminishing and weak temporal correlation structure in neural predictive models. Additional difficulties were the lack of training data and limited computing power. Most speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government\'s NSA and DARPA: SRI studied deep neural networks (DNNs) in speech and speaker recognition. The speaker recognition team led by Larry Heck reported significant success with deep neural networks in speech processing in the 1998 National Institute of Standards and Technology Speaker Recognition evaluation. The SRI deep neural network was then deployed in the Nuance Verifier: representing the first major industrial application of deep learning. The principle of elevating ""raw"" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the ""raw"" spectrogram or linear filter-bank features in the late 1990s: showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech: waveforms: later produced excellent larger-scale results.Speech recognition was taken over by LSTM. In 2003: LSTM started to become competitive with traditional speech recognizers on certain tasks. In 2006: Alex Graves: Santiago Fernández: Faustino Gomez: and Schmidhuber combined it with connectionist temporal classification (CTC) in stacks of LSTM RNNs. In 2015: Google\'s speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM: which they made available through Google Voice Search.The impact of deep learning in industry began in the early 2000s: when CNNs already processed an estimated 10% to 20% of all the checks written in the US: according to Yann LeCun. Industrial applications of deep learning to large-scale speech recognition started around 2010. In 2006: publications by Geoff Hinton: Ruslan Salakhutdinov: Osindero and Teh showed how a many-layered feedforward neural network could be effectively pre-trained one layer at a time: treating each layer in turn as an unsupervised restricted Boltzmann machine: then fine-tuning it using supervised backpropagation. The papers referred to learning for deep belief nets. The 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech: and the possibility that given more capable hardware and large-scale data sets that deep neural nets might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets. However: it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large: context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems. The nature of the recognition errors produced by the two types of systems was characteristically different: offering technical insights into how to integrate deep learning into the existing highly\nefficient: run-time speech decoding system deployed by all major speech recognition systems. Analysis around 2009–2010: contrasting the GMM (and other generative speech models) vs. DNN models: stimulated early industrial investment in deep learning for speech recognition. That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models. In 2010: researchers extended deep learning from TIMIT to large vocabulary speech recognition: by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees.Deep learning is part of state-of-the-art systems in various disciplines: particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification): as well as a range of large-vocabulary speech recognition tasks have steadily improved. Convolutional neural networks were superseded for ASR by CTC for LSTM. but are more successful in computer vision. Advances in hardware have driven renewed interest in deep learning. In 2009: Nvidia was involved in what was called the': 'original_similarity': 0.8416084906482092: 'slide': 165}"
0.45339120001881383,What is the role of message passing in GNNs?,"In GNNs, message passing is the key design element wherein graph nodes update their representations by exchanging information with their neighbors iteratively. Different GNN architectures implement different kinds of message passing. This aspect of defining GNN architectures ""going beyond"" message passing or basing every GNN on message passing over appropriately defined graphs is still a subject of open research.","{'chunk_text': 'for GNNs include Natural Language Processing: social networks: citation networks: molecular biology: chemistry: physics and NP-hard combinatorial optimization problems.Several open source libraries implementing graph neural networks are available: such as PyTorch Geometric (PyTorch): TensorFlow GNN (TensorFlow): jraph (Google JAX): and GraphNeuralNetworks.jl/GeometricFlux.jl (Julia: Flux). == Architecture == The architecture of a generic GNN implements the following fundamental layers: Permutation equivariant: a permutation equivariant layer maps a representation of a graph into an updated representation of the same graph. In the literature: permutation equivariant layers are implemented via pairwise message passing between graph nodes. Intuitively: in a message passing layer: nodes update their representations by aggregating the messages received from their immediate neighbours. As such: each message passing layer increases the receptive field of the GNN by one hop. Local pooling: a local pooling layer coarsens the graph via downsampling. Local pooling is used to increase the receptive field of a GNN: in a similar fashion to pooling layers in convolutional neural networks. Examples include k-nearest neighbours pooling: top-k pooling: and self-attention pooling. Global pooling: a global pooling layer: also known as readout layer: provides fixed-size representation of the whole graph. The global pooling layer must be permutation invariant: such that permutations in the ordering of graph nodes and edges do not alter the final output. Examples include element-wise sum: mean or maximum.It has been demonstrated that GNNs cannot be more expressive than the Weisfeiler–Leman Graph Isomorphism Test. In practice: this means that there exist different graph structures (e.g.: molecules with the same atoms but different bonds) that cannot be distinguished by GNNs. More powerful GNNs operating on higher-dimension geometries such as simplicial complexes can be designed. As of 2022: whether or not future architectures will overcome the message passing primitive is an open research question. == Message passing layers == Message passing layers are permutation-equivariant layers mapping a graph into an updated representation of the same graph. Formally: they can be expressed as message passing neural networks (MPNNs).Let G=(V:E){\\displaystyle G=(V:E)} be a graph: where V{\\displaystyle V} is the node set and E{\\displaystyle E} is the edge set. Let Nu{\\displaystyle N_{u}} be the neighbourhood of some node u∈V{\\displaystyle u\\in V}. Additionally: let xu{\\displaystyle \\mathbf {x} _{u}} be the features of node u∈V{\\displaystyle u\\in V}: and euv{\\displaystyle \\mathbf {e} _{uv}} be the features of edge (u:v)∈E{\\displaystyle (u:v)\\in E}. An MPNN layer can be expressed as follows: hu=ϕ(xu:⨁v∈Nuψ(xu:xv:euv)){\\displaystyle \\mathbf {h} _{u}=\\phi \\left(\\mathbf {x} _{u}:\\bigoplus _{v\\in N_{u}}\\psi (\\mathbf {x} _{u}:\\mathbf {x} _{v}:\\mathbf {e} _{uv})\\right)}where ϕ{\\displaystyle \\phi } and ψ{\\displaystyle \\psi } are differentiable functions (e.g.: artificial neural networks): and ⨁{\\displaystyle \\bigoplus } is a permutation invariant aggregation operator that can accept an arbitrary number of inputs (e.g.: element-wise sum: mean: or max). In particular: ϕ{\\displaystyle \\phi } and ψ{\\displaystyle \\psi } are referred to as update and message functions: respectively. Intuitively: in an MPNN computational block: graph nodes update their representations by aggregating the messages received from their neighbours. The outputs of one or more MPNN layers are node representations hu{\\displaystyle \\mathbf {h} _{u}} for each node u∈V{\\displaystyle u\\in V} in the graph. Node representations can be employed for any downstream task: such as node/graph classification or edge prediction. Graph nodes in an MPNN update their representation aggregating information from their immediate neighbours. As such: stacking n{\\displaystyle n} MPNN layers means that one node will be able to communicate with nodes that are at most n{\\displaystyle n} ""hops"" away. In principle: to ensure that every node receives information from every other node: one would need to stack a number of MPNN layers equal to the graph diameter. However: stacking many MPNN layers may cause issues such as oversmoothing and oversquashing. Oversmoothing refers to the issue of node representations becoming indistinguishable. Oversquashing refers to the bottleneck that is created by squeezing long-range dependencies into fixed-size representations. Countermeasures such as skip connections (as in residual neural networks): gated update rules and jumping knowledge can mitigate oversmoothing. Modifying the final layer to be a fully-adjacent layer: i.e.: by considering the graph as a complete graph: can mitigate oversquashing in problems where long-range dependencies are required.Other ""flavours"" of MPNN have been developed in the literature: such as graph convolutional networks and graph attention networks: whose definitions can be expressed in terms of the MPNN formalism. === Graph convolutional network === The graph convolutional network (GCN) was first introduced by Thomas Kipf and Max Welling in 2017.A GCN layer defines a first-order approximation of a localized spectral filter on graphs. GCNs can be understood as a generalization of convolutional neural networks to graph-structured data. The formal expression of a GCN layer reads as follows: H=σ(D~−12A~D~−12XΘ){\\displaystyle \\mathbf {H} =\\sigma \\left({\\tilde {\\mathbf {D} }}^{-{\\frac {1}{2}}}{\\tilde {\\mathbf {A} }}{\\tilde {\\mathbf {D} }}^{-{\\frac {1}{2}}}\\mathbf {X} \\mathbf {\\Theta } \\right)}where H{\\displaystyle \\mathbf {H} } is the matrix of node representations hu{\\displaystyle \\mathbf {h} _{u}}: X{\\displaystyle \\mathbf {X}\n} is the matrix of node features xu{\\displaystyle \\mathbf {x} _{u}}: σ(⋅){\\displaystyle \\sigma (\\cdot )} is an activation function (e.g.: ReLU): A~{\\displaystyle {\\tilde {\\mathbf {A} }}} is the graph adjacency matrix with the addition of self-loops: D~{\\displaystyle {\\tilde {\\mathbf {D} }}} is the graph degree matrix with the addition of self-loops: and Θ{\\displaystyle \\mathbf {\\Theta } } is a matrix of trainable parameters. In particular: let A{\\displaystyle \\mathbf {A} } be the graph adjacency matrix: then: one can define A~=A+I{\\displaystyle {\\tilde {\\mathbf {A} }}=\\mathbf {A} +\\mathbf {I} } and D~ii=∑j∈VA~ij{\\displaystyle {\\tilde {\\mathbf {D} }}_{ii}=\\sum _{j\\in V}{\\tilde {A}}_{ij}}: where I{\\displaystyle \\mathbf {I} } denotes the identity matrix. This normalization ensures that the eigenvalues of D~−12A~D~−12{\\displaystyle {\\tilde {\\mathbf {D} }}^{-{\\frac {1}{2}}}{\\tilde {\\mathbf {A} }}{\\tilde {\\mathbf {D} }}^{-{\\frac {1}{2}}}} are bounded in the range [0:1]{\\displaystyle [0:1]}: avoiding numerical instabilities and exploding/vanishing gradients. A limitation of GCNs is that they do not allow multidimensional edge features euv{\\displaystyle \\mathbf {e} _{uv}}. It is however possible to associate scalar weights wuv{\\displaystyle w_{uv}} to each edge by imposing Auv=wuv{\\displaystyle A_{uv}=w_{uv}}: i.e.: by setting each nonzero entry in the adjacency matrix equal to the weight of the corresponding edge. === Graph attention network === The graph attention': 'original_similarity': 0.8373300405762928: 'slide': 197}","{'chunk_text': ""Protein folding === Graph neural networks are one of the main building blocks of AlphaFold: an artificial intelligence program developed by Google's DeepMind for solving the protein folding problem in biology. AlphaFold achieved first place in several CASP competitions. === Social networks === Social networks are a major application domain for GNNs due to their natural representation as social graphs. GNNs are used to develop recommender systems based on both social relations and item relations. === Combinatorial optimization === GNNs are used as fundamental building blocks for several combinatorial optimization algorithms. Examples include computing shortest paths or Eulerian circuits for a given graph: deriving chip placements superior or competitive to handcrafted human solutions: and improving expert-designed branching rules in branch and bound. === Cyber security === When viewed as a graph: a network of computers can be analyzed with GNNs for anomaly detection. Anomalies within provenance graphs often correlate to malicious activity within the network. GNNs have been used to identify these anomalies on individual nodes and within paths to detect malicious processes: or on the edge level to detect lateral movement. == References == == External links == https://distill.pub/2021/gnn-intro/\nArtificial neural networks (ANNs) are models created using machine learning to perform a number of tasks. Their creation was inspired by neural circuitry. While some of the computational implementations ANNs relate to earlier"": 'original_similarity': 0.7978212295379054: 'slide': 31}","{'chunk_text': 'machines and Products of Experts. ==== Self-organizing map ==== The self-organizing map (SOM) uses unsupervised learning. A set of neurons learn to map points in an input space to coordinates in an output space. The input space can have different dimensions and topology from the output space: and SOM attempts to preserve these. ==== Learning vector quantization ==== Learning vector quantization (LVQ) can be interpreted as a neural network architecture. Prototypical representatives of the classes parameterize: together with an appropriate distance measure: in a distance-based classification scheme. === Simple recurrent === Simple recurrent networks have three layers: with the addition of a set of ""context units"" in the input layer. These units connect from the hidden layer or the output layer with a fixed weight of one. At each time step: the input is propagated in a standard feedforward fashion: and then a backpropagation-like learning rule is applied (not performing gradient descent). The fixed back connections leave a copy of the previous values of the hidden units in the context units (since they propagate over the connections before the learning rule is applied). === Reservoir computing === Reservoir computing is a computation framework that may be viewed as an extension of neural networks. Typically an input signal is fed into a fixed (random) dynamical system called a reservoir whose dynamics map the input to a higher dimension. A readout mechanism is trained to map the reservoir to the desired output. Training is performed only at the readout stage. Liquid-state machines are a type of reservoir computing. ==== Echo state ==== The echo state network (ESN) employs a sparsely connected random hidden layer. The weights of output neurons are the only part of the network that are trained. ESN are good at reproducing certain time series. === Long short-term memory === The long short-term memory (LSTM) avoids the vanishing gradient problem. It works even when with long delays between inputs and can handle signals that mix low and high frequency components. LSTM RNN outperformed other RNN and other sequence learning methods such as HMM in applications such as language learning and connected handwriting recognition. === Bi-directional === Bi-directional RNN: or BRNN: use a finite sequence to predict or label each element of a sequence based on both the past and future context of the element. This is done by adding the outputs of two RNNs: one processing the sequence from left to right: the other one from right to left. The combined outputs are the predictions of the teacher-given target signals. This technique proved to be especially useful when combined with LSTM. === Hierarchical === Hierarchical RNN connects elements in various ways to decompose hierarchical behavior into useful subprograms. === Stochastic === A district from conventional neural networks: stochastic artificial neural network used as an approximation to random functions. === Genetic Scale === A RNN (often a LSTM) where a series is decomposed into a number of scales where every scale informs the primary length between two consecutive points. A first order scale consists of a normal RNN: a second order consists of all points separated by two indices and so on. The Nth order RNN connects the first and last node. The outputs from all the various scales are treated as a Committee of Machines and the associated scores are used genetically for the next iteration. == Modular == Biological studies have shown that the human brain operates as a collection of small networks. This realization gave birth to the concept of modular neural networks: in which several small networks cooperate or compete to solve problems. === Committee of machines === A committee of machines (CoM) is a collection of different neural networks that together ""vote"" on a given example. This generally gives a much better result than individual networks. Because neural networks suffer from local minima: starting with the same architecture and training but using randomly different initial weights often gives vastly different results. A CoM tends to stabilize the result. The CoM is similar to the general machine learning bagging method: except that the necessary variety of machines in the committee is obtained by training from different starting weights rather than training on different randomly selected subsets of the training data. === Associative === The associative neural network (ASNN) is an extension of committee of machines that combines multiple feedforward neural networks and the k-nearest neighbor technique. It uses the correlation between ensemble responses as a measure of distance amid the analyzed cases for the kNN. This corrects the Bias of the neural network ensemble. An associative neural network has a memory that can coincide with the training set. If new data become available: the network instantly improves its predictive ability and provides data approximation (self-learns) without\nretraining. Another important feature of ASNN is the possibility to interpret neural network results by analysis of correlations between data cases in the space of models. == Physical == A physical neural network includes electrically adjustable resistance material to simulate artificial synapses. Examples include the ADALINE memristor-based neural network. An optical neural network is a physical implementation of an artificial neural network with optical components. == Dynamic == Dynamic neural networks address nonlinear multivariate behaviour and include (learning of) time-dependent behaviour: such as transient phenomena and delay effects. Techniques to estimate a system process from observed data fall under the general category of system identification. === Cascading === Cascade correlation is an architecture and supervised learning algorithm. Instead of just adjusting the weights in a network of fixed topology: Cascade-Correlation begins with a minimal network: then automatically trains and adds new hidden units one by one: creating a multi-layer structure. Once a new hidden unit has been added to the network: its input-side weights are frozen. This unit then becomes a permanent feature-detector in the network: available for producing outputs or for creating other: more complex feature detectors. The Cascade-Correlation architecture has several advantages: It learns quickly: determines its own size and topology: retains the structures it has built even if the': 'original_similarity': 0.7827179528606681: 'slide': 210}"
0.4519232999882661,What are the possible application domains for GNNs?,"GNNs are applicable in various fields such as Natural Language Processing, social networks, citation networks, molecular biology, chemistry, physics, and NP-hard combinatorial optimization problems.","{'chunk_text': ""=== Protein folding === Graph neural networks are one of the main building blocks of AlphaFold: an artificial intelligence program developed by Google's DeepMind for solving the protein folding problem in biology. AlphaFold achieved first place in several CASP competitions. === Social networks === Social networks are a major application domain for GNNs due to their natural representation as social graphs. GNNs are used to develop recommender systems based on both social relations and item relations. === Combinatorial optimization === GNNs are used as fundamental building blocks for several combinatorial optimization algorithms. Examples include computing shortest paths or Eulerian circuits for a given graph: deriving chip placements superior or competitive to handcrafted human solutions: and improving expert-designed branching rules in branch and bound. === Cyber security === When viewed as a graph: a network of computers can be analyzed with GNNs for anomaly detection. Anomalies within provenance graphs often correlate to malicious activity within the network. GNNs have been used to identify these anomalies on individual nodes and within paths to detect malicious processes: or on the edge level to detect lateral movement. == References == == External links == https://distill.pub/2021/gnn-intro/\nArtificial neural networks (ANNs) are models created using machine learning to perform a number of tasks. Their creation was inspired by neural circuitry. While some of the computational implementations ANNs relate to"": 'original_similarity': 0.8671855331787469: 'slide': 30}","{'chunk_text': 'of message passing: started by recursive or convolutional constructive approaches. As of 2022: whether it is possible to define GNN architectures ""going beyond"" message passing: or if every GNN can be built on message passing over suitably defined graphs: is an open research question.Relevant application domains for GNNs include Natural Language Processing: social networks: citation networks: molecular biology: chemistry: physics and NP-hard combinatorial optimization problems.Several open source libraries implementing graph neural networks are available: such as PyTorch Geometric (PyTorch): TensorFlow GNN (TensorFlow): jraph (Google JAX): and GraphNeuralNetworks.jl/GeometricFlux.jl (Julia: Flux). == Architecture == The architecture of a generic GNN implements the following fundamental layers: Permutation equivariant: a permutation equivariant layer maps a representation of a graph into an updated representation of the same graph. In the literature: permutation equivariant layers are implemented via pairwise message passing between graph nodes. Intuitively: in a message passing layer: nodes update their representations by aggregating the messages received from their immediate neighbours. As such: each message passing layer increases the receptive field of the GNN by one hop. Local pooling: a local pooling layer coarsens the graph via downsampling. Local pooling is used to increase the receptive field of a GNN: in a similar fashion to pooling layers in convolutional neural networks. Examples include k-nearest neighbours pooling: top-k pooling: and self-attention pooling. Global pooling: a global pooling layer: also known as readout layer: provides fixed-size representation of the whole graph. The global pooling layer must be permutation invariant: such that permutations in the ordering of graph nodes and edges do not alter the final output. Examples include element-wise sum: mean or maximum.It has been demonstrated that GNNs cannot be more expressive than the Weisfeiler–Leman Graph Isomorphism Test. In practice: this means that there exist different graph structures (e.g.: molecules with the same atoms but different bonds) that cannot be distinguished by GNNs. More powerful GNNs operating on higher-dimension geometries such as simplicial complexes can be designed. As of 2022: whether or not future architectures will overcome the message passing primitive is an open research question. == Message passing layers == Message passing layers are permutation-equivariant layers mapping a graph into an updated representation of the same graph. Formally: they can be expressed as message passing neural networks (MPNNs).Let G=(V:E){\\displaystyle G=(V:E)} be a graph: where V{\\displaystyle V} is the node set and E{\\displaystyle E} is the edge set. Let Nu{\\displaystyle N_{u}} be the neighbourhood of some node u∈V{\\displaystyle u\\in V}. Additionally: let xu{\\displaystyle \\mathbf {x} _{u}} be the features of node u∈V{\\displaystyle u\\in V}: and euv{\\displaystyle \\mathbf {e} _{uv}} be the features of edge (u:v)∈E{\\displaystyle (u:v)\\in E}. An MPNN layer can be expressed as follows: hu=ϕ(xu:⨁v∈Nuψ(xu:xv:euv)){\\displaystyle \\mathbf {h} _{u}=\\phi \\left(\\mathbf {x} _{u}:\\bigoplus _{v\\in N_{u}}\\psi (\\mathbf {x} _{u}:\\mathbf {x} _{v}:\\mathbf {e} _{uv})\\right)}where ϕ{\\displaystyle \\phi } and ψ{\\displaystyle \\psi } are differentiable functions (e.g.: artificial neural networks): and ⨁{\\displaystyle \\bigoplus } is a permutation invariant aggregation operator that can accept an arbitrary number of inputs (e.g.: element-wise sum: mean: or max). In particular: ϕ{\\displaystyle \\phi } and ψ{\\displaystyle \\psi } are referred to as update and message functions: respectively. Intuitively: in an MPNN computational block: graph nodes update their representations by aggregating the messages received from their neighbours. The outputs of one or more MPNN layers are node representations hu{\\displaystyle \\mathbf {h} _{u}} for each node u∈V{\\displaystyle u\\in V} in the graph. Node representations can be employed for any downstream task: such as node/graph classification or edge prediction. Graph nodes in an MPNN update their representation aggregating information from their immediate neighbours. As such: stacking n{\\displaystyle n} MPNN layers means that one node will be able to communicate with nodes that are at most n{\\displaystyle n} ""hops"" away. In principle: to ensure that every node receives information from every other node: one would need to stack a number of MPNN layers equal to the graph diameter. However: stacking many MPNN layers may cause issues such as oversmoothing and oversquashing. Oversmoothing refers to the issue of node representations becoming indistinguishable. Oversquashing refers to the bottleneck that is created by squeezing long-range dependencies into fixed-size representations. Countermeasures such as skip connections (as in residual neural networks): gated update rules and jumping knowledge can mitigate oversmoothing. Modifying the final layer to be a fully-adjacent layer: i.e.: by considering the graph as a complete graph: can mitigate oversquashing in problems where long-range dependencies are required.Other ""flavours"" of MPNN have been developed in the literature: such as graph convolutional networks and graph attention networks: whose definitions can be expressed in terms of the MPNN formalism. === Graph convolutional network === The graph convolutional network (GCN) was first introduced by Thomas Kipf and Max Welling in 2017.A GCN layer defines a first-order approximation of a localized spectral filter on graphs. GCNs can be understood as a generalization of convolutional neural networks to graph-structured data. The formal expression of a GCN layer reads as follows: H=σ(D~−12A~D~−12XΘ){\\displaystyle \\mathbf {H} =\\sigma \\left({\\tilde {\\mathbf {D} }}^{-{\\frac {1}{2}}}{\\tilde {\\mathbf {A} }}{\\tilde {\\mathbf {D} }}^{-{\\frac {1}{2}}}\\mathbf {X} \\mathbf {\\Theta } \\right)}where H{\\displaystyle \\mathbf {H} } is the matrix of node representations hu{\\displaystyle \\mathbf {h} _{u}}: X{\\displaystyle \\mathbf {X}\n} is the matrix of node features xu{\\displaystyle \\mathbf {x} _{u}}: σ(⋅){\\displaystyle \\sigma (\\cdot )} is an activation function (e.g.: ReLU): A~{\\displaystyle {\\tilde {\\mathbf {A} }}} is the graph adjacency matrix with the addition of self-loops: D~{\\displaystyle {\\tilde {\\mathbf {D} }}} is the graph degree matrix with the addition of self-loops: and Θ{\\displaystyle \\mathbf {\\Theta } } is a matrix of trainable parameters. In particular: let A{\\displaystyle \\mathbf {A} } be the graph adjacency matrix: then: one can define A~=A+I{\\displaystyle {\\tilde {\\mathbf {A} }}=\\mathbf {A} +\\mathbf {I} } and D~ii=∑j∈VA~ij{\\displaystyle {\\tilde {\\mathbf {D} }}_{ii}=\\sum _{j\\in V}{\\tilde {A}}_{ij}}: where I{\\displaystyle \\mathbf {I} } denotes the identity matrix. This normalization ensures that the eigenvalues of D~−12A~D~−12{\\displaystyle {\\tilde {\\mathbf {D} }}^{-{\\frac {1}{2}}}{\\tilde {\\mathbf {A} }}{\\tilde {\\mathbf {D} }}^{-{\\frac {1}{2}}}} are bounded in the range [0:1]{\\displaystyle [0:1]}: avoiding numerical instabilities and exploding/vanishing gradients. A limitation of GCNs is that they do not allow multidimensional edge features euv{\\displaystyle': 'original_similarity': 0.8407613209367161: 'slide': 151}","{'chunk_text': 'been summarized since 1991. The debut of DNNs for speaker recognition in the late 1990s and speech recognition around 2009-2011 and of LSTM around 2003–2007: accelerated progress in eight major areas: Scale-up/out and accelerated DNN training and decoding Sequence discriminative training Feature processing by deep models with solid understanding of the underlying mechanisms Adaptation of DNNs and related deep models Multi-task and transfer learning by DNNs and related deep models CNNs and how to design them to best exploit domain knowledge of speech RNN and its rich LSTM variants Other types of deep models including tensor-based models and integrated deep generative/discriminative models.All major commercial speech recognition systems (e.g.: Microsoft Cortana: Xbox: Skype Translator: Amazon Alexa: Google Now: Apple Siri: Baidu and iFlyTek voice search: and a range of Nuance speech products: etc.) are based on deep learning. === Image recognition === A common evaluation set for image classification is the MNIST database data set. MNIST is composed of handwritten digits and includes 60:000 training examples and 10:000 test examples. As with TIMIT: its small size lets users test multiple configurations. A comprehensive list of results on this set is available.Deep learning-based image recognition has become ""superhuman"": producing more accurate results than human contestants. This first occurred in 2011 in recognition of traffic signs: and in 2014: with recognition of human faces.Deep learning-trained vehicles now interpret 360° camera views. Another example is Facial Dysmorphology Novel Analysis (FDNA) used to analyze cases of human malformation connected to a large database of genetic syndromes. === Visual art processing === Closely related to the progress that has been made in image recognition is the increasing application of deep learning techniques to various visual art tasks. DNNs have proven themselves capable: for example: of identifying the style period of a given painting Neural Style Transfer – capturing the style of a given artwork and applying it in a visually pleasing manner to an arbitrary photograph or video generating striking imagery based on random visual input fields. === Natural language processing === Neural networks have been used for implementing language models since the early 2000s. LSTM helped to improve machine translation and language modeling.Other key techniques in this field are negative sampling and word embedding. Word embedding: such as word2vec: can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a vector space. Using word embedding as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar. A compositional vector grammar can be thought of as probabilistic context free grammar (PCFG) implemented by an RNN. Recursive auto-encoders built atop word embeddings can assess sentence similarity and detect paraphrasing. Deep neural architectures provide the best results for constituency parsing: sentiment analysis: information retrieval: spoken language understanding: machine translation: contextual entity linking: writing style recognition: named-entity recognition (token classification): text classification: and others.Recent developments generalize word embedding to sentence embedding. Google Translate (GT) uses a large end-to-end long short-term memory (LSTM) network. Google Neural Machine Translation (GNMT) uses an example-based machine translation method in which the system ""learns from millions of examples"". It translates ""whole sentences at a time: rather than pieces"". Google Translate supports over one hundred languages. The network encodes the ""semantics of the sentence rather than simply memorizing phrase-to-phrase translations"". GT uses English as an intermediate between most language pairs. === Drug discovery and toxicology === A large percentage of candidate drugs fail to win regulatory approval. These failures are caused by insufficient efficacy (on-target effect): undesired interactions (off-target effects): or unanticipated toxic effects. Research has explored use of deep learning to predict the biomolecular targets: off-targets: and toxic effects of environmental chemicals in nutrients: household products and drugs.AtomNet is a deep learning system for structure-based rational drug design. AtomNet was used to predict novel candidate biomolecules for disease targets such as the Ebola virus and multiple sclerosis.In 2017 graph neural networks were used for the first time to predict various properties of molecules in a large toxicology data set. In 2019: generative neural networks were used to produce molecules that were validated experimentally all the way into mice. === Customer relationship management === Deep reinforcement learning has been used to approximate the value of possible direct marketing actions: defined in terms of RFM variables. The estimated value function was shown to have a natural interpretation as customer lifetime value. === Recommendation systems === Recommendation systems have used deep learning to extract meaningful features for a latent factor model for content-based music and journal recommendations. Multi-view deep learning has been applied for learning user preferences from multiple domains. The model uses a hybrid collaborative and content-based approach and enhances recommendations in multiple tasks. === Bioinformatics === An autoencoder ANN was used in bioinformatics: to predict gene ontology annotations and gene-function relationships.In medical informatics: deep learning was used to predict sleep quality based on data\nfrom wearables and predictions of health complications from electronic health record data. === Deep Neural Network Estimations === Deep neural networks can be used to estimate the entropy of a stochastic process and called Neural Joint Entropy Estimator (NJEE). Such an estimation provides insights on the effects of input random variables on an independent random variable. Practically: the DNN is trained as a classifier that maps an input vector or matrix X to an output probability distribution over the possible classes of random variable Y: given input X. For example: in image classification tasks: the NJEE maps a vector of pixels\' color values to probabilities over possible image classes. In practice: the probability distribution of Y is obtained by a Softmax layer with number of nodes that is equal to the alphabet size of Y. NJEE uses continuously differentiable activation functions: such that the conditions for the universal approximation theorem holds. It is shown that this method provides a strongly consistent estimator and outperforms': 'original_similarity': 0.8324049119784593: 'slide': 162}"
0.4418971999839414,What are some examples of libraries implementing graph neural networks?,"Several open-source libraries implement graph neural networks. Some examples include PyTorch Geometric (PyTorch), TensorFlow GNN (TensorFlow), jraph (Google JAX), and GraphNeuralNetworks.jl/GeometricFlux.jl (Julia, Flux).","{'chunk_text': ""main building blocks of AlphaFold: an artificial intelligence program developed by Google's DeepMind for solving the protein folding problem in biology. AlphaFold achieved first place in several CASP competitions. === Social networks === Social networks are a major application domain for GNNs due to their natural representation as social graphs. GNNs are used to develop recommender systems based on both social relations and item relations. === Combinatorial optimization === GNNs are used as fundamental building blocks for several combinatorial optimization algorithms. Examples include computing shortest paths or Eulerian circuits for a given graph: deriving chip placements superior or competitive to handcrafted human solutions: and improving expert-designed branching rules in branch and bound. === Cyber security === When viewed as a graph: a network of computers can be analyzed with GNNs for anomaly detection. Anomalies within provenance graphs often correlate to malicious activity within the network. GNNs have been used to identify these anomalies on individual nodes and within paths to detect malicious processes: or on the edge level to detect lateral movement. == References == == External links == https://distill.pub/2021/gnn-intro/\nArtificial neural networks (ANNs) are models created using machine learning to perform a number of tasks. Their creation was inspired by neural circuitry. While some of the computational implementations ANNs relate to earlier discoveries in mathematics: the first implementation of ANNs was by"": 'original_similarity': 0.8248319758599935: 'slide': 41}","{'chunk_text': 'possible to define GNN architectures ""going beyond"" message passing: or if every GNN can be built on message passing over suitably defined graphs: is an open research question.Relevant application domains for GNNs include Natural Language Processing: social networks: citation networks: molecular biology: chemistry: physics and NP-hard combinatorial optimization problems.Several open source libraries implementing graph neural networks are available: such as PyTorch Geometric (PyTorch): TensorFlow GNN (TensorFlow): jraph (Google JAX): and GraphNeuralNetworks.jl/GeometricFlux.jl (Julia: Flux). == Architecture == The architecture of a generic GNN implements the following fundamental layers: Permutation equivariant: a permutation equivariant layer maps a representation of a graph into an updated representation of the same graph. In the literature: permutation equivariant layers are implemented via pairwise message passing between graph nodes. Intuitively: in a message passing layer: nodes update their representations by aggregating the messages received from their immediate neighbours. As such: each message passing layer increases the receptive field of the GNN by one hop. Local pooling: a local pooling layer coarsens the graph via downsampling. Local pooling is used to increase the receptive field of a GNN: in a similar fashion to pooling layers in convolutional neural networks. Examples include k-nearest neighbours pooling: top-k pooling: and self-attention pooling. Global pooling: a global pooling layer: also known as readout layer: provides fixed-size representation of the whole graph. The global pooling layer must be permutation invariant: such that permutations in the ordering of graph nodes and edges do not alter the final output. Examples include element-wise sum: mean or maximum.It has been demonstrated that GNNs cannot be more expressive than the Weisfeiler–Leman Graph Isomorphism Test. In practice: this means that there exist different graph structures (e.g.: molecules with the same atoms but different bonds) that cannot be distinguished by GNNs. More powerful GNNs operating on higher-dimension geometries such as simplicial complexes can be designed. As of 2022: whether or not future architectures will overcome the message passing primitive is an open research question. == Message passing layers == Message passing layers are permutation-equivariant layers mapping a graph into an updated representation of the same graph. Formally: they can be expressed as message passing neural networks (MPNNs).Let G=(V:E){\\displaystyle G=(V:E)} be a graph: where V{\\displaystyle V} is the node set and E{\\displaystyle E} is the edge set. Let Nu{\\displaystyle N_{u}} be the neighbourhood of some node u∈V{\\displaystyle u\\in V}. Additionally: let xu{\\displaystyle \\mathbf {x} _{u}} be the features of node u∈V{\\displaystyle u\\in V}: and euv{\\displaystyle \\mathbf {e} _{uv}} be the features of edge (u:v)∈E{\\displaystyle (u:v)\\in E}. An MPNN layer can be expressed as follows: hu=ϕ(xu:⨁v∈Nuψ(xu:xv:euv)){\\displaystyle \\mathbf {h} _{u}=\\phi \\left(\\mathbf {x} _{u}:\\bigoplus _{v\\in N_{u}}\\psi (\\mathbf {x} _{u}:\\mathbf {x} _{v}:\\mathbf {e} _{uv})\\right)}where ϕ{\\displaystyle \\phi } and ψ{\\displaystyle \\psi } are differentiable functions (e.g.: artificial neural networks): and ⨁{\\displaystyle \\bigoplus } is a permutation invariant aggregation operator that can accept an arbitrary number of inputs (e.g.: element-wise sum: mean: or max). In particular: ϕ{\\displaystyle \\phi } and ψ{\\displaystyle \\psi } are referred to as update and message functions: respectively. Intuitively: in an MPNN computational block: graph nodes update their representations by aggregating the messages received from their neighbours. The outputs of one or more MPNN layers are node representations hu{\\displaystyle \\mathbf {h} _{u}} for each node u∈V{\\displaystyle u\\in V} in the graph. Node representations can be employed for any downstream task: such as node/graph classification or edge prediction. Graph nodes in an MPNN update their representation aggregating information from their immediate neighbours. As such: stacking n{\\displaystyle n} MPNN layers means that one node will be able to communicate with nodes that are at most n{\\displaystyle n} ""hops"" away. In principle: to ensure that every node receives information from every other node: one would need to stack a number of MPNN layers equal to the graph diameter. However: stacking many MPNN layers may cause issues such as oversmoothing and oversquashing. Oversmoothing refers to the issue of node representations becoming indistinguishable. Oversquashing refers to the bottleneck that is created by squeezing long-range dependencies into fixed-size representations. Countermeasures such as skip connections (as in residual neural networks): gated update rules and jumping knowledge can mitigate oversmoothing. Modifying the final layer to be a fully-adjacent layer: i.e.: by considering the graph as a complete graph: can mitigate oversquashing in problems where long-range dependencies are required.Other ""flavours"" of MPNN have been developed in the literature: such as graph convolutional networks and graph attention networks: whose definitions can be expressed in terms of the MPNN formalism. === Graph convolutional network === The graph convolutional network (GCN) was first introduced by Thomas Kipf and Max Welling in 2017.A GCN layer defines a first-order approximation of a localized spectral filter on graphs. GCNs can be understood as a generalization of convolutional neural networks to graph-structured data. The formal expression of a GCN layer reads as follows: H=σ(D~−12A~D~−12XΘ){\\displaystyle \\mathbf {H} =\\sigma \\left({\\tilde {\\mathbf {D} }}^{-{\\frac {1}{2}}}{\\tilde {\\mathbf {A} }}{\\tilde {\\mathbf {D} }}^{-{\\frac {1}{2}}}\\mathbf {X} \\mathbf {\\Theta } \\right)}where H{\\displaystyle \\mathbf {H} } is the matrix of node representations hu{\\displaystyle \\mathbf {h} _{u}}: X{\\displaystyle \\mathbf {X}\n} is the matrix of node features xu{\\displaystyle \\mathbf {x} _{u}}: σ(⋅){\\displaystyle \\sigma (\\cdot )} is an activation function (e.g.: ReLU): A~{\\displaystyle {\\tilde {\\mathbf {A} }}} is the graph adjacency matrix with the addition of self-loops: D~{\\displaystyle {\\tilde {\\mathbf {D} }}} is the graph degree matrix with the addition of self-loops: and Θ{\\displaystyle \\mathbf {\\Theta } } is a matrix of trainable parameters. In particular: let A{\\displaystyle \\mathbf {A} } be the graph adjacency matrix: then: one can define A~=A+I{\\displaystyle {\\tilde {\\mathbf {A} }}=\\mathbf {A} +\\mathbf {I} } and D~ii=∑j∈VA~ij{\\displaystyle {\\tilde {\\mathbf {D} }}_{ii}=\\sum _{j\\in V}{\\tilde {A}}_{ij}}: where I{\\displaystyle \\mathbf {I} } denotes the identity matrix. This normalization ensures that the eigenvalues of D~−12A~D~−12{\\displaystyle {\\tilde {\\mathbf {D} }}^{-{\\frac {1}{2}}}{\\tilde {\\mathbf {A} }}{\\tilde {\\mathbf {D} }}^{-{\\frac {1}{2}}}} are bounded in the range [0:1]{\\displaystyle [0:1]}: avoiding numerical instabilities and exploding/vanishing gradients. A limitation of GCNs is that they do not allow multidimensional edge features euv{\\displaystyle \\mathbf {e} _{uv}}. It is however possible to associate scalar weights wuv{\\displaystyle w_{uv}} to each edge': 'original_similarity': 0.8125644257519032: 'slide': 167}","{'chunk_text': 'of DNNs for speaker recognition in the late 1990s and speech recognition around 2009-2011 and of LSTM around 2003–2007: accelerated progress in eight major areas: Scale-up/out and accelerated DNN training and decoding Sequence discriminative training Feature processing by deep models with solid understanding of the underlying mechanisms Adaptation of DNNs and related deep models Multi-task and transfer learning by DNNs and related deep models CNNs and how to design them to best exploit domain knowledge of speech RNN and its rich LSTM variants Other types of deep models including tensor-based models and integrated deep generative/discriminative models.All major commercial speech recognition systems (e.g.: Microsoft Cortana: Xbox: Skype Translator: Amazon Alexa: Google Now: Apple Siri: Baidu and iFlyTek voice search: and a range of Nuance speech products: etc.) are based on deep learning. === Image recognition === A common evaluation set for image classification is the MNIST database data set. MNIST is composed of handwritten digits and includes 60:000 training examples and 10:000 test examples. As with TIMIT: its small size lets users test multiple configurations. A comprehensive list of results on this set is available.Deep learning-based image recognition has become ""superhuman"": producing more accurate results than human contestants. This first occurred in 2011 in recognition of traffic signs: and in 2014: with recognition of human faces.Deep learning-trained vehicles now interpret 360° camera views. Another example is Facial Dysmorphology Novel Analysis (FDNA) used to analyze cases of human malformation connected to a large database of genetic syndromes. === Visual art processing === Closely related to the progress that has been made in image recognition is the increasing application of deep learning techniques to various visual art tasks. DNNs have proven themselves capable: for example: of identifying the style period of a given painting Neural Style Transfer – capturing the style of a given artwork and applying it in a visually pleasing manner to an arbitrary photograph or video generating striking imagery based on random visual input fields. === Natural language processing === Neural networks have been used for implementing language models since the early 2000s. LSTM helped to improve machine translation and language modeling.Other key techniques in this field are negative sampling and word embedding. Word embedding: such as word2vec: can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a vector space. Using word embedding as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar. A compositional vector grammar can be thought of as probabilistic context free grammar (PCFG) implemented by an RNN. Recursive auto-encoders built atop word embeddings can assess sentence similarity and detect paraphrasing. Deep neural architectures provide the best results for constituency parsing: sentiment analysis: information retrieval: spoken language understanding: machine translation: contextual entity linking: writing style recognition: named-entity recognition (token classification): text classification: and others.Recent developments generalize word embedding to sentence embedding. Google Translate (GT) uses a large end-to-end long short-term memory (LSTM) network. Google Neural Machine Translation (GNMT) uses an example-based machine translation method in which the system ""learns from millions of examples"". It translates ""whole sentences at a time: rather than pieces"". Google Translate supports over one hundred languages. The network encodes the ""semantics of the sentence rather than simply memorizing phrase-to-phrase translations"". GT uses English as an intermediate between most language pairs. === Drug discovery and toxicology === A large percentage of candidate drugs fail to win regulatory approval. These failures are caused by insufficient efficacy (on-target effect): undesired interactions (off-target effects): or unanticipated toxic effects. Research has explored use of deep learning to predict the biomolecular targets: off-targets: and toxic effects of environmental chemicals in nutrients: household products and drugs.AtomNet is a deep learning system for structure-based rational drug design. AtomNet was used to predict novel candidate biomolecules for disease targets such as the Ebola virus and multiple sclerosis.In 2017 graph neural networks were used for the first time to predict various properties of molecules in a large toxicology data set. In 2019: generative neural networks were used to produce molecules that were validated experimentally all the way into mice. === Customer relationship management === Deep reinforcement learning has been used to approximate the value of possible direct marketing actions: defined in terms of RFM variables. The estimated value function was shown to have a natural interpretation as customer lifetime value. === Recommendation systems === Recommendation systems have used deep learning to extract meaningful features for a latent factor model for content-based music and journal recommendations. Multi-view deep learning has been applied for learning user preferences from multiple domains. The model uses a hybrid collaborative and content-based approach and enhances recommendations in multiple tasks. === Bioinformatics === An autoencoder ANN was used in bioinformatics: to predict gene ontology annotations and gene-function relationships.In medical informatics: deep learning was used to predict sleep quality based on data\nfrom wearables and predictions of health complications from electronic health record data. === Deep Neural Network Estimations === Deep neural networks can be used to estimate the entropy of a stochastic process and called Neural Joint Entropy Estimator (NJEE). Such an estimation provides insights on the effects of input random variables on an independent random variable. Practically: the DNN is trained as a classifier that maps an input vector or matrix X to an output probability distribution over the possible classes of random variable Y: given input X. For example: in image classification tasks: the NJEE maps a vector of pixels\' color values to probabilities over possible image classes. In practice: the probability distribution of Y is obtained by a Softmax layer with number of nodes that is equal to the alphabet size of Y. NJEE uses continuously differentiable activation functions: such that the conditions for the universal approximation theorem holds. It is shown that this method provides a strongly consistent estimator and outperforms other methods in case of large': 'original_similarity': 0.8121758630237926: 'slide': 168}"
0.42725169999175705,Who was behind the first implementation of artificial neural networks (ANNs)?,The first implementation of ANNs was by psychologist Frank Rosenblatt.,"{'chunk_text': 'transformer architecture was first described in 2017 as a method to teach ANNs grammatical dependencies in language: and is the predominant architecture used by large language models: such as GPT-4. Diffusion models were first described in 2015: and began to be used by image generation models such as DALL-E in the 2020s. == Linear neural network == The simplest kind of feedforward neural network is a linear network: which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated in each node. The mean squared errors between these calculated outputs and a given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement. == Perceptrons and other early neural networks == Warren McCulloch and Walter Pitts (1943) also considered a non-learning computational model for neural networks. This model paved the way for research to split into two approaches. One approach focused on biological processes while the other focused on the application of neural networks to artificial intelligence. This work led to work on nerve networks and their link to finite automata.In the early 1940s: D. O. Hebb created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Hebbian learning is unsupervised learning. This evolved into models for long-term potentiation. Researchers started applying these ideas to computational models in 1948 with Turing\'s B-type machines. Farley and Clark (1954) first used computational machines: then called ""calculators"": to simulate a Hebbian network. Other neural network computational machines were created by Rochester: Holland: Habit and Duda (1956).Rosenblatt (1958) created the perceptron: an algorithm for pattern recognition. With mathematical notation: Rosenblatt described circuitry not in the basic perceptron: such as the exclusive-or circuit that could not be processed by neural networks at the time. In 1959: a biological model proposed by Nobel laureates Hubel and Wiesel was based on their discovery of two types of cells in the primary visual cortex: simple cells and complex cells.Some say that research stagnated following Minsky and Papert (1969): who discovered that basic perceptrons were incapable of processing the exclusive-or circuit and that computers lacked sufficient power to process useful neural networks. However: by the time this book came out: methods for training multilayer perceptrons (MLPs) by deep learning were already known. == First deep learning == The first deep learning MLP was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa in 1965: as the Group Method of Data Handling. This method employs incremental layer by layer training based on regression analysis: where useless units in hidden layers are pruned with the help of a validation set. The first deep learning MLP trained by stochastic gradient descent was published in 1967 by Shun\'ichi Amari. In computer experiments conducted by Amari\'s student Saito: a five layer MLP with two modifiable layers learned useful internal representations to classify non-linearily separable pattern classes. == Backpropagation == The backpropagation algorithm is an efficient application of the Leibniz chain rule (1673) to networks of differentiable nodes. It is also known as the reverse mode of automatic differentiation or reverse accumulation: due to Seppo Linnainmaa (1970). The term ""back-propagating errors"" was introduced in 1962 by Frank Rosenblatt: but he did not have an implementation of this procedure: although Henry J. Kelley had a continuous precursor of backpropagation already in 1960 in the context of control theory. In 1982: Paul Werbos applied backpropagation to MLPs in the way that has become standard. In 1986: David E. Rumelhart et al. published an experimental analysis of the technique. == Recurrent network architectures == Wilhelm Lenz and Ernst Ising created and analyzed the Ising model (1925) which is essentially a non-learning artificial recurrent neural network (RNN) consisting of neuron-like threshold elements. In 1972: Shun\'ichi Amari made this architecture adaptive. His learning RNN was popularised by John Hopfield in 1982. == Self-organizing maps == Self-organizing maps (SOMs) were described by Teuvo Kohonen in 1982. SOMs are neurophysiologically inspired artificial neural networks that learn low-dimensional representations of high-dimensional data while preserving the topological structure of the data. They are trained using competitive learning. SOMs create internal representations reminiscent of the cortical homunculus: a distorted representation of the human body: based on a neurological ""map"" of the areas and proportions of the human brain dedicated to processing sensory functions: for different parts of the body. == Convolutional neural networks (CNNs) == The origin of the CNN architecture is the ""neocognitron"" introduced by Kunihiko Fukushima in 1980. It was inspired by work of Hubel and Wiesel in the 1950s and 1960s which showed that cat visual cortices contain neurons that individually respond to small regions of the visual field. The neocognitron introduced the two basic types of layers in CNNs: convolutional layers: and downsampling layers. A convolutional layer contains units whose receptive fields cover a patch of\nthe previous layer. The weight vector (the set of adaptive parameters) of such a unit is often called a filter. Units can share filters. Downsampling layers contain units whose receptive fields cover patches of previous convolutional layers. Such a unit typically computes the average of the activations of the units in its patch. This downsampling helps to correctly classify objects in visual scenes even when the objects are shifted. In 1969: Kunihiko Fukushima also introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for CNNs and deep neural networks in general.The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel and was one of the first CNNs: as it achieved shift invariance. It did so by utilizing weight sharing in combination with backpropagation training. Thus: while': 'original_similarity': 0.8784568097060195: 'slide': 135}","{'chunk_text': 'multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least 2 hidden layers.Artificial neural networks are used for predictive modeling: adaptive control: and other applications where they can be trained via a dataset. They are also used to solve problems in artificial intelligence. Networks can learn from experience: and can derive conclusions from a complex and seemingly unrelated set of information. == Training == Neural networks are typically trained through empirical risk minimization. This method is based on the idea of optimizing the network\'s parameters to minimize the difference: or empirical risk: between the predicted output and the actual target values in a given dataset. Gradient based methods such as backpropagation are usually used to estimate the parameters of the network. During the training phase: ANNs learn from labeled training data by iteratively updating their parameters to minimize a defined loss function. This method allows the network to generalize to unseen data. == History == Historically: digital computers evolved from the von Neumann model: and operate via the execution of explicit instructions via access to memory by a number of processors. Neural networks: on the other hand: originated from efforts to model information processing in biological systems through the framework of connectionism. Unlike the von Neumann model: connectionist computing does not separate memory and processing. The simplest kind of feedforward neural network (FNN) is a linear network: which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated at each node. The mean squared errors between these calculated outputs and the given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement.Warren McCulloch and Walter Pitts (1943) also considered a non-learning computational model for neural networks.In the late 1940s: D. O. Hebb created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Hebbian learning is considered to be a \'typical\' unsupervised learning rule and its later variants were early models for long term potentiation. These ideas started being applied to computational models in 1948 with Turing\'s ""unorganized machines"". Farley and Wesley A. Clark were the first to simulate a Hebbian network in 1954 at MIT. They used computational machines: then called ""calculators"". Other neural network computational machines were created by Rochester: Holland: Habit: and Duda in 1956. In 1958: psychologist Frank Rosenblatt invented the perceptron: the first implemented artificial neural network: funded by the United States Office of Naval Research. The invention of the perceptron raised public excitement for research in Artificial Neural Networks: causing the US government to drastically increase funding into deep learning research. This led to ""the golden age of AI"" fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence. For example: in 1957 Herbert Simon famously said:It is not my aim to surprise or shock you—but the simplest way I can summarize is to say that there are now in the world machines that think: that learn and that create. Moreover: their ability to do these things is going to increase rapidly until—in a visible future—the range of problems they can handle will be coextensive with the range to which the human mind has been applied.However: this wasn\'t the case as research stagnated in the United States following the work of Minsky and Papert (1969): who discovered that basic perceptrons were incapable of processing the exclusive-or circuit and that computers lacked sufficient power to train useful neural networks. This: along with other factors such as the 1973 Lighthill report by James Lighthill stating that research in Artificial Intelligence has not ""produced the major impact that was then promised:"" shutting funding in research into the field of AI in all but two universities in the UK and in many major institutions across the world. This ushered an era called the AI Winter with reduced research into connectionism due to a decrease in government funding and an increased stress on symbolic artificial intelligence in the United States and other Western countries.During the AI Winter era: however: research outside the United States continued: especially in Eastern Europe. By the time Minsky and Papert\'s book on Perceptrons came out: methods for training multilayer perceptrons (MLPs) were already known. The first deep learning MLP was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa in 1965: as the Group Method of Data Handling. The first deep learning MLP trained by stochastic gradient descent was published in 1967 by Shun\'ichi Amari. In computer experiments conducted by Amari\'s student Saito: a five layer MLP with\ntwo modifiable layers learned useful internal representations to classify non-linearily separable pattern classes.Self-organizing maps (SOMs) were described by Teuvo Kohonen in 1982. SOMs are neurophysiologically inspired neural networks that learn low-dimensional representations of high-dimensional data while preserving the topological structure of the data. They are trained using competitive learning.The convolutional neural network (CNN) architecture with convolutional layers and downsampling layers was introduced by Kunihiko Fukushima in 1980. He called it the neocognitron. In 1969: he also introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for CNNs and deep neural networks in general. CNNs have become an essential tool for computer vision. A key in later advances in artificial neural network research was the backpropagation algorithm: an efficient application of the Leibniz chain rule (1673) to networks of differentiable nodes. It is also known as the reverse mode of automatic differentiation or reverse accumulation: due to Seppo Linnainmaa (1970). The term ""back-propagating errors"" was introduced in 1962 by Frank Rosenblatt: but he did not': 'original_similarity': 0.8609752335629591: 'slide': 170}","{'chunk_text': 'ANN Next Generation of Neural Networks - Google Tech Talks Performance of Neural Networks Neural Networks and Information Sanderson G (5 October 2017). ""But what is a Neural Network?"". 3Blue1Brown. Archived from the original on 7 November 2021 – via YouTube. == Notes == == References == == Bibliography ==\nA neural network is a group of interconnected units': 'original_similarity': 0.8481905733738857: 'slide': 7}"
0.43296610002289526,What is the AlexNet and what is its significance in the field of artificial neural networks?,"AlexNet is a deep neural network developed in the 2010s. It greatly outperformed other image recognition models and is thought to have launched the ongoing AI spring, contributing to further increased interest in ANNs.","{'chunk_text': 'the predominant architecture used by large language models: such as GPT-4. Diffusion models were first described in 2015: and began to be used by image generation models such as DALL-E in the 2020s. == Linear neural network == The simplest kind of feedforward neural network is a linear network: which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated in each node. The mean squared errors between these calculated outputs and a given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement. == Perceptrons and other early neural networks == Warren McCulloch and Walter Pitts (1943) also considered a non-learning computational model for neural networks. This model paved the way for research to split into two approaches. One approach focused on biological processes while the other focused on the application of neural networks to artificial intelligence. This work led to work on nerve networks and their link to finite automata.In the early 1940s: D. O. Hebb created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Hebbian learning is unsupervised learning. This evolved into models for long-term potentiation. Researchers started applying these ideas to computational models in 1948 with Turing\'s B-type machines. Farley and Clark (1954) first used computational machines: then called ""calculators"": to simulate a Hebbian network. Other neural network computational machines were created by Rochester: Holland: Habit and Duda (1956).Rosenblatt (1958) created the perceptron: an algorithm for pattern recognition. With mathematical notation: Rosenblatt described circuitry not in the basic perceptron: such as the exclusive-or circuit that could not be processed by neural networks at the time. In 1959: a biological model proposed by Nobel laureates Hubel and Wiesel was based on their discovery of two types of cells in the primary visual cortex: simple cells and complex cells.Some say that research stagnated following Minsky and Papert (1969): who discovered that basic perceptrons were incapable of processing the exclusive-or circuit and that computers lacked sufficient power to process useful neural networks. However: by the time this book came out: methods for training multilayer perceptrons (MLPs) by deep learning were already known. == First deep learning == The first deep learning MLP was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa in 1965: as the Group Method of Data Handling. This method employs incremental layer by layer training based on regression analysis: where useless units in hidden layers are pruned with the help of a validation set. The first deep learning MLP trained by stochastic gradient descent was published in 1967 by Shun\'ichi Amari. In computer experiments conducted by Amari\'s student Saito: a five layer MLP with two modifiable layers learned useful internal representations to classify non-linearily separable pattern classes. == Backpropagation == The backpropagation algorithm is an efficient application of the Leibniz chain rule (1673) to networks of differentiable nodes. It is also known as the reverse mode of automatic differentiation or reverse accumulation: due to Seppo Linnainmaa (1970). The term ""back-propagating errors"" was introduced in 1962 by Frank Rosenblatt: but he did not have an implementation of this procedure: although Henry J. Kelley had a continuous precursor of backpropagation already in 1960 in the context of control theory. In 1982: Paul Werbos applied backpropagation to MLPs in the way that has become standard. In 1986: David E. Rumelhart et al. published an experimental analysis of the technique. == Recurrent network architectures == Wilhelm Lenz and Ernst Ising created and analyzed the Ising model (1925) which is essentially a non-learning artificial recurrent neural network (RNN) consisting of neuron-like threshold elements. In 1972: Shun\'ichi Amari made this architecture adaptive. His learning RNN was popularised by John Hopfield in 1982. == Self-organizing maps == Self-organizing maps (SOMs) were described by Teuvo Kohonen in 1982. SOMs are neurophysiologically inspired artificial neural networks that learn low-dimensional representations of high-dimensional data while preserving the topological structure of the data. They are trained using competitive learning. SOMs create internal representations reminiscent of the cortical homunculus: a distorted representation of the human body: based on a neurological ""map"" of the areas and proportions of the human brain dedicated to processing sensory functions: for different parts of the body. == Convolutional neural networks (CNNs) == The origin of the CNN architecture is the ""neocognitron"" introduced by Kunihiko Fukushima in 1980. It was inspired by work of Hubel and Wiesel in the 1950s and 1960s which showed that cat visual cortices contain neurons that individually respond to small regions of the visual field. The neocognitron introduced the two basic types of layers in CNNs: convolutional layers: and downsampling layers. A convolutional layer contains units whose receptive fields cover a patch of\nthe previous layer. The weight vector (the set of adaptive parameters) of such a unit is often called a filter. Units can share filters. Downsampling layers contain units whose receptive fields cover patches of previous convolutional layers. Such a unit typically computes the average of the activations of the units in its patch. This downsampling helps to correctly classify objects in visual scenes even when the objects are shifted. In 1969: Kunihiko Fukushima also introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for CNNs and deep neural networks in general.The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel and was one of the first CNNs: as it achieved shift invariance. It did so by utilizing weight sharing in combination with backpropagation training. Thus: while also using a pyramidal structure as in the neocognitron: it performed a global optimization of the weights instead of': 'original_similarity': 0.8398130737457206: 'slide': 154}","{'chunk_text': 'possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least 2 hidden layers.Artificial neural networks are used for predictive modeling: adaptive control: and other applications where they can be trained via a dataset. They are also used to solve problems in artificial intelligence. Networks can learn from experience: and can derive conclusions from a complex and seemingly unrelated set of information. == Training == Neural networks are typically trained through empirical risk minimization. This method is based on the idea of optimizing the network\'s parameters to minimize the difference: or empirical risk: between the predicted output and the actual target values in a given dataset. Gradient based methods such as backpropagation are usually used to estimate the parameters of the network. During the training phase: ANNs learn from labeled training data by iteratively updating their parameters to minimize a defined loss function. This method allows the network to generalize to unseen data. == History == Historically: digital computers evolved from the von Neumann model: and operate via the execution of explicit instructions via access to memory by a number of processors. Neural networks: on the other hand: originated from efforts to model information processing in biological systems through the framework of connectionism. Unlike the von Neumann model: connectionist computing does not separate memory and processing. The simplest kind of feedforward neural network (FNN) is a linear network: which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated at each node. The mean squared errors between these calculated outputs and the given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement.Warren McCulloch and Walter Pitts (1943) also considered a non-learning computational model for neural networks.In the late 1940s: D. O. Hebb created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Hebbian learning is considered to be a \'typical\' unsupervised learning rule and its later variants were early models for long term potentiation. These ideas started being applied to computational models in 1948 with Turing\'s ""unorganized machines"". Farley and Wesley A. Clark were the first to simulate a Hebbian network in 1954 at MIT. They used computational machines: then called ""calculators"". Other neural network computational machines were created by Rochester: Holland: Habit: and Duda in 1956. In 1958: psychologist Frank Rosenblatt invented the perceptron: the first implemented artificial neural network: funded by the United States Office of Naval Research. The invention of the perceptron raised public excitement for research in Artificial Neural Networks: causing the US government to drastically increase funding into deep learning research. This led to ""the golden age of AI"" fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence. For example: in 1957 Herbert Simon famously said:It is not my aim to surprise or shock you—but the simplest way I can summarize is to say that there are now in the world machines that think: that learn and that create. Moreover: their ability to do these things is going to increase rapidly until—in a visible future—the range of problems they can handle will be coextensive with the range to which the human mind has been applied.However: this wasn\'t the case as research stagnated in the United States following the work of Minsky and Papert (1969): who discovered that basic perceptrons were incapable of processing the exclusive-or circuit and that computers lacked sufficient power to train useful neural networks. This: along with other factors such as the 1973 Lighthill report by James Lighthill stating that research in Artificial Intelligence has not ""produced the major impact that was then promised:"" shutting funding in research into the field of AI in all but two universities in the UK and in many major institutions across the world. This ushered an era called the AI Winter with reduced research into connectionism due to a decrease in government funding and an increased stress on symbolic artificial intelligence in the United States and other Western countries.During the AI Winter era: however: research outside the United States continued: especially in Eastern Europe. By the time Minsky and Papert\'s book on Perceptrons came out: methods for training multilayer perceptrons (MLPs) were already known. The first deep learning MLP was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa in 1965: as the Group Method of Data Handling. The first deep learning MLP trained by stochastic gradient descent was published in 1967 by Shun\'ichi Amari. In computer experiments conducted by Amari\'s student Saito: a five layer MLP with\ntwo modifiable layers learned useful internal representations to classify non-linearily separable pattern classes.Self-organizing maps (SOMs) were described by Teuvo Kohonen in 1982. SOMs are neurophysiologically inspired neural networks that learn low-dimensional representations of high-dimensional data while preserving the topological structure of the data. They are trained using competitive learning.The convolutional neural network (CNN) architecture with convolutional layers and downsampling layers was introduced by Kunihiko Fukushima in 1980. He called it the neocognitron. In 1969: he also introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for CNNs and deep neural networks in general. CNNs have become an essential tool for computer vision. A key in later advances in artificial neural network research was the backpropagation algorithm: an efficient application of the Leibniz chain rule (1673) to networks of differentiable nodes. It is also known as the reverse mode of automatic differentiation or reverse accumulation: due to Seppo Linnainmaa (1970). The term ""back-propagating errors"" was introduced in 1962 by Frank Rosenblatt: but': 'original_similarity': 0.8379147731360196: 'slide': 167}","{'chunk_text': 'to translate between more than 100 languages. In 2017: Covariant.ai was launched: which focuses on integrating deep learning into factories.As of 2008: researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement: or TAMER: which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor. First developed as TAMER: a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot with the ability to learn new tasks through observation. Using Deep TAMER: a robot learned a task with a human trainer: watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer: who provided feedback such as ""good job"" and ""bad job"". == Criticism and comment == Deep learning has attracted both criticism and comment: in some cases from outside the field of computer science. === Theory === A main criticism concerns the lack of theory surrounding some methods. Learning in the most common deep architectures is implemented using well-understood gradient descent. However: the theory surrounding other algorithms: such as contrastive divergence is less clear. (e.g.: Does it converge? If so: how fast? What is it approximating?) Deep learning methods are often looked at as a black box: with most confirmations done empirically: rather than theoretically.Others point out that deep learning should be looked at as a step towards realizing strong AI: not as an all-encompassing solution. Despite the power of deep learning methods: they still lack much of the functionality needed to realize this goal entirely. Research psychologist Gary Marcus noted: Realistically: deep learning is only part of the larger challenge of building intelligent machines. Such techniques lack ways of representing causal relationships (...) have no obvious ways of performing logical inferences: and they are also still a long way from integrating abstract knowledge: such as information about what objects are: what they are for: and how they are typically used. The most powerful A.I. systems: like Watson (...) use techniques like deep learning as just one element in a very complicated ensemble of techniques: ranging from the statistical technique of Bayesian inference to deductive reasoning. In further reference to the idea that artistic sensitivity might be inherent in relatively low levels of the cognitive hierarchy: a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained demonstrate a visual appeal: the original research notice received well over 1:000 comments: and was the subject of what was for a time the most frequently accessed article on The Guardian\'s website. === Errors === Some deep learning architectures display problematic behaviors: such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images (2014) and misclassifying minuscule perturbations of correctly classified images (2013). Goertzel hypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-component artificial general intelligence (AGI) architectures. These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar decompositions of observed entities and events. Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic goal of both human language acquisition and artificial intelligence (AI). === Cyber threat === As deep learning moves from the lab into the world: research and experience show that artificial neural networks are vulnerable to hacks and deception. By identifying patterns that these systems use to function: attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize. For example: an attacker can make subtle changes to an image such that the ANN finds a match even though the image looks to a human nothing like the search target. Such manipulation is termed an ""adversarial attack"".In 2016 researchers used one ANN to doctor images in trial and error fashion: identify another\'s focal points: and thereby generate images that deceived it. The modified images looked no different to human eyes. Another group showed that printouts of doctored images then photographed successfully tricked an image classification system. One defense is reverse image search: in which a possible fake image is submitted to a site such as TinEye that can then find other instances of it. A refinement is to search using only parts of the image: to identify images from which that piece may have been taken.Another group showed that certain psychedelic spectacles could fool a facial recognition system into thinking ordinary people were celebrities: potentially allowing one person to impersonate another. In 2017 researchers added stickers to stop signs and caused an ANN to misclassify them.ANNs\ncan however be further trained to detect attempts at deception: potentially leading attackers and defenders into an arms race similar to the kind that already defines the malware defense industry. ANNs have been trained to defeat ANN-based anti-malware software by repeatedly attacking a defense with malware that was continually altered by a genetic algorithm until it tricked the anti-malware while retaining its ability to damage the target.In 2016: another group demonstrated that certain sounds could make the Google Now voice command system open a particular web address: and hypothesized that this could ""serve as a stepping stone for further attacks (e.g.: opening a web page hosting drive-by malware)"".In ""data poisoning"": false data is continually smuggled into a machine learning system\'s training set to prevent it from achieving mastery. === Data collection ethics === Most Deep Learning systems rely on training and verification data that is generated and/or annotated by humans. It has been argued in media philosophy that not': 'original_similarity': 0.8357930171107709: 'slide': 157}"
0.8425000999995973,How does a linear neural network work?,A linear network consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated in each node. The mean squared errors between these calculated outputs and a given target values are minimized by creating an adjustment to the weights.,"{'chunk_text': 'network: which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated in each node. The mean squared errors between these calculated outputs and a given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement. === Perceptron === If using a threshold: i.e. a linear activation function: the resulting linear threshold unit is called a perceptron. (Often the term is used to denote just one of these units.) Multiple parallel linear units are able to approximate any continuous function from a compact interval of the real numbers into the interval [−1:1] despite the limited computational power of single unit with a linear threshold function. This result can be found in Peter Auer: Harald Burgsteiner and Wolfgang Maass ""A learning rule for very simple universal approximators consisting of a single layer of perceptrons"".Perceptrons can be trained by a simple learning algorithm that is usually called the delta rule. It calculates the errors between calculated output and sample output data: and uses this to create an adjustment to the weights: thus implementing a form of gradient descent. === Multilayer perceptron === A multilayer perceptron (MLP) is a misnomer for a modern feedforward artificial neural network: consisting of fully connected neurons with a nonlinear kind of activation function: organized in at least three layers: notable for being able to distinguish data that is not linearly separable. It is a misnomer because the original perceptron used a Heaviside step function: instead of a nonlinear kind of activation function (used by modern networks). == Other feedforward networks == Examples of other feedforward networks include convolutional neural networks and radial basis function networks: which use a different activation function. == See also == Hopfield network Feed-forward Backpropagation Rprop == References == == External links == Feedforward neural networks tutorial Feedforward Neural Network: Example Feedforward Neural Networks: An Introduction\nGemini is a family of multimodal large language models': 'original_similarity': 0.852130693167664: 'slide': 7}","{'chunk_text': 'The term (i.e. ""back-propagating errors"") itself has been used by Rosenblatt himself: but he did not know how to implement it: although a continuous precursor of backpropagation was already used in the context of control theory in 1960 by Henry J. Kelley. It is known also as a reverse mode of automatic differentiation.In 1982: backpropagation was applied in the way that has become standard: for the first time by Paul Werbos.In 1985: an experimental analysis of the technique was conducted by David E. Rumelhart et al.. Many improvements to the approach have been made in subsequent decades.In 1987: using a stochastic gradient descent within a (wide 12-layer nonlinear) feed-forward network: Matthew Brand has trained it to reproduce logic functions of nontrivial circuit depth: using small batches of random input/output samples. He: however: concluded that on hardware (sub-megaflop computers) available at the time it was impractical: and proposed using fixed random early layers as an input hash for a single modifiable layer.In 1990s: an (much simpler) alternative to using neural networks: although still related support vector machine approach was developed by Vladimir Vapnik and his colleagues. In addition to performing linear classification: they were able to efficiently perform a non-linear classification using what is called the kernel trick: using high-dimensional feature spaces.In 2003: interest in backpropagation networks returned due to the successes of deep learning being applied to language modelling by Yoshua Bengio with co-authors.In 2017: modern transformer architectures were introduced. == Mathematical foundations == === Activation function === The two historically common activation functions are both sigmoids: and are described by y(vi)=tanh\u2061(vi) and y(vi)=(1+e−vi)−1{\\displaystyle y(v_{i})=\\tanh(v_{i})~~{\\textrm {and}}~~y(v_{i})=(1+e^{-v_{i}})^{-1}}.The first is a hyperbolic tangent that ranges from -1 to 1: while the other is the logistic function: which is similar in shape but ranges from 0 to 1. Here yi{\\displaystyle y_{i}} is the output of the i{\\displaystyle i}th node (neuron) and vi{\\displaystyle v_{i}} is the weighted sum of the input connections. Alternative activation functions have been proposed: including the rectifier and softplus functions. More specialized activation functions include radial basis functions (used in radial basis networks: another class of supervised neural network models). In recent developments of deep learning the rectified linear unit (ReLU) is more frequently used as one of the possible ways to overcome the numerical problems related to the sigmoids. === Learning === Learning occurs by changing connection weights after each piece of data is processed: based on the amount of error in the output compared to the expected result. This is an example of supervised learning: and is carried out through backpropagation. We can represent the degree of error in an output node j{\\displaystyle j} in the n{\\displaystyle n}th data point (training example) by ej(n)=dj(n)−yj(n){\\displaystyle e_{j}(n)=d_{j}(n)-y_{j}(n)}: where dj(n){\\displaystyle d_{j}(n)} is the desired target value for n{\\displaystyle n}th data point at node j{\\displaystyle j}: and yj(n){\\displaystyle y_{j}(n)} is the value produced at node j{\\displaystyle j} when the n{\\displaystyle n}th data point is given as an input. The node weights can then be adjusted based on corrections that minimize the error in the entire output for the n{\\displaystyle n}th data point: given by E(n)=12∑output node jej2(n){\\displaystyle {\\mathcal {E}}(n)={\\frac {1}{2}}\\sum _{{\\text{output node }}j}e_{j}^{2}(n)}.Using gradient descent: the change in each weight wij{\\displaystyle w_{ij}} is Δwji(n)=−η∂E(n)∂vj(n)yi(n){\\displaystyle \\Delta w_{ji}(n)=-\\eta {\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}y_{i}(n)}where yi(n){\\displaystyle y_{i}(n)} is the output of the previous neuron i{\\displaystyle i}: and η{\\displaystyle \\eta } is the learning rate: which is selected to ensure that the weights quickly converge to a response: without oscillations. In the previous expression: ∂E(n)∂vj(n){\\displaystyle {\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}} denotes the partial derivate of the error E(n){\\displaystyle {\\mathcal {E}}(n)} according to the weighted sum vj(n){\\displaystyle v_{j}(n)} of the input connections of neuron i{\\displaystyle i}. The derivative to be calculated depends on the induced local field vj{\\displaystyle v_{j}}: which itself varies. It is easy to prove that for an output node this derivative can be simplified to −∂E(n)∂vj(n)=ej(n)ϕ′(vj(n)){\\displaystyle -{\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}=e_{j}(n)\\phi ^{\\prime }(v_{j}(n))}where ϕ′{\\displaystyle \\phi ^{\\prime }} is the derivative of the activation function described above: which itself does not vary. The analysis is more difficult for the change in weights to a hidden node: but it can be shown that the relevant derivative is −∂E(n)∂vj(n)=ϕ′(vj(n))∑k−∂E(n)∂vk(n)wkj(n){\\displaystyle -{\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}=\\phi ^{\\prime }(v_{j}(n))\\sum _{k}-{\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{k}(n)}}w_{kj}(n)}.This depends on the change in weights of the k{\\displaystyle k}th nodes: which represent the output layer. So to change the hidden layer weights: the output layer weights change according to the derivative of the activation function: and so this algorithm represents a backpropagation of the activation function. == History == === Linear neural network === The simplest\nkind of feedforward neural network is a linear network: which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated in each node. The mean squared errors between these calculated outputs and a given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement. === Perceptron === If using a threshold: i.e. a linear activation function: the resulting linear threshold unit is called a perceptron. (Often the term is used to denote just one of these units.) Multiple parallel linear units are able to approximate any continuous function from a compact interval of the real numbers into the interval [−1:1] despite the limited computational power of single unit with a linear threshold function. This result can be found in Peter Auer: Harald Burgsteiner and Wolfgang Maass ""A learning rule for very simple universal approximators consisting of a single layer of perceptrons"".Perceptrons can be trained by a simple learning algorithm that is usually called the delta rule. It calculates the errors between calculated output and sample output data: and': 'original_similarity': 0.8344956549709811: 'slide': 239}","{'chunk_text': ""Feedforward networks can be constructed with various types of units: such as binary McCulloch–Pitts neurons: the simplest of which is the perceptron. Continuous neurons: frequently with sigmoidal activation: are used in the context of backpropagation. === Group method of data handling === The Group Method of Data Handling (GMDH) features fully automatic structural and parametric model optimization. The node activation functions are Kolmogorov–Gabor polynomials that permit additions and multiplications. It uses a deep multilayer perceptron with eight layers. It is a supervised learning network that grows layer by layer: where each layer is trained by regression analysis. Useless items are detected using a validation set: and pruned through regularization. The size and depth of the resulting network depends on the task. === Autoencoder === An autoencoder: autoassociator or Diabolo network: 19 is similar to the multilayer perceptron (MLP) – with an input layer: an output layer and one or more hidden layers connecting them. However: the output layer has the same number of units as the input layer. Its purpose is to reconstruct its own inputs (instead of emitting a target value). Therefore: autoencoders are unsupervised learning models. An autoencoder is used for unsupervised learning of efficient codings: typically for the purpose of dimensionality reduction and for learning generative models of data. === Probabilistic === A probabilistic neural network (PNN) is a four-layer feedforward neural network. The layers are Input: hidden pattern/summation: and output. In the PNN algorithm: the parent probability distribution function (PDF) of each class is approximated by a Parzen window and a non-parametric function. Then: using PDF of each class: the class probability of a new input is estimated and Bayes’ rule is employed to allocate it to the class with the highest posterior probability. It was derived from the Bayesian network and a statistical algorithm called Kernel Fisher discriminant analysis. It is used for classification and pattern recognition. === Time delay === A time delay neural network (TDNN) is a feedforward architecture for sequential data that recognizes features independent of sequence position. In order to achieve time-shift invariance: delays are added to the input so that multiple data points (points in time) are analyzed together. It usually forms part of a larger pattern recognition system. It has been implemented using a perceptron network whose connection weights were trained with back propagation (supervised learning). === Convolutional === A convolutional neural network (CNN: or ConvNet or shift invariant or space invariant) is a class of deep network: composed of one or more convolutional layers with fully connected layers (matching those in typical ANNs) on top. It uses tied weights and pooling layers. In particular: max-pooling. It is often structured via Fukushima's convolutional architecture. They are variations of multilayer perceptrons that use minimal preprocessing. This architecture allows CNNs to take advantage of the 2D structure of input data. Its unit connectivity pattern is inspired by the organization of the visual cortex. Units respond to stimuli in a restricted region of space known as the receptive field. Receptive fields partially overlap: over-covering the entire visual field. Unit response can be approximated mathematically by a convolution operation.CNNs are suitable for processing visual and other two-dimensional data. They have shown superior results in both image and speech applications. They can be trained with standard backpropagation. CNNs are easier to train than other regular: deep: feed-forward neural networks and have many fewer parameters to estimate.Capsule Neural Networks (CapsNet) add structures called capsules to a CNN and reuse output from several capsules to form more stable (with respect to various perturbations) representations.Examples of applications in computer vision include DeepDream and robot navigation. They have wide applications in image and video recognition: recommender systems and natural language processing. === Deep stacking network === A deep stacking network (DSN) (deep convex network) is based on a hierarchy of blocks of simplified neural network modules. It was introduced in 2011 by Deng and Yu. It formulates the learning as a convex optimization problem with a closed-form solution: emphasizing the mechanism's similarity to stacked generalization. Each DSN block is a simple module that is easy to train by itself in a supervised fashion without backpropagation for the entire blocks.Each block consists of a simplified multi-layer perceptron (MLP) with a single hidden layer. The hidden layer h has logistic sigmoidal units: and the output layer has linear units. Connections between these layers are represented by weight matrix U; input-to-hidden-layer connections have weight matrix W. Target vectors t form the columns of matrix T: and the input data vectors x form the columns of matrix X. The matrix of hidden units is H=σ(WTX){\\displaystyle {\\boldsymbol {H}}=\\sigma ({\\boldsymbol {W}}^{T}{\\boldsymbol {X}})}. Modules are trained in order: so lower-layer weights W are known at each stage. The function performs the element-wise logistic sigmoid operation. Each block estimates the same final label class y: and its estimate is concatenated with original input X to form the\nexpanded input for the next block. Thus: the input to the first block contains the original data only: while downstream blocks' input adds the output of preceding blocks. Then learning the upper-layer weight matrix U given other weights in the network can be formulated as a convex optimization problem: minUTf=‖UTH−T‖F2:{\\displaystyle \\min _{U^{T}}f=\\|{\\boldsymbol {U}}^{T}{\\boldsymbol {H}}-{\\boldsymbol {T}}\\|_{F}^{2}:}which has a closed-form solution.Unlike other deep architectures: such as DBNs: the goal is not to discover the transformed feature representation. The structure of the hierarchy of this kind of architecture makes parallel learning straightforward: as a batch-mode optimization problem. In purely discriminative tasks: DSNs outperform conventional DBNs. ==== Tensor deep stacking networks ==== This architecture is a DSN extension. It offers two important improvements: it uses higher-order information from covariance statistics: and it transforms the non-convex problem of a lower-layer to a convex sub-problem of an upper-layer. TDSNs use covariance statistics in a bilinear mapping from each of two distinct sets of hidden units in the same layer to predictions: via a third-order tensor. While parallelization and scalability are not considered seriously in conventional DNNs: all learning for DSNs and TDSNs is done in batch"": 'original_similarity': 0.8332369052108468: 'slide': 188}"
0.4473520000174176,Who developed the perceptron and what was its function?,"The perceptron, an algorithm for pattern recognition, was developed by Rosenblatt. It described circuitries not in the basic perceptron, including the exclusive-or circuit, which could not be processed by neural networks at the time.","{'chunk_text': 'multilayer perceptron).: section 16 In addition: term deep learning was proposed in 1986 by Rina Dechter although the history of its appearance is apparently more complicated.The first general: working learning algorithm for supervised: deep: feedforward: multilayer perceptrons was published by Alexey Ivakhnenko and Lapa in 1967. A 1971 paper described a deep network with eight layers trained by the group method of data handling.The first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun\'ichi Amari. In computer experiments conducted by Amari\'s student Saito: a five layer MLP with two modifiable layers learned internal representations to classify non-linearily separable pattern classes. In 1987 Matthew Brand reported that wide 12-layer nonlinear perceptrons could be fully end-to-end trained to reproduce logic functions of nontrivial circuit depth via gradient descent on small batches of random input/output samples: but concluded that training time on contemporary hardware (sub-megaflop computers) made the technique impractical: and proposed using fixed random early layers as an input hash for a single modifiable layer. Instead: subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique. In 1970: Seppo Linnainmaa published the reverse mode of automatic differentiation of discrete connected networks of nested differentiable functions. This became known as backpropagation. It is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes. The terminology ""back-propagating errors"" was actually introduced in 1962 by Rosenblatt: but he did not know how to implement this: although Henry J. Kelley had a continuous precursor of backpropagation already in 1960 in the context of control theory. In 1982: Paul Werbos applied backpropagation to MLPs in the way that has become standard. In 1985: David E. Rumelhart et al. published an experimental analysis of the technique.Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers began with the Neocognitron introduced by Kunihiko Fukushima in 1980. In 1969: he also introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for CNNs and deep learning in general. CNNs have become an essential tool for computer vision. The term Deep Learning was introduced to the machine learning community by Rina Dechter in 1986: and to artificial neural networks by Igor Aizenberg and colleagues in 2000: in the context of Boolean threshold neurons.In 1988: Wei Zhang et al. applied the backpropagation algorithm to a convolutional neural network (a simplified Neocognitron with convolutional interconnections between the image feature layers and the last fully connected layer) for alphabet recognition. They also proposed an implementation of the CNN with an optical computing system. In 1989: Yann LeCun et al. applied backpropagation to a CNN with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked: training required 3 days. Subsequently: Wei Zhang: et al. modified their model by removing the last fully connected layer and applied it for medical image object segmentation in 1991 and breast cancer detection in mammograms in 1994. LeNet-5 (1998): a 7-level CNN by Yann LeCun et al.: that classifies digits: was applied by several banks to recognize hand-written numbers on checks digitized in 32x32 pixel images. In the 1980s: backpropagation did not work well for deep learning with long credit assignment paths. To overcome this problem: Jürgen Schmidhuber (1992) proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning. It uses predictive coding to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN: by distilling a higher level chunker network into a lower level automatizer network. In 1993: a chunker solved a deep learning task whose depth exceeded 1000.In 1992: Jürgen Schmidhuber also published an alternative to RNNs which is now called a linear Transformer or a Transformer with linearized self-attention (save for a normalization operator). It learns internal spotlights of attention: a slow feedforward neural network learns by gradient descent to control the fast weights of another neural network through outer products of self-generated activation patterns FROM and TO (which are now called key and value for self-attention). This fast weight attention mapping is applied to a query pattern. The modern Transformer was introduced by Ashish Vaswani et al. in their 2017 paper ""Attention Is All You Need"". It combines this with a softmax operator and a projection matrix. Transformers have increasingly become the model of choice for natural language processing. Many modern large language models such as ChatGPT: GPT-4: and BERT use it. Transformers are also increasingly being used in computer vision.In 1991: Jürgen Schmidhuber also published adversarial neural networks that contest with each other in the form of a zero-sum game: where one network\'s gain is the other network\'s loss. The first network is a\ngenerative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called ""artificial curiosity"". In 2014: this principle was used in a generative adversarial network (GAN) by Ian Goodfellow et al. Here the environmental reaction is 1 or 0 depending on whether the first network\'s output is in a given set. This can be used to create realistic deepfakes. Excellent image quality is achieved by Nvidia\'s StyleGAN (2018) based on the Progressive GAN by Tero Karras et al. Here the GAN generator is grown from small to large scale in a pyramidal fashion. Sepp Hochreiter\'s diploma thesis (1991) was called ""one of the most important documents in the history of machine learning"" by his supervisor Schmidhuber. It not only tested the neural history compressor: but also identified and analyzed the vanishing gradient problem. Hochreiter proposed recurrent residual connections to solve this problem. This led to the deep learning method called long short-term memory (LSTM): published in 1997. LSTM recurrent neural networks can learn ""very deep learning"" tasks with long credit assignment paths that require memories of events': 'original_similarity': 0.8242976512209411: 'slide': 193}","{'chunk_text': 'of the weights and the inputs is calculated in each node. The mean squared errors between these calculated outputs and a given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement. === Perceptron === If using a threshold: i.e. a linear activation function: the resulting linear threshold unit is called a perceptron. (Often the term is used to denote just one of these units.) Multiple parallel linear units are able to approximate any continuous function from a compact interval of the real numbers into the interval [−1:1] despite the limited computational power of single unit with a linear threshold function. This result can be found in Peter Auer: Harald Burgsteiner and Wolfgang Maass ""A learning rule for very simple universal approximators consisting of a single layer of perceptrons"".Perceptrons can be trained by a simple learning algorithm that is usually called the delta rule. It calculates the errors between calculated output and sample output data: and uses this to create an adjustment to the weights: thus implementing a form of gradient descent. === Multilayer perceptron === A multilayer perceptron (MLP) is a misnomer for a modern feedforward artificial neural network: consisting of fully connected neurons with a nonlinear kind of activation function: organized in at least three layers: notable for being able to distinguish data that is not linearly separable. It is a misnomer because the original perceptron used a Heaviside step function: instead of a nonlinear kind of activation function (used by modern networks). == Other feedforward networks == Examples of other feedforward networks include convolutional neural networks and radial basis function networks: which use a different activation function. == See also == Hopfield network Feed-forward Backpropagation Rprop == References == == External links == Feedforward neural networks tutorial Feedforward Neural Network: Example Feedforward Neural Networks: An Introduction\nGemini is a family of multimodal large language models developed by Google DeepMind: serving as the successor to LaMDA and PaLM 2. Comprising Gemini Ultra: Gemini Pro: and Gemini Nano: it was announced on December 6: 2023:': 'original_similarity': 0.8184613136966888: 'slide': 35}","{'chunk_text': 'but he did not have an implementation of this procedure: although Henry J. Kelley and Bryson had dynamic programming based continuous precursors of backpropagation already in 1960–61 in the context of control theory. In 1973: Dreyfus used backpropagation to adapt parameters of controllers in proportion to error gradients. In 1982: Paul Werbos applied backpropagation to MLPs in the way that has become standard. In 1986 Rumelhart: Hinton and Williams showed that backpropagation learned interesting internal representations of words as feature vectors when trained to predict the next word in a sequence.In the late 1970s to early 1980s: interest briefly emerged in theoretically investigating the Ising model created by Wilhelm Lenz (1920) and Ernst Ising (1925) in relation to Cayley tree topologies and large neural networks. The Ising model is essentially a non-learning artificial recurrent neural network (RNN) consisting of neuron-like threshold elements. In 1972: Shun\'ichi Amari described an adaptive version of this architecture: In 1981: the Ising model was solved exactly by Peter Barth for the general case of closed Cayley trees (with loops) with an arbitrary branching ratio and found to exhibit unusual phase transition behavior in its local-apex and long-range site-site correlations.John Hopfield popularised this architecture in 1982: and it is now known as a Hopfield network. The time delay neural network (TDNN) of Alex Waibel (1987) combined convolutions and weight sharing and backpropagation. In 1988: Wei Zhang et al. applied backpropagation to a CNN (a simplified Neocognitron with convolutional interconnections between the image feature layers and the last fully connected layer) for alphabet recognition. In 1989: Yann LeCun et al. trained a CNN to recognize handwritten ZIP codes on mail. In 1992: max-pooling for CNNs was introduced by Juan Weng et al. to help with least-shift invariance and tolerance to deformation to aid 3D object recognition. LeNet-5 (1998): a 7-level CNN by Yann LeCun et al.: that classifies digits: was applied by several banks to recognize hand-written numbers on checks digitized in 32x32 pixel images. From 1988 onward: the use of neural networks transformed the field of protein structure prediction: in particular when the first cascading networks were trained on profiles (matrices) produced by multiple sequence alignments.In 1991: Sepp Hochreiter\'s diploma thesis identified and analyzed the vanishing gradient problem and proposed recurrent residual connections to solve it. His thesis was called ""one of the most important documents in the history of machine learning"" by his supervisor Juergen Schmidhuber.In 1991: Juergen Schmidhuber published adversarial neural networks that contest with each other in the form of a zero-sum game: where one network\'s gain is the other network\'s loss. The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called ""artificial curiosity."" In 1992: Juergen Schmidhuber proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning. It uses predictive coding to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN: by distilling a higher level chunker network into a lower level automatizer network. In the same year he also published an alternative to RNNs which is a precursor of a linear Transformer. It introduces the concept internal spotlights of attention: a slow feedforward neural network learns by gradient descent to control the fast weights of another neural network through outer products of self-generated activation patterns. The development of metal–oxide–semiconductor (MOS) very-large-scale integration (VLSI): in the form of complementary MOS (CMOS) technology: enabled increasing MOS transistor counts in digital electronics. This provided more processing power for the development of practical artificial neural networks in the 1980s.Neural networks\' early successes included predicting the stock market and in 1995 a (mostly) self-driving car.1997: Sepp Hochreite and Juergen Schmidhuber introduced the deep learning method called long short-term memory (LSTM): published in Neural Computation. LSTM recurrent neural networks can learn ""very deep learning"" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. The ""vanilla LSTM"" with forget gate was introduced in 1999 by Felix Gers: Schmidhuber and Fred Cummins.Geoffrey Hinton et al. (2006) proposed learning a high-level representation using successive layers of binary or real-valued latent variables with a restricted Boltzmann machine to model each layer. In 2012: Ng and Dean created a network that learned to recognize higher-level concepts: such as cats: only from watching unlabeled images. Unsupervised pre-training and increased computing power from GPUs and distributed computing allowed the use of larger networks: particularly in image and visual recognition problems: which became known as ""deep learning"".Variants of the back-propagation algorithm: as well as unsupervised methods by Geoff Hinton and colleagues at the University of Toronto: can be used to train deep: highly nonlinear neural architectures: similar to the 1980 Neocognitron by Kunihiko Fukushima: and the ""standard architecture of vision"": inspired by the simple and complex cells identified by David H. Hubel\nand Torsten Wiesel in the primary visual cortex. Computational devices have been created in CMOS for both biophysical simulation and neuromorphic computing. More recent efforts show promise for creating nanodevices for very large scale principal components analyses and convolution. If successful: these efforts could usher in a new era of neural computing that is a step beyond digital computing: because it depends on learning rather than programming and because it is fundamentally analog rather than digital even though the first instantiations may in fact be with CMOS digital devices. Ciresan and colleagues (2010) showed that despite the vanishing gradient problem: GPUs make backpropagation feasible for many-layered feedforward neural networks. Between 2009 and 2012: ANNs began winning prizes in image recognition contests: approaching human level performance on various tasks: initially in pattern recognition and handwriting recognition. For example: the bi-directional and multi-dimensional long short-term memory (LSTM) of Graves et al. won three competitions in connected handwriting recognition in 2009 without any prior knowledge about the three languages to be learned.Ciresan': 'original_similarity': 0.8171015040456568: 'slide': 167}"
0.4593024000059813,What is the role of the backpropagation algorithm in artificial neural networks?,The backpropagation algorithm is an efficient application of the Leibniz chain rule to networks of differentiable nodes. It's crucial for adjusting the weight of various nodes in the network to improve prediction accuracy.,"{'chunk_text': 'hidden layer with randomized weights that did not learn: and an output layer. It also introduced variants: including a version with four-layer perceptrons where the last two layers have learned weights (and thus a proper multilayer perceptron).: section 16 In addition: term deep learning was proposed in 1986 by Rina Dechter although the history of its appearance is apparently more complicated.The first general: working learning algorithm for supervised: deep: feedforward: multilayer perceptrons was published by Alexey Ivakhnenko and Lapa in 1967. A 1971 paper described a deep network with eight layers trained by the group method of data handling.The first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun\'ichi Amari. In computer experiments conducted by Amari\'s student Saito: a five layer MLP with two modifiable layers learned internal representations to classify non-linearily separable pattern classes. In 1987 Matthew Brand reported that wide 12-layer nonlinear perceptrons could be fully end-to-end trained to reproduce logic functions of nontrivial circuit depth via gradient descent on small batches of random input/output samples: but concluded that training time on contemporary hardware (sub-megaflop computers) made the technique impractical: and proposed using fixed random early layers as an input hash for a single modifiable layer. Instead: subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique. In 1970: Seppo Linnainmaa published the reverse mode of automatic differentiation of discrete connected networks of nested differentiable functions. This became known as backpropagation. It is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes. The terminology ""back-propagating errors"" was actually introduced in 1962 by Rosenblatt: but he did not know how to implement this: although Henry J. Kelley had a continuous precursor of backpropagation already in 1960 in the context of control theory. In 1982: Paul Werbos applied backpropagation to MLPs in the way that has become standard. In 1985: David E. Rumelhart et al. published an experimental analysis of the technique.Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers began with the Neocognitron introduced by Kunihiko Fukushima in 1980. In 1969: he also introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for CNNs and deep learning in general. CNNs have become an essential tool for computer vision. The term Deep Learning was introduced to the machine learning community by Rina Dechter in 1986: and to artificial neural networks by Igor Aizenberg and colleagues in 2000: in the context of Boolean threshold neurons.In 1988: Wei Zhang et al. applied the backpropagation algorithm to a convolutional neural network (a simplified Neocognitron with convolutional interconnections between the image feature layers and the last fully connected layer) for alphabet recognition. They also proposed an implementation of the CNN with an optical computing system. In 1989: Yann LeCun et al. applied backpropagation to a CNN with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked: training required 3 days. Subsequently: Wei Zhang: et al. modified their model by removing the last fully connected layer and applied it for medical image object segmentation in 1991 and breast cancer detection in mammograms in 1994. LeNet-5 (1998): a 7-level CNN by Yann LeCun et al.: that classifies digits: was applied by several banks to recognize hand-written numbers on checks digitized in 32x32 pixel images. In the 1980s: backpropagation did not work well for deep learning with long credit assignment paths. To overcome this problem: Jürgen Schmidhuber (1992) proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning. It uses predictive coding to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN: by distilling a higher level chunker network into a lower level automatizer network. In 1993: a chunker solved a deep learning task whose depth exceeded 1000.In 1992: Jürgen Schmidhuber also published an alternative to RNNs which is now called a linear Transformer or a Transformer with linearized self-attention (save for a normalization operator). It learns internal spotlights of attention: a slow feedforward neural network learns by gradient descent to control the fast weights of another neural network through outer products of self-generated activation patterns FROM and TO (which are now called key and value for self-attention). This fast weight attention mapping is applied to a query pattern. The modern Transformer was introduced by Ashish Vaswani et al. in their 2017 paper ""Attention Is All You Need"". It combines this with a softmax operator and a projection matrix. Transformers have increasingly become the model of choice for natural language processing. Many modern large language models such as ChatGPT: GPT-4: and BERT use it. Transformers are also increasingly being used in computer vision.In 1991: Jürgen Schmidhuber also published adversarial neural networks that contest with each other in the form of a zero-sum game: where one network\'s gain is the other network\'s loss. The first network is a\ngenerative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called ""artificial curiosity"". In 2014: this principle was used in a generative adversarial network (GAN) by Ian Goodfellow et al. Here the environmental reaction is 1 or 0 depending on whether the first network\'s output is in a given set. This can be used to create realistic deepfakes. Excellent image quality is achieved by Nvidia\'s StyleGAN (2018) based on the Progressive GAN by Tero Karras et al. Here the GAN generator is grown from small to large scale in a pyramidal fashion. Sepp Hochreiter\'s diploma thesis (1991) was called ""one of the most important documents in the history of machine learning"" by his supervisor Schmidhuber. It not only tested the neural history compressor: but also identified and analyzed the vanishing gradient problem. Hochreiter proposed recurrent residual connections to solve this problem.': 'original_similarity': 0.8432930846697947: 'slide': 158}","{'chunk_text': 'as reported Shun\'ichi Amari. Amari\'s student Saito conducted the computer experiments: using a five-layered feedforward network with two learning layers.In 1970: modern backpropagation method: an efficient application of a chain-rule-based supervised learning: was for the first time published by the Finnish researcher Seppo Linnainmaa. The term (i.e. ""back-propagating errors"") itself has been used by Rosenblatt himself: but he did not know how to implement it: although a continuous precursor of backpropagation was already used in the context of control theory in 1960 by Henry J. Kelley. It is known also as a reverse mode of automatic differentiation.In 1982: backpropagation was applied in the way that has become standard: for the first time by Paul Werbos.In 1985: an experimental analysis of the technique was conducted by David E. Rumelhart et al.. Many improvements to the approach have been made in subsequent decades.In 1987: using a stochastic gradient descent within a (wide 12-layer nonlinear) feed-forward network: Matthew Brand has trained it to reproduce logic functions of nontrivial circuit depth: using small batches of random input/output samples. He: however: concluded that on hardware (sub-megaflop computers) available at the time it was impractical: and proposed using fixed random early layers as an input hash for a single modifiable layer.In 1990s: an (much simpler) alternative to using neural networks: although still related support vector machine approach was developed by Vladimir Vapnik and his colleagues. In addition to performing linear classification: they were able to efficiently perform a non-linear classification using what is called the kernel trick: using high-dimensional feature spaces.In 2003: interest in backpropagation networks returned due to the successes of deep learning being applied to language modelling by Yoshua Bengio with co-authors.In 2017: modern transformer architectures were introduced. == Mathematical foundations == === Activation function === The two historically common activation functions are both sigmoids: and are described by y(vi)=tanh\u2061(vi) and y(vi)=(1+e−vi)−1{\\displaystyle y(v_{i})=\\tanh(v_{i})~~{\\textrm {and}}~~y(v_{i})=(1+e^{-v_{i}})^{-1}}.The first is a hyperbolic tangent that ranges from -1 to 1: while the other is the logistic function: which is similar in shape but ranges from 0 to 1. Here yi{\\displaystyle y_{i}} is the output of the i{\\displaystyle i}th node (neuron) and vi{\\displaystyle v_{i}} is the weighted sum of the input connections. Alternative activation functions have been proposed: including the rectifier and softplus functions. More specialized activation functions include radial basis functions (used in radial basis networks: another class of supervised neural network models). In recent developments of deep learning the rectified linear unit (ReLU) is more frequently used as one of the possible ways to overcome the numerical problems related to the sigmoids. === Learning === Learning occurs by changing connection weights after each piece of data is processed: based on the amount of error in the output compared to the expected result. This is an example of supervised learning: and is carried out through backpropagation. We can represent the degree of error in an output node j{\\displaystyle j} in the n{\\displaystyle n}th data point (training example) by ej(n)=dj(n)−yj(n){\\displaystyle e_{j}(n)=d_{j}(n)-y_{j}(n)}: where dj(n){\\displaystyle d_{j}(n)} is the desired target value for n{\\displaystyle n}th data point at node j{\\displaystyle j}: and yj(n){\\displaystyle y_{j}(n)} is the value produced at node j{\\displaystyle j} when the n{\\displaystyle n}th data point is given as an input. The node weights can then be adjusted based on corrections that minimize the error in the entire output for the n{\\displaystyle n}th data point: given by E(n)=12∑output node jej2(n){\\displaystyle {\\mathcal {E}}(n)={\\frac {1}{2}}\\sum _{{\\text{output node }}j}e_{j}^{2}(n)}.Using gradient descent: the change in each weight wij{\\displaystyle w_{ij}} is Δwji(n)=−η∂E(n)∂vj(n)yi(n){\\displaystyle \\Delta w_{ji}(n)=-\\eta {\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}y_{i}(n)}where yi(n){\\displaystyle y_{i}(n)} is the output of the previous neuron i{\\displaystyle i}: and η{\\displaystyle \\eta } is the learning rate: which is selected to ensure that the weights quickly converge to a response: without oscillations. In the previous expression: ∂E(n)∂vj(n){\\displaystyle {\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}} denotes the partial derivate of the error E(n){\\displaystyle {\\mathcal {E}}(n)} according to the weighted sum vj(n){\\displaystyle v_{j}(n)} of the input connections of neuron i{\\displaystyle i}. The derivative to be calculated depends on the induced local field vj{\\displaystyle v_{j}}: which itself varies. It is easy to prove that for an output node this derivative can be simplified to −∂E(n)∂vj(n)=ej(n)ϕ′(vj(n)){\\displaystyle -{\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}=e_{j}(n)\\phi ^{\\prime }(v_{j}(n))}where ϕ′{\\displaystyle \\phi ^{\\prime }} is the derivative of the activation function described above: which itself does not vary. The analysis is more difficult for the change in weights to a hidden node: but it can be shown that the relevant derivative is −∂E(n)∂vj(n)=ϕ′(vj(n))∑k−∂E(n)∂vk(n)wkj(n){\\displaystyle -{\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}=\\phi ^{\\prime }(v_{j}(n))\\sum _{k}-{\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{k}(n)}}w_{kj}(n)}.This depends on the change in weights of the k{\\displaystyle k}th nodes: which represent the output layer. So to change the hidden layer weights: the output layer weights change according to the derivative of the activation function: and so this algorithm represents a backpropagation of the activation function. == History == === Linear neural network === The simplest\nkind of feedforward neural network is a linear network: which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated in each node. The mean squared errors between these calculated outputs and a given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement. === Perceptron === If using a threshold: i.e. a linear activation function: the resulting linear threshold unit is called a perceptron. (Often the term is used to denote just one of these units.) Multiple parallel linear units are able to approximate any continuous function from a compact interval of the real numbers into the interval [−1:1] despite the limited computational power of single unit with a linear threshold function. This result can be found in Peter Auer: Harald Burgsteiner and': 'original_similarity': 0.8413009210587465: 'slide': 195}","{'chunk_text': 'use formal statistical methods to determine the confidence of the trained model. The MSE on a validation set can be used as an estimate for variance. This value can then be used to calculate the confidence interval of network output: assuming a normal distribution. A confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified. By assigning a softmax activation function: a generalization of the logistic function: on the output layer of the neural network (or a softmax component in a component-based network) for categorical target variables: the outputs can be interpreted as posterior probabilities. This is useful in classification as it gives a certainty measure on classifications. The softmax activation function is: yi=exi∑j=1cexj{\\displaystyle y_{i}={\\frac {e^{x_{i}}}{\\sum _{j=1}^{c}e^{x_{j}}}}} == Criticism == === Training === A common criticism of neural networks: particularly in robotics: is that they require too many training samples for real-world operation. Any learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases. Potential solutions include randomly shuffling training examples: by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example: grouping examples in so-called mini-batches and/or introducing a recursive least squares algorithm for CMAC. Dean Pomerleau uses a neural network to train a robotic vehicle to drive on multiple types of roads (single lane: multi-lane: dirt: etc.): and a large amount of his research is devoted to extrapolating multiple training scenarios from a single training experience: and preserving past training diversity so that the system does not become overtrained (if: for example: it is presented with a series of right turns—it should not learn to always turn right). === Theory === A central claim of ANNs is that they embody new and powerful general principles for processing information. These principles are ill-defined. It is often claimed that they are emergent from the network itself. This allows simple statistical association (the basic function of artificial neural networks) to be described as learning or recognition. In 1997: Alexander Dewdney: a former Scientific American columnist: commented that as a result: artificial neural networks have a ""something-for-nothing quality: one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are. No human hand (or mind) intervenes; solutions are found as if by magic; and no one: it seems: has learned anything"". One response to Dewdney is that neural networks have been successfully used to handle many complex and diverse tasks: ranging from autonomously flying aircraft to detecting credit card fraud to mastering the game of Go. Technology writer Roger Bridgman commented: Neural networks: for instance: are in the dock not only because they have been hyped to high heaven: (what hasn\'t?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be ""an opaque: unreadable table...valueless as a scientific resource"". In spite of his emphatic declaration that science is not technology: Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having. Although it is true that analyzing what has been learned by an artificial neural network is difficult: it is much easier to do so than to analyze what has been learned by a biological neural network. Moreover: recent emphasis on the explainability of AI has contributed towards the development of methods: notably those based on attention mechanisms: for visualizing and explaining learned neural networks. Furthermore: researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles that allow a learning machine to be successful. For example: Bengio and LeCun (2007) wrote an article regarding local vs non-local learning: as well as shallow vs deep architecture.Biological brains use both shallow and deep circuits as reported by brain anatomy: displaying a wide variety of invariance. Weng argued that the brain self-wires largely according to signal statistics and therefore: a serial cascade cannot catch all major statistical dependencies. === Hardware === Large and effective neural networks require considerable computing resources. While the brain has hardware tailored to the task of processing signals through a graph of neurons: simulating even a simplified neuron on von Neumann architecture may consume vast amounts of memory and storage. Furthermore: the designer often needs to transmit signals through many of these connections and their associated neurons – which require enormous CPU power and time. Schmidhuber noted that the resurgence of neural networks in the twenty-first century is largely attributable to advances in hardware: from 1991 to 2015: computing power: especially as delivered by GPGPUs (on GPUs): has increased around a million-fold: making the standard backpropagation algorithm feasible for training networks that are several layers deeper than before. The use of accelerators such as FPGAs and GPUs can reduce training times from months to days.Neuromorphic engineering or a physical neural network addresses the hardware difficulty directly: by constructing non-von-Neumann chips to directly implement neural networks\nin circuitry. Another type of chip optimized for neural network processing is called a Tensor Processing Unit: or TPU. === Practical counterexamples === Analyzing what has been learned by an ANN is much easier than analyzing what has been learned by a biological neural network. Furthermore: researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a learning machine to be successful. For example: local vs. non-local learning and shallow vs. deep architecture. === Hybrid approaches === Advocates of hybrid models (combining neural networks and symbolic approaches) say that such a mixture can better capture the mechanisms of the human mind. === Dataset bias === Neural networks are dependent on the quality of the data they are trained on: thus low quality data with imbalanced representativeness can lead': 'original_similarity': 0.8402589356386208: 'slide': 132}"
0.4419595999934245,What is a modeling language?,"A modeling language is an artificial language that can be used to express data, information or knowledge or systems in a structure defined by a consistent set of rules. These rules help interpret the meaning of components in the structure. They can be graphical or textual, and can be used in various fields including computer science, information management, business process modeling, software engineering, and systems engineering.","{'chunk_text': '== Type of modeling languages == === Graphical types === Example of graphical modeling languages in the field of computer science: project management and systems engineering: Behavior Trees are a formal: graphical modeling language used primarily in systems and software engineering. Commonly used to unambiguously represent the hundreds or even thousands of natural language requirements that are typically used to express the stakeholder needs for a large-scale software-integrated system. Business Process Modeling Notation (BPMN: and the XML form BPML) is an example of a Process Modeling language. C-K theory consists of a modeling language for design processes. DRAKON is a general-purpose algorithmic modeling language for specifying software-intensive systems: a schematic representation of an algorithm or a stepwise process: and a family of programming languages. EXPRESS and EXPRESS-G (ISO 10303-11) is an international standard general-purpose data modeling language. Extended Enterprise Modeling Language (EEML) is commonly used for business process modeling across a number of layers. Flowchart is a schematic representation of an algorithm or a stepwise process. Fundamental Modeling Concepts (FMC) modeling language for software-intensive systems. IDEF is a family of modeling languages: which include IDEF0 for functional modeling: IDEF1X for information modeling: IDEF3 for business process modeling: IDEF4 for Object-Oriented Design and IDEF5 for modeling ontologies. Jackson Structured Programming (JSP) is a method for structured programming based on correspondences between data stream structure and program structure. LePUS3 is an object-oriented visual Design Description Language and a formal specification language that is suitable primarily for modeling large object-oriented (Java: C++: C#) programs and design patterns. Lifecycle Modeling Language is an open-standard language for systems engineering that supports the full system lifecycle: conceptual: utilization: support and retirement stages. Object-Role Modeling (ORM) in the field of software engineering is a method for conceptual modeling: and can be used as a tool for information and rules analysis. Petri nets use variations on exactly one diagramming technique and topology: namely the bipartite graph. The simplicity of its basic user interface easily enabled extensive tool support over the years: particularly in the areas of model checking: graphically oriented simulation: and software verification. Southbeach Notation is a visual modeling language used to describe situations in terms of agents that are considered useful or harmful from the modeler\'s perspective. The notation shows how the agents interact with each other and whether this interaction improves or worsens the situation. Specification and Description Language (SDL) is a specification language targeted at the unambiguous specification and description of the behavior of reactive and distributed systems. SysML is a Domain-Specific Modeling language for systems engineering that is defined as a UML profile (customization). Unified Modeling Language (UML) is a general-purpose modeling language that is an industry standard for specifying software-intensive systems. UML 2.0: the current version: supports thirteen different diagram techniques: and has widespread tool support. Service-oriented modeling framework (SOMF) is a holistic language for designing enterprise and application level architecture models in the space of enterprise architecture: virtualization: service-oriented architecture (SOA): cloud computing: and more. Architecture description language (ADL) is a language used to describe and represent the systems architecture of a system. Architecture Analysis & Design Language (AADL) is a modeling language that supports early and repeated analyses of a system\'s architecture with respect to performance-critical properties through an extendable notation: a tool framework: and precisely defined semantics.Examples of graphical modeling languages in other fields of science. EAST-ADL is a Domain-Specific Modeling language dedicated to automotive system design. Energy Systems Language (ESL): a language that aims to model ecological energetics & global economics. IEC 61499 defines Domain-Specific Modeling language dedicated to distribute industrial process measurement and control systems. === Textual types === Information models can also be expressed in formalized natural languages: such as Gellish. Gellish has natural language variants such as Gellish Formal English and Gellish Formal Dutch (Gellish Formeel Nederlands): etc. Gellish Formal English is an information representation language or semantic modeling language that is defined in the Gellish English Dictionary-Taxonomy: which has the form of a Taxonomy-Ontology (similarly for Dutch). Gellish Formal English is not only suitable to express knowledge: requirements and dictionaries: taxonomies and ontologies: but also information about individual things. All that information is expressed in one language and therefore it can all be integrated: independent of the question whether it is stored in central or distributed or in federated databases. Information models in Gellish Formal English consists of collections of Gellish Formal English expressions: that use natural language terms and formalized phrases. For example: a geographic information model might consist of a number of Gellish Formal English expressions: such as: - the Eiffel tower <is located in> Paris - Paris <is classified as a> city whereas information requirements and knowledge can be expressed for example as follows: - tower <shall be located in a> geographical area - city <is a kind of> geographical area Such Gellish Formal English expressions use names of concepts (such as ""city"") and phrases\nthat represent relation types (such as ⟨is located in⟩ and ⟨is classified as a⟩) that should be selected from the Gellish English Dictionary-Taxonomy (or of your own domain dictionary). The Gellish English Dictionary-Taxonomy enables the creation of semantically rich information models: because the dictionary contains more than 600 standard relation types and contains definitions of more than 40000 concepts. An information model in Gellish can express facts or make statements: queries and answers. === More specific types === In the field of computer science recently more specific types of modeling languages have emerged. ==== Algebraic ==== Algebraic Modeling Languages (AML) are high-level programming languages for describing and solving high complexity problems for large scale mathematical computation (i.e. large scale optimization type problems). One particular advantage of AMLs like AIMMS: AMPL: GAMS: Gekko: Mosel: OPL and OptimJ is the similarity of its syntax to the mathematical notation of optimization problems. This allows for a very concise and readable definition of problems in the domain of optimization: which is supported by certain language elements like sets: indices: algebraic expressions: powerful sparse index and data handling variables: constraints with arbitrary': 'original_similarity': 0.8834411021674667: 'slide': 185}","{'chunk_text': ""relation types and contains definitions of more than 40000 concepts. An information model in Gellish can express facts or make statements: queries and answers. === More specific types === In the field of computer science recently more specific types of modeling languages have emerged. ==== Algebraic ==== Algebraic Modeling Languages (AML) are high-level programming languages for describing and solving high complexity problems for large scale mathematical computation (i.e. large scale optimization type problems). One particular advantage of AMLs like AIMMS: AMPL: GAMS: Gekko: Mosel: OPL and OptimJ is the similarity of its syntax to the mathematical notation of optimization problems. This allows for a very concise and readable definition of problems in the domain of optimization: which is supported by certain language elements like sets: indices: algebraic expressions: powerful sparse index and data handling variables: constraints with arbitrary names. The algebraic formulation of a model does not contain any hints how to process it. ==== Behavioral ==== Behavioral languages are designed to describe the observable behavior of complex systems consisting of components that execute concurrently. These languages focus on the description of key concepts such as: concurrency: nondeterminism: synchronization: and communication. The semantic foundations of Behavioral languages are process calculus or process algebra. ==== Discipline-specific ==== A discipline-specific modeling (DspM) language is focused on deliverables affiliated with a specific software development life cycle stage. Therefore: such language offers a distinct vocabulary: syntax: and notation for each stage: such as discovery: analysis: design: architecture: contraction: etc. For example: for the analysis phase of a project: the modeler employs specific analysis notation to deliver an analysis proposition diagram. During the design phase: however: logical design notation is used to depict relationship between software entities. In addition: the discipline-specific modeling language best practices does not preclude practitioners from combining the various notations in a single diagram. ==== Domain-specific ==== Domain-specific modeling (DSM) is a software engineering methodology for designing and developing systems: most often IT systems such as computer software. It involves systematic use of a graphical domain-specific language (DSL) to represent the various facets of a system. DSM languages tend to support higher-level abstractions than General-purpose modeling languages: so they require less effort and fewer low-level details to specify a given system. ==== Framework-specific ==== A framework-specific modeling language (FSML) is a kind of domain-specific modeling language which is designed for an object-oriented application framework. FSMLs define framework-provided abstractions as FSML concepts and decompose the abstractions into features. The features represent implementation steps or choices. A FSML concept can be configured by selecting features and providing values for features. Such a concept configuration represents how the concept should be implemented in the code. In other words: concept configuration describes how the framework should be completed in order to create the implementation of the concept. ==== Information and knowledge modeling ==== Linked data and ontology engineering require 'host languages' to represent entities and the relations between them: constraints between the properties of entities and relations: and metadata attributes. JSON-LD and RDF are two major (and semantically almost equivalent) languages in this context: primarily because they support statement reification and contextualisation which are essential properties to support the higher-order logic needed to reason about models. Model transformation is a common example of such reasoning. ==== Object-oriented ==== Object modeling languages are modeling languages based on a standardized set of symbols and ways of arranging them to model (part of) an object oriented software design or system design. Some organizations use them extensively in combination with a software development methodology to progress from initial specification to an implementation plan and to communicate that plan to an entire team of developers and stakeholders. Because a modeling language is visual and at a higher-level of abstraction than code: using models encourages the generation of a shared vision that may prevent problems of differing interpretation later in development. Often software modeling tools are used to construct these models: which may then be capable of automatic translation to code. ==== Virtual reality ==== Virtual Reality Modeling Language (VRML): before 1995 known as the Virtual Reality Markup Language is a standard file format for representing 3-dimensional (3D) interactive vector graphics: designed particularly with the World Wide Web in mind. ==== Others ==== Architecture Description Language Face Modeling Language Generative Modelling Language Java Modeling Language Promela Rebeca Modeling Language Service Modeling Language Web Services Modeling Language X3D == Applications == Various kinds of modeling languages are applied in different disciplines: including computer science: information management: business process modeling: software engineering: and systems engineering. Modeling languages can be used to specify: system requirements: structures and behaviors.Modeling languages are intended to be used to precisely specify systems so that stakeholders (e.g.: customers: operators: analysts: designers) can better understand the system being modeled. The more mature modeling languages are precise: consistent and executable. Informal diagramming techniques applied with drawing tools are expected to produce useful pictorial representations of system requirements: structures and behaviors: which can be useful for communication: design: and problem solving but cannot be used programmatically.: 539 Executable modeling languages applied with proper tool support: however: are expected to automate system verification and validation: simulation and code generation from the same representations. == Quality == A review of modelling languages is essential to be able to assign which languages are appropriate for different modelling settings. In the term settings we include stakeholders: domain and the knowledge connected. Assessing the language quality is a means that aims to achieve better models. === Framework for evaluation === Here language quality is stated in accordance with the SEQUAL framework for quality of models developed by Krogstie: Sindre and Lindland (2003): since this is a framework that connects the language quality to a framework for general model\nquality. Five areas are used in this framework to describe language quality and these are supposed to express both the conceptual as well as the visual notation of the language. We will not go into a thoroughly explanation of the underlying quality framework of models but concentrate on the areas"": 'original_similarity': 0.8650912793688725: 'slide': 48}","{'chunk_text': 'introduced the transformer architecture in their landmark paper ""Attention Is All You Need"". This paper\'s goal was to improve upon 2014 Seq2seq technology: and was based mainly on the attention mechanism developed by Bahdanau et al. in 2014. The following year in 2018: BERT was introduced and quickly became ""ubiquitous"". Though the original transformer has both encoder and decoder blocks: BERT is an encoder-only model. Although decoder-only GPT-1 was introduced in 2018: it was GPT-2 in 2019 that caught widespread attention because OpenAI at first deemed it too powerful to release publicly: out of fear of malicious use. GPT-3 in 2020 went a step further and as of 2024 is available only via API with no offering of downloading the model to execute locally. But it was the 2022 consumer-facing browser-based ChatGPT that captured the imaginations of the general population and caused some media hype and online buzz. The 2023 GPT-4 was praised for its increased accuracy and as a ""holy grail"" for its multimodal capabilities. OpenAI did not reveal high-level architecture and the number of parameters of GPT-4. In the meantime: competing language models have for the most part been playing catch-up to the GPT series: at least in terms of number of parameters. Notable exceptions in terms of either number of parameters or measured accuracy include Google\'s 2019 T5-11B and 2022 PaLM-E: and Anthropic\'s 2024 Claude 3. In terms of Elo ratings: on January 26: 2024: Google\'s Bard (Gemini Pro) surpassed the regular GPT-4: but not the limited-availability GPT-4-Turbo.Since 2022: source-available models have been gaining popularity: especially at first with BLOOM and LLaMA: though both have restrictions on the field of use. Mistral AI\'s models Mistral 7B and Mixtral 8x7b have the more permissive Apache License. As of January 2024: Mixtral 8x7b is the most powerful open LLM according to the LMSYS Chatbot Arena Leaderboard: being more powerful than GPT-3.5 but not as powerful as GPT-4. == Dataset preprocessing == === Probabilistic tokenization === Because machine learning algorithms process numbers rather than text: the text must be converted to numbers. In the first step: a vocabulary is decided upon: then integer indexes are arbitrarily but uniquely assigned to each vocabulary entry: and finally: an embedding is associated to the integer index. Algorithms include byte-pair encoding and WordPiece. Probabilistic tokenization also compresses the datasets. Because LLMs generally require input to be an array that is not jagged: the shorter texts must be ""padded"" until they match the length of the longest one. How many tokens are: on average: needed per word depends on the language of the dataset. ==== BPE ==== Using a modification of byte-pair encoding: in the first step: all unique characters (including blanks and punctuation marks) are treated as an initial set of n-grams (i.e. initial set of uni-grams). Successively the most frequent pair of adjacent characters is merged into a bi-gram and all instances of the pair are replaced by it. All occurrences of adjacent pairs of (previously merged) n-grams that most frequently occur together are then again merged into even lengthier n-gram repeatedly until a vocabulary of prescribed size is obtained (in case of GPT-3: the size is 50257). Token vocabulary consists of integers: spanning from zero up to the size of the token vocabulary. New words can always be interpreted as combinations of the tokens and the initial-set uni-grams.A token vocabulary based on the frequencies extracted from mainly English corpora uses as few tokens as possible for an average English word. An average word in another language encoded by such an English-optimized tokenizer is however split into suboptimal amount of tokens. GPT-2 tokenizer can use up to 15 times more tokens per word for some languages: for example for Shan language from Myanmar. Even more widespread languages such as Portuguese and German have ""a premium of 50%"" compared to English.For example: here is how tokenizer used by GPT-3 (Legacy) split the following sentence tokenizer: texts -> series of numerical ""tokens"". === Dataset cleaning === In the context of training LLMs: datasets are typically cleaned by removing toxic passages from the dataset: discarding low-quality data: and de-duplication. Cleaned datasets can increase training efficiency and lead to improved downstream performance.With the increasing proportion of LLM-generated content on the web: data cleaning in the future may include filtering out such content. LLM-generated content can pose a problem if the content is similar to human text (making filtering difficult) but of lower quality (degrading performance of models trained on it). == Training and architecture == === Reinforcement learning from human feedback (RLHF) === Reinforcement learning from human feedback (RLHF) through algorithms: such\nas proximal policy optimization: is used to further fine-tune a model based on a dataset of human preferences. === Instruction tuning === Using ""self-instruct"" approaches: LLMs have been able to bootstrap correct responses: replacing any naive responses: starting from human-generated corrections of a few cases. For example: in the instruction ""Write an essay about the main themes represented in Hamlet:"" an initial naive completion might be \'If you submit the essay after March 17: your grade will be reduced by 10% for each day of delay:"" based on the frequency of this textual sequence in the corpus. === Mixture of experts === The largest LLM may be too expensive to train and use directly. For such models: mixture of experts (MoE) can be applied: a line of research pursued by Google researchers since 2017 to train models reaching up to 1 trillion parameters. === Prompt engineering: attention mechanism: and context window === Most results previously achievable only by (costly) fine-tuning: can be achieved through prompt engineering: although limited to the scope of a single conversation (more precisely: limited to the scope of a context window). In order to find out which tokens are relevant to each other within the scope of the context window: the attention mechanism calculates ""soft"" weights for each token: more precisely for its embedding: by using multiple attention heads: each with its own ""relevance"" for calculating its own soft weights. For example:': 'original_similarity': 0.8184037031057536: 'slide': 234}"
0.6671497999923304,What is the difference between graphical and textual modeling languages?,"Graphical modeling languages use a diagram technique with named symbols to represent concepts and lines to depict relationships between symbols and other graphical notations to represent constraints. On the other hand, textual modeling languages use standardized keywords along with parameters or natural language terms and phrases to form computer interpretable expressions.","{'chunk_text': 'that they can address more challenging problems: such as parallel computing and distributed systems. A large number of modeling languages appear in the literature. == Type of modeling languages == === Graphical types === Example of graphical modeling languages in the field of computer science: project management and systems engineering: Behavior Trees are a formal: graphical modeling language used primarily in systems and software engineering. Commonly used to unambiguously represent the hundreds or even thousands of natural language requirements that are typically used to express the stakeholder needs for a large-scale software-integrated system. Business Process Modeling Notation (BPMN: and the XML form BPML) is an example of a Process Modeling language. C-K theory consists of a modeling language for design processes. DRAKON is a general-purpose algorithmic modeling language for specifying software-intensive systems: a schematic representation of an algorithm or a stepwise process: and a family of programming languages. EXPRESS and EXPRESS-G (ISO 10303-11) is an international standard general-purpose data modeling language. Extended Enterprise Modeling Language (EEML) is commonly used for business process modeling across a number of layers. Flowchart is a schematic representation of an algorithm or a stepwise process. Fundamental Modeling Concepts (FMC) modeling language for software-intensive systems. IDEF is a family of modeling languages: which include IDEF0 for functional modeling: IDEF1X for information modeling: IDEF3 for business process modeling: IDEF4 for Object-Oriented Design and IDEF5 for modeling ontologies. Jackson Structured Programming (JSP) is a method for structured programming based on correspondences between data stream structure and program structure. LePUS3 is an object-oriented visual Design Description Language and a formal specification language that is suitable primarily for modeling large object-oriented (Java: C++: C#) programs and design patterns. Lifecycle Modeling Language is an open-standard language for systems engineering that supports the full system lifecycle: conceptual: utilization: support and retirement stages. Object-Role Modeling (ORM) in the field of software engineering is a method for conceptual modeling: and can be used as a tool for information and rules analysis. Petri nets use variations on exactly one diagramming technique and topology: namely the bipartite graph. The simplicity of its basic user interface easily enabled extensive tool support over the years: particularly in the areas of model checking: graphically oriented simulation: and software verification. Southbeach Notation is a visual modeling language used to describe situations in terms of agents that are considered useful or harmful from the modeler\'s perspective. The notation shows how the agents interact with each other and whether this interaction improves or worsens the situation. Specification and Description Language (SDL) is a specification language targeted at the unambiguous specification and description of the behavior of reactive and distributed systems. SysML is a Domain-Specific Modeling language for systems engineering that is defined as a UML profile (customization). Unified Modeling Language (UML) is a general-purpose modeling language that is an industry standard for specifying software-intensive systems. UML 2.0: the current version: supports thirteen different diagram techniques: and has widespread tool support. Service-oriented modeling framework (SOMF) is a holistic language for designing enterprise and application level architecture models in the space of enterprise architecture: virtualization: service-oriented architecture (SOA): cloud computing: and more. Architecture description language (ADL) is a language used to describe and represent the systems architecture of a system. Architecture Analysis & Design Language (AADL) is a modeling language that supports early and repeated analyses of a system\'s architecture with respect to performance-critical properties through an extendable notation: a tool framework: and precisely defined semantics.Examples of graphical modeling languages in other fields of science. EAST-ADL is a Domain-Specific Modeling language dedicated to automotive system design. Energy Systems Language (ESL): a language that aims to model ecological energetics & global economics. IEC 61499 defines Domain-Specific Modeling language dedicated to distribute industrial process measurement and control systems. === Textual types === Information models can also be expressed in formalized natural languages: such as Gellish. Gellish has natural language variants such as Gellish Formal English and Gellish Formal Dutch (Gellish Formeel Nederlands): etc. Gellish Formal English is an information representation language or semantic modeling language that is defined in the Gellish English Dictionary-Taxonomy: which has the form of a Taxonomy-Ontology (similarly for Dutch). Gellish Formal English is not only suitable to express knowledge: requirements and dictionaries: taxonomies and ontologies: but also information about individual things. All that information is expressed in one language and therefore it can all be integrated: independent of the question whether it is stored in central or distributed or in federated databases. Information models in Gellish Formal English consists of collections of Gellish Formal English expressions: that use natural language terms and formalized phrases. For example: a geographic information model might consist of a number of Gellish Formal English expressions: such as: - the Eiffel tower <is located in> Paris - Paris <is classified as a> city whereas information requirements and knowledge can be expressed for example as follows: - tower <shall be located in a> geographical area - city <is a kind of> geographical area Such Gellish Formal English expressions use names of concepts (such as ""city"") and phrases\nthat represent relation types (such as ⟨is located in⟩ and ⟨is classified as a⟩) that should be selected from the Gellish English Dictionary-Taxonomy (or of your own domain dictionary). The Gellish English Dictionary-Taxonomy enables the creation of semantically rich information models: because the dictionary contains more than 600 standard relation types and contains definitions of more than 40000 concepts. An information model in Gellish can express facts or make statements: queries and answers. === More specific types === In the field of computer science recently more specific types of modeling languages have emerged. ==== Algebraic ==== Algebraic Modeling Languages (AML) are high-level programming languages for describing and solving high complexity problems for large scale mathematical computation (i.e. large scale optimization type problems). One particular advantage of AMLs like AIMMS: AMPL: GAMS: Gekko: Mosel: OPL and OptimJ is the similarity of its syntax to the mathematical notation of optimization problems. This allows for a very concise and readable definition of problems in the domain': 'original_similarity': 0.8785638434469338: 'slide': 161}","{'chunk_text': ""science recently more specific types of modeling languages have emerged. ==== Algebraic ==== Algebraic Modeling Languages (AML) are high-level programming languages for describing and solving high complexity problems for large scale mathematical computation (i.e. large scale optimization type problems). One particular advantage of AMLs like AIMMS: AMPL: GAMS: Gekko: Mosel: OPL and OptimJ is the similarity of its syntax to the mathematical notation of optimization problems. This allows for a very concise and readable definition of problems in the domain of optimization: which is supported by certain language elements like sets: indices: algebraic expressions: powerful sparse index and data handling variables: constraints with arbitrary names. The algebraic formulation of a model does not contain any hints how to process it. ==== Behavioral ==== Behavioral languages are designed to describe the observable behavior of complex systems consisting of components that execute concurrently. These languages focus on the description of key concepts such as: concurrency: nondeterminism: synchronization: and communication. The semantic foundations of Behavioral languages are process calculus or process algebra. ==== Discipline-specific ==== A discipline-specific modeling (DspM) language is focused on deliverables affiliated with a specific software development life cycle stage. Therefore: such language offers a distinct vocabulary: syntax: and notation for each stage: such as discovery: analysis: design: architecture: contraction: etc. For example: for the analysis phase of a project: the modeler employs specific analysis notation to deliver an analysis proposition diagram. During the design phase: however: logical design notation is used to depict relationship between software entities. In addition: the discipline-specific modeling language best practices does not preclude practitioners from combining the various notations in a single diagram. ==== Domain-specific ==== Domain-specific modeling (DSM) is a software engineering methodology for designing and developing systems: most often IT systems such as computer software. It involves systematic use of a graphical domain-specific language (DSL) to represent the various facets of a system. DSM languages tend to support higher-level abstractions than General-purpose modeling languages: so they require less effort and fewer low-level details to specify a given system. ==== Framework-specific ==== A framework-specific modeling language (FSML) is a kind of domain-specific modeling language which is designed for an object-oriented application framework. FSMLs define framework-provided abstractions as FSML concepts and decompose the abstractions into features. The features represent implementation steps or choices. A FSML concept can be configured by selecting features and providing values for features. Such a concept configuration represents how the concept should be implemented in the code. In other words: concept configuration describes how the framework should be completed in order to create the implementation of the concept. ==== Information and knowledge modeling ==== Linked data and ontology engineering require 'host languages' to represent entities and the relations between them: constraints between the properties of entities and relations: and metadata attributes. JSON-LD and RDF are two major (and semantically almost equivalent) languages in this context: primarily because they support statement reification and contextualisation which are essential properties to support the higher-order logic needed to reason about models. Model transformation is a common example of such reasoning. ==== Object-oriented ==== Object modeling languages are modeling languages based on a standardized set of symbols and ways of arranging them to model (part of) an object oriented software design or system design. Some organizations use them extensively in combination with a software development methodology to progress from initial specification to an implementation plan and to communicate that plan to an entire team of developers and stakeholders. Because a modeling language is visual and at a higher-level of abstraction than code: using models encourages the generation of a shared vision that may prevent problems of differing interpretation later in development. Often software modeling tools are used to construct these models: which may then be capable of automatic translation to code. ==== Virtual reality ==== Virtual Reality Modeling Language (VRML): before 1995 known as the Virtual Reality Markup Language is a standard file format for representing 3-dimensional (3D) interactive vector graphics: designed particularly with the World Wide Web in mind. ==== Others ==== Architecture Description Language Face Modeling Language Generative Modelling Language Java Modeling Language Promela Rebeca Modeling Language Service Modeling Language Web Services Modeling Language X3D == Applications == Various kinds of modeling languages are applied in different disciplines: including computer science: information management: business process modeling: software engineering: and systems engineering. Modeling languages can be used to specify: system requirements: structures and behaviors.Modeling languages are intended to be used to precisely specify systems so that stakeholders (e.g.: customers: operators: analysts: designers) can better understand the system being modeled. The more mature modeling languages are precise: consistent and executable. Informal diagramming techniques applied with drawing tools are expected to produce useful pictorial representations of system requirements: structures and behaviors: which can be useful for communication: design: and problem solving but cannot be used programmatically.: 539 Executable modeling languages applied with proper tool support: however: are expected to automate system verification and validation: simulation and code generation from the same representations. == Quality == A review of modelling languages is essential to be able to assign which languages are appropriate for different modelling settings. In the term settings we include stakeholders: domain and the knowledge connected. Assessing the language quality is a means that aims to achieve better models. === Framework for evaluation === Here language quality is stated in accordance with the SEQUAL framework for quality of models developed by Krogstie: Sindre and Lindland (2003): since this is a framework that connects the language quality to a framework for general model\nquality. Five areas are used in this framework to describe language quality and these are supposed to express both the conceptual as well as the visual notation of the language. We will not go into a thoroughly explanation of the underlying quality framework of models but concentrate on the areas used to explain the language quality framework. ==== Domain appropriateness ==== The framework states the ability to represent the domain as domain appropriateness. The statement appropriateness can be a bit vague: but in this"": 'original_similarity': 0.8375164795818063: 'slide': 82}","{'chunk_text': 'a system is supposed to do. == Diagrams == UML 2 has many types of diagrams: which are divided into two categories. Some types represent structural information: and the rest represent general types of behavior: including a few that represent different aspects of interactions. These diagrams can be categorized hierarchically as shown in the following class diagram: These diagrams may all contain comments or notes explaining usage: constraint: or intent. === Structure diagrams === Structure diagrams represent the static aspects of the system. It emphasizes the things that must be present in the system being modeled. Since structure diagrams represent the structure: they are used extensively in documenting the software architecture of software systems. For example: the component diagram describes how a software system is split up into components and shows the dependencies among these components. === Behavior diagrams === Behavior diagrams represent the dynamic aspect of the system. It emphasizes what must happen in the system being modeled. Since behavior diagrams illustrate the behavior of a system: they are used extensively to describe the functionality of software systems. As an example: the activity diagram describes the business and operational step-by-step activities of the components in a system. ==== Interaction diagrams ==== Interaction diagrams: a subset of behavior diagrams: emphasize the flow of control and data among the things in the system being modeled. For example: the sequence diagram shows how objects communicate with each other regarding a sequence of messages. == Metamodeling == The Object Management Group (OMG) has developed a metamodeling architecture to define the UML: called the Meta-Object Facility. MOF is designed as a four-layered architecture: as shown in the image at right. It provides a meta-meta model at the top: called the M3 layer. This M3-model is the language used by Meta-Object Facility to build metamodels: called M2-models. The most prominent example of a Layer 2 Meta-Object Facility model is the UML metamodel: which describes the UML itself. These M2-models describe elements of the M1-layer: and thus M1-models. These would be: for example: models written in UML. The last layer is the M0-layer or data layer. It is used to describe runtime instances of the system.The meta-model can be extended using a mechanism called stereotyping. This has been criticized as being insufficient/untenable by Brian Henderson-Sellers and Cesar Gonzalez-Perez in ""Uses and Abuses of the Stereotype Mechanism in UML 1.x and 2.0"". == Adoption == Back in 2013 UML has been marketed by OMG for many contexts: but aimed primarily at software development with limited success.It has been treated: at times: as a design silver bullet: which leads to problems. UML misuse includes overuse (designing every part of the system with it: which is unnecessary) and assuming that novices can design with it.It is considered a large language: with many constructs. Some people (including Jacobson) feel that UML\'s size hinders learning (and therefore using) it.MS Visual Studio dropped support for UML in 2016 due to lack of usage.According to Google Trends UML has been on steady decline since 2004. == See also == Applications of UML Business Process Model and Notation (BPMN) C4 model Department of Defense Architecture Framework DOT (graph description language) List of Unified Modeling Language tools MODAF Model-based testing Model-driven engineering Object-oriented role analysis and modeling Process Specification Language Systems Modeling Language (SysML) == References == == Further reading == Ambler: Scott William (2004). The Object Primer: Agile Model Driven Development with UML 2. Cambridge University Press. ISBN 0-521-54018-6. Archived from the original on 31 January 2010. Retrieved 29 April 2006. Chonoles: Michael Jesse; James A. Schardt (2003). UML 2 for Dummies. Wiley Publishing. ISBN 0-7645-2614-6. Fowler: Martin (2004). UML Distilled: A Brief Guide to the Standard Object Modeling Language (3rd ed.). Addison-Wesley. ISBN 0-321-19368-7. Jacobson: Ivar; Grady Booch; James Rumbaugh (1998). The Unified Software Development Process. Addison Wesley Longman. ISBN 0-201-57169-2. Martin: Robert Cecil (2003). UML for Java Programmers. Prentice Hall. ISBN 0-13-142848-9. Noran: Ovidiu S. ""Business Modelling: UML vs. IDEF"" (PDF). Retrieved 14 November 2022. Horst Kargl. ""Interactive UML Metamodel with additional Examples"". Penker: Magnus; Hans-Erik Eriksson (2000). Business Modeling with UML. John Wiley & Sons. ISBN 0-471-29551-5. Douglass: Bruce Powel. ""Bruce Douglass: Real-Time Agile Systems and Software Development"" (web). Retrieved 1 January 2019. Douglass: Bruce (2014). Real-Time UML Workshop 2nd Edition. Newnes. ISBN 978-0-471-29551-8. Douglass: Bruce (2004). Real-Time UML 3rd Edition. Newnes. ISBN 978-0321160768. Douglass: Bruce (2002). Real-Time Design Patterns. Addison-Wesley Professional. ISBN 978-0201699562. Douglass: Bruce (2009). Real-Time Agility. Addison-Wesley Professional. ISBN 978-0321545497. Douglass: Bruce (2010). Design Patterns for Embedded Systems in C. Newnes. ISBN 978-1856177078. == External links == Official website Current Version Specification\nmodel and the set of diagrams of a system. A diagram is a partial graphic representation of a system\'s model. The set of diagrams need not completely cover the model and deleting a diagram does not change the model. The model may also contain documentation that drives the model elements and diagrams (such as written use cases). UML diagrams represent two different views of a system model: Static (or structural) view: emphasizes the static structure of the system using objects: attributes: operations and relationships. It includes class diagrams and composite structure diagrams. Dynamic (or behavioral) view: emphasizes the dynamic behavior of the system by showing collaborations among objects and changes to the internal states of objects. This view includes sequence diagrams: activity diagrams and state machine diagrams.UML models can be exchanged among UML tools by using the XML Metadata Interchange (XMI) format. In UML: one of the key tools for behavior modeling is the use-case model: caused by OOSE. Use cases are a way of specifying required usages of a system. Typically: they are used to capture the requirements of a system: that is: what a': 'original_similarity': 0.8073596775108995: 'slide': 184}"
0.45465169998351485,How do executable modeling languages benefit programmers?,"Executable modeling languages aim to boost the productivity of skilled programmers. They allow programmers to tackle more complex problems, like parallel computing and distributed systems. However, their use does not imply that the need for programmers is eliminated. ","{'chunk_text': 'of modeling languages appear in the literature. == Type of modeling languages == === Graphical types === Example of graphical modeling languages in the field of computer science: project management and systems engineering: Behavior Trees are a formal: graphical modeling language used primarily in systems and software engineering. Commonly used to unambiguously represent the hundreds or even thousands of natural language requirements that are typically used to express the stakeholder needs for a large-scale software-integrated system. Business Process Modeling Notation (BPMN: and the XML form BPML) is an example of a Process Modeling language. C-K theory consists of a modeling language for design processes. DRAKON is a general-purpose algorithmic modeling language for specifying software-intensive systems: a schematic representation of an algorithm or a stepwise process: and a family of programming languages. EXPRESS and EXPRESS-G (ISO 10303-11) is an international standard general-purpose data modeling language. Extended Enterprise Modeling Language (EEML) is commonly used for business process modeling across a number of layers. Flowchart is a schematic representation of an algorithm or a stepwise process. Fundamental Modeling Concepts (FMC) modeling language for software-intensive systems. IDEF is a family of modeling languages: which include IDEF0 for functional modeling: IDEF1X for information modeling: IDEF3 for business process modeling: IDEF4 for Object-Oriented Design and IDEF5 for modeling ontologies. Jackson Structured Programming (JSP) is a method for structured programming based on correspondences between data stream structure and program structure. LePUS3 is an object-oriented visual Design Description Language and a formal specification language that is suitable primarily for modeling large object-oriented (Java: C++: C#) programs and design patterns. Lifecycle Modeling Language is an open-standard language for systems engineering that supports the full system lifecycle: conceptual: utilization: support and retirement stages. Object-Role Modeling (ORM) in the field of software engineering is a method for conceptual modeling: and can be used as a tool for information and rules analysis. Petri nets use variations on exactly one diagramming technique and topology: namely the bipartite graph. The simplicity of its basic user interface easily enabled extensive tool support over the years: particularly in the areas of model checking: graphically oriented simulation: and software verification. Southbeach Notation is a visual modeling language used to describe situations in terms of agents that are considered useful or harmful from the modeler\'s perspective. The notation shows how the agents interact with each other and whether this interaction improves or worsens the situation. Specification and Description Language (SDL) is a specification language targeted at the unambiguous specification and description of the behavior of reactive and distributed systems. SysML is a Domain-Specific Modeling language for systems engineering that is defined as a UML profile (customization). Unified Modeling Language (UML) is a general-purpose modeling language that is an industry standard for specifying software-intensive systems. UML 2.0: the current version: supports thirteen different diagram techniques: and has widespread tool support. Service-oriented modeling framework (SOMF) is a holistic language for designing enterprise and application level architecture models in the space of enterprise architecture: virtualization: service-oriented architecture (SOA): cloud computing: and more. Architecture description language (ADL) is a language used to describe and represent the systems architecture of a system. Architecture Analysis & Design Language (AADL) is a modeling language that supports early and repeated analyses of a system\'s architecture with respect to performance-critical properties through an extendable notation: a tool framework: and precisely defined semantics.Examples of graphical modeling languages in other fields of science. EAST-ADL is a Domain-Specific Modeling language dedicated to automotive system design. Energy Systems Language (ESL): a language that aims to model ecological energetics & global economics. IEC 61499 defines Domain-Specific Modeling language dedicated to distribute industrial process measurement and control systems. === Textual types === Information models can also be expressed in formalized natural languages: such as Gellish. Gellish has natural language variants such as Gellish Formal English and Gellish Formal Dutch (Gellish Formeel Nederlands): etc. Gellish Formal English is an information representation language or semantic modeling language that is defined in the Gellish English Dictionary-Taxonomy: which has the form of a Taxonomy-Ontology (similarly for Dutch). Gellish Formal English is not only suitable to express knowledge: requirements and dictionaries: taxonomies and ontologies: but also information about individual things. All that information is expressed in one language and therefore it can all be integrated: independent of the question whether it is stored in central or distributed or in federated databases. Information models in Gellish Formal English consists of collections of Gellish Formal English expressions: that use natural language terms and formalized phrases. For example: a geographic information model might consist of a number of Gellish Formal English expressions: such as: - the Eiffel tower <is located in> Paris - Paris <is classified as a> city whereas information requirements and knowledge can be expressed for example as follows: - tower <shall be located in a> geographical area - city <is a kind of> geographical area Such Gellish Formal English expressions use names of concepts (such as ""city"") and phrases\nthat represent relation types (such as ⟨is located in⟩ and ⟨is classified as a⟩) that should be selected from the Gellish English Dictionary-Taxonomy (or of your own domain dictionary). The Gellish English Dictionary-Taxonomy enables the creation of semantically rich information models: because the dictionary contains more than 600 standard relation types and contains definitions of more than 40000 concepts. An information model in Gellish can express facts or make statements: queries and answers. === More specific types === In the field of computer science recently more specific types of modeling languages have emerged. ==== Algebraic ==== Algebraic Modeling Languages (AML) are high-level programming languages for describing and solving high complexity problems for large scale mathematical computation (i.e. large scale optimization type problems). One particular advantage of AMLs like AIMMS: AMPL: GAMS: Gekko: Mosel: OPL and OptimJ is the similarity of its syntax to the mathematical notation of optimization problems. This allows for a very concise and readable definition of problems in the domain of optimization: which is supported by certain language elements like sets: indices: algebraic expressions: powerful sparse index': 'original_similarity': 0.8329792897558689: 'slide': 178}","{'chunk_text': ""large scale optimization type problems). One particular advantage of AMLs like AIMMS: AMPL: GAMS: Gekko: Mosel: OPL and OptimJ is the similarity of its syntax to the mathematical notation of optimization problems. This allows for a very concise and readable definition of problems in the domain of optimization: which is supported by certain language elements like sets: indices: algebraic expressions: powerful sparse index and data handling variables: constraints with arbitrary names. The algebraic formulation of a model does not contain any hints how to process it. ==== Behavioral ==== Behavioral languages are designed to describe the observable behavior of complex systems consisting of components that execute concurrently. These languages focus on the description of key concepts such as: concurrency: nondeterminism: synchronization: and communication. The semantic foundations of Behavioral languages are process calculus or process algebra. ==== Discipline-specific ==== A discipline-specific modeling (DspM) language is focused on deliverables affiliated with a specific software development life cycle stage. Therefore: such language offers a distinct vocabulary: syntax: and notation for each stage: such as discovery: analysis: design: architecture: contraction: etc. For example: for the analysis phase of a project: the modeler employs specific analysis notation to deliver an analysis proposition diagram. During the design phase: however: logical design notation is used to depict relationship between software entities. In addition: the discipline-specific modeling language best practices does not preclude practitioners from combining the various notations in a single diagram. ==== Domain-specific ==== Domain-specific modeling (DSM) is a software engineering methodology for designing and developing systems: most often IT systems such as computer software. It involves systematic use of a graphical domain-specific language (DSL) to represent the various facets of a system. DSM languages tend to support higher-level abstractions than General-purpose modeling languages: so they require less effort and fewer low-level details to specify a given system. ==== Framework-specific ==== A framework-specific modeling language (FSML) is a kind of domain-specific modeling language which is designed for an object-oriented application framework. FSMLs define framework-provided abstractions as FSML concepts and decompose the abstractions into features. The features represent implementation steps or choices. A FSML concept can be configured by selecting features and providing values for features. Such a concept configuration represents how the concept should be implemented in the code. In other words: concept configuration describes how the framework should be completed in order to create the implementation of the concept. ==== Information and knowledge modeling ==== Linked data and ontology engineering require 'host languages' to represent entities and the relations between them: constraints between the properties of entities and relations: and metadata attributes. JSON-LD and RDF are two major (and semantically almost equivalent) languages in this context: primarily because they support statement reification and contextualisation which are essential properties to support the higher-order logic needed to reason about models. Model transformation is a common example of such reasoning. ==== Object-oriented ==== Object modeling languages are modeling languages based on a standardized set of symbols and ways of arranging them to model (part of) an object oriented software design or system design. Some organizations use them extensively in combination with a software development methodology to progress from initial specification to an implementation plan and to communicate that plan to an entire team of developers and stakeholders. Because a modeling language is visual and at a higher-level of abstraction than code: using models encourages the generation of a shared vision that may prevent problems of differing interpretation later in development. Often software modeling tools are used to construct these models: which may then be capable of automatic translation to code. ==== Virtual reality ==== Virtual Reality Modeling Language (VRML): before 1995 known as the Virtual Reality Markup Language is a standard file format for representing 3-dimensional (3D) interactive vector graphics: designed particularly with the World Wide Web in mind. ==== Others ==== Architecture Description Language Face Modeling Language Generative Modelling Language Java Modeling Language Promela Rebeca Modeling Language Service Modeling Language Web Services Modeling Language X3D == Applications == Various kinds of modeling languages are applied in different disciplines: including computer science: information management: business process modeling: software engineering: and systems engineering. Modeling languages can be used to specify: system requirements: structures and behaviors.Modeling languages are intended to be used to precisely specify systems so that stakeholders (e.g.: customers: operators: analysts: designers) can better understand the system being modeled. The more mature modeling languages are precise: consistent and executable. Informal diagramming techniques applied with drawing tools are expected to produce useful pictorial representations of system requirements: structures and behaviors: which can be useful for communication: design: and problem solving but cannot be used programmatically.: 539 Executable modeling languages applied with proper tool support: however: are expected to automate system verification and validation: simulation and code generation from the same representations. == Quality == A review of modelling languages is essential to be able to assign which languages are appropriate for different modelling settings. In the term settings we include stakeholders: domain and the knowledge connected. Assessing the language quality is a means that aims to achieve better models. === Framework for evaluation === Here language quality is stated in accordance with the SEQUAL framework for quality of models developed by Krogstie: Sindre and Lindland (2003): since this is a framework that connects the language quality to a framework for general model\nquality. Five areas are used in this framework to describe language quality and these are supposed to express both the conceptual as well as the visual notation of the language. We will not go into a thoroughly explanation of the underlying quality framework of models but concentrate on the areas used to explain the language quality framework. ==== Domain appropriateness ==== The framework states the ability to represent the domain as domain appropriateness. The statement appropriateness can be a bit vague: but in this particular context it means able to express. You should ideally only be able to express things that are in the domain but be powerful enough to include everything that is in the domain. This"": 'original_similarity': 0.8200162645543956: 'slide': 116}","{'chunk_text': 'misuse. For example: the availability of large language models could reduce the skill-level required to commit bioterrorism; biosecurity researcher Kevin Esvelt has suggested that LLM creators should exclude from their training data papers on creating or enhancing pathogens.A study by researchers at Google and several universities: including Cornell University and University of California: Berkeley: showed that there are potential security risks in language models such as ChatGPT. In their study: they examined the possibility that questioners could get: from ChatGPT: the training data that the AI model used; they found that they could get the training data from the AI model. For example: when asking ChatGPT 3.5 turbo to repeat the word ""poem"" forever: the AI model will say ""poem"" hundreds of times and then diverge: deviating from the standard dialogue style and spitting out nonsense phrases: thus spitting out the training data as it is. The researchers have seen more than 10:000 examples of the AI model exposing their training data in a similar method. The researchers said that it was hard to tell if the AI model was actually safe or not.The potential presence of ""sleeper agents"" within LLM models is another emerging security concern. These are hidden functionalities built into the model that remain dormant until triggered by a specific event or condition. Upon activation: the LLM deviates from its expected behavior to make insecure actions. === Algorithmic bias === While LLMs have shown remarkable capabilities in generating human-like text: they are susceptible to inheriting and amplifying biases present in their training data. This can manifest in skewed representations or unfair treatment of different demographics: such as those based on race: gender: language: and cultural groups. Since English data is overrepresented in current large language models\' training data: it may also downplay non-English views. ==== Stereotyping ==== AI models can reinforce a wide range of stereotypes: including those based on gender: ethnicity: age: nationality: religion: or occupation. This can lead to outputs that unfairly generalize or caricature groups of people: sometimes in harmful or derogatory ways.Notably: gender bias refers to the tendency of these models to produce outputs that are unfairly prejudiced towards one gender over another. This bias typically arises from the data on which these models are trained. Large language models often assign roles and characteristics based on traditional gender norms. For example: it might associate nurses or secretaries predominantly with women and engineers or CEOs with men. ==== Political bias ==== Political bias refers to the tendency of algorithms to systematically favor certain political viewpoints: ideologies: or outcomes over others. Language models may also exhibit political biases. Since the training data includes a wide range of political opinions and coverage: the models might generate responses that lean towards particular political ideologies or viewpoints: depending on the prevalence of those views in the data. == List == For the training cost column: 1 petaFLOP-day = 1 petaFLOP/sec × 1 day = 8.64E19 FLOP. == See also == Foundation models == Notes == == References == == Further reading == Jurafsky: Dan: Martin: James. H. Speech and Language Processing: An Introduction to Natural Language Processing: Computational Linguistics: and Speech Recognition: 3rd Edition draft: 2023. Phuong: Mary; Hutter: Marcus (2022). ""Formal Algorithms for Transformers"". arXiv:2207.09238 [cs.LG]. Eloundou: Tyna; Manning: Sam; Mishkin: Pamela; Rock: Daniel (2023). ""GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models"". arXiv:2303.10130 [econ.GN]. Eldan: Ronen; Li: Yuanzhi (2023). ""TinyStories: How Small Can Language Models Be and Still Speak Coherent English?"". arXiv:2305.07759 [cs.CL]. Frank: Michael C. (27 June 2023). ""Baby steps in evaluating the capacities of large language models"". Nature Reviews Psychology. 2 (8): 451–452. doi:10.1038/s44159-023-00211-x. ISSN 2731-0574. S2CID 259713140. Retrieved 2 July 2023. Zhao: Wayne Xin; et al. (2023). ""A Survey of Large Language Models"". arXiv:2303.18223 [cs.CL]. Kaddour: Jean; et al. (2023). ""Challenges and Applications of Large Language Models"". arXiv:2307.10169 [cs.CL]. Yin: Shukang; Fu: Chaoyou; Zhao: Sirui; Li: Ke; Sun: Xing; Xu: Tong; Chen: Enhong (2023-06-01). ""A Survey on Multimodal Large Language Models"". arXiv:2306.13549 [cs.CV]. Open LLMs repository on GitHub.\nA modeling language is any artificial language that can be used to express data: information or knowledge or systems in a structure that is defined by a consistent set of rules. The rules are used for interpretation of the meaning of components in the structure Programing language. == Overview == A modeling language can be graphical or textual. Graphical modeling languages use a diagram technique with named symbols that represent concepts and lines that connect the symbols and represent relationships and various other graphical notation to represent constraints. Textual modeling languages may use standardized keywords accompanied by parameters or natural language terms and phrases to make computer-interpretable expressions.An example of a graphical modeling language and a corresponding textual modeling language is EXPRESS. Not all modeling languages are executable: and for those that are: the use of them doesn\'t necessarily mean that programmers are no longer required. On the contrary: executable modeling languages are intended to amplify the productivity of skilled programmers: so that they can address more challenging problems: such as parallel computing and distributed systems. A large number of modeling languages appear in the literature. == Type of modeling languages == === Graphical types === Example of graphical modeling languages in the field of computer science: project management and systems engineering: Behavior Trees are a formal: graphical modeling language used primarily in systems and software engineering. Commonly': 'original_similarity': 0.8054735872547235: 'slide': 226}"
0.45077349999337457,What is an example of a graphical modeling language and its corresponding textual modeling language?,EXPRESS is an example of a graphical modeling language and its corresponding textual modeling language.,"{'chunk_text': 'the contrary: executable modeling languages are intended to amplify the productivity of skilled programmers: so that they can address more challenging problems: such as parallel computing and distributed systems. A large number of modeling languages appear in the literature. == Type of modeling languages == === Graphical types === Example of graphical modeling languages in the field of computer science: project management and systems engineering: Behavior Trees are a formal: graphical modeling language used primarily in systems and software engineering. Commonly used to unambiguously represent the hundreds or even thousands of natural language requirements that are typically used to express the stakeholder needs for a large-scale software-integrated system. Business Process Modeling Notation (BPMN: and the XML form BPML) is an example of a Process Modeling language. C-K theory consists of a modeling language for design processes. DRAKON is a general-purpose algorithmic modeling language for specifying software-intensive systems: a schematic representation of an algorithm or a stepwise process: and a family of programming languages. EXPRESS and EXPRESS-G (ISO 10303-11) is an international standard general-purpose data modeling language. Extended Enterprise Modeling Language (EEML) is commonly used for business process modeling across a number of layers. Flowchart is a schematic representation of an algorithm or a stepwise process. Fundamental Modeling Concepts (FMC) modeling language for software-intensive systems. IDEF is a family of modeling languages: which include IDEF0 for functional modeling: IDEF1X for information modeling: IDEF3 for business process modeling: IDEF4 for Object-Oriented Design and IDEF5 for modeling ontologies. Jackson Structured Programming (JSP) is a method for structured programming based on correspondences between data stream structure and program structure. LePUS3 is an object-oriented visual Design Description Language and a formal specification language that is suitable primarily for modeling large object-oriented (Java: C++: C#) programs and design patterns. Lifecycle Modeling Language is an open-standard language for systems engineering that supports the full system lifecycle: conceptual: utilization: support and retirement stages. Object-Role Modeling (ORM) in the field of software engineering is a method for conceptual modeling: and can be used as a tool for information and rules analysis. Petri nets use variations on exactly one diagramming technique and topology: namely the bipartite graph. The simplicity of its basic user interface easily enabled extensive tool support over the years: particularly in the areas of model checking: graphically oriented simulation: and software verification. Southbeach Notation is a visual modeling language used to describe situations in terms of agents that are considered useful or harmful from the modeler\'s perspective. The notation shows how the agents interact with each other and whether this interaction improves or worsens the situation. Specification and Description Language (SDL) is a specification language targeted at the unambiguous specification and description of the behavior of reactive and distributed systems. SysML is a Domain-Specific Modeling language for systems engineering that is defined as a UML profile (customization). Unified Modeling Language (UML) is a general-purpose modeling language that is an industry standard for specifying software-intensive systems. UML 2.0: the current version: supports thirteen different diagram techniques: and has widespread tool support. Service-oriented modeling framework (SOMF) is a holistic language for designing enterprise and application level architecture models in the space of enterprise architecture: virtualization: service-oriented architecture (SOA): cloud computing: and more. Architecture description language (ADL) is a language used to describe and represent the systems architecture of a system. Architecture Analysis & Design Language (AADL) is a modeling language that supports early and repeated analyses of a system\'s architecture with respect to performance-critical properties through an extendable notation: a tool framework: and precisely defined semantics.Examples of graphical modeling languages in other fields of science. EAST-ADL is a Domain-Specific Modeling language dedicated to automotive system design. Energy Systems Language (ESL): a language that aims to model ecological energetics & global economics. IEC 61499 defines Domain-Specific Modeling language dedicated to distribute industrial process measurement and control systems. === Textual types === Information models can also be expressed in formalized natural languages: such as Gellish. Gellish has natural language variants such as Gellish Formal English and Gellish Formal Dutch (Gellish Formeel Nederlands): etc. Gellish Formal English is an information representation language or semantic modeling language that is defined in the Gellish English Dictionary-Taxonomy: which has the form of a Taxonomy-Ontology (similarly for Dutch). Gellish Formal English is not only suitable to express knowledge: requirements and dictionaries: taxonomies and ontologies: but also information about individual things. All that information is expressed in one language and therefore it can all be integrated: independent of the question whether it is stored in central or distributed or in federated databases. Information models in Gellish Formal English consists of collections of Gellish Formal English expressions: that use natural language terms and formalized phrases. For example: a geographic information model might consist of a number of Gellish Formal English expressions: such as: - the Eiffel tower <is located in> Paris - Paris <is classified as a> city whereas information requirements and knowledge can be expressed for example as follows: - tower <shall be located in a> geographical area - city <is a kind of> geographical area Such Gellish Formal English expressions use names of concepts (such as ""city"") and phrases\nthat represent relation types (such as ⟨is located in⟩ and ⟨is classified as a⟩) that should be selected from the Gellish English Dictionary-Taxonomy (or of your own domain dictionary). The Gellish English Dictionary-Taxonomy enables the creation of semantically rich information models: because the dictionary contains more than 600 standard relation types and contains definitions of more than 40000 concepts. An information model in Gellish can express facts or make statements: queries and answers. === More specific types === In the field of computer science recently more specific types of modeling languages have emerged. ==== Algebraic ==== Algebraic Modeling Languages (AML) are high-level programming languages for describing and solving high complexity problems for large scale mathematical computation (i.e. large scale optimization type problems). One particular advantage of AMLs like AIMMS: AMPL: GAMS: Gekko: Mosel: OPL and OptimJ is the similarity of its syntax to the mathematical notation of optimization': 'original_similarity': 0.8926064176622946: 'slide': 146}","{'chunk_text': ""information model in Gellish can express facts or make statements: queries and answers. === More specific types === In the field of computer science recently more specific types of modeling languages have emerged. ==== Algebraic ==== Algebraic Modeling Languages (AML) are high-level programming languages for describing and solving high complexity problems for large scale mathematical computation (i.e. large scale optimization type problems). One particular advantage of AMLs like AIMMS: AMPL: GAMS: Gekko: Mosel: OPL and OptimJ is the similarity of its syntax to the mathematical notation of optimization problems. This allows for a very concise and readable definition of problems in the domain of optimization: which is supported by certain language elements like sets: indices: algebraic expressions: powerful sparse index and data handling variables: constraints with arbitrary names. The algebraic formulation of a model does not contain any hints how to process it. ==== Behavioral ==== Behavioral languages are designed to describe the observable behavior of complex systems consisting of components that execute concurrently. These languages focus on the description of key concepts such as: concurrency: nondeterminism: synchronization: and communication. The semantic foundations of Behavioral languages are process calculus or process algebra. ==== Discipline-specific ==== A discipline-specific modeling (DspM) language is focused on deliverables affiliated with a specific software development life cycle stage. Therefore: such language offers a distinct vocabulary: syntax: and notation for each stage: such as discovery: analysis: design: architecture: contraction: etc. For example: for the analysis phase of a project: the modeler employs specific analysis notation to deliver an analysis proposition diagram. During the design phase: however: logical design notation is used to depict relationship between software entities. In addition: the discipline-specific modeling language best practices does not preclude practitioners from combining the various notations in a single diagram. ==== Domain-specific ==== Domain-specific modeling (DSM) is a software engineering methodology for designing and developing systems: most often IT systems such as computer software. It involves systematic use of a graphical domain-specific language (DSL) to represent the various facets of a system. DSM languages tend to support higher-level abstractions than General-purpose modeling languages: so they require less effort and fewer low-level details to specify a given system. ==== Framework-specific ==== A framework-specific modeling language (FSML) is a kind of domain-specific modeling language which is designed for an object-oriented application framework. FSMLs define framework-provided abstractions as FSML concepts and decompose the abstractions into features. The features represent implementation steps or choices. A FSML concept can be configured by selecting features and providing values for features. Such a concept configuration represents how the concept should be implemented in the code. In other words: concept configuration describes how the framework should be completed in order to create the implementation of the concept. ==== Information and knowledge modeling ==== Linked data and ontology engineering require 'host languages' to represent entities and the relations between them: constraints between the properties of entities and relations: and metadata attributes. JSON-LD and RDF are two major (and semantically almost equivalent) languages in this context: primarily because they support statement reification and contextualisation which are essential properties to support the higher-order logic needed to reason about models. Model transformation is a common example of such reasoning. ==== Object-oriented ==== Object modeling languages are modeling languages based on a standardized set of symbols and ways of arranging them to model (part of) an object oriented software design or system design. Some organizations use them extensively in combination with a software development methodology to progress from initial specification to an implementation plan and to communicate that plan to an entire team of developers and stakeholders. Because a modeling language is visual and at a higher-level of abstraction than code: using models encourages the generation of a shared vision that may prevent problems of differing interpretation later in development. Often software modeling tools are used to construct these models: which may then be capable of automatic translation to code. ==== Virtual reality ==== Virtual Reality Modeling Language (VRML): before 1995 known as the Virtual Reality Markup Language is a standard file format for representing 3-dimensional (3D) interactive vector graphics: designed particularly with the World Wide Web in mind. ==== Others ==== Architecture Description Language Face Modeling Language Generative Modelling Language Java Modeling Language Promela Rebeca Modeling Language Service Modeling Language Web Services Modeling Language X3D == Applications == Various kinds of modeling languages are applied in different disciplines: including computer science: information management: business process modeling: software engineering: and systems engineering. Modeling languages can be used to specify: system requirements: structures and behaviors.Modeling languages are intended to be used to precisely specify systems so that stakeholders (e.g.: customers: operators: analysts: designers) can better understand the system being modeled. The more mature modeling languages are precise: consistent and executable. Informal diagramming techniques applied with drawing tools are expected to produce useful pictorial representations of system requirements: structures and behaviors: which can be useful for communication: design: and problem solving but cannot be used programmatically.: 539 Executable modeling languages applied with proper tool support: however: are expected to automate system verification and validation: simulation and code generation from the same representations. == Quality == A review of modelling languages is essential to be able to assign which languages are appropriate for different modelling settings. In the term settings we include stakeholders: domain and the knowledge connected. Assessing the language quality is a means that aims to achieve better models. === Framework for evaluation === Here language quality is stated in accordance with the SEQUAL framework for quality of models developed by Krogstie: Sindre and Lindland (2003): since this is a framework that connects the language quality to a framework for general model\nquality. Five areas are used in this framework to describe language quality and these are supposed to express both the conceptual as well as the visual notation of the language. We will not go into a thoroughly explanation of the underlying quality framework of models but concentrate on the areas used to explain the language quality framework. ==== Domain appropriateness ===="": 'original_similarity': 0.8394919096694498: 'slide': 59}","{'chunk_text': 'system: that is: what a system is supposed to do. == Diagrams == UML 2 has many types of diagrams: which are divided into two categories. Some types represent structural information: and the rest represent general types of behavior: including a few that represent different aspects of interactions. These diagrams can be categorized hierarchically as shown in the following class diagram: These diagrams may all contain comments or notes explaining usage: constraint: or intent. === Structure diagrams === Structure diagrams represent the static aspects of the system. It emphasizes the things that must be present in the system being modeled. Since structure diagrams represent the structure: they are used extensively in documenting the software architecture of software systems. For example: the component diagram describes how a software system is split up into components and shows the dependencies among these components. === Behavior diagrams === Behavior diagrams represent the dynamic aspect of the system. It emphasizes what must happen in the system being modeled. Since behavior diagrams illustrate the behavior of a system: they are used extensively to describe the functionality of software systems. As an example: the activity diagram describes the business and operational step-by-step activities of the components in a system. ==== Interaction diagrams ==== Interaction diagrams: a subset of behavior diagrams: emphasize the flow of control and data among the things in the system being modeled. For example: the sequence diagram shows how objects communicate with each other regarding a sequence of messages. == Metamodeling == The Object Management Group (OMG) has developed a metamodeling architecture to define the UML: called the Meta-Object Facility. MOF is designed as a four-layered architecture: as shown in the image at right. It provides a meta-meta model at the top: called the M3 layer. This M3-model is the language used by Meta-Object Facility to build metamodels: called M2-models. The most prominent example of a Layer 2 Meta-Object Facility model is the UML metamodel: which describes the UML itself. These M2-models describe elements of the M1-layer: and thus M1-models. These would be: for example: models written in UML. The last layer is the M0-layer or data layer. It is used to describe runtime instances of the system.The meta-model can be extended using a mechanism called stereotyping. This has been criticized as being insufficient/untenable by Brian Henderson-Sellers and Cesar Gonzalez-Perez in ""Uses and Abuses of the Stereotype Mechanism in UML 1.x and 2.0"". == Adoption == Back in 2013 UML has been marketed by OMG for many contexts: but aimed primarily at software development with limited success.It has been treated: at times: as a design silver bullet: which leads to problems. UML misuse includes overuse (designing every part of the system with it: which is unnecessary) and assuming that novices can design with it.It is considered a large language: with many constructs. Some people (including Jacobson) feel that UML\'s size hinders learning (and therefore using) it.MS Visual Studio dropped support for UML in 2016 due to lack of usage.According to Google Trends UML has been on steady decline since 2004. == See also == Applications of UML Business Process Model and Notation (BPMN) C4 model Department of Defense Architecture Framework DOT (graph description language) List of Unified Modeling Language tools MODAF Model-based testing Model-driven engineering Object-oriented role analysis and modeling Process Specification Language Systems Modeling Language (SysML) == References == == Further reading == Ambler: Scott William (2004). The Object Primer: Agile Model Driven Development with UML 2. Cambridge University Press. ISBN 0-521-54018-6. Archived from the original on 31 January 2010. Retrieved 29 April 2006. Chonoles: Michael Jesse; James A. Schardt (2003). UML 2 for Dummies. Wiley Publishing. ISBN 0-7645-2614-6. Fowler: Martin (2004). UML Distilled: A Brief Guide to the Standard Object Modeling Language (3rd ed.). Addison-Wesley. ISBN 0-321-19368-7. Jacobson: Ivar; Grady Booch; James Rumbaugh (1998). The Unified Software Development Process. Addison Wesley Longman. ISBN 0-201-57169-2. Martin: Robert Cecil (2003). UML for Java Programmers. Prentice Hall. ISBN 0-13-142848-9. Noran: Ovidiu S. ""Business Modelling: UML vs. IDEF"" (PDF). Retrieved 14 November 2022. Horst Kargl. ""Interactive UML Metamodel with additional Examples"". Penker: Magnus; Hans-Erik Eriksson (2000). Business Modeling with UML. John Wiley & Sons. ISBN 0-471-29551-5. Douglass: Bruce Powel. ""Bruce Douglass: Real-Time Agile Systems and Software Development"" (web). Retrieved 1 January 2019. Douglass: Bruce (2014). Real-Time UML Workshop 2nd Edition. Newnes. ISBN 978-0-471-29551-8. Douglass: Bruce (2004). Real-Time UML 3rd Edition. Newnes. ISBN 978-0321160768. Douglass: Bruce (2002). Real-Time Design Patterns. Addison-Wesley Professional. ISBN 978-0201699562. Douglass: Bruce (2009). Real-Time Agility. Addison-Wesley Professional. ISBN 978-0321545497. Douglass: Bruce (2010). Design Patterns for Embedded Systems in C. Newnes. ISBN 978-1856177078. == External links == Official website Current Version Specification\nmodel and the set of diagrams of a system. A diagram is a partial graphic representation of a system\'s model. The set of diagrams need not completely cover the model and deleting a diagram does not change the model. The model may also contain documentation that drives the model elements and diagrams (such as written use cases). UML diagrams represent two different views of a system model: Static (or structural) view: emphasizes the static structure of the system using objects: attributes: operations and relationships. It includes class diagrams and composite structure diagrams. Dynamic (or behavioral) view: emphasizes the dynamic behavior of the system by showing collaborations among objects and changes to the internal states of objects. This view includes sequence diagrams: activity diagrams and state machine diagrams.UML models can be exchanged among UML tools by using the XML Metadata Interchange (XMI) format. In UML: one of the key tools for behavior modeling is the use-case model: caused by OOSE. Use cases are a way of specifying required usages of a system. Typically: they are used to capture the requirements of a system:': 'original_similarity': 0.8157092051112506: 'slide': 180}"
0.4470629999996163,What is the role of modeling languages in system development?,"Modeling languages can be used to specify system requirements, structures, and behaviors, offering a precise specification of systems to better understand the system being modeled by stakeholders like customers, operators, analysts, and designers. More mature modeling languages that are precise, consistent, and executable can automate system verification and validation, simulation, and code generation.","{'chunk_text': 'Graphical types === Example of graphical modeling languages in the field of computer science: project management and systems engineering: Behavior Trees are a formal: graphical modeling language used primarily in systems and software engineering. Commonly used to unambiguously represent the hundreds or even thousands of natural language requirements that are typically used to express the stakeholder needs for a large-scale software-integrated system. Business Process Modeling Notation (BPMN: and the XML form BPML) is an example of a Process Modeling language. C-K theory consists of a modeling language for design processes. DRAKON is a general-purpose algorithmic modeling language for specifying software-intensive systems: a schematic representation of an algorithm or a stepwise process: and a family of programming languages. EXPRESS and EXPRESS-G (ISO 10303-11) is an international standard general-purpose data modeling language. Extended Enterprise Modeling Language (EEML) is commonly used for business process modeling across a number of layers. Flowchart is a schematic representation of an algorithm or a stepwise process. Fundamental Modeling Concepts (FMC) modeling language for software-intensive systems. IDEF is a family of modeling languages: which include IDEF0 for functional modeling: IDEF1X for information modeling: IDEF3 for business process modeling: IDEF4 for Object-Oriented Design and IDEF5 for modeling ontologies. Jackson Structured Programming (JSP) is a method for structured programming based on correspondences between data stream structure and program structure. LePUS3 is an object-oriented visual Design Description Language and a formal specification language that is suitable primarily for modeling large object-oriented (Java: C++: C#) programs and design patterns. Lifecycle Modeling Language is an open-standard language for systems engineering that supports the full system lifecycle: conceptual: utilization: support and retirement stages. Object-Role Modeling (ORM) in the field of software engineering is a method for conceptual modeling: and can be used as a tool for information and rules analysis. Petri nets use variations on exactly one diagramming technique and topology: namely the bipartite graph. The simplicity of its basic user interface easily enabled extensive tool support over the years: particularly in the areas of model checking: graphically oriented simulation: and software verification. Southbeach Notation is a visual modeling language used to describe situations in terms of agents that are considered useful or harmful from the modeler\'s perspective. The notation shows how the agents interact with each other and whether this interaction improves or worsens the situation. Specification and Description Language (SDL) is a specification language targeted at the unambiguous specification and description of the behavior of reactive and distributed systems. SysML is a Domain-Specific Modeling language for systems engineering that is defined as a UML profile (customization). Unified Modeling Language (UML) is a general-purpose modeling language that is an industry standard for specifying software-intensive systems. UML 2.0: the current version: supports thirteen different diagram techniques: and has widespread tool support. Service-oriented modeling framework (SOMF) is a holistic language for designing enterprise and application level architecture models in the space of enterprise architecture: virtualization: service-oriented architecture (SOA): cloud computing: and more. Architecture description language (ADL) is a language used to describe and represent the systems architecture of a system. Architecture Analysis & Design Language (AADL) is a modeling language that supports early and repeated analyses of a system\'s architecture with respect to performance-critical properties through an extendable notation: a tool framework: and precisely defined semantics.Examples of graphical modeling languages in other fields of science. EAST-ADL is a Domain-Specific Modeling language dedicated to automotive system design. Energy Systems Language (ESL): a language that aims to model ecological energetics & global economics. IEC 61499 defines Domain-Specific Modeling language dedicated to distribute industrial process measurement and control systems. === Textual types === Information models can also be expressed in formalized natural languages: such as Gellish. Gellish has natural language variants such as Gellish Formal English and Gellish Formal Dutch (Gellish Formeel Nederlands): etc. Gellish Formal English is an information representation language or semantic modeling language that is defined in the Gellish English Dictionary-Taxonomy: which has the form of a Taxonomy-Ontology (similarly for Dutch). Gellish Formal English is not only suitable to express knowledge: requirements and dictionaries: taxonomies and ontologies: but also information about individual things. All that information is expressed in one language and therefore it can all be integrated: independent of the question whether it is stored in central or distributed or in federated databases. Information models in Gellish Formal English consists of collections of Gellish Formal English expressions: that use natural language terms and formalized phrases. For example: a geographic information model might consist of a number of Gellish Formal English expressions: such as: - the Eiffel tower <is located in> Paris - Paris <is classified as a> city whereas information requirements and knowledge can be expressed for example as follows: - tower <shall be located in a> geographical area - city <is a kind of> geographical area Such Gellish Formal English expressions use names of concepts (such as ""city"") and phrases\nthat represent relation types (such as ⟨is located in⟩ and ⟨is classified as a⟩) that should be selected from the Gellish English Dictionary-Taxonomy (or of your own domain dictionary). The Gellish English Dictionary-Taxonomy enables the creation of semantically rich information models: because the dictionary contains more than 600 standard relation types and contains definitions of more than 40000 concepts. An information model in Gellish can express facts or make statements: queries and answers. === More specific types === In the field of computer science recently more specific types of modeling languages have emerged. ==== Algebraic ==== Algebraic Modeling Languages (AML) are high-level programming languages for describing and solving high complexity problems for large scale mathematical computation (i.e. large scale optimization type problems). One particular advantage of AMLs like AIMMS: AMPL: GAMS: Gekko: Mosel: OPL and OptimJ is the similarity of its syntax to the mathematical notation of optimization problems. This allows for a very concise and readable definition of problems in the domain of optimization: which is supported by certain language elements like sets: indices: algebraic expressions: powerful sparse index and data handling variables: constraints with arbitrary names. The algebraic formulation of a model': 'original_similarity': 0.8607119334828455: 'slide': 192}","{'chunk_text': ""==== Algebraic Modeling Languages (AML) are high-level programming languages for describing and solving high complexity problems for large scale mathematical computation (i.e. large scale optimization type problems). One particular advantage of AMLs like AIMMS: AMPL: GAMS: Gekko: Mosel: OPL and OptimJ is the similarity of its syntax to the mathematical notation of optimization problems. This allows for a very concise and readable definition of problems in the domain of optimization: which is supported by certain language elements like sets: indices: algebraic expressions: powerful sparse index and data handling variables: constraints with arbitrary names. The algebraic formulation of a model does not contain any hints how to process it. ==== Behavioral ==== Behavioral languages are designed to describe the observable behavior of complex systems consisting of components that execute concurrently. These languages focus on the description of key concepts such as: concurrency: nondeterminism: synchronization: and communication. The semantic foundations of Behavioral languages are process calculus or process algebra. ==== Discipline-specific ==== A discipline-specific modeling (DspM) language is focused on deliverables affiliated with a specific software development life cycle stage. Therefore: such language offers a distinct vocabulary: syntax: and notation for each stage: such as discovery: analysis: design: architecture: contraction: etc. For example: for the analysis phase of a project: the modeler employs specific analysis notation to deliver an analysis proposition diagram. During the design phase: however: logical design notation is used to depict relationship between software entities. In addition: the discipline-specific modeling language best practices does not preclude practitioners from combining the various notations in a single diagram. ==== Domain-specific ==== Domain-specific modeling (DSM) is a software engineering methodology for designing and developing systems: most often IT systems such as computer software. It involves systematic use of a graphical domain-specific language (DSL) to represent the various facets of a system. DSM languages tend to support higher-level abstractions than General-purpose modeling languages: so they require less effort and fewer low-level details to specify a given system. ==== Framework-specific ==== A framework-specific modeling language (FSML) is a kind of domain-specific modeling language which is designed for an object-oriented application framework. FSMLs define framework-provided abstractions as FSML concepts and decompose the abstractions into features. The features represent implementation steps or choices. A FSML concept can be configured by selecting features and providing values for features. Such a concept configuration represents how the concept should be implemented in the code. In other words: concept configuration describes how the framework should be completed in order to create the implementation of the concept. ==== Information and knowledge modeling ==== Linked data and ontology engineering require 'host languages' to represent entities and the relations between them: constraints between the properties of entities and relations: and metadata attributes. JSON-LD and RDF are two major (and semantically almost equivalent) languages in this context: primarily because they support statement reification and contextualisation which are essential properties to support the higher-order logic needed to reason about models. Model transformation is a common example of such reasoning. ==== Object-oriented ==== Object modeling languages are modeling languages based on a standardized set of symbols and ways of arranging them to model (part of) an object oriented software design or system design. Some organizations use them extensively in combination with a software development methodology to progress from initial specification to an implementation plan and to communicate that plan to an entire team of developers and stakeholders. Because a modeling language is visual and at a higher-level of abstraction than code: using models encourages the generation of a shared vision that may prevent problems of differing interpretation later in development. Often software modeling tools are used to construct these models: which may then be capable of automatic translation to code. ==== Virtual reality ==== Virtual Reality Modeling Language (VRML): before 1995 known as the Virtual Reality Markup Language is a standard file format for representing 3-dimensional (3D) interactive vector graphics: designed particularly with the World Wide Web in mind. ==== Others ==== Architecture Description Language Face Modeling Language Generative Modelling Language Java Modeling Language Promela Rebeca Modeling Language Service Modeling Language Web Services Modeling Language X3D == Applications == Various kinds of modeling languages are applied in different disciplines: including computer science: information management: business process modeling: software engineering: and systems engineering. Modeling languages can be used to specify: system requirements: structures and behaviors.Modeling languages are intended to be used to precisely specify systems so that stakeholders (e.g.: customers: operators: analysts: designers) can better understand the system being modeled. The more mature modeling languages are precise: consistent and executable. Informal diagramming techniques applied with drawing tools are expected to produce useful pictorial representations of system requirements: structures and behaviors: which can be useful for communication: design: and problem solving but cannot be used programmatically.: 539 Executable modeling languages applied with proper tool support: however: are expected to automate system verification and validation: simulation and code generation from the same representations. == Quality == A review of modelling languages is essential to be able to assign which languages are appropriate for different modelling settings. In the term settings we include stakeholders: domain and the knowledge connected. Assessing the language quality is a means that aims to achieve better models. === Framework for evaluation === Here language quality is stated in accordance with the SEQUAL framework for quality of models developed by Krogstie: Sindre and Lindland (2003): since this is a framework that connects the language quality to a framework for general model\nquality. Five areas are used in this framework to describe language quality and these are supposed to express both the conceptual as well as the visual notation of the language. We will not go into a thoroughly explanation of the underlying quality framework of models but concentrate on the areas used to explain the language quality framework. ==== Domain appropriateness ==== The framework states the ability to represent the domain as domain appropriateness. The statement appropriateness can be a bit vague: but in this particular context it means able to express. You should ideally only be"": 'original_similarity': 0.8565885524354862: 'slide': 94}","{'chunk_text': 'quality and these are supposed to express both the conceptual as well as the visual notation of the language. We will not go into a thoroughly explanation of the underlying quality framework of models but concentrate on the areas used to explain the language quality framework. ==== Domain appropriateness ==== The framework states the ability to represent the domain as domain appropriateness. The statement appropriateness can be a bit vague: but in this particular context it means able to express. You should ideally only be able to express things that are in the domain but be powerful enough to include everything that is in the domain. This requirement might seem a bit strict: but the aim is to get a visually expressed model which includes everything relevant to the domain and excludes everything not appropriate for the domain. To achieve this: the language has to have a good distinction of which notations and syntaxes that are advantageous to present. ==== Participant appropriateness ==== To evaluate the participant appropriateness we try to identify how well the language expresses the knowledge held by the stakeholders. This involves challenges since a stakeholder\'s knowledge is subjective. The knowledge of the stakeholder is both tacit and explicit. Both types of knowledge are of dynamic character. In this framework only the explicit type of knowledge is taken into account. The language should to a large extent express all the explicit knowledge of the stakeholders relevant to the domain. ==== Modeller appropriateness ==== Last paragraph stated that knowledge of the stakeholders should be presented in a good way. In addition it is imperative that the language should be able to express all possible explicit knowledge of the stakeholders. No knowledge should be left unexpressed due to lacks in the language. ==== Comprehensibility appropriateness ==== Comprehensibility appropriateness makes sure that the social actors understand the model due to a consistent use of the language. To achieve this the framework includes a set of criteria. The general importance that these express is that the language should be flexible: easy to organize and easy to distinguish different parts of the language internally as well as from other languages. In addition to this: the goal should be as simple as possible and that each symbol in the language has a unique representation. This is in connection to also to the structure of the development requirements. . ==== Tool appropriateness ==== To ensure that the domain actually modelled is usable for analyzing and further processing: the language has to ensure that it is possible to reason in an automatic way. To achieve this it has to include formal syntax and semantics. Another advantage by formalizing is the ability to discover errors in an early stage. It is not always that the language best fitted for the technical actors is the same as for the social actors. ==== Organizational appropriateness ==== The language used is appropriate for the organizational context: e.g. that the language is standardized within the organization: or that it is supported by tools that are chosen as standard in the organization. == See also == == References == == Further reading == John Krogstie (2003) ""Evaluating UML using a generic quality framework"" . SINTEF Telecom and Informatics and IDI: NTNU: Norway Krogstie and Sølvsberg (2003). Information Systems Engineering: Conceptual Modeling in a Quality Perspective. Institute of computer and information sciences.\\ Anna Gunhild Nysetvold and John Krogstie (2005). ""Assessing business processing modeling languages using a generic quality framework"". Institute of computer and information sciences. == External links == Fundamental Modeling Concepts Software Modeling Languages Portal BIP -- Incremental Component-based Construction of Real-time Systems Gellish Formal English\nA neural network: also called a neuronal network: is an interconnected population': 'original_similarity': 0.8178848570002414: 'slide': 10}"
0.47539700000197627,What is a neural network and how closely is it related to artificial neural networks?,"A neural network, also known as a neuronal network, is an interconnected population of neurons that typically contain multiple neural circuits. These networks are studied to understand the organization and functioning of nervous systems. Artificial neural networks are machine learning models, which are very closely related to biological neural networks as they are inspired and designed based on the mechanisms used by neural circuits in biological networks.","{'chunk_text': 'play Atari video games using only pixels as data input. In 2015 they demonstrated their AlphaGo system: which learned the game of Go well enough to beat a professional Go player. Google Translate uses a neural network to translate between more than 100 languages. In 2017: Covariant.ai was launched: which focuses on integrating deep learning into factories.As of 2008: researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement: or TAMER: which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor. First developed as TAMER: a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot with the ability to learn new tasks through observation. Using Deep TAMER: a robot learned a task with a human trainer: watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer: who provided feedback such as ""good job"" and ""bad job"". == Criticism and comment == Deep learning has attracted both criticism and comment: in some cases from outside the field of computer science. === Theory === A main criticism concerns the lack of theory surrounding some methods. Learning in the most common deep architectures is implemented using well-understood gradient descent. However: the theory surrounding other algorithms: such as contrastive divergence is less clear. (e.g.: Does it converge? If so: how fast? What is it approximating?) Deep learning methods are often looked at as a black box: with most confirmations done empirically: rather than theoretically.Others point out that deep learning should be looked at as a step towards realizing strong AI: not as an all-encompassing solution. Despite the power of deep learning methods: they still lack much of the functionality needed to realize this goal entirely. Research psychologist Gary Marcus noted: Realistically: deep learning is only part of the larger challenge of building intelligent machines. Such techniques lack ways of representing causal relationships (...) have no obvious ways of performing logical inferences: and they are also still a long way from integrating abstract knowledge: such as information about what objects are: what they are for: and how they are typically used. The most powerful A.I. systems: like Watson (...) use techniques like deep learning as just one element in a very complicated ensemble of techniques: ranging from the statistical technique of Bayesian inference to deductive reasoning. In further reference to the idea that artistic sensitivity might be inherent in relatively low levels of the cognitive hierarchy: a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained demonstrate a visual appeal: the original research notice received well over 1:000 comments: and was the subject of what was for a time the most frequently accessed article on The Guardian\'s website. === Errors === Some deep learning architectures display problematic behaviors: such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images (2014) and misclassifying minuscule perturbations of correctly classified images (2013). Goertzel hypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-component artificial general intelligence (AGI) architectures. These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar decompositions of observed entities and events. Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic goal of both human language acquisition and artificial intelligence (AI). === Cyber threat === As deep learning moves from the lab into the world: research and experience show that artificial neural networks are vulnerable to hacks and deception. By identifying patterns that these systems use to function: attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize. For example: an attacker can make subtle changes to an image such that the ANN finds a match even though the image looks to a human nothing like the search target. Such manipulation is termed an ""adversarial attack"".In 2016 researchers used one ANN to doctor images in trial and error fashion: identify another\'s focal points: and thereby generate images that deceived it. The modified images looked no different to human eyes. Another group showed that printouts of doctored images then photographed successfully tricked an image classification system. One defense is reverse image search: in which a possible fake image is submitted to a site such as TinEye that can then find other instances of it. A refinement is to search using only parts of the image: to identify images from which that piece may have been taken.Another group showed that certain psychedelic spectacles could fool a facial recognition system into thinking ordinary people were celebrities: potentially allowing one person to impersonate another. In 2017 researchers added stickers to stop signs and caused an ANN to misclassify them.ANNs\ncan however be further trained to detect attempts at deception: potentially leading attackers and defenders into an arms race similar to the kind that already defines the malware defense industry. ANNs have been trained to defeat ANN-based anti-malware software by repeatedly attacking a defense with malware that was continually altered by a genetic algorithm until it tricked the anti-malware while retaining its ability to damage the target.In 2016: another group demonstrated that certain sounds could make the Google Now voice command system open a particular web address: and hypothesized that this could ""serve as a stepping stone for further attacks (e.g.: opening a web page hosting drive-by malware)"".In ""data poisoning"": false data is continually smuggled into a machine learning system\'s training set': 'original_similarity': 0.8535765344685453: 'slide': 120}","{'chunk_text': 'inputs: called the activation function. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Typically: neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer): possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least 2 hidden layers.Artificial neural networks are used for predictive modeling: adaptive control: and other applications where they can be trained via a dataset. They are also used to solve problems in artificial intelligence. Networks can learn from experience: and can derive conclusions from a complex and seemingly unrelated set of information. == Training == Neural networks are typically trained through empirical risk minimization. This method is based on the idea of optimizing the network\'s parameters to minimize the difference: or empirical risk: between the predicted output and the actual target values in a given dataset. Gradient based methods such as backpropagation are usually used to estimate the parameters of the network. During the training phase: ANNs learn from labeled training data by iteratively updating their parameters to minimize a defined loss function. This method allows the network to generalize to unseen data. == History == Historically: digital computers evolved from the von Neumann model: and operate via the execution of explicit instructions via access to memory by a number of processors. Neural networks: on the other hand: originated from efforts to model information processing in biological systems through the framework of connectionism. Unlike the von Neumann model: connectionist computing does not separate memory and processing. The simplest kind of feedforward neural network (FNN) is a linear network: which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated at each node. The mean squared errors between these calculated outputs and the given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement.Warren McCulloch and Walter Pitts (1943) also considered a non-learning computational model for neural networks.In the late 1940s: D. O. Hebb created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Hebbian learning is considered to be a \'typical\' unsupervised learning rule and its later variants were early models for long term potentiation. These ideas started being applied to computational models in 1948 with Turing\'s ""unorganized machines"". Farley and Wesley A. Clark were the first to simulate a Hebbian network in 1954 at MIT. They used computational machines: then called ""calculators"". Other neural network computational machines were created by Rochester: Holland: Habit: and Duda in 1956. In 1958: psychologist Frank Rosenblatt invented the perceptron: the first implemented artificial neural network: funded by the United States Office of Naval Research. The invention of the perceptron raised public excitement for research in Artificial Neural Networks: causing the US government to drastically increase funding into deep learning research. This led to ""the golden age of AI"" fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence. For example: in 1957 Herbert Simon famously said:It is not my aim to surprise or shock you—but the simplest way I can summarize is to say that there are now in the world machines that think: that learn and that create. Moreover: their ability to do these things is going to increase rapidly until—in a visible future—the range of problems they can handle will be coextensive with the range to which the human mind has been applied.However: this wasn\'t the case as research stagnated in the United States following the work of Minsky and Papert (1969): who discovered that basic perceptrons were incapable of processing the exclusive-or circuit and that computers lacked sufficient power to train useful neural networks. This: along with other factors such as the 1973 Lighthill report by James Lighthill stating that research in Artificial Intelligence has not ""produced the major impact that was then promised:"" shutting funding in research into the field of AI in all but two universities in the UK and in many major institutions across the world. This ushered an era called the AI Winter with reduced research into connectionism due to a decrease in government funding and an increased stress on symbolic artificial intelligence in the United States and other Western countries.During the AI Winter era: however: research outside the United States continued: especially in Eastern Europe. By the time Minsky and Papert\'s book on Perceptrons came out: methods for training multilayer perceptrons (MLPs) were already known. The first deep learning MLP was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa in 1965: as the Group Method of Data Handling. The first deep learning MLP trained by stochastic gradient descent was published in 1967 by Shun\'ichi Amari. In computer experiments conducted by Amari\'s student Saito: a five layer MLP with\ntwo modifiable layers learned useful internal representations to classify non-linearily separable pattern classes.Self-organizing maps (SOMs) were described by Teuvo Kohonen in 1982. SOMs are neurophysiologically inspired neural networks that learn low-dimensional representations of high-dimensional data while preserving the topological structure of the data. They are trained using competitive learning.The convolutional neural network (CNN) architecture with convolutional layers and downsampling layers was introduced by Kunihiko Fukushima in 1980. He called it the neocognitron. In 1969: he also introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for CNNs and deep neural networks in general. CNNs have become an essential tool for': 'original_similarity': 0.8527562787275619: 'slide': 106}","{'chunk_text': 'nervous systems – a population of nerve cells connected by synapses. In machine learning: an artificial neural network is a mathematical model used to approximate nonlinear functions. Artificial neural networks are used to solve artificial intelligence problems. == Biological neural network == A biological neural network is a population of biological neurons chemically connected to each other by synapses. A given neuron can be connected to hundreds of thousands of synapses. Each neuron sends and receives electrochemical signals called action potentials to its connected neighbors. A neuron can serve an excitatory role: amplifying and propagating signals it receives: or an inhibitory role: suppressing signals instead.Populations of interconnected neurons that are smaller than neural networks are called neural circuits. Very large interconnected networks are called large scale brain networks: and many of these together form brains and nervous systems. Signals generated by neural networks in the brain eventually travel through the nervous system and across neuromuscular junctions to muscle cells: where they cause contraction and thereby motion. == Artificial neural network == An artificial neural network is a mathematical model used to approximate nonlinear functions. While early artificial neural networks were physical machines: today they are almost always implemented in software. Neurons in an artificial neural network are usually arranged into layers: with information passing from the first layer (the input layer) through one or more intermediate layers (hidden layers) to the final layer (the output layer). The ""signal"" input to each neuron is a number: specifically a linear combination of the outputs of the connected neurons in the previous layer. The signal each neuron outputs is calculated from this number: according to its activation function. The behavior of the network depends on the strengths (or weights) of the connections between neurons. A network is trained by modifying these weights through empirical risk minimization or backpropagation in order to fit some preexisting dataset.Neural networks are used to solve problems in artificial intelligence: and have thereby found applications in many disciplines: including predictive modeling: adaptive control: facial recognition: handwriting recognition: general game playing: and generative AI. == History == The theoretical base for contemporary neural networks was independently proposed by Alexander Bain in 1873 and William James in 1890. Both posited that human thought emerged from interactions among large numbers of neurons inside the brain. In 1949: Donald Hebb described Hebbian learning: the idea that neural networks can change and learn over time by strengthening a synapse every time a signal travels along it.Artificial neural networks were originally used to model biological neural networks starting in the 1930s under the approach of connectionism. However: starting with the invention of the perceptron: a simple artificial neural network: by Warren McCulloch and Walter Pitts in 1943: followed by the implementation of one in hardware by Frank Rosenblatt in 1957: artificial neural networks became increasingly used for machine learning applications instead: and increasingly different from their biological counterparts. == See also == Emergence Biological cybernetics Biologically-inspired computing == References ==\nAn optical neural network is a physical implementation of an artificial neural network with optical components. Early optical neural networks used a photorefractive Volume hologram to interconnect arrays of input neurons to arrays of output with synaptic weights in proportion to the multiplexed hologram\'s strength. Volume holograms were further multiplexed using spectral hole burning to add one dimension of wavelength to space to achieve four dimensional': 'original_similarity': 0.8494039606144995: 'slide': 64}"
0.4579958000103943,What are some applications of artificial neural networks in the field of artificial intelligence?,"In the field of artificial intelligence, artificial neural networks have been applied successfully to several areas including speech recognition, image analysis, and adaptive control. They are also used to create software agents in computer and video games or in the construction of autonomous robots.","{'chunk_text': 'weak phone bigram language models. This lets the strength of the acoustic modeling aspects of speech recognition be more easily analyzed. The error rates listed below: including these early results and measured as percent phone error rates (PER): have been summarized since 1991. The debut of DNNs for speaker recognition in the late 1990s and speech recognition around 2009-2011 and of LSTM around 2003–2007: accelerated progress in eight major areas: Scale-up/out and accelerated DNN training and decoding Sequence discriminative training Feature processing by deep models with solid understanding of the underlying mechanisms Adaptation of DNNs and related deep models Multi-task and transfer learning by DNNs and related deep models CNNs and how to design them to best exploit domain knowledge of speech RNN and its rich LSTM variants Other types of deep models including tensor-based models and integrated deep generative/discriminative models.All major commercial speech recognition systems (e.g.: Microsoft Cortana: Xbox: Skype Translator: Amazon Alexa: Google Now: Apple Siri: Baidu and iFlyTek voice search: and a range of Nuance speech products: etc.) are based on deep learning. === Image recognition === A common evaluation set for image classification is the MNIST database data set. MNIST is composed of handwritten digits and includes 60:000 training examples and 10:000 test examples. As with TIMIT: its small size lets users test multiple configurations. A comprehensive list of results on this set is available.Deep learning-based image recognition has become ""superhuman"": producing more accurate results than human contestants. This first occurred in 2011 in recognition of traffic signs: and in 2014: with recognition of human faces.Deep learning-trained vehicles now interpret 360° camera views. Another example is Facial Dysmorphology Novel Analysis (FDNA) used to analyze cases of human malformation connected to a large database of genetic syndromes. === Visual art processing === Closely related to the progress that has been made in image recognition is the increasing application of deep learning techniques to various visual art tasks. DNNs have proven themselves capable: for example: of identifying the style period of a given painting Neural Style Transfer – capturing the style of a given artwork and applying it in a visually pleasing manner to an arbitrary photograph or video generating striking imagery based on random visual input fields. === Natural language processing === Neural networks have been used for implementing language models since the early 2000s. LSTM helped to improve machine translation and language modeling.Other key techniques in this field are negative sampling and word embedding. Word embedding: such as word2vec: can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a vector space. Using word embedding as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar. A compositional vector grammar can be thought of as probabilistic context free grammar (PCFG) implemented by an RNN. Recursive auto-encoders built atop word embeddings can assess sentence similarity and detect paraphrasing. Deep neural architectures provide the best results for constituency parsing: sentiment analysis: information retrieval: spoken language understanding: machine translation: contextual entity linking: writing style recognition: named-entity recognition (token classification): text classification: and others.Recent developments generalize word embedding to sentence embedding. Google Translate (GT) uses a large end-to-end long short-term memory (LSTM) network. Google Neural Machine Translation (GNMT) uses an example-based machine translation method in which the system ""learns from millions of examples"". It translates ""whole sentences at a time: rather than pieces"". Google Translate supports over one hundred languages. The network encodes the ""semantics of the sentence rather than simply memorizing phrase-to-phrase translations"". GT uses English as an intermediate between most language pairs. === Drug discovery and toxicology === A large percentage of candidate drugs fail to win regulatory approval. These failures are caused by insufficient efficacy (on-target effect): undesired interactions (off-target effects): or unanticipated toxic effects. Research has explored use of deep learning to predict the biomolecular targets: off-targets: and toxic effects of environmental chemicals in nutrients: household products and drugs.AtomNet is a deep learning system for structure-based rational drug design. AtomNet was used to predict novel candidate biomolecules for disease targets such as the Ebola virus and multiple sclerosis.In 2017 graph neural networks were used for the first time to predict various properties of molecules in a large toxicology data set. In 2019: generative neural networks were used to produce molecules that were validated experimentally all the way into mice. === Customer relationship management === Deep reinforcement learning has been used to approximate the value of possible direct marketing actions: defined in terms of RFM variables. The estimated value function was shown to have a natural interpretation as customer lifetime value. === Recommendation systems === Recommendation systems have used deep learning to extract meaningful features for a latent factor model for content-based music and journal recommendations. Multi-view deep learning has been applied for learning user preferences from multiple domains. The model uses a hybrid collaborative and content-based approach and enhances recommendations in multiple tasks. === Bioinformatics === An autoencoder ANN was used in bioinformatics: to predict gene ontology annotations and gene-function relationships.In medical informatics: deep learning was used to predict sleep quality based on data\nfrom wearables and predictions of health complications from electronic health record data. === Deep Neural Network Estimations === Deep neural networks can be used to estimate the entropy of a stochastic process and called Neural Joint Entropy Estimator (NJEE). Such an estimation provides insights on the effects of input random variables on an independent random variable. Practically: the DNN is trained as a classifier that maps an input vector or matrix X to an output probability distribution over the possible classes of random variable Y: given input X. For example: in image classification tasks: the NJEE maps a vector of pixels\' color values to probabilities over possible image classes. In practice: the probability distribution of Y is obtained by a Softmax layer with number of': 'original_similarity': 0.8708115985831357: 'slide': 123}","{'chunk_text': 'a professional Go player. Google Translate uses a neural network to translate between more than 100 languages. In 2017: Covariant.ai was launched: which focuses on integrating deep learning into factories.As of 2008: researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement: or TAMER: which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor. First developed as TAMER: a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot with the ability to learn new tasks through observation. Using Deep TAMER: a robot learned a task with a human trainer: watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer: who provided feedback such as ""good job"" and ""bad job"". == Criticism and comment == Deep learning has attracted both criticism and comment: in some cases from outside the field of computer science. === Theory === A main criticism concerns the lack of theory surrounding some methods. Learning in the most common deep architectures is implemented using well-understood gradient descent. However: the theory surrounding other algorithms: such as contrastive divergence is less clear. (e.g.: Does it converge? If so: how fast? What is it approximating?) Deep learning methods are often looked at as a black box: with most confirmations done empirically: rather than theoretically.Others point out that deep learning should be looked at as a step towards realizing strong AI: not as an all-encompassing solution. Despite the power of deep learning methods: they still lack much of the functionality needed to realize this goal entirely. Research psychologist Gary Marcus noted: Realistically: deep learning is only part of the larger challenge of building intelligent machines. Such techniques lack ways of representing causal relationships (...) have no obvious ways of performing logical inferences: and they are also still a long way from integrating abstract knowledge: such as information about what objects are: what they are for: and how they are typically used. The most powerful A.I. systems: like Watson (...) use techniques like deep learning as just one element in a very complicated ensemble of techniques: ranging from the statistical technique of Bayesian inference to deductive reasoning. In further reference to the idea that artistic sensitivity might be inherent in relatively low levels of the cognitive hierarchy: a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained demonstrate a visual appeal: the original research notice received well over 1:000 comments: and was the subject of what was for a time the most frequently accessed article on The Guardian\'s website. === Errors === Some deep learning architectures display problematic behaviors: such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images (2014) and misclassifying minuscule perturbations of correctly classified images (2013). Goertzel hypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-component artificial general intelligence (AGI) architectures. These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar decompositions of observed entities and events. Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic goal of both human language acquisition and artificial intelligence (AI). === Cyber threat === As deep learning moves from the lab into the world: research and experience show that artificial neural networks are vulnerable to hacks and deception. By identifying patterns that these systems use to function: attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize. For example: an attacker can make subtle changes to an image such that the ANN finds a match even though the image looks to a human nothing like the search target. Such manipulation is termed an ""adversarial attack"".In 2016 researchers used one ANN to doctor images in trial and error fashion: identify another\'s focal points: and thereby generate images that deceived it. The modified images looked no different to human eyes. Another group showed that printouts of doctored images then photographed successfully tricked an image classification system. One defense is reverse image search: in which a possible fake image is submitted to a site such as TinEye that can then find other instances of it. A refinement is to search using only parts of the image: to identify images from which that piece may have been taken.Another group showed that certain psychedelic spectacles could fool a facial recognition system into thinking ordinary people were celebrities: potentially allowing one person to impersonate another. In 2017 researchers added stickers to stop signs and caused an ANN to misclassify them.ANNs\ncan however be further trained to detect attempts at deception: potentially leading attackers and defenders into an arms race similar to the kind that already defines the malware defense industry. ANNs have been trained to defeat ANN-based anti-malware software by repeatedly attacking a defense with malware that was continually altered by a genetic algorithm until it tricked the anti-malware while retaining its ability to damage the target.In 2016: another group demonstrated that certain sounds could make the Google Now voice command system open a particular web address: and hypothesized that this could ""serve as a stepping stone for further attacks (e.g.: opening a web page hosting drive-by malware)"".In ""data poisoning"": false data is continually smuggled into a machine learning system\'s training set to prevent it from achieving mastery. === Data collection ethics === Most Deep Learning systems rely on training and verification data that is generated and/or annotated by': 'original_similarity': 0.8632344872376596: 'slide': 147}","{'chunk_text': ""counterparts: but are very effective at their intended tasks (e.g. classification or segmentation). Some artificial neural networks are adaptive systems and are used for example to model populations and environments: which constantly change. Neural networks can be hardware- (neurons are represented by physical components) or software-based (computer models): and can use a variety of topologies and learning algorithms. == Feedforward == The feedforward neural network was the first and simplest type. In this network the information moves only from the input layer directly through any hidden layers to the output layer without cycles/loops. Feedforward networks can be constructed with various types of units: such as binary McCulloch–Pitts neurons: the simplest of which is the perceptron. Continuous neurons: frequently with sigmoidal activation: are used in the context of backpropagation. === Group method of data handling === The Group Method of Data Handling (GMDH) features fully automatic structural and parametric model optimization. The node activation functions are Kolmogorov–Gabor polynomials that permit additions and multiplications. It uses a deep multilayer perceptron with eight layers. It is a supervised learning network that grows layer by layer: where each layer is trained by regression analysis. Useless items are detected using a validation set: and pruned through regularization. The size and depth of the resulting network depends on the task. === Autoencoder === An autoencoder: autoassociator or Diabolo network: 19 is similar to the multilayer perceptron (MLP) – with an input layer: an output layer and one or more hidden layers connecting them. However: the output layer has the same number of units as the input layer. Its purpose is to reconstruct its own inputs (instead of emitting a target value). Therefore: autoencoders are unsupervised learning models. An autoencoder is used for unsupervised learning of efficient codings: typically for the purpose of dimensionality reduction and for learning generative models of data. === Probabilistic === A probabilistic neural network (PNN) is a four-layer feedforward neural network. The layers are Input: hidden pattern/summation: and output. In the PNN algorithm: the parent probability distribution function (PDF) of each class is approximated by a Parzen window and a non-parametric function. Then: using PDF of each class: the class probability of a new input is estimated and Bayes’ rule is employed to allocate it to the class with the highest posterior probability. It was derived from the Bayesian network and a statistical algorithm called Kernel Fisher discriminant analysis. It is used for classification and pattern recognition. === Time delay === A time delay neural network (TDNN) is a feedforward architecture for sequential data that recognizes features independent of sequence position. In order to achieve time-shift invariance: delays are added to the input so that multiple data points (points in time) are analyzed together. It usually forms part of a larger pattern recognition system. It has been implemented using a perceptron network whose connection weights were trained with back propagation (supervised learning). === Convolutional === A convolutional neural network (CNN: or ConvNet or shift invariant or space invariant) is a class of deep network: composed of one or more convolutional layers with fully connected layers (matching those in typical ANNs) on top. It uses tied weights and pooling layers. In particular: max-pooling. It is often structured via Fukushima's convolutional architecture. They are variations of multilayer perceptrons that use minimal preprocessing. This architecture allows CNNs to take advantage of the 2D structure of input data. Its unit connectivity pattern is inspired by the organization of the visual cortex. Units respond to stimuli in a restricted region of space known as the receptive field. Receptive fields partially overlap: over-covering the entire visual field. Unit response can be approximated mathematically by a convolution operation.CNNs are suitable for processing visual and other two-dimensional data. They have shown superior results in both image and speech applications. They can be trained with standard backpropagation. CNNs are easier to train than other regular: deep: feed-forward neural networks and have many fewer parameters to estimate.Capsule Neural Networks (CapsNet) add structures called capsules to a CNN and reuse output from several capsules to form more stable (with respect to various perturbations) representations.Examples of applications in computer vision include DeepDream and robot navigation. They have wide applications in image and video recognition: recommender systems and natural language processing. === Deep stacking network === A deep stacking network (DSN) (deep convex network) is based on a hierarchy of blocks of simplified neural network modules. It was introduced in 2011 by Deng and Yu. It formulates the learning as a convex optimization problem with a closed-form solution: emphasizing the mechanism's similarity to stacked generalization. Each DSN block is a simple module that is easy to train by itself in a supervised fashion without backpropagation for the entire blocks.Each block consists of a simplified multi-layer perceptron (MLP) with a single hidden layer. The hidden layer h has logistic sigmoidal units: and the output layer has linear units. Connections between these layers are represented by weight matrix U; input-to-hidden-layer connections have weight matrix W. Target vectors t form the columns of matrix T: and the input data vectors x form the columns of matrix X. The matrix of hidden units is H=σ(WTX){\\displaystyle {\\boldsymbol {H}}=\\sigma ({\\boldsymbol {W}}^{T}{\\boldsymbol {X}})}. Modules are trained in order: so lower-layer weights W are known at each stage. The function performs the element-wise logistic sigmoid operation. Each block estimates the same final label class y: and its estimate is concatenated with original input X to form the\nexpanded input for the next block. Thus: the input to the first block contains the original data only: while downstream blocks' input adds the output of preceding blocks. Then learning the upper-layer weight matrix U given other weights in the network can be formulated as a convex optimization problem: minUTf=‖UTH−T‖F2:{\\displaystyle \\min _{U^{T}}f=\\|{\\boldsymbol {U}}^{T}{\\boldsymbol {H}}-{\\boldsymbol {T}}\\|_{F}^{2}:}which has a closed-form solution.Unlike other deep architectures: such as DBNs: the goal is not to discover the transformed feature representation. The structure of the hierarchy of this kind of architecture makes parallel learning straightforward: as a batch-mode optimization problem. In purely discriminative"": 'original_similarity': 0.8564765998902373: 'slide': 95}"
0.46027030001278035,What theories did Alexander Bain and William James propose regarding neural networks?,"Alexander Bain proposed that every activity led to the firing of a certain set of neurons and when activities were repeated, the connections between those neurons strengthened, leading to the formation of memory. William James suggested that memories and actions resulted from electrical currents flowing among the neurons in the brain and did not require individual neural connections for each memory or action.","{'chunk_text': 'William James (1890). In their work: both thoughts and body activity resulted from interactions among neurons within the brain. For Bain: every activity led to the firing of a certain set of neurons. When activities were repeated: the connections between those neurons strengthened. According to his theory: this repetition was what led to the formation of memory. The general scientific community at the time was skeptical of Bain\'s theory because it required what appeared to be an inordinate number of neural connections within the brain. It is now apparent that the brain is exceedingly complex and that the same brain “wiring” can handle multiple problems and inputs. James\' theory was similar to Bain\'s; however: he suggested that memories and actions resulted from electrical currents flowing among the neurons in the brain. His model: by focusing on the flow of electrical currents: did not require individual neural connections for each memory or action. C. S. Sherrington (1898) conducted experiments to test James\' theory. He ran electrical currents down the spinal cords of rats. However: instead of demonstrating an increase in electrical current as projected by James: Sherrington found that the electrical current strength decreased as the testing continued over time. Importantly: this work led to the discovery of the concept of habituation. McCulloch and Pitts (1943) also created a computational model for neural networks based on mathematics and algorithms. They called this model threshold logic. These early models paved the way for neural network research to split into two distinct approaches. One approach focused on biological processes in the brain and the other focused on the application of neural networks to artificial intelligence. The parallel distributed processing of the mid-1980s became popular under the name connectionism. The text by Rumelhart and McClelland (1986) provided a full exposition on the use of connectionism in computers to simulate neural processes. Artificial neural networks: as used in artificial intelligence: have traditionally been viewed as simplified models of neural processing in the brain: even though the relation between this model and brain biological architecture is debated: as it is not clear to what degree artificial neural networks mirror brain function. == Neuroscience == Theoretical and computational neuroscience is the field concerned with the analysis and computational modeling of biological neural systems. Since neural systems are intimately related to cognitive processes and behaviour: the field is closely related to cognitive and behavioural modeling. The aim of the field is to create models of biological neural systems in order to understand how biological systems work. To gain this understanding: neuroscientists strive to make a link between observed biological processes (data): biologically plausible mechanisms for neural processing and learning (neural network models) and theory (statistical learning theory and information theory). === Types of models === Many models are used; defined at different levels of abstraction: and modeling different aspects of neural systems. They range from models of the short-term behaviour of individual neurons: through models of the dynamics of neural circuitry arising from interactions between individual neurons: to models of behaviour arising from abstract neural modules that represent complete subsystems. These include models of the long-term and short-term plasticity of neural systems and their relation to learning and memory: from the individual neuron to the system level. === Connectivity === In August 2020 scientists reported that bi-directional connections: or added appropriate feedback connections: can accelerate and improve communication between and in modular neural networks of the brain\'s cerebral cortex and lower the threshold for their successful communication. They showed that adding feedback connections between a resonance pair can support successful propagation of a single pulse packet throughout the entire network. == Recent improvements == While initially research had been concerned mostly with the electrical characteristics of neurons: a particularly important part of the investigation in recent years has been the exploration of the role of neuromodulators such as dopamine: acetylcholine: and serotonin on behaviour and learning.Biophysical models: such as BCM theory: has been important in understanding mechanisms for synaptic plasticity: and have had applications in both computer science and neuroscience. == See also == Adaptive resonance theory Biological cybernetics Cognitive architecture Cognitive science Connectomics Cultured neuronal networks Parallel constraint satisfaction processes == References ==\nIn machine learning: an artificial neural network (also neural network or neural net: abbreviated ANN or NN) is a model inspired by the neuronal organization found in the biological neural networks in animal brains.An ANN is made of connected units or nodes called artificial neurons: which loosely model the neurons in a brain. These are connected by edges: which model the synapses in a brain. An artificial neuron receives signals from connected neurons: then processes them and sends a signal to other connected neurons. The ""signal"" is a real number: and the output of each neuron is computed by some non-linear function of the sum of its inputs: called the activation function. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Typically: neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer): possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least 2 hidden layers.Artificial neural networks are used for predictive modeling: adaptive control: and other applications where they can be trained via a dataset. They are also used to solve problems in artificial intelligence. Networks can learn from experience: and can derive conclusions from a complex and seemingly unrelated set of information. == Training == Neural networks are typically trained through empirical risk minimization. This method is based on': 'original_similarity': 0.8283123222052705: 'slide': 256}","{'chunk_text': 'used to approximate nonlinear functions. Artificial neural networks are used to solve artificial intelligence problems. == Biological neural network == A biological neural network is a population of biological neurons chemically connected to each other by synapses. A given neuron can be connected to hundreds of thousands of synapses. Each neuron sends and receives electrochemical signals called action potentials to its connected neighbors. A neuron can serve an excitatory role: amplifying and propagating signals it receives: or an inhibitory role: suppressing signals instead.Populations of interconnected neurons that are smaller than neural networks are called neural circuits. Very large interconnected networks are called large scale brain networks: and many of these together form brains and nervous systems. Signals generated by neural networks in the brain eventually travel through the nervous system and across neuromuscular junctions to muscle cells: where they cause contraction and thereby motion. == Artificial neural network == An artificial neural network is a mathematical model used to approximate nonlinear functions. While early artificial neural networks were physical machines: today they are almost always implemented in software. Neurons in an artificial neural network are usually arranged into layers: with information passing from the first layer (the input layer) through one or more intermediate layers (hidden layers) to the final layer (the output layer). The ""signal"" input to each neuron is a number: specifically a linear combination of the outputs of the connected neurons in the previous layer. The signal each neuron outputs is calculated from this number: according to its activation function. The behavior of the network depends on the strengths (or weights) of the connections between neurons. A network is trained by modifying these weights through empirical risk minimization or backpropagation in order to fit some preexisting dataset.Neural networks are used to solve problems in artificial intelligence: and have thereby found applications in many disciplines: including predictive modeling: adaptive control: facial recognition: handwriting recognition: general game playing: and generative AI. == History == The theoretical base for contemporary neural networks was independently proposed by Alexander Bain in 1873 and William James in 1890. Both posited that human thought emerged from interactions among large numbers of neurons inside the brain. In 1949: Donald Hebb described Hebbian learning: the idea that neural networks can change and learn over time by strengthening a synapse every time a signal travels along it.Artificial neural networks were originally used to model biological neural networks starting in the 1930s under the approach of connectionism. However: starting with the invention of the perceptron: a simple artificial neural network: by Warren McCulloch and Walter Pitts in 1943: followed by the implementation of one in hardware by Frank Rosenblatt in 1957: artificial neural networks became increasingly used for machine learning applications instead: and increasingly different from their biological counterparts. == See also == Emergence Biological cybernetics Biologically-inspired computing == References ==\nAn optical neural network is a physical implementation of an artificial neural network with optical components. Early optical neural networks used a photorefractive Volume hologram to interconnect arrays of input neurons to arrays of output with synaptic weights in proportion to the multiplexed hologram\'s strength. Volume holograms were further multiplexed using spectral hole burning to add one dimension of wavelength to space to achieve four dimensional interconnects of two dimensional arrays of neural inputs and outputs. This research led to extensive research on alternative methods using the strength': 'original_similarity': 0.8219595045996418: 'slide': 86}","{'chunk_text': 'Bryson had dynamic programming based continuous precursors of backpropagation already in 1960–61 in the context of control theory. In 1973: Dreyfus used backpropagation to adapt parameters of controllers in proportion to error gradients. In 1982: Paul Werbos applied backpropagation to MLPs in the way that has become standard. In 1986 Rumelhart: Hinton and Williams showed that backpropagation learned interesting internal representations of words as feature vectors when trained to predict the next word in a sequence.In the late 1970s to early 1980s: interest briefly emerged in theoretically investigating the Ising model created by Wilhelm Lenz (1920) and Ernst Ising (1925) in relation to Cayley tree topologies and large neural networks. The Ising model is essentially a non-learning artificial recurrent neural network (RNN) consisting of neuron-like threshold elements. In 1972: Shun\'ichi Amari described an adaptive version of this architecture: In 1981: the Ising model was solved exactly by Peter Barth for the general case of closed Cayley trees (with loops) with an arbitrary branching ratio and found to exhibit unusual phase transition behavior in its local-apex and long-range site-site correlations.John Hopfield popularised this architecture in 1982: and it is now known as a Hopfield network. The time delay neural network (TDNN) of Alex Waibel (1987) combined convolutions and weight sharing and backpropagation. In 1988: Wei Zhang et al. applied backpropagation to a CNN (a simplified Neocognitron with convolutional interconnections between the image feature layers and the last fully connected layer) for alphabet recognition. In 1989: Yann LeCun et al. trained a CNN to recognize handwritten ZIP codes on mail. In 1992: max-pooling for CNNs was introduced by Juan Weng et al. to help with least-shift invariance and tolerance to deformation to aid 3D object recognition. LeNet-5 (1998): a 7-level CNN by Yann LeCun et al.: that classifies digits: was applied by several banks to recognize hand-written numbers on checks digitized in 32x32 pixel images. From 1988 onward: the use of neural networks transformed the field of protein structure prediction: in particular when the first cascading networks were trained on profiles (matrices) produced by multiple sequence alignments.In 1991: Sepp Hochreiter\'s diploma thesis identified and analyzed the vanishing gradient problem and proposed recurrent residual connections to solve it. His thesis was called ""one of the most important documents in the history of machine learning"" by his supervisor Juergen Schmidhuber.In 1991: Juergen Schmidhuber published adversarial neural networks that contest with each other in the form of a zero-sum game: where one network\'s gain is the other network\'s loss. The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called ""artificial curiosity."" In 1992: Juergen Schmidhuber proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning. It uses predictive coding to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN: by distilling a higher level chunker network into a lower level automatizer network. In the same year he also published an alternative to RNNs which is a precursor of a linear Transformer. It introduces the concept internal spotlights of attention: a slow feedforward neural network learns by gradient descent to control the fast weights of another neural network through outer products of self-generated activation patterns. The development of metal–oxide–semiconductor (MOS) very-large-scale integration (VLSI): in the form of complementary MOS (CMOS) technology: enabled increasing MOS transistor counts in digital electronics. This provided more processing power for the development of practical artificial neural networks in the 1980s.Neural networks\' early successes included predicting the stock market and in 1995 a (mostly) self-driving car.1997: Sepp Hochreite and Juergen Schmidhuber introduced the deep learning method called long short-term memory (LSTM): published in Neural Computation. LSTM recurrent neural networks can learn ""very deep learning"" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. The ""vanilla LSTM"" with forget gate was introduced in 1999 by Felix Gers: Schmidhuber and Fred Cummins.Geoffrey Hinton et al. (2006) proposed learning a high-level representation using successive layers of binary or real-valued latent variables with a restricted Boltzmann machine to model each layer. In 2012: Ng and Dean created a network that learned to recognize higher-level concepts: such as cats: only from watching unlabeled images. Unsupervised pre-training and increased computing power from GPUs and distributed computing allowed the use of larger networks: particularly in image and visual recognition problems: which became known as ""deep learning"".Variants of the back-propagation algorithm: as well as unsupervised methods by Geoff Hinton and colleagues at the University of Toronto: can be used to train deep: highly nonlinear neural architectures: similar to the 1980 Neocognitron by Kunihiko Fukushima: and the ""standard architecture of vision"": inspired by the simple and complex cells identified by David H. Hubel\nand Torsten Wiesel in the primary visual cortex. Computational devices have been created in CMOS for both biophysical simulation and neuromorphic computing. More recent efforts show promise for creating nanodevices for very large scale principal components analyses and convolution. If successful: these efforts could usher in a new era of neural computing that is a step beyond digital computing: because it depends on learning rather than programming and because it is fundamentally analog rather than digital even though the first instantiations may in fact be with CMOS digital devices. Ciresan and colleagues (2010) showed that despite the vanishing gradient problem: GPUs make backpropagation feasible for many-layered feedforward neural networks. Between 2009 and 2012: ANNs began winning prizes in image recognition contests: approaching human level performance on various tasks: initially in pattern recognition and handwriting recognition. For example: the bi-directional and multi-dimensional long short-term memory (LSTM) of Graves et al. won three competitions in connected handwriting recognition in 2009 without any prior knowledge about the three languages to be learned.Ciresan and colleagues built the first pattern recognizers to achieve human-competitive/superhuman performance on benchmarks such as': 'original_similarity': 0.8160770992109844: 'slide': 182}"
0.4491885000024922,Who were McCulloch and Pitts and what was their contribution to neural network research?,"McCulloch and Pitts were researchers who, in 1943, created a computational model for neural networks based on mathematics and algorithms. They called this model threshold logic. Their work contributed significantly to the evolution of neural network research, which split into two distinct approaches - one focusing on biological processes in the brain, and the other on the application of neural networks to artificial intelligence.","{'chunk_text': 'in the context of control theory. In 1973: Dreyfus used backpropagation to adapt parameters of controllers in proportion to error gradients. In 1982: Paul Werbos applied backpropagation to MLPs in the way that has become standard. In 1986 Rumelhart: Hinton and Williams showed that backpropagation learned interesting internal representations of words as feature vectors when trained to predict the next word in a sequence.In the late 1970s to early 1980s: interest briefly emerged in theoretically investigating the Ising model created by Wilhelm Lenz (1920) and Ernst Ising (1925) in relation to Cayley tree topologies and large neural networks. The Ising model is essentially a non-learning artificial recurrent neural network (RNN) consisting of neuron-like threshold elements. In 1972: Shun\'ichi Amari described an adaptive version of this architecture: In 1981: the Ising model was solved exactly by Peter Barth for the general case of closed Cayley trees (with loops) with an arbitrary branching ratio and found to exhibit unusual phase transition behavior in its local-apex and long-range site-site correlations.John Hopfield popularised this architecture in 1982: and it is now known as a Hopfield network. The time delay neural network (TDNN) of Alex Waibel (1987) combined convolutions and weight sharing and backpropagation. In 1988: Wei Zhang et al. applied backpropagation to a CNN (a simplified Neocognitron with convolutional interconnections between the image feature layers and the last fully connected layer) for alphabet recognition. In 1989: Yann LeCun et al. trained a CNN to recognize handwritten ZIP codes on mail. In 1992: max-pooling for CNNs was introduced by Juan Weng et al. to help with least-shift invariance and tolerance to deformation to aid 3D object recognition. LeNet-5 (1998): a 7-level CNN by Yann LeCun et al.: that classifies digits: was applied by several banks to recognize hand-written numbers on checks digitized in 32x32 pixel images. From 1988 onward: the use of neural networks transformed the field of protein structure prediction: in particular when the first cascading networks were trained on profiles (matrices) produced by multiple sequence alignments.In 1991: Sepp Hochreiter\'s diploma thesis identified and analyzed the vanishing gradient problem and proposed recurrent residual connections to solve it. His thesis was called ""one of the most important documents in the history of machine learning"" by his supervisor Juergen Schmidhuber.In 1991: Juergen Schmidhuber published adversarial neural networks that contest with each other in the form of a zero-sum game: where one network\'s gain is the other network\'s loss. The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called ""artificial curiosity."" In 1992: Juergen Schmidhuber proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning. It uses predictive coding to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN: by distilling a higher level chunker network into a lower level automatizer network. In the same year he also published an alternative to RNNs which is a precursor of a linear Transformer. It introduces the concept internal spotlights of attention: a slow feedforward neural network learns by gradient descent to control the fast weights of another neural network through outer products of self-generated activation patterns. The development of metal–oxide–semiconductor (MOS) very-large-scale integration (VLSI): in the form of complementary MOS (CMOS) technology: enabled increasing MOS transistor counts in digital electronics. This provided more processing power for the development of practical artificial neural networks in the 1980s.Neural networks\' early successes included predicting the stock market and in 1995 a (mostly) self-driving car.1997: Sepp Hochreite and Juergen Schmidhuber introduced the deep learning method called long short-term memory (LSTM): published in Neural Computation. LSTM recurrent neural networks can learn ""very deep learning"" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. The ""vanilla LSTM"" with forget gate was introduced in 1999 by Felix Gers: Schmidhuber and Fred Cummins.Geoffrey Hinton et al. (2006) proposed learning a high-level representation using successive layers of binary or real-valued latent variables with a restricted Boltzmann machine to model each layer. In 2012: Ng and Dean created a network that learned to recognize higher-level concepts: such as cats: only from watching unlabeled images. Unsupervised pre-training and increased computing power from GPUs and distributed computing allowed the use of larger networks: particularly in image and visual recognition problems: which became known as ""deep learning"".Variants of the back-propagation algorithm: as well as unsupervised methods by Geoff Hinton and colleagues at the University of Toronto: can be used to train deep: highly nonlinear neural architectures: similar to the 1980 Neocognitron by Kunihiko Fukushima: and the ""standard architecture of vision"": inspired by the simple and complex cells identified by David H. Hubel\nand Torsten Wiesel in the primary visual cortex. Computational devices have been created in CMOS for both biophysical simulation and neuromorphic computing. More recent efforts show promise for creating nanodevices for very large scale principal components analyses and convolution. If successful: these efforts could usher in a new era of neural computing that is a step beyond digital computing: because it depends on learning rather than programming and because it is fundamentally analog rather than digital even though the first instantiations may in fact be with CMOS digital devices. Ciresan and colleagues (2010) showed that despite the vanishing gradient problem: GPUs make backpropagation feasible for many-layered feedforward neural networks. Between 2009 and 2012: ANNs began winning prizes in image recognition contests: approaching human level performance on various tasks: initially in pattern recognition and handwriting recognition. For example: the bi-directional and multi-dimensional long short-term memory (LSTM) of Graves et al. won three competitions in connected handwriting recognition in 2009 without any prior knowledge about the three languages to be learned.Ciresan and colleagues built the first pattern recognizers to achieve human-competitive/superhuman performance on benchmarks such as traffic sign recognition (IJCNN 2012). Radial basis function and wavelet networks were': 'original_similarity': 0.8035416005396597: 'slide': 194}","{'chunk_text': 'Alexey Ivakhnenko and Lapa in 1967. A 1971 paper described a deep network with eight layers trained by the group method of data handling.The first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun\'ichi Amari. In computer experiments conducted by Amari\'s student Saito: a five layer MLP with two modifiable layers learned internal representations to classify non-linearily separable pattern classes. In 1987 Matthew Brand reported that wide 12-layer nonlinear perceptrons could be fully end-to-end trained to reproduce logic functions of nontrivial circuit depth via gradient descent on small batches of random input/output samples: but concluded that training time on contemporary hardware (sub-megaflop computers) made the technique impractical: and proposed using fixed random early layers as an input hash for a single modifiable layer. Instead: subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique. In 1970: Seppo Linnainmaa published the reverse mode of automatic differentiation of discrete connected networks of nested differentiable functions. This became known as backpropagation. It is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes. The terminology ""back-propagating errors"" was actually introduced in 1962 by Rosenblatt: but he did not know how to implement this: although Henry J. Kelley had a continuous precursor of backpropagation already in 1960 in the context of control theory. In 1982: Paul Werbos applied backpropagation to MLPs in the way that has become standard. In 1985: David E. Rumelhart et al. published an experimental analysis of the technique.Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers began with the Neocognitron introduced by Kunihiko Fukushima in 1980. In 1969: he also introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for CNNs and deep learning in general. CNNs have become an essential tool for computer vision. The term Deep Learning was introduced to the machine learning community by Rina Dechter in 1986: and to artificial neural networks by Igor Aizenberg and colleagues in 2000: in the context of Boolean threshold neurons.In 1988: Wei Zhang et al. applied the backpropagation algorithm to a convolutional neural network (a simplified Neocognitron with convolutional interconnections between the image feature layers and the last fully connected layer) for alphabet recognition. They also proposed an implementation of the CNN with an optical computing system. In 1989: Yann LeCun et al. applied backpropagation to a CNN with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked: training required 3 days. Subsequently: Wei Zhang: et al. modified their model by removing the last fully connected layer and applied it for medical image object segmentation in 1991 and breast cancer detection in mammograms in 1994. LeNet-5 (1998): a 7-level CNN by Yann LeCun et al.: that classifies digits: was applied by several banks to recognize hand-written numbers on checks digitized in 32x32 pixel images. In the 1980s: backpropagation did not work well for deep learning with long credit assignment paths. To overcome this problem: Jürgen Schmidhuber (1992) proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning. It uses predictive coding to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN: by distilling a higher level chunker network into a lower level automatizer network. In 1993: a chunker solved a deep learning task whose depth exceeded 1000.In 1992: Jürgen Schmidhuber also published an alternative to RNNs which is now called a linear Transformer or a Transformer with linearized self-attention (save for a normalization operator). It learns internal spotlights of attention: a slow feedforward neural network learns by gradient descent to control the fast weights of another neural network through outer products of self-generated activation patterns FROM and TO (which are now called key and value for self-attention). This fast weight attention mapping is applied to a query pattern. The modern Transformer was introduced by Ashish Vaswani et al. in their 2017 paper ""Attention Is All You Need"". It combines this with a softmax operator and a projection matrix. Transformers have increasingly become the model of choice for natural language processing. Many modern large language models such as ChatGPT: GPT-4: and BERT use it. Transformers are also increasingly being used in computer vision.In 1991: Jürgen Schmidhuber also published adversarial neural networks that contest with each other in the form of a zero-sum game: where one network\'s gain is the other network\'s loss. The first network is a\ngenerative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called ""artificial curiosity"". In 2014: this principle was used in a generative adversarial network (GAN) by Ian Goodfellow et al. Here the environmental reaction is 1 or 0 depending on whether the first network\'s output is in a given set. This can be used to create realistic deepfakes. Excellent image quality is achieved by Nvidia\'s StyleGAN (2018) based on the Progressive GAN by Tero Karras et al. Here the GAN generator is grown from small to large scale in a pyramidal fashion. Sepp Hochreiter\'s diploma thesis (1991) was called ""one of the most important documents in the history of machine learning"" by his supervisor Schmidhuber. It not only tested the neural history compressor: but also identified and analyzed the vanishing gradient problem. Hochreiter proposed recurrent residual connections to solve this problem. This led to the deep learning method called long short-term memory (LSTM): published in 1997. LSTM recurrent neural networks can learn ""very deep learning"" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. The ""vanilla LSTM"" with forget gate was introduced in 1999 by Felix Gers: Schmidhuber and Fred Cummins. LSTM has become the most cited neural network of the 20th century. In 2015: Rupesh': 'original_similarity': 0.7967648594867641: 'slide': 233}","{'chunk_text': 'the weights and the inputs is calculated in each node. The mean squared errors between these calculated outputs and a given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement. == Perceptrons and other early neural networks == Warren McCulloch and Walter Pitts (1943) also considered a non-learning computational model for neural networks. This model paved the way for research to split into two approaches. One approach focused on biological processes while the other focused on the application of neural networks to artificial intelligence. This work led to work on nerve networks and their link to finite automata.In the early 1940s: D. O. Hebb created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Hebbian learning is unsupervised learning. This evolved into models for long-term potentiation. Researchers started applying these ideas to computational models in 1948 with Turing\'s B-type machines. Farley and Clark (1954) first used computational machines: then called ""calculators"": to simulate a Hebbian network. Other neural network computational machines were created by Rochester: Holland: Habit and Duda (1956).Rosenblatt (1958) created the perceptron: an algorithm for pattern recognition. With mathematical notation: Rosenblatt described circuitry not in the basic perceptron: such as the exclusive-or circuit that could not be processed by neural networks at the time. In 1959: a biological model proposed by Nobel laureates Hubel and Wiesel was based on their discovery of two types of cells in the primary visual cortex: simple cells and complex cells.Some say that research stagnated following Minsky and Papert (1969): who discovered that basic perceptrons were incapable of processing the exclusive-or circuit and that computers lacked sufficient power to process useful neural networks. However: by the time this book came out: methods for training multilayer perceptrons (MLPs) by deep learning were already known. == First deep learning == The first deep learning MLP was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa in 1965: as the Group Method of Data Handling. This method employs incremental layer by layer training based on regression analysis: where useless units in hidden layers are pruned with the help of a validation set. The first deep learning MLP trained by stochastic gradient descent was published in 1967 by Shun\'ichi Amari. In computer experiments conducted by Amari\'s student Saito: a five layer MLP with two modifiable layers learned useful internal representations to classify non-linearily separable pattern classes. == Backpropagation == The backpropagation algorithm is an efficient application of the Leibniz chain rule (1673) to networks of differentiable nodes. It is also known as the reverse mode of automatic differentiation or reverse accumulation: due to Seppo Linnainmaa (1970). The term ""back-propagating errors"" was introduced in 1962 by Frank Rosenblatt: but he did not have an implementation of this procedure: although Henry J. Kelley had a continuous precursor of backpropagation already in 1960 in the context of control theory. In 1982: Paul Werbos applied backpropagation to MLPs in the way that has become standard. In 1986: David E. Rumelhart et al. published an experimental analysis of the technique. == Recurrent network architectures == Wilhelm Lenz and Ernst Ising created and analyzed the Ising model (1925) which is essentially a non-learning artificial recurrent neural network (RNN) consisting of neuron-like threshold elements. In 1972: Shun\'ichi Amari made this architecture adaptive. His learning RNN was popularised by John Hopfield in 1982. == Self-organizing maps == Self-organizing maps (SOMs) were described by Teuvo Kohonen in 1982. SOMs are neurophysiologically inspired artificial neural networks that learn low-dimensional representations of high-dimensional data while preserving the topological structure of the data. They are trained using competitive learning. SOMs create internal representations reminiscent of the cortical homunculus: a distorted representation of the human body: based on a neurological ""map"" of the areas and proportions of the human brain dedicated to processing sensory functions: for different parts of the body. == Convolutional neural networks (CNNs) == The origin of the CNN architecture is the ""neocognitron"" introduced by Kunihiko Fukushima in 1980. It was inspired by work of Hubel and Wiesel in the 1950s and 1960s which showed that cat visual cortices contain neurons that individually respond to small regions of the visual field. The neocognitron introduced the two basic types of layers in CNNs: convolutional layers: and downsampling layers. A convolutional layer contains units whose receptive fields cover a patch of\nthe previous layer. The weight vector (the set of adaptive parameters) of such a unit is often called a filter. Units can share filters. Downsampling layers contain units whose receptive fields cover patches of previous convolutional layers. Such a unit typically computes the average of the activations of the units in its patch. This downsampling helps to correctly classify objects in visual scenes even when the objects are shifted. In 1969: Kunihiko Fukushima also introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for CNNs and deep neural networks in general.The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel and was one of the first CNNs: as it achieved shift invariance. It did so by utilizing weight sharing in combination with backpropagation training. Thus: while also using a pyramidal structure as in the neocognitron: it performed a global optimization of the weights instead of a local one.In 1988: Wei Zhang et al. applied backpropagation to a CNN (a simplified Neocognitron with convolutional interconnections between the image feature layers and the last fully connected layer) for alphabet recognition. They also proposed an implementation of the CNN with an optical computing system.In 1989: Yann LeCun et al. trained a CNN with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked: training required 3 days. Learning was fully automatic: performed': 'original_similarity': 0.7923654851615457: 'slide': 231}"
0.40379480001865886,What are some recent improvements in the research of neural networks?,"Recent improvements in the research of neural networks have mostly been concerned with the exploration of the role of neuromodulators, such as dopamine, acetylcholine, and serotonin, on behavior and learning. Biophysical models, like BCM theory, have been instrumental for understanding synaptic plasticity mechanisms, and have influenced both computer science and neuroscience.","{'chunk_text': 'games using only pixels as data input. In 2015 they demonstrated their AlphaGo system: which learned the game of Go well enough to beat a professional Go player. Google Translate uses a neural network to translate between more than 100 languages. In 2017: Covariant.ai was launched: which focuses on integrating deep learning into factories.As of 2008: researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement: or TAMER: which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor. First developed as TAMER: a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot with the ability to learn new tasks through observation. Using Deep TAMER: a robot learned a task with a human trainer: watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer: who provided feedback such as ""good job"" and ""bad job"". == Criticism and comment == Deep learning has attracted both criticism and comment: in some cases from outside the field of computer science. === Theory === A main criticism concerns the lack of theory surrounding some methods. Learning in the most common deep architectures is implemented using well-understood gradient descent. However: the theory surrounding other algorithms: such as contrastive divergence is less clear. (e.g.: Does it converge? If so: how fast? What is it approximating?) Deep learning methods are often looked at as a black box: with most confirmations done empirically: rather than theoretically.Others point out that deep learning should be looked at as a step towards realizing strong AI: not as an all-encompassing solution. Despite the power of deep learning methods: they still lack much of the functionality needed to realize this goal entirely. Research psychologist Gary Marcus noted: Realistically: deep learning is only part of the larger challenge of building intelligent machines. Such techniques lack ways of representing causal relationships (...) have no obvious ways of performing logical inferences: and they are also still a long way from integrating abstract knowledge: such as information about what objects are: what they are for: and how they are typically used. The most powerful A.I. systems: like Watson (...) use techniques like deep learning as just one element in a very complicated ensemble of techniques: ranging from the statistical technique of Bayesian inference to deductive reasoning. In further reference to the idea that artistic sensitivity might be inherent in relatively low levels of the cognitive hierarchy: a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained demonstrate a visual appeal: the original research notice received well over 1:000 comments: and was the subject of what was for a time the most frequently accessed article on The Guardian\'s website. === Errors === Some deep learning architectures display problematic behaviors: such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images (2014) and misclassifying minuscule perturbations of correctly classified images (2013). Goertzel hypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-component artificial general intelligence (AGI) architectures. These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar decompositions of observed entities and events. Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic goal of both human language acquisition and artificial intelligence (AI). === Cyber threat === As deep learning moves from the lab into the world: research and experience show that artificial neural networks are vulnerable to hacks and deception. By identifying patterns that these systems use to function: attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize. For example: an attacker can make subtle changes to an image such that the ANN finds a match even though the image looks to a human nothing like the search target. Such manipulation is termed an ""adversarial attack"".In 2016 researchers used one ANN to doctor images in trial and error fashion: identify another\'s focal points: and thereby generate images that deceived it. The modified images looked no different to human eyes. Another group showed that printouts of doctored images then photographed successfully tricked an image classification system. One defense is reverse image search: in which a possible fake image is submitted to a site such as TinEye that can then find other instances of it. A refinement is to search using only parts of the image: to identify images from which that piece may have been taken.Another group showed that certain psychedelic spectacles could fool a facial recognition system into thinking ordinary people were celebrities: potentially allowing one person to impersonate another. In 2017 researchers added stickers to stop signs and caused an ANN to misclassify them.ANNs\ncan however be further trained to detect attempts at deception: potentially leading attackers and defenders into an arms race similar to the kind that already defines the malware defense industry. ANNs have been trained to defeat ANN-based anti-malware software by repeatedly attacking a defense with malware that was continually altered by a genetic algorithm until it tricked the anti-malware while retaining its ability to damage the target.In 2016: another group demonstrated that certain sounds could make the Google Now voice command system open a particular web address: and hypothesized that this could ""serve as a stepping stone for further attacks (e.g.: opening a web page hosting drive-by malware)"".In ""data poisoning"": false data is continually smuggled into a machine learning system\'s training set to prevent it': 'original_similarity': 0.8532192982182732: 'slide': 123}","{'chunk_text': 'ANN Next Generation of Neural Networks - Google Tech Talks Performance of Neural Networks Neural Networks and Information Sanderson G (5 October 2017). ""But what is a Neural Network?"". 3Blue1Brown. Archived from the original on 7 November 2021 – via YouTube. == Notes == == References == == Bibliography ==\nA neural network is a group of interconnected units': 'original_similarity': 0.8501756176575225: 'slide': 7}","{'chunk_text': 'interval of network output: assuming a normal distribution. A confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified. By assigning a softmax activation function: a generalization of the logistic function: on the output layer of the neural network (or a softmax component in a component-based network) for categorical target variables: the outputs can be interpreted as posterior probabilities. This is useful in classification as it gives a certainty measure on classifications. The softmax activation function is: yi=exi∑j=1cexj{\\displaystyle y_{i}={\\frac {e^{x_{i}}}{\\sum _{j=1}^{c}e^{x_{j}}}}} == Criticism == === Training === A common criticism of neural networks: particularly in robotics: is that they require too many training samples for real-world operation. Any learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases. Potential solutions include randomly shuffling training examples: by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example: grouping examples in so-called mini-batches and/or introducing a recursive least squares algorithm for CMAC. Dean Pomerleau uses a neural network to train a robotic vehicle to drive on multiple types of roads (single lane: multi-lane: dirt: etc.): and a large amount of his research is devoted to extrapolating multiple training scenarios from a single training experience: and preserving past training diversity so that the system does not become overtrained (if: for example: it is presented with a series of right turns—it should not learn to always turn right). === Theory === A central claim of ANNs is that they embody new and powerful general principles for processing information. These principles are ill-defined. It is often claimed that they are emergent from the network itself. This allows simple statistical association (the basic function of artificial neural networks) to be described as learning or recognition. In 1997: Alexander Dewdney: a former Scientific American columnist: commented that as a result: artificial neural networks have a ""something-for-nothing quality: one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are. No human hand (or mind) intervenes; solutions are found as if by magic; and no one: it seems: has learned anything"". One response to Dewdney is that neural networks have been successfully used to handle many complex and diverse tasks: ranging from autonomously flying aircraft to detecting credit card fraud to mastering the game of Go. Technology writer Roger Bridgman commented: Neural networks: for instance: are in the dock not only because they have been hyped to high heaven: (what hasn\'t?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be ""an opaque: unreadable table...valueless as a scientific resource"". In spite of his emphatic declaration that science is not technology: Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having. Although it is true that analyzing what has been learned by an artificial neural network is difficult: it is much easier to do so than to analyze what has been learned by a biological neural network. Moreover: recent emphasis on the explainability of AI has contributed towards the development of methods: notably those based on attention mechanisms: for visualizing and explaining learned neural networks. Furthermore: researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles that allow a learning machine to be successful. For example: Bengio and LeCun (2007) wrote an article regarding local vs non-local learning: as well as shallow vs deep architecture.Biological brains use both shallow and deep circuits as reported by brain anatomy: displaying a wide variety of invariance. Weng argued that the brain self-wires largely according to signal statistics and therefore: a serial cascade cannot catch all major statistical dependencies. === Hardware === Large and effective neural networks require considerable computing resources. While the brain has hardware tailored to the task of processing signals through a graph of neurons: simulating even a simplified neuron on von Neumann architecture may consume vast amounts of memory and storage. Furthermore: the designer often needs to transmit signals through many of these connections and their associated neurons – which require enormous CPU power and time. Schmidhuber noted that the resurgence of neural networks in the twenty-first century is largely attributable to advances in hardware: from 1991 to 2015: computing power: especially as delivered by GPGPUs (on GPUs): has increased around a million-fold: making the standard backpropagation algorithm feasible for training networks that are several layers deeper than before. The use of accelerators such as FPGAs and GPUs can reduce training times from months to days.Neuromorphic engineering or a physical neural network addresses the hardware difficulty directly: by constructing non-von-Neumann chips to directly implement neural networks\nin circuitry. Another type of chip optimized for neural network processing is called a Tensor Processing Unit: or TPU. === Practical counterexamples === Analyzing what has been learned by an ANN is much easier than analyzing what has been learned by a biological neural network. Furthermore: researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a learning machine to be successful. For example: local vs. non-local learning and shallow vs. deep architecture. === Hybrid approaches === Advocates of hybrid models (combining neural networks and symbolic approaches) say that such a mixture can better capture the mechanisms of the human mind. === Dataset bias === Neural networks are dependent on the quality of the data they are trained on: thus low quality data with imbalanced representativeness can lead to the model learning and perpetuating societal biases. These inherited biases become especially critical when the ANNs are integrated into real-world scenarios where the training data may be imbalanced due to the scarcity of data for': 'original_similarity': 0.8467846791643909: 'slide': 168}"
0.6524295999843162,What is a neural network?,"A neural network refers to a group of interconnected units, or neurons, that send signals to each other. These neurons can be either biological cells or mathematical models. A complex network of neurons can execute complex tasks. There are two main types of such networks - biological neural networks seen in brains and complex nervous systems, and artificial neural networks, which are mathematical models used in machine learning for approximating nonlinear functions.","{'chunk_text': 'biological neural network is a physical structure found in brains and complex nervous systems – a population of nerve cells connected by synapses. In machine learning: an artificial neural network is a mathematical model used to approximate nonlinear functions. Artificial neural networks are used to solve artificial intelligence problems. == Biological neural network == A biological neural network is a population of biological neurons chemically connected to each other by synapses. A given neuron can be connected to hundreds of thousands of synapses. Each neuron sends and receives electrochemical signals called action potentials to its connected neighbors. A neuron can serve an excitatory role: amplifying and propagating signals it receives: or an inhibitory role: suppressing signals instead.Populations of interconnected neurons that are smaller than neural networks are called neural circuits. Very large interconnected networks are called large scale brain networks: and many of these together form brains and nervous systems. Signals generated by neural networks in the brain eventually travel through the nervous system and across neuromuscular junctions to muscle cells: where they cause contraction and thereby motion. == Artificial neural network == An artificial neural network is a mathematical model used to approximate nonlinear functions. While early artificial neural networks were physical machines: today they are almost always implemented in software. Neurons in an artificial neural network are usually arranged into layers: with information passing from the first layer (the input layer) through one or more intermediate layers (hidden layers) to the final layer (the output layer). The ""signal"" input to each neuron is a number: specifically a linear combination of the outputs of the connected neurons in the previous layer. The signal each neuron outputs is calculated from this number: according to its activation function. The behavior of the network depends on the strengths (or weights) of the connections between neurons. A network is trained by modifying these weights through empirical risk minimization or backpropagation in order to fit some preexisting dataset.Neural networks are used to solve problems in artificial intelligence: and have thereby found applications in many disciplines: including predictive modeling: adaptive control: facial recognition: handwriting recognition: general game playing: and generative AI. == History == The theoretical base for contemporary neural networks was independently proposed by Alexander Bain in 1873 and William James in 1890. Both posited that human thought emerged from interactions among large numbers of neurons inside the brain. In 1949: Donald Hebb described Hebbian learning: the idea that neural networks can change and learn over time by strengthening a synapse every time a signal travels along it.Artificial neural networks were originally used to model biological neural networks starting in the 1930s under the approach of connectionism. However: starting with the invention of the perceptron: a simple artificial neural network: by Warren McCulloch and Walter Pitts in 1943: followed by the implementation of one in hardware by Frank Rosenblatt in 1957: artificial neural networks became increasingly used for machine learning applications instead: and increasingly different from their biological counterparts. == See also == Emergence Biological cybernetics Biologically-inspired computing == References ==\nAn optical neural network is a physical implementation of an artificial neural network with optical components. Early optical neural networks used a photorefractive Volume hologram to interconnect arrays of input neurons to arrays of output with synaptic weights in proportion to the multiplexed hologram\'s strength. Volume holograms were further multiplexed using spectral hole burning': 'original_similarity': 0.8691370869402736: 'slide': 52}","{'chunk_text': 'each neuron is computed by some non-linear function of the sum of its inputs: called the activation function. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Typically: neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer): possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least 2 hidden layers.Artificial neural networks are used for predictive modeling: adaptive control: and other applications where they can be trained via a dataset. They are also used to solve problems in artificial intelligence. Networks can learn from experience: and can derive conclusions from a complex and seemingly unrelated set of information. == Training == Neural networks are typically trained through empirical risk minimization. This method is based on the idea of optimizing the network\'s parameters to minimize the difference: or empirical risk: between the predicted output and the actual target values in a given dataset. Gradient based methods such as backpropagation are usually used to estimate the parameters of the network. During the training phase: ANNs learn from labeled training data by iteratively updating their parameters to minimize a defined loss function. This method allows the network to generalize to unseen data. == History == Historically: digital computers evolved from the von Neumann model: and operate via the execution of explicit instructions via access to memory by a number of processors. Neural networks: on the other hand: originated from efforts to model information processing in biological systems through the framework of connectionism. Unlike the von Neumann model: connectionist computing does not separate memory and processing. The simplest kind of feedforward neural network (FNN) is a linear network: which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated at each node. The mean squared errors between these calculated outputs and the given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement.Warren McCulloch and Walter Pitts (1943) also considered a non-learning computational model for neural networks.In the late 1940s: D. O. Hebb created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Hebbian learning is considered to be a \'typical\' unsupervised learning rule and its later variants were early models for long term potentiation. These ideas started being applied to computational models in 1948 with Turing\'s ""unorganized machines"". Farley and Wesley A. Clark were the first to simulate a Hebbian network in 1954 at MIT. They used computational machines: then called ""calculators"". Other neural network computational machines were created by Rochester: Holland: Habit: and Duda in 1956. In 1958: psychologist Frank Rosenblatt invented the perceptron: the first implemented artificial neural network: funded by the United States Office of Naval Research. The invention of the perceptron raised public excitement for research in Artificial Neural Networks: causing the US government to drastically increase funding into deep learning research. This led to ""the golden age of AI"" fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence. For example: in 1957 Herbert Simon famously said:It is not my aim to surprise or shock you—but the simplest way I can summarize is to say that there are now in the world machines that think: that learn and that create. Moreover: their ability to do these things is going to increase rapidly until—in a visible future—the range of problems they can handle will be coextensive with the range to which the human mind has been applied.However: this wasn\'t the case as research stagnated in the United States following the work of Minsky and Papert (1969): who discovered that basic perceptrons were incapable of processing the exclusive-or circuit and that computers lacked sufficient power to train useful neural networks. This: along with other factors such as the 1973 Lighthill report by James Lighthill stating that research in Artificial Intelligence has not ""produced the major impact that was then promised:"" shutting funding in research into the field of AI in all but two universities in the UK and in many major institutions across the world. This ushered an era called the AI Winter with reduced research into connectionism due to a decrease in government funding and an increased stress on symbolic artificial intelligence in the United States and other Western countries.During the AI Winter era: however: research outside the United States continued: especially in Eastern Europe. By the time Minsky and Papert\'s book on Perceptrons came out: methods for training multilayer perceptrons (MLPs) were already known. The first deep learning MLP was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa in 1965: as the Group Method of Data Handling. The first deep learning MLP trained by stochastic gradient descent was published in 1967 by Shun\'ichi Amari. In computer experiments conducted by Amari\'s student Saito: a five layer MLP with\ntwo modifiable layers learned useful internal representations to classify non-linearily separable pattern classes.Self-organizing maps (SOMs) were described by Teuvo Kohonen in 1982. SOMs are neurophysiologically inspired neural networks that learn low-dimensional representations of high-dimensional data while preserving the topological structure of the data. They are trained using competitive learning.The convolutional neural network (CNN) architecture with convolutional layers and downsampling layers was introduced by Kunihiko Fukushima in 1980. He called it the neocognitron. In 1969: he also introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for CNNs': 'original_similarity': 0.8626344372866616: 'slide': 93}","{'chunk_text': 'every activity led to the firing of a certain set of neurons. When activities were repeated: the connections between those neurons strengthened. According to his theory: this repetition was what led to the formation of memory. The general scientific community at the time was skeptical of Bain\'s theory because it required what appeared to be an inordinate number of neural connections within the brain. It is now apparent that the brain is exceedingly complex and that the same brain “wiring” can handle multiple problems and inputs. James\' theory was similar to Bain\'s; however: he suggested that memories and actions resulted from electrical currents flowing among the neurons in the brain. His model: by focusing on the flow of electrical currents: did not require individual neural connections for each memory or action. C. S. Sherrington (1898) conducted experiments to test James\' theory. He ran electrical currents down the spinal cords of rats. However: instead of demonstrating an increase in electrical current as projected by James: Sherrington found that the electrical current strength decreased as the testing continued over time. Importantly: this work led to the discovery of the concept of habituation. McCulloch and Pitts (1943) also created a computational model for neural networks based on mathematics and algorithms. They called this model threshold logic. These early models paved the way for neural network research to split into two distinct approaches. One approach focused on biological processes in the brain and the other focused on the application of neural networks to artificial intelligence. The parallel distributed processing of the mid-1980s became popular under the name connectionism. The text by Rumelhart and McClelland (1986) provided a full exposition on the use of connectionism in computers to simulate neural processes. Artificial neural networks: as used in artificial intelligence: have traditionally been viewed as simplified models of neural processing in the brain: even though the relation between this model and brain biological architecture is debated: as it is not clear to what degree artificial neural networks mirror brain function. == Neuroscience == Theoretical and computational neuroscience is the field concerned with the analysis and computational modeling of biological neural systems. Since neural systems are intimately related to cognitive processes and behaviour: the field is closely related to cognitive and behavioural modeling. The aim of the field is to create models of biological neural systems in order to understand how biological systems work. To gain this understanding: neuroscientists strive to make a link between observed biological processes (data): biologically plausible mechanisms for neural processing and learning (neural network models) and theory (statistical learning theory and information theory). === Types of models === Many models are used; defined at different levels of abstraction: and modeling different aspects of neural systems. They range from models of the short-term behaviour of individual neurons: through models of the dynamics of neural circuitry arising from interactions between individual neurons: to models of behaviour arising from abstract neural modules that represent complete subsystems. These include models of the long-term and short-term plasticity of neural systems and their relation to learning and memory: from the individual neuron to the system level. === Connectivity === In August 2020 scientists reported that bi-directional connections: or added appropriate feedback connections: can accelerate and improve communication between and in modular neural networks of the brain\'s cerebral cortex and lower the threshold for their successful communication. They showed that adding feedback connections between a resonance pair can support successful propagation of a single pulse packet throughout the entire network. == Recent improvements == While initially research had been concerned mostly with the electrical characteristics of neurons: a particularly important part of the investigation in recent years has been the exploration of the role of neuromodulators such as dopamine: acetylcholine: and serotonin on behaviour and learning.Biophysical models: such as BCM theory: has been important in understanding mechanisms for synaptic plasticity: and have had applications in both computer science and neuroscience. == See also == Adaptive resonance theory Biological cybernetics Cognitive architecture Cognitive science Connectomics Cultured neuronal networks Parallel constraint satisfaction processes == References ==\nIn machine learning: an artificial neural network (also neural network or neural net: abbreviated ANN or NN) is a model inspired by the neuronal organization found in the biological neural networks in animal brains.An ANN is made of connected units or nodes called artificial neurons: which loosely model the neurons in a brain. These are connected by edges: which model the synapses in a brain. An artificial neuron receives signals from connected neurons: then processes them and sends a signal to other connected neurons. The ""signal"" is a real number: and the output of each neuron is computed by some non-linear function of the sum of its inputs: called the activation function. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Typically: neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer): possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least 2 hidden layers.Artificial neural networks are used for predictive modeling: adaptive control: and other applications where they can be trained via a dataset. They are also used to solve problems in artificial intelligence. Networks can learn from experience: and can derive conclusions from a complex and seemingly unrelated set of information. == Training == Neural networks are typically trained through empirical risk minimization. This method is based on the idea of optimizing the network\'s parameters to minimize the difference: or empirical risk: between the predicted output and the actual': 'original_similarity': 0.8619693316022716: 'slide': 277}"
0.4191685999976471,What is the difference between a biological neural network and an artificial neural network?,"A biological neural network comprises biological neurons that are chemically connected to each other by synapses. These neurons send and receive electrochemical signals. An artificial neural network, on the other hand, is a mathematical model that approximates non-linear functions. These networks are mostly used in software form to help solve artificial intelligence problems.","{'chunk_text': 'and complex nervous systems – a population of nerve cells connected by synapses. In machine learning: an artificial neural network is a mathematical model used to approximate nonlinear functions. Artificial neural networks are used to solve artificial intelligence problems. == Biological neural network == A biological neural network is a population of biological neurons chemically connected to each other by synapses. A given neuron can be connected to hundreds of thousands of synapses. Each neuron sends and receives electrochemical signals called action potentials to its connected neighbors. A neuron can serve an excitatory role: amplifying and propagating signals it receives: or an inhibitory role: suppressing signals instead.Populations of interconnected neurons that are smaller than neural networks are called neural circuits. Very large interconnected networks are called large scale brain networks: and many of these together form brains and nervous systems. Signals generated by neural networks in the brain eventually travel through the nervous system and across neuromuscular junctions to muscle cells: where they cause contraction and thereby motion. == Artificial neural network == An artificial neural network is a mathematical model used to approximate nonlinear functions. While early artificial neural networks were physical machines: today they are almost always implemented in software. Neurons in an artificial neural network are usually arranged into layers: with information passing from the first layer (the input layer) through one or more intermediate layers (hidden layers) to the final layer (the output layer). The ""signal"" input to each neuron is a number: specifically a linear combination of the outputs of the connected neurons in the previous layer. The signal each neuron outputs is calculated from this number: according to its activation function. The behavior of the network depends on the strengths (or weights) of the connections between neurons. A network is trained by modifying these weights through empirical risk minimization or backpropagation in order to fit some preexisting dataset.Neural networks are used to solve problems in artificial intelligence: and have thereby found applications in many disciplines: including predictive modeling: adaptive control: facial recognition: handwriting recognition: general game playing: and generative AI. == History == The theoretical base for contemporary neural networks was independently proposed by Alexander Bain in 1873 and William James in 1890. Both posited that human thought emerged from interactions among large numbers of neurons inside the brain. In 1949: Donald Hebb described Hebbian learning: the idea that neural networks can change and learn over time by strengthening a synapse every time a signal travels along it.Artificial neural networks were originally used to model biological neural networks starting in the 1930s under the approach of connectionism. However: starting with the invention of the perceptron: a simple artificial neural network: by Warren McCulloch and Walter Pitts in 1943: followed by the implementation of one in hardware by Frank Rosenblatt in 1957: artificial neural networks became increasingly used for machine learning applications instead: and increasingly different from their biological counterparts. == See also == Emergence Biological cybernetics Biologically-inspired computing == References ==\nAn optical neural network is a physical implementation of an artificial neural network with optical components. Early optical neural networks used a photorefractive Volume hologram to interconnect arrays of input neurons to arrays of output with synaptic weights in proportion to the multiplexed hologram\'s strength. Volume holograms were further multiplexed using spectral hole burning to add one dimension of wavelength to space to achieve': 'original_similarity': 0.8806798501570672: 'slide': 62}","{'chunk_text': 'work: both thoughts and body activity resulted from interactions among neurons within the brain. For Bain: every activity led to the firing of a certain set of neurons. When activities were repeated: the connections between those neurons strengthened. According to his theory: this repetition was what led to the formation of memory. The general scientific community at the time was skeptical of Bain\'s theory because it required what appeared to be an inordinate number of neural connections within the brain. It is now apparent that the brain is exceedingly complex and that the same brain “wiring” can handle multiple problems and inputs. James\' theory was similar to Bain\'s; however: he suggested that memories and actions resulted from electrical currents flowing among the neurons in the brain. His model: by focusing on the flow of electrical currents: did not require individual neural connections for each memory or action. C. S. Sherrington (1898) conducted experiments to test James\' theory. He ran electrical currents down the spinal cords of rats. However: instead of demonstrating an increase in electrical current as projected by James: Sherrington found that the electrical current strength decreased as the testing continued over time. Importantly: this work led to the discovery of the concept of habituation. McCulloch and Pitts (1943) also created a computational model for neural networks based on mathematics and algorithms. They called this model threshold logic. These early models paved the way for neural network research to split into two distinct approaches. One approach focused on biological processes in the brain and the other focused on the application of neural networks to artificial intelligence. The parallel distributed processing of the mid-1980s became popular under the name connectionism. The text by Rumelhart and McClelland (1986) provided a full exposition on the use of connectionism in computers to simulate neural processes. Artificial neural networks: as used in artificial intelligence: have traditionally been viewed as simplified models of neural processing in the brain: even though the relation between this model and brain biological architecture is debated: as it is not clear to what degree artificial neural networks mirror brain function. == Neuroscience == Theoretical and computational neuroscience is the field concerned with the analysis and computational modeling of biological neural systems. Since neural systems are intimately related to cognitive processes and behaviour: the field is closely related to cognitive and behavioural modeling. The aim of the field is to create models of biological neural systems in order to understand how biological systems work. To gain this understanding: neuroscientists strive to make a link between observed biological processes (data): biologically plausible mechanisms for neural processing and learning (neural network models) and theory (statistical learning theory and information theory). === Types of models === Many models are used; defined at different levels of abstraction: and modeling different aspects of neural systems. They range from models of the short-term behaviour of individual neurons: through models of the dynamics of neural circuitry arising from interactions between individual neurons: to models of behaviour arising from abstract neural modules that represent complete subsystems. These include models of the long-term and short-term plasticity of neural systems and their relation to learning and memory: from the individual neuron to the system level. === Connectivity === In August 2020 scientists reported that bi-directional connections: or added appropriate feedback connections: can accelerate and improve communication between and in modular neural networks of the brain\'s cerebral cortex and lower the threshold for their successful communication. They showed that adding feedback connections between a resonance pair can support successful propagation of a single pulse packet throughout the entire network. == Recent improvements == While initially research had been concerned mostly with the electrical characteristics of neurons: a particularly important part of the investigation in recent years has been the exploration of the role of neuromodulators such as dopamine: acetylcholine: and serotonin on behaviour and learning.Biophysical models: such as BCM theory: has been important in understanding mechanisms for synaptic plasticity: and have had applications in both computer science and neuroscience. == See also == Adaptive resonance theory Biological cybernetics Cognitive architecture Cognitive science Connectomics Cultured neuronal networks Parallel constraint satisfaction processes == References ==\nIn machine learning: an artificial neural network (also neural network or neural net: abbreviated ANN or NN) is a model inspired by the neuronal organization found in the biological neural networks in animal brains.An ANN is made of connected units or nodes called artificial neurons: which loosely model the neurons in a brain. These are connected by edges: which model the synapses in a brain. An artificial neuron receives signals from connected neurons: then processes them and sends a signal to other connected neurons. The ""signal"" is a real number: and the output of each neuron is computed by some non-linear function of the sum of its inputs: called the activation function. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Typically: neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer): possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least 2 hidden layers.Artificial neural networks are used for predictive modeling: adaptive control: and other applications where they can be trained via a dataset. They are also used to solve problems in artificial intelligence. Networks can learn from experience: and can derive conclusions from a complex and seemingly unrelated set of information. == Training == Neural networks are typically trained through empirical risk minimization. This method is based on the idea of optimizing the': 'original_similarity': 0.8700958449750746: 'slide': 261}","{'chunk_text': 'to other connected neurons. The ""signal"" is a real number: and the output of each neuron is computed by some non-linear function of the sum of its inputs: called the activation function. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Typically: neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer): possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least 2 hidden layers.Artificial neural networks are used for predictive modeling: adaptive control: and other applications where they can be trained via a dataset. They are also used to solve problems in artificial intelligence. Networks can learn from experience: and can derive conclusions from a complex and seemingly unrelated set of information. == Training == Neural networks are typically trained through empirical risk minimization. This method is based on the idea of optimizing the network\'s parameters to minimize the difference: or empirical risk: between the predicted output and the actual target values in a given dataset. Gradient based methods such as backpropagation are usually used to estimate the parameters of the network. During the training phase: ANNs learn from labeled training data by iteratively updating their parameters to minimize a defined loss function. This method allows the network to generalize to unseen data. == History == Historically: digital computers evolved from the von Neumann model: and operate via the execution of explicit instructions via access to memory by a number of processors. Neural networks: on the other hand: originated from efforts to model information processing in biological systems through the framework of connectionism. Unlike the von Neumann model: connectionist computing does not separate memory and processing. The simplest kind of feedforward neural network (FNN) is a linear network: which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated at each node. The mean squared errors between these calculated outputs and the given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement.Warren McCulloch and Walter Pitts (1943) also considered a non-learning computational model for neural networks.In the late 1940s: D. O. Hebb created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Hebbian learning is considered to be a \'typical\' unsupervised learning rule and its later variants were early models for long term potentiation. These ideas started being applied to computational models in 1948 with Turing\'s ""unorganized machines"". Farley and Wesley A. Clark were the first to simulate a Hebbian network in 1954 at MIT. They used computational machines: then called ""calculators"". Other neural network computational machines were created by Rochester: Holland: Habit: and Duda in 1956. In 1958: psychologist Frank Rosenblatt invented the perceptron: the first implemented artificial neural network: funded by the United States Office of Naval Research. The invention of the perceptron raised public excitement for research in Artificial Neural Networks: causing the US government to drastically increase funding into deep learning research. This led to ""the golden age of AI"" fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence. For example: in 1957 Herbert Simon famously said:It is not my aim to surprise or shock you—but the simplest way I can summarize is to say that there are now in the world machines that think: that learn and that create. Moreover: their ability to do these things is going to increase rapidly until—in a visible future—the range of problems they can handle will be coextensive with the range to which the human mind has been applied.However: this wasn\'t the case as research stagnated in the United States following the work of Minsky and Papert (1969): who discovered that basic perceptrons were incapable of processing the exclusive-or circuit and that computers lacked sufficient power to train useful neural networks. This: along with other factors such as the 1973 Lighthill report by James Lighthill stating that research in Artificial Intelligence has not ""produced the major impact that was then promised:"" shutting funding in research into the field of AI in all but two universities in the UK and in many major institutions across the world. This ushered an era called the AI Winter with reduced research into connectionism due to a decrease in government funding and an increased stress on symbolic artificial intelligence in the United States and other Western countries.During the AI Winter era: however: research outside the United States continued: especially in Eastern Europe. By the time Minsky and Papert\'s book on Perceptrons came out: methods for training multilayer perceptrons (MLPs) were already known. The first deep learning MLP was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa in 1965: as the Group Method of Data Handling. The first deep learning MLP trained by stochastic gradient descent was published in 1967 by Shun\'ichi Amari. In computer experiments conducted by Amari\'s student Saito: a five layer MLP with\ntwo modifiable layers learned useful internal representations to classify non-linearily separable pattern classes.Self-organizing maps (SOMs) were described by Teuvo Kohonen in 1982. SOMs are neurophysiologically inspired neural networks that learn low-dimensional representations of high-dimensional data while preserving the topological structure of the data. They are trained using competitive learning.The convolutional neural network (CNN) architecture with convolutional layers and downsampling layers was introduced by Kunihiko Fukushima in 1980. He called it the neocognitron. In 1969: he also introduced the ReLU (rectified linear': 'original_similarity': 0.8582290032834808: 'slide': 79}"
0.385464699997101,How do neurons in an artificial neural network operate?,"In artificial neural networks, neurons are arranged into layers, with information passing through from the input layer to the output layer, through one or more intermediate or hidden layers. Each neuron receives an input signal - a number based on the outputs of neurons from the previous layer. The signal a neuron puts out is computed based on this number and its activation function. The strengths, or weights, of the connections between neurons shape the behavior of the network. Training a network involves modifying these weights to reach the desired output.","{'chunk_text': ""are adaptive systems and are used for example to model populations and environments: which constantly change. Neural networks can be hardware- (neurons are represented by physical components) or software-based (computer models): and can use a variety of topologies and learning algorithms. == Feedforward == The feedforward neural network was the first and simplest type. In this network the information moves only from the input layer directly through any hidden layers to the output layer without cycles/loops. Feedforward networks can be constructed with various types of units: such as binary McCulloch–Pitts neurons: the simplest of which is the perceptron. Continuous neurons: frequently with sigmoidal activation: are used in the context of backpropagation. === Group method of data handling === The Group Method of Data Handling (GMDH) features fully automatic structural and parametric model optimization. The node activation functions are Kolmogorov–Gabor polynomials that permit additions and multiplications. It uses a deep multilayer perceptron with eight layers. It is a supervised learning network that grows layer by layer: where each layer is trained by regression analysis. Useless items are detected using a validation set: and pruned through regularization. The size and depth of the resulting network depends on the task. === Autoencoder === An autoencoder: autoassociator or Diabolo network: 19 is similar to the multilayer perceptron (MLP) – with an input layer: an output layer and one or more hidden layers connecting them. However: the output layer has the same number of units as the input layer. Its purpose is to reconstruct its own inputs (instead of emitting a target value). Therefore: autoencoders are unsupervised learning models. An autoencoder is used for unsupervised learning of efficient codings: typically for the purpose of dimensionality reduction and for learning generative models of data. === Probabilistic === A probabilistic neural network (PNN) is a four-layer feedforward neural network. The layers are Input: hidden pattern/summation: and output. In the PNN algorithm: the parent probability distribution function (PDF) of each class is approximated by a Parzen window and a non-parametric function. Then: using PDF of each class: the class probability of a new input is estimated and Bayes’ rule is employed to allocate it to the class with the highest posterior probability. It was derived from the Bayesian network and a statistical algorithm called Kernel Fisher discriminant analysis. It is used for classification and pattern recognition. === Time delay === A time delay neural network (TDNN) is a feedforward architecture for sequential data that recognizes features independent of sequence position. In order to achieve time-shift invariance: delays are added to the input so that multiple data points (points in time) are analyzed together. It usually forms part of a larger pattern recognition system. It has been implemented using a perceptron network whose connection weights were trained with back propagation (supervised learning). === Convolutional === A convolutional neural network (CNN: or ConvNet or shift invariant or space invariant) is a class of deep network: composed of one or more convolutional layers with fully connected layers (matching those in typical ANNs) on top. It uses tied weights and pooling layers. In particular: max-pooling. It is often structured via Fukushima's convolutional architecture. They are variations of multilayer perceptrons that use minimal preprocessing. This architecture allows CNNs to take advantage of the 2D structure of input data. Its unit connectivity pattern is inspired by the organization of the visual cortex. Units respond to stimuli in a restricted region of space known as the receptive field. Receptive fields partially overlap: over-covering the entire visual field. Unit response can be approximated mathematically by a convolution operation.CNNs are suitable for processing visual and other two-dimensional data. They have shown superior results in both image and speech applications. They can be trained with standard backpropagation. CNNs are easier to train than other regular: deep: feed-forward neural networks and have many fewer parameters to estimate.Capsule Neural Networks (CapsNet) add structures called capsules to a CNN and reuse output from several capsules to form more stable (with respect to various perturbations) representations.Examples of applications in computer vision include DeepDream and robot navigation. They have wide applications in image and video recognition: recommender systems and natural language processing. === Deep stacking network === A deep stacking network (DSN) (deep convex network) is based on a hierarchy of blocks of simplified neural network modules. It was introduced in 2011 by Deng and Yu. It formulates the learning as a convex optimization problem with a closed-form solution: emphasizing the mechanism's similarity to stacked generalization. Each DSN block is a simple module that is easy to train by itself in a supervised fashion without backpropagation for the entire blocks.Each block consists of a simplified multi-layer perceptron (MLP) with a single hidden layer. The hidden layer h has logistic sigmoidal units: and the output layer has linear units. Connections between these layers are represented by weight matrix U; input-to-hidden-layer connections have weight matrix W. Target vectors t form the columns of matrix T: and the input data vectors x form the columns of matrix X. The matrix of hidden units is H=σ(WTX){\\displaystyle {\\boldsymbol {H}}=\\sigma ({\\boldsymbol {W}}^{T}{\\boldsymbol {X}})}. Modules are trained in order: so lower-layer weights W are known at each stage. The function performs the element-wise logistic sigmoid operation. Each block estimates the same final label class y: and its estimate is concatenated with original input X to form the\nexpanded input for the next block. Thus: the input to the first block contains the original data only: while downstream blocks' input adds the output of preceding blocks. Then learning the upper-layer weight matrix U given other weights in the network can be formulated as a convex optimization problem: minUTf=‖UTH−T‖F2:{\\displaystyle \\min _{U^{T}}f=\\|{\\boldsymbol {U}}^{T}{\\boldsymbol {H}}-{\\boldsymbol {T}}\\|_{F}^{2}:}which has a closed-form solution.Unlike other deep architectures: such as DBNs: the goal is not to discover the transformed feature representation. The structure of the hierarchy of this kind of architecture makes parallel learning straightforward: as a batch-mode optimization problem. In purely discriminative tasks: DSNs outperform conventional DBNs. ==== Tensor deep stacking networks ==== This architecture is a DSN extension."": 'original_similarity': 0.8583758449577726: 'slide': 112}","{'chunk_text': 'the sum of its inputs: called the activation function. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Typically: neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer): possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least 2 hidden layers.Artificial neural networks are used for predictive modeling: adaptive control: and other applications where they can be trained via a dataset. They are also used to solve problems in artificial intelligence. Networks can learn from experience: and can derive conclusions from a complex and seemingly unrelated set of information. == Training == Neural networks are typically trained through empirical risk minimization. This method is based on the idea of optimizing the network\'s parameters to minimize the difference: or empirical risk: between the predicted output and the actual target values in a given dataset. Gradient based methods such as backpropagation are usually used to estimate the parameters of the network. During the training phase: ANNs learn from labeled training data by iteratively updating their parameters to minimize a defined loss function. This method allows the network to generalize to unseen data. == History == Historically: digital computers evolved from the von Neumann model: and operate via the execution of explicit instructions via access to memory by a number of processors. Neural networks: on the other hand: originated from efforts to model information processing in biological systems through the framework of connectionism. Unlike the von Neumann model: connectionist computing does not separate memory and processing. The simplest kind of feedforward neural network (FNN) is a linear network: which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated at each node. The mean squared errors between these calculated outputs and the given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement.Warren McCulloch and Walter Pitts (1943) also considered a non-learning computational model for neural networks.In the late 1940s: D. O. Hebb created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Hebbian learning is considered to be a \'typical\' unsupervised learning rule and its later variants were early models for long term potentiation. These ideas started being applied to computational models in 1948 with Turing\'s ""unorganized machines"". Farley and Wesley A. Clark were the first to simulate a Hebbian network in 1954 at MIT. They used computational machines: then called ""calculators"". Other neural network computational machines were created by Rochester: Holland: Habit: and Duda in 1956. In 1958: psychologist Frank Rosenblatt invented the perceptron: the first implemented artificial neural network: funded by the United States Office of Naval Research. The invention of the perceptron raised public excitement for research in Artificial Neural Networks: causing the US government to drastically increase funding into deep learning research. This led to ""the golden age of AI"" fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence. For example: in 1957 Herbert Simon famously said:It is not my aim to surprise or shock you—but the simplest way I can summarize is to say that there are now in the world machines that think: that learn and that create. Moreover: their ability to do these things is going to increase rapidly until—in a visible future—the range of problems they can handle will be coextensive with the range to which the human mind has been applied.However: this wasn\'t the case as research stagnated in the United States following the work of Minsky and Papert (1969): who discovered that basic perceptrons were incapable of processing the exclusive-or circuit and that computers lacked sufficient power to train useful neural networks. This: along with other factors such as the 1973 Lighthill report by James Lighthill stating that research in Artificial Intelligence has not ""produced the major impact that was then promised:"" shutting funding in research into the field of AI in all but two universities in the UK and in many major institutions across the world. This ushered an era called the AI Winter with reduced research into connectionism due to a decrease in government funding and an increased stress on symbolic artificial intelligence in the United States and other Western countries.During the AI Winter era: however: research outside the United States continued: especially in Eastern Europe. By the time Minsky and Papert\'s book on Perceptrons came out: methods for training multilayer perceptrons (MLPs) were already known. The first deep learning MLP was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa in 1965: as the Group Method of Data Handling. The first deep learning MLP trained by stochastic gradient descent was published in 1967 by Shun\'ichi Amari. In computer experiments conducted by Amari\'s student Saito: a five layer MLP with\ntwo modifiable layers learned useful internal representations to classify non-linearily separable pattern classes.Self-organizing maps (SOMs) were described by Teuvo Kohonen in 1982. SOMs are neurophysiologically inspired neural networks that learn low-dimensional representations of high-dimensional data while preserving the topological structure of the data. They are trained using competitive learning.The convolutional neural network (CNN) architecture with convolutional layers and downsampling layers was introduced by Kunihiko Fukushima in 1980. He called it the neocognitron. In 1969: he also introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for CNNs and deep neural networks in general. CNNs have become': 'original_similarity': 0.8573237038594451: 'slide': 102}","{'chunk_text': '2012: ANNs began winning prizes in image recognition contests: approaching human level performance on various tasks: initially in pattern recognition and handwriting recognition. For example: the bi-directional and multi-dimensional long short-term memory (LSTM) of Graves et al. won three competitions in connected handwriting recognition in 2009 without any prior knowledge about the three languages to be learned.Ciresan and colleagues built the first pattern recognizers to achieve human-competitive/superhuman performance on benchmarks such as traffic sign recognition (IJCNN 2012). Radial basis function and wavelet networks were introduced in 2013. These can be shown to offer best approximation properties and have been applied in nonlinear system identification and classification applications.In 2014: the adversarial network principle was used in a generative adversarial network (GAN) by Ian Goodfellow et al. Here the adversarial network (discriminator) outputs a value between 1 and 0 depending on the likelihood of the first network\'s (generator) output is in a given set. This can be used to create realistic deepfakes. Excellent image quality is achieved by Nvidia\'s StyleGAN (2018) based on the Progressive GAN by Tero Karras: Timo Aila: Samuli Laine: and Jaakko Lehtinen. Here the GAN generator is grown from small to large scale in a pyramidal fashion. In 2015: Rupesh Kumar Srivastava: Klaus Greff: and Schmidhuber used the LSTM principle to create the Highway network: a feedforward neural network with hundreds of layers: much deeper than previous networks. 7 months later: Kaiming He: Xiangyu Zhang; Shaoqing Ren: and Jian Sun won the ImageNet 2015 competition with an open-gated or gateless Highway network variant called Residual neural network.In 2017: Ashish Vaswani et al. introduced the modern Transformer architecture in their paper ""Attention Is All You Need."" It combines this with a softmax operator and a projection matrix. Transformers have increasingly become the model of choice for natural language processing. Many modern large language models such as ChatGPT: GPT-4: and BERT use it. Transformers are also increasingly being used in computer vision.Ramenzanpour et al. showed in 2020 that analytical and computational techniques derived from statistical physics of disordered systems can be extended to large-scale problems: including machine learning: e.g.: to analyze the weight space of deep neural networks. == Models == ANNs began as an attempt to exploit the architecture of the human brain to perform tasks that conventional algorithms had little success with. They soon reoriented towards improving empirical results: abandoning attempts to remain true to their biological precursors. ANNs have the ability to learn and model non-linearities and complex relationships. This is achieved by neurons being connected in various patterns: allowing the output of some neurons to become the input of others. The network forms a directed: weighted graph.An artificial neural network consists of simulated neurons. Each neuron is connected to other nodes via links like a biological axon-synapse-dendrite connection. All the nodes connected by links take in some data and use it to perform specific operations and tasks on the data. Each link has a weight: determining the strength of one node\'s influence on another: allowing weights to choose the signal between neurons. === Artificial neurons === ANNs are composed of artificial neurons which are conceptually derived from biological neurons. Each artificial neuron has inputs and produces a single output which can be sent to multiple other neurons. The inputs can be the feature values of a sample of external data: such as images or documents: or they can be the outputs of other neurons. The outputs of the final output neurons of the neural net accomplish the task: such as recognizing an object in an image. To find the output of the neuron we take the weighted sum of all the inputs: weighted by the weights of the connections from the inputs to the neuron. We add a bias term to this sum. This weighted sum is sometimes called the activation. This weighted sum is then passed through a (usually nonlinear) activation function to produce the output. The initial inputs are external data: such as images and documents. The ultimate outputs accomplish the task: such as recognizing an object in an image. === Organization === The neurons are typically organized into multiple layers: especially in deep learning. Neurons of one layer connect only to neurons of the immediately preceding and immediately following layers. The layer that receives external data is the input layer. The layer that produces the ultimate result is the output layer. In between them are zero or more hidden layers. Single layer and unlayered networks are also used. Between two layers: multiple connection patterns are possible. They can be \'fully connected\': with every neuron in one layer connecting to every neuron in the next layer. They can be pooling: where a group of neurons in one layer connects to a single neuron in the next layer: thereby reducing the number of neurons in that layer. Neurons with only such connections form a directed acyclic graph and are known as feedforward networks. Alternatively: networks that allow connections between neurons in the same or previous layers are known as recurrent networks. === Hyperparameter === A hyperparameter is a constant parameter whose value is set before the learning process begins. The values of parameters are derived via learning. Examples of hyperparameters include learning rate: the number of hidden layers and batch size. The values of some hyperparameters can be dependent on those of other\nhyperparameters. For example: the size of some layers can depend on the overall number of layers. === Learning === Learning is the adaptation of the network to better handle a task by considering sample observations. Learning involves adjusting the weights (and optional thresholds) of the network to improve the accuracy of the result. This is done by minimizing the observed errors. Learning is complete when examining additional observations does not usefully reduce the error rate. Even after learning: the error rate typically does not reach 0. If after learning: the error rate is too high: the network typically must be redesigned. Practically this is done by defining a cost function that is evaluated': 'original_similarity': 0.8564706779554601: 'slide': 111}"
0.4285816000192426,How do signals in a biological neural network function?,"In a biological neural network, each neuron sends and receives electrochemical signals known as action potentials. Depending on its role, a neuron can either amplify and propagate or suppress these signals. These signals travel through the nervous system to muscle cells, inducing contraction and subsequent motion.","{'chunk_text': 'learning: an artificial neural network is a mathematical model used to approximate nonlinear functions. Artificial neural networks are used to solve artificial intelligence problems. == Biological neural network == A biological neural network is a population of biological neurons chemically connected to each other by synapses. A given neuron can be connected to hundreds of thousands of synapses. Each neuron sends and receives electrochemical signals called action potentials to its connected neighbors. A neuron can serve an excitatory role: amplifying and propagating signals it receives: or an inhibitory role: suppressing signals instead.Populations of interconnected neurons that are smaller than neural networks are called neural circuits. Very large interconnected networks are called large scale brain networks: and many of these together form brains and nervous systems. Signals generated by neural networks in the brain eventually travel through the nervous system and across neuromuscular junctions to muscle cells: where they cause contraction and thereby motion. == Artificial neural network == An artificial neural network is a mathematical model used to approximate nonlinear functions. While early artificial neural networks were physical machines: today they are almost always implemented in software. Neurons in an artificial neural network are usually arranged into layers: with information passing from the first layer (the input layer) through one or more intermediate layers (hidden layers) to the final layer (the output layer). The ""signal"" input to each neuron is a number: specifically a linear combination of the outputs of the connected neurons in the previous layer. The signal each neuron outputs is calculated from this number: according to its activation function. The behavior of the network depends on the strengths (or weights) of the connections between neurons. A network is trained by modifying these weights through empirical risk minimization or backpropagation in order to fit some preexisting dataset.Neural networks are used to solve problems in artificial intelligence: and have thereby found applications in many disciplines: including predictive modeling: adaptive control: facial recognition: handwriting recognition: general game playing: and generative AI. == History == The theoretical base for contemporary neural networks was independently proposed by Alexander Bain in 1873 and William James in 1890. Both posited that human thought emerged from interactions among large numbers of neurons inside the brain. In 1949: Donald Hebb described Hebbian learning: the idea that neural networks can change and learn over time by strengthening a synapse every time a signal travels along it.Artificial neural networks were originally used to model biological neural networks starting in the 1930s under the approach of connectionism. However: starting with the invention of the perceptron: a simple artificial neural network: by Warren McCulloch and Walter Pitts in 1943: followed by the implementation of one in hardware by Frank Rosenblatt in 1957: artificial neural networks became increasingly used for machine learning applications instead: and increasingly different from their biological counterparts. == See also == Emergence Biological cybernetics Biologically-inspired computing == References ==\nAn optical neural network is a physical implementation of an artificial neural network with optical components. Early optical neural networks used a photorefractive Volume hologram to interconnect arrays of input neurons to arrays of output with synaptic weights in proportion to the multiplexed hologram\'s strength. Volume holograms were further multiplexed using spectral hole burning to add one dimension of wavelength to space to achieve four dimensional interconnects of two dimensional arrays of neural inputs and outputs. This research led': 'original_similarity': 0.872413319640226: 'slide': 77}","{'chunk_text': 'for contemporary neural networks was independently proposed by Alexander Bain (1873) and William James (1890). In their work: both thoughts and body activity resulted from interactions among neurons within the brain. For Bain: every activity led to the firing of a certain set of neurons. When activities were repeated: the connections between those neurons strengthened. According to his theory: this repetition was what led to the formation of memory. The general scientific community at the time was skeptical of Bain\'s theory because it required what appeared to be an inordinate number of neural connections within the brain. It is now apparent that the brain is exceedingly complex and that the same brain “wiring” can handle multiple problems and inputs. James\' theory was similar to Bain\'s; however: he suggested that memories and actions resulted from electrical currents flowing among the neurons in the brain. His model: by focusing on the flow of electrical currents: did not require individual neural connections for each memory or action. C. S. Sherrington (1898) conducted experiments to test James\' theory. He ran electrical currents down the spinal cords of rats. However: instead of demonstrating an increase in electrical current as projected by James: Sherrington found that the electrical current strength decreased as the testing continued over time. Importantly: this work led to the discovery of the concept of habituation. McCulloch and Pitts (1943) also created a computational model for neural networks based on mathematics and algorithms. They called this model threshold logic. These early models paved the way for neural network research to split into two distinct approaches. One approach focused on biological processes in the brain and the other focused on the application of neural networks to artificial intelligence. The parallel distributed processing of the mid-1980s became popular under the name connectionism. The text by Rumelhart and McClelland (1986) provided a full exposition on the use of connectionism in computers to simulate neural processes. Artificial neural networks: as used in artificial intelligence: have traditionally been viewed as simplified models of neural processing in the brain: even though the relation between this model and brain biological architecture is debated: as it is not clear to what degree artificial neural networks mirror brain function. == Neuroscience == Theoretical and computational neuroscience is the field concerned with the analysis and computational modeling of biological neural systems. Since neural systems are intimately related to cognitive processes and behaviour: the field is closely related to cognitive and behavioural modeling. The aim of the field is to create models of biological neural systems in order to understand how biological systems work. To gain this understanding: neuroscientists strive to make a link between observed biological processes (data): biologically plausible mechanisms for neural processing and learning (neural network models) and theory (statistical learning theory and information theory). === Types of models === Many models are used; defined at different levels of abstraction: and modeling different aspects of neural systems. They range from models of the short-term behaviour of individual neurons: through models of the dynamics of neural circuitry arising from interactions between individual neurons: to models of behaviour arising from abstract neural modules that represent complete subsystems. These include models of the long-term and short-term plasticity of neural systems and their relation to learning and memory: from the individual neuron to the system level. === Connectivity === In August 2020 scientists reported that bi-directional connections: or added appropriate feedback connections: can accelerate and improve communication between and in modular neural networks of the brain\'s cerebral cortex and lower the threshold for their successful communication. They showed that adding feedback connections between a resonance pair can support successful propagation of a single pulse packet throughout the entire network. == Recent improvements == While initially research had been concerned mostly with the electrical characteristics of neurons: a particularly important part of the investigation in recent years has been the exploration of the role of neuromodulators such as dopamine: acetylcholine: and serotonin on behaviour and learning.Biophysical models: such as BCM theory: has been important in understanding mechanisms for synaptic plasticity: and have had applications in both computer science and neuroscience. == See also == Adaptive resonance theory Biological cybernetics Cognitive architecture Cognitive science Connectomics Cultured neuronal networks Parallel constraint satisfaction processes == References ==\nIn machine learning: an artificial neural network (also neural network or neural net: abbreviated ANN or NN) is a model inspired by the neuronal organization found in the biological neural networks in animal brains.An ANN is made of connected units or nodes called artificial neurons: which loosely model the neurons in a brain. These are connected by edges: which model the synapses in a brain. An artificial neuron receives signals from connected neurons: then processes them and sends a signal to other connected neurons. The ""signal"" is a real number: and the output of each neuron is computed by some non-linear function of the sum of its inputs: called the activation function. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Typically: neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer): possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least 2 hidden layers.Artificial neural networks are used for predictive modeling: adaptive control: and other applications where they can be trained via a dataset. They are also used to solve problems in artificial intelligence. Networks can learn from experience: and can derive conclusions from a complex and seemingly unrelated set of information. == Training == Neural networks': 'original_similarity': 0.8684104002561913: 'slide': 244}","{'chunk_text': ""(computer models): and can use a variety of topologies and learning algorithms. == Feedforward == The feedforward neural network was the first and simplest type. In this network the information moves only from the input layer directly through any hidden layers to the output layer without cycles/loops. Feedforward networks can be constructed with various types of units: such as binary McCulloch–Pitts neurons: the simplest of which is the perceptron. Continuous neurons: frequently with sigmoidal activation: are used in the context of backpropagation. === Group method of data handling === The Group Method of Data Handling (GMDH) features fully automatic structural and parametric model optimization. The node activation functions are Kolmogorov–Gabor polynomials that permit additions and multiplications. It uses a deep multilayer perceptron with eight layers. It is a supervised learning network that grows layer by layer: where each layer is trained by regression analysis. Useless items are detected using a validation set: and pruned through regularization. The size and depth of the resulting network depends on the task. === Autoencoder === An autoencoder: autoassociator or Diabolo network: 19 is similar to the multilayer perceptron (MLP) – with an input layer: an output layer and one or more hidden layers connecting them. However: the output layer has the same number of units as the input layer. Its purpose is to reconstruct its own inputs (instead of emitting a target value). Therefore: autoencoders are unsupervised learning models. An autoencoder is used for unsupervised learning of efficient codings: typically for the purpose of dimensionality reduction and for learning generative models of data. === Probabilistic === A probabilistic neural network (PNN) is a four-layer feedforward neural network. The layers are Input: hidden pattern/summation: and output. In the PNN algorithm: the parent probability distribution function (PDF) of each class is approximated by a Parzen window and a non-parametric function. Then: using PDF of each class: the class probability of a new input is estimated and Bayes’ rule is employed to allocate it to the class with the highest posterior probability. It was derived from the Bayesian network and a statistical algorithm called Kernel Fisher discriminant analysis. It is used for classification and pattern recognition. === Time delay === A time delay neural network (TDNN) is a feedforward architecture for sequential data that recognizes features independent of sequence position. In order to achieve time-shift invariance: delays are added to the input so that multiple data points (points in time) are analyzed together. It usually forms part of a larger pattern recognition system. It has been implemented using a perceptron network whose connection weights were trained with back propagation (supervised learning). === Convolutional === A convolutional neural network (CNN: or ConvNet or shift invariant or space invariant) is a class of deep network: composed of one or more convolutional layers with fully connected layers (matching those in typical ANNs) on top. It uses tied weights and pooling layers. In particular: max-pooling. It is often structured via Fukushima's convolutional architecture. They are variations of multilayer perceptrons that use minimal preprocessing. This architecture allows CNNs to take advantage of the 2D structure of input data. Its unit connectivity pattern is inspired by the organization of the visual cortex. Units respond to stimuli in a restricted region of space known as the receptive field. Receptive fields partially overlap: over-covering the entire visual field. Unit response can be approximated mathematically by a convolution operation.CNNs are suitable for processing visual and other two-dimensional data. They have shown superior results in both image and speech applications. They can be trained with standard backpropagation. CNNs are easier to train than other regular: deep: feed-forward neural networks and have many fewer parameters to estimate.Capsule Neural Networks (CapsNet) add structures called capsules to a CNN and reuse output from several capsules to form more stable (with respect to various perturbations) representations.Examples of applications in computer vision include DeepDream and robot navigation. They have wide applications in image and video recognition: recommender systems and natural language processing. === Deep stacking network === A deep stacking network (DSN) (deep convex network) is based on a hierarchy of blocks of simplified neural network modules. It was introduced in 2011 by Deng and Yu. It formulates the learning as a convex optimization problem with a closed-form solution: emphasizing the mechanism's similarity to stacked generalization. Each DSN block is a simple module that is easy to train by itself in a supervised fashion without backpropagation for the entire blocks.Each block consists of a simplified multi-layer perceptron (MLP) with a single hidden layer. The hidden layer h has logistic sigmoidal units: and the output layer has linear units. Connections between these layers are represented by weight matrix U; input-to-hidden-layer connections have weight matrix W. Target vectors t form the columns of matrix T: and the input data vectors x form the columns of matrix X. The matrix of hidden units is H=σ(WTX){\\displaystyle {\\boldsymbol {H}}=\\sigma ({\\boldsymbol {W}}^{T}{\\boldsymbol {X}})}. Modules are trained in order: so lower-layer weights W are known at each stage. The function performs the element-wise logistic sigmoid operation. Each block estimates the same final label class y: and its estimate is concatenated with original input X to form the\nexpanded input for the next block. Thus: the input to the first block contains the original data only: while downstream blocks' input adds the output of preceding blocks. Then learning the upper-layer weight matrix U given other weights in the network can be formulated as a convex optimization problem: minUTf=‖UTH−T‖F2:{\\displaystyle \\min _{U^{T}}f=\\|{\\boldsymbol {U}}^{T}{\\boldsymbol {H}}-{\\boldsymbol {T}}\\|_{F}^{2}:}which has a closed-form solution.Unlike other deep architectures: such as DBNs: the goal is not to discover the transformed feature representation. The structure of the hierarchy of this kind of architecture makes parallel learning straightforward: as a batch-mode optimization problem. In purely discriminative tasks: DSNs outperform conventional DBNs. ==== Tensor deep stacking networks ==== This architecture is a DSN extension. It offers two important improvements: it uses higher-order information from covariance statistics: and it transforms the non-convex problem of a lower-layer to a convex sub-problem of an upper-layer. TDSNs"": 'original_similarity': 0.8431648227477911: 'slide': 141}"
0.4186057999904733,How has the application of artificial neural networks evolved?,"Artificial neural networks were initially developed to model biological neural networks under the concept of connectionism in the 1930s. However, with the development of the perceptron, a simple artificial neural network, around 1943 and subsequent implementations in hardware, they started being utilized more for machine learning applications, and thereby deviated considerably from their biological counterparts.","{'chunk_text': 'AlphaGo system: which learned the game of Go well enough to beat a professional Go player. Google Translate uses a neural network to translate between more than 100 languages. In 2017: Covariant.ai was launched: which focuses on integrating deep learning into factories.As of 2008: researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement: or TAMER: which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor. First developed as TAMER: a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot with the ability to learn new tasks through observation. Using Deep TAMER: a robot learned a task with a human trainer: watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer: who provided feedback such as ""good job"" and ""bad job"". == Criticism and comment == Deep learning has attracted both criticism and comment: in some cases from outside the field of computer science. === Theory === A main criticism concerns the lack of theory surrounding some methods. Learning in the most common deep architectures is implemented using well-understood gradient descent. However: the theory surrounding other algorithms: such as contrastive divergence is less clear. (e.g.: Does it converge? If so: how fast? What is it approximating?) Deep learning methods are often looked at as a black box: with most confirmations done empirically: rather than theoretically.Others point out that deep learning should be looked at as a step towards realizing strong AI: not as an all-encompassing solution. Despite the power of deep learning methods: they still lack much of the functionality needed to realize this goal entirely. Research psychologist Gary Marcus noted: Realistically: deep learning is only part of the larger challenge of building intelligent machines. Such techniques lack ways of representing causal relationships (...) have no obvious ways of performing logical inferences: and they are also still a long way from integrating abstract knowledge: such as information about what objects are: what they are for: and how they are typically used. The most powerful A.I. systems: like Watson (...) use techniques like deep learning as just one element in a very complicated ensemble of techniques: ranging from the statistical technique of Bayesian inference to deductive reasoning. In further reference to the idea that artistic sensitivity might be inherent in relatively low levels of the cognitive hierarchy: a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained demonstrate a visual appeal: the original research notice received well over 1:000 comments: and was the subject of what was for a time the most frequently accessed article on The Guardian\'s website. === Errors === Some deep learning architectures display problematic behaviors: such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images (2014) and misclassifying minuscule perturbations of correctly classified images (2013). Goertzel hypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-component artificial general intelligence (AGI) architectures. These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar decompositions of observed entities and events. Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic goal of both human language acquisition and artificial intelligence (AI). === Cyber threat === As deep learning moves from the lab into the world: research and experience show that artificial neural networks are vulnerable to hacks and deception. By identifying patterns that these systems use to function: attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize. For example: an attacker can make subtle changes to an image such that the ANN finds a match even though the image looks to a human nothing like the search target. Such manipulation is termed an ""adversarial attack"".In 2016 researchers used one ANN to doctor images in trial and error fashion: identify another\'s focal points: and thereby generate images that deceived it. The modified images looked no different to human eyes. Another group showed that printouts of doctored images then photographed successfully tricked an image classification system. One defense is reverse image search: in which a possible fake image is submitted to a site such as TinEye that can then find other instances of it. A refinement is to search using only parts of the image: to identify images from which that piece may have been taken.Another group showed that certain psychedelic spectacles could fool a facial recognition system into thinking ordinary people were celebrities: potentially allowing one person to impersonate another. In 2017 researchers added stickers to stop signs and caused an ANN to misclassify them.ANNs\ncan however be further trained to detect attempts at deception: potentially leading attackers and defenders into an arms race similar to the kind that already defines the malware defense industry. ANNs have been trained to defeat ANN-based anti-malware software by repeatedly attacking a defense with malware that was continually altered by a genetic algorithm until it tricked the anti-malware while retaining its ability to damage the target.In 2016: another group demonstrated that certain sounds could make the Google Now voice command system open a particular web address: and hypothesized that this could ""serve as a stepping stone for further attacks (e.g.: opening a web page hosting drive-by malware)"".In ""data poisoning"": false data is continually smuggled into a machine learning system\'s training set to prevent it from achieving mastery. === Data collection ethics === Most Deep Learning systems': 'original_similarity': 0.864135493767899: 'slide': 135}","{'chunk_text': 'in ANNs. The transformer architecture was first described in 2017 as a method to teach ANNs grammatical dependencies in language: and is the predominant architecture used by large language models: such as GPT-4. Diffusion models were first described in 2015: and began to be used by image generation models such as DALL-E in the 2020s. == Linear neural network == The simplest kind of feedforward neural network is a linear network: which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated in each node. The mean squared errors between these calculated outputs and a given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement. == Perceptrons and other early neural networks == Warren McCulloch and Walter Pitts (1943) also considered a non-learning computational model for neural networks. This model paved the way for research to split into two approaches. One approach focused on biological processes while the other focused on the application of neural networks to artificial intelligence. This work led to work on nerve networks and their link to finite automata.In the early 1940s: D. O. Hebb created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Hebbian learning is unsupervised learning. This evolved into models for long-term potentiation. Researchers started applying these ideas to computational models in 1948 with Turing\'s B-type machines. Farley and Clark (1954) first used computational machines: then called ""calculators"": to simulate a Hebbian network. Other neural network computational machines were created by Rochester: Holland: Habit and Duda (1956).Rosenblatt (1958) created the perceptron: an algorithm for pattern recognition. With mathematical notation: Rosenblatt described circuitry not in the basic perceptron: such as the exclusive-or circuit that could not be processed by neural networks at the time. In 1959: a biological model proposed by Nobel laureates Hubel and Wiesel was based on their discovery of two types of cells in the primary visual cortex: simple cells and complex cells.Some say that research stagnated following Minsky and Papert (1969): who discovered that basic perceptrons were incapable of processing the exclusive-or circuit and that computers lacked sufficient power to process useful neural networks. However: by the time this book came out: methods for training multilayer perceptrons (MLPs) by deep learning were already known. == First deep learning == The first deep learning MLP was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa in 1965: as the Group Method of Data Handling. This method employs incremental layer by layer training based on regression analysis: where useless units in hidden layers are pruned with the help of a validation set. The first deep learning MLP trained by stochastic gradient descent was published in 1967 by Shun\'ichi Amari. In computer experiments conducted by Amari\'s student Saito: a five layer MLP with two modifiable layers learned useful internal representations to classify non-linearily separable pattern classes. == Backpropagation == The backpropagation algorithm is an efficient application of the Leibniz chain rule (1673) to networks of differentiable nodes. It is also known as the reverse mode of automatic differentiation or reverse accumulation: due to Seppo Linnainmaa (1970). The term ""back-propagating errors"" was introduced in 1962 by Frank Rosenblatt: but he did not have an implementation of this procedure: although Henry J. Kelley had a continuous precursor of backpropagation already in 1960 in the context of control theory. In 1982: Paul Werbos applied backpropagation to MLPs in the way that has become standard. In 1986: David E. Rumelhart et al. published an experimental analysis of the technique. == Recurrent network architectures == Wilhelm Lenz and Ernst Ising created and analyzed the Ising model (1925) which is essentially a non-learning artificial recurrent neural network (RNN) consisting of neuron-like threshold elements. In 1972: Shun\'ichi Amari made this architecture adaptive. His learning RNN was popularised by John Hopfield in 1982. == Self-organizing maps == Self-organizing maps (SOMs) were described by Teuvo Kohonen in 1982. SOMs are neurophysiologically inspired artificial neural networks that learn low-dimensional representations of high-dimensional data while preserving the topological structure of the data. They are trained using competitive learning. SOMs create internal representations reminiscent of the cortical homunculus: a distorted representation of the human body: based on a neurological ""map"" of the areas and proportions of the human brain dedicated to processing sensory functions: for different parts of the body. == Convolutional neural networks (CNNs) == The origin of the CNN architecture is the ""neocognitron"" introduced by Kunihiko Fukushima in 1980. It was inspired by work of Hubel and Wiesel in the 1950s and 1960s which showed that cat visual cortices contain neurons that individually respond to small regions of the visual field. The neocognitron introduced the two basic types of layers in CNNs: convolutional layers: and downsampling layers. A convolutional layer contains units whose receptive fields cover a patch of\nthe previous layer. The weight vector (the set of adaptive parameters) of such a unit is often called a filter. Units can share filters. Downsampling layers contain units whose receptive fields cover patches of previous convolutional layers. Such a unit typically computes the average of the activations of the units in its patch. This downsampling helps to correctly classify objects in visual scenes even when the objects are shifted. In 1969: Kunihiko Fukushima also introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for CNNs and deep neural networks in general.The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel and was one of the first CNNs: as it achieved shift invariance. It did so by utilizing weight sharing in combination with backpropagation': 'original_similarity': 0.8589766668242452: 'slide': 132}","{'chunk_text': 'in computer vision. Advances in hardware have driven renewed interest in deep learning. In 2009: Nvidia was involved in what was called the ""big bang"" of deep learning: ""as deep-learning neural networks were trained with Nvidia graphics processing units (GPUs)"". That year: Andrew Ng determined that GPUs could increase the speed of deep-learning systems by about 100 times. In particular: GPUs are well-suited for the matrix/vector computations involved in machine learning. GPUs speed up training algorithms by orders of magnitude: reducing running times from weeks to days. Further: specialized hardware and algorithm optimizations can be used for efficient processing of deep learning models. === Deep learning revolution === In the late 2000s: deep learning started to outperform other methods in machine learning competitions. In 2009: a long short-term memory trained by connectionist temporal classification (Alex Graves: Santiago Fernández: Faustino Gomez: and Jürgen Schmidhuber: 2006) was the first RNN to win pattern recognition contests: winning three competitions in connected handwriting recognition. Google later used CTC-trained LSTM for speech recognition on the smartphone.Significant impacts in image or object recognition were felt from 2011 to 2012. Although CNNs trained by backpropagation had been around for decades: and GPU implementations of NNs for years: including CNNs: faster implementations of CNNs on GPUs were needed to progress on computer vision. In 2011: the DanNet by Dan Ciresan: Ueli Meier: Jonathan Masci: Luca Maria Gambardella: and Jürgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest: outperforming traditional methods by a factor of 3. Also in 2011: DanNet won the ICDAR Chinese handwriting contest: and in May 2012: it won the ISBI image segmentation contest. Until 2011: CNNs did not play a major role at computer vision conferences: but in June 2012: a paper by Ciresan et al. at the leading conference CVPR showed how max-pooling CNNs on GPU can dramatically improve many vision benchmark records. In September 2012: DanNet also won the ICPR contest on analysis of large medical images for cancer detection: and in the following year also the MICCAI Grand Challenge on the same topic. In October 2012: the similar AlexNet by Alex Krizhevsky: Ilya Sutskever: and Geoffrey Hinton won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. The VGG-16 network by Karen Simonyan and Andrew Zisserman further reduced the error rate and won the ImageNet 2014 competition: following a similar trend in large-scale speech recognition. Image classification was then extended to the more challenging task of generating descriptions (captions) for images: often as a combination of CNNs and LSTMs.In 2012: a team led by George E. Dahl won the ""Merck Molecular Activity Challenge"" using multi-task deep neural networks to predict the biomolecular target of one drug. In 2014: Sepp Hochreiter\'s group used deep learning to detect off-target and toxic effects of environmental chemicals in nutrients: household products and drugs and won the ""Tox21 Data Challenge"" of NIH: FDA and NCATS.In 2016: Roger Parloff mentioned a ""deep learning revolution"" that has transformed the AI industry.In March 2019: Yoshua Bengio: Geoffrey Hinton and Yann LeCun were awarded the Turing Award for conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing. == Neural networks == Artificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve their ability) to do tasks by considering examples: generally without task-specific programming. For example: in image recognition: they might learn to identify images that contain cats by analyzing example images that have been manually labeled as ""cat"" or ""no cat"" and using the analytic results to identify cats in other images. They have found most use in applications difficult to express with a traditional computer algorithm using rule-based programming. An ANN is based on a collection of connected units called artificial neurons: (analogous to biological neurons in a biological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it. Neurons may have state: generally represented by real numbers: typically between 0 and 1. Neurons and synapses may also have a weight that varies as learning proceeds: which can increase or decrease the strength of the signal that it sends downstream. Typically: neurons are organized in layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first (input): to the last (output) layer: possibly after traversing the layers multiple times. The original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time: attention focused on matching specific mental abilities: leading to deviations from biology such as backpropagation: or passing information in the reverse direction and adjusting the network to reflect that information. Neural networks have been used on a variety of tasks: including computer vision: speech recognition: machine translation: social network filtering: playing board and video games and medical diagnosis. As of 2017: neural networks typically have a few thousand to a few million units and millions of connections. Despite\nthis number being several order of magnitude less than the number of neurons on a human brain: these networks can perform many tasks at a level beyond that of humans (e.g.: recognizing faces: or playing ""Go""). === Deep neural networks === A deep neural network (DNN) is an artificial neural network with multiple layers between the input and output layers. There are different types of neural networks but they always consist of the same components: neurons: synapses: weights: biases: and functions. These components as a whole function in a way that mimics functions of the human brain: and can be trained like any other ML algorithm.For example: a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed. The user can review the results and select which': 'original_similarity': 0.8548999598560781: 'slide': 143}"
0.40636839999933727,What is an optical neural network?,An optical neural network is a physical implementation of an artificial neural network with optical components. It uses the strength of the optical interconnect for implementing neuronal communications.,"{'chunk_text': ""proportion to the multiplexed hologram's strength. Volume holograms were further multiplexed using spectral hole burning to add one dimension of wavelength to space to achieve four dimensional interconnects of two dimensional arrays of neural inputs and outputs. This research led to extensive research on alternative methods using the strength of the optical interconnect for implementing neuronal communications.Some artificial neural networks that have been implemented as optical neural networks include the Hopfield neural network and the Kohonen self-organizing map with liquid crystal spatial light modulators Optical neural networks can also be based on the principles of neuromorphic engineering: creating neuromorphic photonic systems. Typically: these systems encode information in the networks using spikes: mimicking the functionality of spiking neural networks in optical and photonic hardware. Photonic devices that have demonstrated neuromorphic functionalities include (among others) vertical-cavity surface-emitting lasers: integrated photonic modulators: optoelectronic systems based on superconducting Josephson junctions or systems based on resonant tunnelling diodes. == Electrochemical vs. optical neural networks == Biological neural networks function on an electrochemical basis: while optical neural networks use electromagnetic waves. Optical interfaces to biological neural networks can be created with optogenetics: but is not the same as an optical neural networks. In biological neural networks there exist a lot of different mechanisms for dynamically changing the state of the neurons: these include short-term and long-term synaptic plasticity. Synaptic plasticity is among the electrophysiological phenomena used to control the efficiency of synaptic transmission: long-term for learning and memory: and short-term for short transient changes in synaptic transmission efficiency. Implementing this with optical components is difficult: and ideally requires advanced photonic materials. Properties that might be desirable in photonic materials for optical neural networks include the ability to change their efficiency of transmitting light: based on the intensity of incoming light. == Rising Era of Optical Neural Networks == With the increasing significance of computer vision in various domains: the computational cost of these tasks has increased: making it more important to develop the new approaches of the processing acceleration. Optical computing has emerged as a potential alternative to GPU acceleration for modern neural networks: particularly considering the looming obsolescence of Moore's Law. Consequently: optical neural networks have garnered increased attention in the research community. Presently: two primary methods of optical neural computing are under research: silicon photonics-based and free-space optics. Each approach has its benefits and drawbacks; while silicon photonics may offer superior speed: it lacks the massive parallelism that free-space optics can deliver. Given the substantial parallelism capabilities of free-space optics: researchers have focused on taking advantage of it. One implementation: proposed by Lin et al.: involves the training and fabrication of phase masks for a handwritten digit classifier. By stacking 3D-printed phase masks: light passing through the fabricated network can be read by a photodetector array of ten detectors: each representing a digit class ranging from 1 to 10. Although this network can achieve terahertz-range classification: it lacks flexibility: as the phase masks are fabricated for a specific task and cannot be retrained. An alternative method for classification in free-space optics: introduced by Cahng et al.: employs a 4F system that is based on the convolution theorem to perform convolution operations. This system uses two lenses to execute the Fourier transforms of the convolution operation: enabling passive conversion into the Fourier domain without power consumption or latency. However: the convolution operation kernels in this implementation are also fabricated phase masks: limiting the device's functionality to specific convolutional layers of the network only. In contrast: Li et al. proposed a technique involving kernel tiling to use the parallelism of the 4F system while using a Digital Micromirror Device (DMD) instead of a phase mask. This approach allows users to upload various kernels into the 4F system and execute the entire network's inference on a single device. Unfortunately: modern neural networks are not designed for the 4F systems: as they were primarily developed during the CPU/GPU era. Mostly because they tend to use a lower resolution and a high number of channels in their feature maps. == Other Implementations == In 2007 there was one model of Optical Neural Network: the Programmable Optical Array/Analogic Computer (POAC). It had been implemented in the year 2000 and reported based on modified Joint Fourier Transform Correlator (JTC) and Bacteriorhodopsin (BR) as a holographic optical memory. Full parallelism: large array size and the speed of light are three promises offered by POAC to implement an optical CNN. They had been investigated during the last years with their practical limitations and considerations yielding the design of the first portable POAC version. The practical details – hardware (optical setups) and software (optical templates) – were published. However: POAC is a general purpose and programmable array computer that has a wide range of applications including: image processing pattern recognition target tracking real-time video processing document security optical switching == See also == Optical computing Quantum neural network == References ==\nPhysics-informed neural networks (PINNs) are a type of universal function approximators that can embed the knowledge of any physical laws that govern a given data-set in the learning process: and can be described by partial differential equations (PDEs). They overcome"": 'original_similarity': 0.9055055519642565: 'slide': 38}","{'chunk_text': 'a permanent feature-detector in the network: available for producing outputs or for creating other: more complex feature detectors. The Cascade-Correlation architecture has several advantages: It learns quickly: determines its own size and topology: retains the structures it has built even if the training set changes and requires no backpropagation. === Neuro-fuzzy === A neuro-fuzzy network is a fuzzy inference system in the body of an artificial neural network. Depending on the FIS type: several layers simulate the processes involved in a fuzzy inference-like fuzzification: inference: aggregation and defuzzification. Embedding an FIS in a general structure of an ANN has the benefit of using available ANN training methods to find the parameters of a fuzzy system. === Compositional pattern-producing === Compositional pattern-producing networks (CPPNs) are a variation of artificial neural networks which differ in their set of activation functions and how they are applied. While typical artificial neural networks often contain only sigmoid functions (and sometimes Gaussian functions): CPPNs can include both types of functions and many others. Furthermore: unlike typical artificial neural networks: CPPNs are applied across the entire space of possible inputs so that they can represent a complete image. Since they are compositions of functions: CPPNs in effect encode images at infinite resolution and can be sampled for a particular display at whatever resolution is optimal. == Memory networks == Memory networks incorporate long-term memory. The long-term memory can be read and written to: with the goal of using it for prediction. These models have been applied in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base and the output is a textual response.In sparse distributed memory or hierarchical temporal memory: the patterns encoded by neural networks are used as addresses for content-addressable memory: with ""neurons"" essentially serving as address encoders and decoders. However: the early controllers of such memories were not differentiable. === One-shot associative memory === This type of network can add new patterns without re-training. It is done by creating a specific memory structure: which assigns each new pattern to an orthogonal plane using adjacently connected hierarchical arrays. The network offers real-time pattern recognition and high scalability; this requires parallel processing and is thus best suited for platforms such as wireless sensor networks: grid computing: and GPGPUs. === Hierarchical temporal memory === Hierarchical temporal memory (HTM) models some of the structural and algorithmic properties of the neocortex. HTM is a biomimetic model based on memory-prediction theory. HTM is a method for discovering and inferring the high-level causes of observed input patterns and sequences: thus building an increasingly complex model of the world. HTM combines existing ideas to mimic the neocortex with a simple design that provides many capabilities. HTM combines and extends approaches used in Bayesian networks: spatial and temporal clustering algorithms: while using a tree-shaped hierarchy of nodes that is common in neural networks. === Holographic associative memory === Holographic Associative Memory (HAM) is an analog: correlation-based: associative: stimulus-response system. Information is mapped onto the phase orientation of complex numbers. The memory is effective for associative memory tasks: generalization and pattern recognition with changeable attention. Dynamic search localization is central to biological memory. In visual perception: humans focus on specific objects in a pattern. Humans can change focus from object to object without learning. HAM can mimic this ability by creating explicit representations for focus. It uses a bi-modal representation of pattern and a hologram-like complex spherical weight state-space. HAMs are useful for optical realization because the underlying hyper-spherical computations can be implemented with optical computation. === LSTM-related differentiable memory structures === Apart from long short-term memory (LSTM): other approaches also added differentiable memory to recurrent functions. For example: Differentiable push and pop actions for alternative memory networks called neural stack machines Memory networks where the control network\'s external differentiable storage is in the fast weights of another network LSTM forget gates Self-referential RNNs with special output units for addressing and rapidly manipulating the RNN\'s own weights in differentiable fashion (internal storage) Learning to transduce with unbounded memory === Neural Turing machines === Neural Turing machines (NTM) couple LSTM networks to external memory resources: with which they can interact by attentional processes. The combined system is analogous to a Turing machine but is differentiable end-to-end: allowing it to be efficiently trained by gradient descent. Preliminary results demonstrate that neural Turing machines can infer simple algorithms such as copying: sorting and associative recall from input and output examples. Differentiable neural computers (DNC) are an NTM extension. They out-performed Neural turing machines: long short-term memory systems and memory networks on sequence-processing tasks. === Semantic hashing === Approaches that represent previous experiences directly and use a similar experience to form a local model are often called nearest neighbour or k-nearest neighbors methods. Deep learning is useful in semantic hashing where a deep graphical model the word-count vectors obtained from a large set of documents. Documents are mapped to memory addresses in such a way that semantically similar documents are\nlocated at nearby addresses. Documents similar to a query document can then be found by accessing all the addresses that differ by only a few bits from the address of the query document. Unlike sparse distributed memory that operates on 1000-bit addresses: semantic hashing works on 32 or 64-bit addresses found in a conventional computer architecture. === Pointer networks === Deep neural networks can be potentially improved by deepening and parameter reduction: while maintaining trainability. While training extremely deep (e.g.: 1 million layers) neural networks might not be practical: CPU-like architectures such as pointer networks and neural random-access machines overcome this limitation by using external random-access memory and other components that typically belong to a computer architecture such as registers: ALU and pointers. Such systems operate on probability distribution vectors stored in memory cells and registers. Thus: the model is fully differentiable and trains end-to-end. The key characteristic of these models is that their depth: the size of their short-term memory: and the number of parameters can be altered independently. ==': 'original_similarity': 0.8267766935737391: 'slide': 169}","{'chunk_text': 'it receives: or an inhibitory role: suppressing signals instead.Populations of interconnected neurons that are smaller than neural networks are called neural circuits. Very large interconnected networks are called large scale brain networks: and many of these together form brains and nervous systems. Signals generated by neural networks in the brain eventually travel through the nervous system and across neuromuscular junctions to muscle cells: where they cause contraction and thereby motion. == Artificial neural network == An artificial neural network is a mathematical model used to approximate nonlinear functions. While early artificial neural networks were physical machines: today they are almost always implemented in software. Neurons in an artificial neural network are usually arranged into layers: with information passing from the first layer (the input layer) through one or more intermediate layers (hidden layers) to the final layer (the output layer). The ""signal"" input to each neuron is a number: specifically a linear combination of the outputs of the connected neurons in the previous layer. The signal each neuron outputs is calculated from this number: according to its activation function. The behavior of the network depends on the strengths (or weights) of the connections between neurons. A network is trained by modifying these weights through empirical risk minimization or backpropagation in order to fit some preexisting dataset.Neural networks are used to solve problems in artificial intelligence: and have thereby found applications in many disciplines: including predictive modeling: adaptive control: facial recognition: handwriting recognition: general game playing: and generative AI. == History == The theoretical base for contemporary neural networks was independently proposed by Alexander Bain in 1873 and William James in 1890. Both posited that human thought emerged from interactions among large numbers of neurons inside the brain. In 1949: Donald Hebb described Hebbian learning: the idea that neural networks can change and learn over time by strengthening a synapse every time a signal travels along it.Artificial neural networks were originally used to model biological neural networks starting in the 1930s under the approach of connectionism. However: starting with the invention of the perceptron: a simple artificial neural network: by Warren McCulloch and Walter Pitts in 1943: followed by the implementation of one in hardware by Frank Rosenblatt in 1957: artificial neural networks became increasingly used for machine learning applications instead: and increasingly different from their biological counterparts. == See also == Emergence Biological cybernetics Biologically-inspired computing == References ==\nAn optical neural network is a physical implementation of an artificial neural network with optical components. Early optical neural networks used a photorefractive Volume hologram to interconnect arrays of input neurons to arrays of output with synaptic weights in proportion to the multiplexed hologram\'s strength. Volume holograms were further multiplexed using spectral hole burning to add one dimension of wavelength to space to achieve four dimensional interconnects of two dimensional arrays of neural inputs and outputs. This research led to extensive research on alternative methods using the strength of the optical interconnect for implementing neuronal communications.Some artificial neural networks that have been implemented as optical neural networks include the Hopfield neural network and the Kohonen self-organizing map with liquid crystal spatial light modulators Optical neural networks can also be based on the principles of neuromorphic engineering: creating neuromorphic photonic systems. Typically: these systems encode information in the networks using spikes: mimicking the functionality of spiking neural networks in optical and photonic hardware.': 'original_similarity': 0.8262104344142864: 'slide': 160}"
0.46897779998835176,What are some types of artificial neural networks that have been implemented as optical neural networks?,"Some artificial neural networks that have been implemented as optical neural networks include the Hopfield neural network and the Kohonen self-organizing map, often with liquid crystal spatial light modulators.","{'chunk_text': ""Volume holograms were further multiplexed using spectral hole burning to add one dimension of wavelength to space to achieve four dimensional interconnects of two dimensional arrays of neural inputs and outputs. This research led to extensive research on alternative methods using the strength of the optical interconnect for implementing neuronal communications.Some artificial neural networks that have been implemented as optical neural networks include the Hopfield neural network and the Kohonen self-organizing map with liquid crystal spatial light modulators Optical neural networks can also be based on the principles of neuromorphic engineering: creating neuromorphic photonic systems. Typically: these systems encode information in the networks using spikes: mimicking the functionality of spiking neural networks in optical and photonic hardware. Photonic devices that have demonstrated neuromorphic functionalities include (among others) vertical-cavity surface-emitting lasers: integrated photonic modulators: optoelectronic systems based on superconducting Josephson junctions or systems based on resonant tunnelling diodes. == Electrochemical vs. optical neural networks == Biological neural networks function on an electrochemical basis: while optical neural networks use electromagnetic waves. Optical interfaces to biological neural networks can be created with optogenetics: but is not the same as an optical neural networks. In biological neural networks there exist a lot of different mechanisms for dynamically changing the state of the neurons: these include short-term and long-term synaptic plasticity. Synaptic plasticity is among the electrophysiological phenomena used to control the efficiency of synaptic transmission: long-term for learning and memory: and short-term for short transient changes in synaptic transmission efficiency. Implementing this with optical components is difficult: and ideally requires advanced photonic materials. Properties that might be desirable in photonic materials for optical neural networks include the ability to change their efficiency of transmitting light: based on the intensity of incoming light. == Rising Era of Optical Neural Networks == With the increasing significance of computer vision in various domains: the computational cost of these tasks has increased: making it more important to develop the new approaches of the processing acceleration. Optical computing has emerged as a potential alternative to GPU acceleration for modern neural networks: particularly considering the looming obsolescence of Moore's Law. Consequently: optical neural networks have garnered increased attention in the research community. Presently: two primary methods of optical neural computing are under research: silicon photonics-based and free-space optics. Each approach has its benefits and drawbacks; while silicon photonics may offer superior speed: it lacks the massive parallelism that free-space optics can deliver. Given the substantial parallelism capabilities of free-space optics: researchers have focused on taking advantage of it. One implementation: proposed by Lin et al.: involves the training and fabrication of phase masks for a handwritten digit classifier. By stacking 3D-printed phase masks: light passing through the fabricated network can be read by a photodetector array of ten detectors: each representing a digit class ranging from 1 to 10. Although this network can achieve terahertz-range classification: it lacks flexibility: as the phase masks are fabricated for a specific task and cannot be retrained. An alternative method for classification in free-space optics: introduced by Cahng et al.: employs a 4F system that is based on the convolution theorem to perform convolution operations. This system uses two lenses to execute the Fourier transforms of the convolution operation: enabling passive conversion into the Fourier domain without power consumption or latency. However: the convolution operation kernels in this implementation are also fabricated phase masks: limiting the device's functionality to specific convolutional layers of the network only. In contrast: Li et al. proposed a technique involving kernel tiling to use the parallelism of the 4F system while using a Digital Micromirror Device (DMD) instead of a phase mask. This approach allows users to upload various kernels into the 4F system and execute the entire network's inference on a single device. Unfortunately: modern neural networks are not designed for the 4F systems: as they were primarily developed during the CPU/GPU era. Mostly because they tend to use a lower resolution and a high number of channels in their feature maps. == Other Implementations == In 2007 there was one model of Optical Neural Network: the Programmable Optical Array/Analogic Computer (POAC). It had been implemented in the year 2000 and reported based on modified Joint Fourier Transform Correlator (JTC) and Bacteriorhodopsin (BR) as a holographic optical memory. Full parallelism: large array size and the speed of light are three promises offered by POAC to implement an optical CNN. They had been investigated during the last years with their practical limitations and considerations yielding the design of the first portable POAC version. The practical details – hardware (optical setups) and software (optical templates) – were published. However: POAC is a general purpose and programmable array computer that has a wide range of applications including: image processing pattern recognition target tracking real-time video processing document security optical switching == See also == Optical computing Quantum neural network == References ==\nPhysics-informed neural networks (PINNs) are a type of universal function approximators that can embed the knowledge of any physical laws that govern a given data-set in the learning process: and can be described by partial differential equations (PDEs). They overcome the low data availability of some"": 'original_similarity': 0.8873198747682131: 'slide': 44}","{'chunk_text': ""segmentation). Some artificial neural networks are adaptive systems and are used for example to model populations and environments: which constantly change. Neural networks can be hardware- (neurons are represented by physical components) or software-based (computer models): and can use a variety of topologies and learning algorithms. == Feedforward == The feedforward neural network was the first and simplest type. In this network the information moves only from the input layer directly through any hidden layers to the output layer without cycles/loops. Feedforward networks can be constructed with various types of units: such as binary McCulloch–Pitts neurons: the simplest of which is the perceptron. Continuous neurons: frequently with sigmoidal activation: are used in the context of backpropagation. === Group method of data handling === The Group Method of Data Handling (GMDH) features fully automatic structural and parametric model optimization. The node activation functions are Kolmogorov–Gabor polynomials that permit additions and multiplications. It uses a deep multilayer perceptron with eight layers. It is a supervised learning network that grows layer by layer: where each layer is trained by regression analysis. Useless items are detected using a validation set: and pruned through regularization. The size and depth of the resulting network depends on the task. === Autoencoder === An autoencoder: autoassociator or Diabolo network: 19 is similar to the multilayer perceptron (MLP) – with an input layer: an output layer and one or more hidden layers connecting them. However: the output layer has the same number of units as the input layer. Its purpose is to reconstruct its own inputs (instead of emitting a target value). Therefore: autoencoders are unsupervised learning models. An autoencoder is used for unsupervised learning of efficient codings: typically for the purpose of dimensionality reduction and for learning generative models of data. === Probabilistic === A probabilistic neural network (PNN) is a four-layer feedforward neural network. The layers are Input: hidden pattern/summation: and output. In the PNN algorithm: the parent probability distribution function (PDF) of each class is approximated by a Parzen window and a non-parametric function. Then: using PDF of each class: the class probability of a new input is estimated and Bayes’ rule is employed to allocate it to the class with the highest posterior probability. It was derived from the Bayesian network and a statistical algorithm called Kernel Fisher discriminant analysis. It is used for classification and pattern recognition. === Time delay === A time delay neural network (TDNN) is a feedforward architecture for sequential data that recognizes features independent of sequence position. In order to achieve time-shift invariance: delays are added to the input so that multiple data points (points in time) are analyzed together. It usually forms part of a larger pattern recognition system. It has been implemented using a perceptron network whose connection weights were trained with back propagation (supervised learning). === Convolutional === A convolutional neural network (CNN: or ConvNet or shift invariant or space invariant) is a class of deep network: composed of one or more convolutional layers with fully connected layers (matching those in typical ANNs) on top. It uses tied weights and pooling layers. In particular: max-pooling. It is often structured via Fukushima's convolutional architecture. They are variations of multilayer perceptrons that use minimal preprocessing. This architecture allows CNNs to take advantage of the 2D structure of input data. Its unit connectivity pattern is inspired by the organization of the visual cortex. Units respond to stimuli in a restricted region of space known as the receptive field. Receptive fields partially overlap: over-covering the entire visual field. Unit response can be approximated mathematically by a convolution operation.CNNs are suitable for processing visual and other two-dimensional data. They have shown superior results in both image and speech applications. They can be trained with standard backpropagation. CNNs are easier to train than other regular: deep: feed-forward neural networks and have many fewer parameters to estimate.Capsule Neural Networks (CapsNet) add structures called capsules to a CNN and reuse output from several capsules to form more stable (with respect to various perturbations) representations.Examples of applications in computer vision include DeepDream and robot navigation. They have wide applications in image and video recognition: recommender systems and natural language processing. === Deep stacking network === A deep stacking network (DSN) (deep convex network) is based on a hierarchy of blocks of simplified neural network modules. It was introduced in 2011 by Deng and Yu. It formulates the learning as a convex optimization problem with a closed-form solution: emphasizing the mechanism's similarity to stacked generalization. Each DSN block is a simple module that is easy to train by itself in a supervised fashion without backpropagation for the entire blocks.Each block consists of a simplified multi-layer perceptron (MLP) with a single hidden layer. The hidden layer h has logistic sigmoidal units: and the output layer has linear units. Connections between these layers are represented by weight matrix U; input-to-hidden-layer connections have weight matrix W. Target vectors t form the columns of matrix T: and the input data vectors x form the columns of matrix X. The matrix of hidden units is H=σ(WTX){\\displaystyle {\\boldsymbol {H}}=\\sigma ({\\boldsymbol {W}}^{T}{\\boldsymbol {X}})}. Modules are trained in order: so lower-layer weights W are known at each stage. The function performs the element-wise logistic sigmoid operation. Each block estimates the same final label class y: and its estimate is concatenated with original input X to form the\nexpanded input for the next block. Thus: the input to the first block contains the original data only: while downstream blocks' input adds the output of preceding blocks. Then learning the upper-layer weight matrix U given other weights in the network can be formulated as a convex optimization problem: minUTf=‖UTH−T‖F2:{\\displaystyle \\min _{U^{T}}f=\\|{\\boldsymbol {U}}^{T}{\\boldsymbol {H}}-{\\boldsymbol {T}}\\|_{F}^{2}:}which has a closed-form solution.Unlike other deep architectures: such as DBNs: the goal is not to discover the transformed feature representation. The structure of the hierarchy of this kind of architecture makes parallel learning straightforward: as a batch-mode optimization problem. In purely discriminative tasks: DSNs outperform conventional DBNs. ==== Tensor deep stacking networks ==== This"": 'original_similarity': 0.8451004544792665: 'slide': 107}","{'chunk_text': 'input-side weights are frozen. This unit then becomes a permanent feature-detector in the network: available for producing outputs or for creating other: more complex feature detectors. The Cascade-Correlation architecture has several advantages: It learns quickly: determines its own size and topology: retains the structures it has built even if the training set changes and requires no backpropagation. === Neuro-fuzzy === A neuro-fuzzy network is a fuzzy inference system in the body of an artificial neural network. Depending on the FIS type: several layers simulate the processes involved in a fuzzy inference-like fuzzification: inference: aggregation and defuzzification. Embedding an FIS in a general structure of an ANN has the benefit of using available ANN training methods to find the parameters of a fuzzy system. === Compositional pattern-producing === Compositional pattern-producing networks (CPPNs) are a variation of artificial neural networks which differ in their set of activation functions and how they are applied. While typical artificial neural networks often contain only sigmoid functions (and sometimes Gaussian functions): CPPNs can include both types of functions and many others. Furthermore: unlike typical artificial neural networks: CPPNs are applied across the entire space of possible inputs so that they can represent a complete image. Since they are compositions of functions: CPPNs in effect encode images at infinite resolution and can be sampled for a particular display at whatever resolution is optimal. == Memory networks == Memory networks incorporate long-term memory. The long-term memory can be read and written to: with the goal of using it for prediction. These models have been applied in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base and the output is a textual response.In sparse distributed memory or hierarchical temporal memory: the patterns encoded by neural networks are used as addresses for content-addressable memory: with ""neurons"" essentially serving as address encoders and decoders. However: the early controllers of such memories were not differentiable. === One-shot associative memory === This type of network can add new patterns without re-training. It is done by creating a specific memory structure: which assigns each new pattern to an orthogonal plane using adjacently connected hierarchical arrays. The network offers real-time pattern recognition and high scalability; this requires parallel processing and is thus best suited for platforms such as wireless sensor networks: grid computing: and GPGPUs. === Hierarchical temporal memory === Hierarchical temporal memory (HTM) models some of the structural and algorithmic properties of the neocortex. HTM is a biomimetic model based on memory-prediction theory. HTM is a method for discovering and inferring the high-level causes of observed input patterns and sequences: thus building an increasingly complex model of the world. HTM combines existing ideas to mimic the neocortex with a simple design that provides many capabilities. HTM combines and extends approaches used in Bayesian networks: spatial and temporal clustering algorithms: while using a tree-shaped hierarchy of nodes that is common in neural networks. === Holographic associative memory === Holographic Associative Memory (HAM) is an analog: correlation-based: associative: stimulus-response system. Information is mapped onto the phase orientation of complex numbers. The memory is effective for associative memory tasks: generalization and pattern recognition with changeable attention. Dynamic search localization is central to biological memory. In visual perception: humans focus on specific objects in a pattern. Humans can change focus from object to object without learning. HAM can mimic this ability by creating explicit representations for focus. It uses a bi-modal representation of pattern and a hologram-like complex spherical weight state-space. HAMs are useful for optical realization because the underlying hyper-spherical computations can be implemented with optical computation. === LSTM-related differentiable memory structures === Apart from long short-term memory (LSTM): other approaches also added differentiable memory to recurrent functions. For example: Differentiable push and pop actions for alternative memory networks called neural stack machines Memory networks where the control network\'s external differentiable storage is in the fast weights of another network LSTM forget gates Self-referential RNNs with special output units for addressing and rapidly manipulating the RNN\'s own weights in differentiable fashion (internal storage) Learning to transduce with unbounded memory === Neural Turing machines === Neural Turing machines (NTM) couple LSTM networks to external memory resources: with which they can interact by attentional processes. The combined system is analogous to a Turing machine but is differentiable end-to-end: allowing it to be efficiently trained by gradient descent. Preliminary results demonstrate that neural Turing machines can infer simple algorithms such as copying: sorting and associative recall from input and output examples. Differentiable neural computers (DNC) are an NTM extension. They out-performed Neural turing machines: long short-term memory systems and memory networks on sequence-processing tasks. === Semantic hashing === Approaches that represent previous experiences directly and use a similar experience to form a local model are often called nearest neighbour or k-nearest neighbors methods. Deep learning is useful in semantic hashing where a deep graphical model the word-count vectors obtained from a large set of documents. Documents are mapped to memory addresses in such a way that semantically similar documents are\nlocated at nearby addresses. Documents similar to a query document can then be found by accessing all the addresses that differ by only a few bits from the address of the query document. Unlike sparse distributed memory that operates on 1000-bit addresses: semantic hashing works on 32 or 64-bit addresses found in a conventional computer architecture. === Pointer networks === Deep neural networks can be potentially improved by deepening and parameter reduction: while maintaining trainability. While training extremely deep (e.g.: 1 million layers) neural networks might not be practical: CPU-like architectures such as pointer networks and neural random-access machines overcome this limitation by using external random-access memory and other components that typically belong to a computer architecture such as registers: ALU and pointers. Such systems operate on probability distribution vectors stored in memory cells and registers. Thus: the model is fully differentiable and trains end-to-end. The key characteristic of these models is that their depth: the size of their short-term memory: and the': 'original_similarity': 0.8356398841861713: 'slide': 161}"
1.1088527000101749,How do biological neural networks differ from optical neural networks?,"Biological neural networks function on an electrochemical basis, while optical neural networks use electromagnetic waves. The mechanisms for dynamically changing the state of the neurons in a biological network include short-term and long-term synaptic plasticity.","{'chunk_text': ""hologram to interconnect arrays of input neurons to arrays of output with synaptic weights in proportion to the multiplexed hologram's strength. Volume holograms were further multiplexed using spectral hole burning to add one dimension of wavelength to space to achieve four dimensional interconnects of two dimensional arrays of neural inputs and outputs. This research led to extensive research on alternative methods using the strength of the optical interconnect for implementing neuronal communications.Some artificial neural networks that have been implemented as optical neural networks include the Hopfield neural network and the Kohonen self-organizing map with liquid crystal spatial light modulators Optical neural networks can also be based on the principles of neuromorphic engineering: creating neuromorphic photonic systems. Typically: these systems encode information in the networks using spikes: mimicking the functionality of spiking neural networks in optical and photonic hardware. Photonic devices that have demonstrated neuromorphic functionalities include (among others) vertical-cavity surface-emitting lasers: integrated photonic modulators: optoelectronic systems based on superconducting Josephson junctions or systems based on resonant tunnelling diodes. == Electrochemical vs. optical neural networks == Biological neural networks function on an electrochemical basis: while optical neural networks use electromagnetic waves. Optical interfaces to biological neural networks can be created with optogenetics: but is not the same as an optical neural networks. In biological neural networks there exist a lot of different mechanisms for dynamically changing the state of the neurons: these include short-term and long-term synaptic plasticity. Synaptic plasticity is among the electrophysiological phenomena used to control the efficiency of synaptic transmission: long-term for learning and memory: and short-term for short transient changes in synaptic transmission efficiency. Implementing this with optical components is difficult: and ideally requires advanced photonic materials. Properties that might be desirable in photonic materials for optical neural networks include the ability to change their efficiency of transmitting light: based on the intensity of incoming light. == Rising Era of Optical Neural Networks == With the increasing significance of computer vision in various domains: the computational cost of these tasks has increased: making it more important to develop the new approaches of the processing acceleration. Optical computing has emerged as a potential alternative to GPU acceleration for modern neural networks: particularly considering the looming obsolescence of Moore's Law. Consequently: optical neural networks have garnered increased attention in the research community. Presently: two primary methods of optical neural computing are under research: silicon photonics-based and free-space optics. Each approach has its benefits and drawbacks; while silicon photonics may offer superior speed: it lacks the massive parallelism that free-space optics can deliver. Given the substantial parallelism capabilities of free-space optics: researchers have focused on taking advantage of it. One implementation: proposed by Lin et al.: involves the training and fabrication of phase masks for a handwritten digit classifier. By stacking 3D-printed phase masks: light passing through the fabricated network can be read by a photodetector array of ten detectors: each representing a digit class ranging from 1 to 10. Although this network can achieve terahertz-range classification: it lacks flexibility: as the phase masks are fabricated for a specific task and cannot be retrained. An alternative method for classification in free-space optics: introduced by Cahng et al.: employs a 4F system that is based on the convolution theorem to perform convolution operations. This system uses two lenses to execute the Fourier transforms of the convolution operation: enabling passive conversion into the Fourier domain without power consumption or latency. However: the convolution operation kernels in this implementation are also fabricated phase masks: limiting the device's functionality to specific convolutional layers of the network only. In contrast: Li et al. proposed a technique involving kernel tiling to use the parallelism of the 4F system while using a Digital Micromirror Device (DMD) instead of a phase mask. This approach allows users to upload various kernels into the 4F system and execute the entire network's inference on a single device. Unfortunately: modern neural networks are not designed for the 4F systems: as they were primarily developed during the CPU/GPU era. Mostly because they tend to use a lower resolution and a high number of channels in their feature maps. == Other Implementations == In 2007 there was one model of Optical Neural Network: the Programmable Optical Array/Analogic Computer (POAC). It had been implemented in the year 2000 and reported based on modified Joint Fourier Transform Correlator (JTC) and Bacteriorhodopsin (BR) as a holographic optical memory. Full parallelism: large array size and the speed of light are three promises offered by POAC to implement an optical CNN. They had been investigated during the last years with their practical limitations and considerations yielding the design of the first portable POAC version. The practical details – hardware (optical setups) and software (optical templates) – were published. However: POAC is a general purpose and programmable array computer that has a wide range of applications including: image processing pattern recognition target tracking real-time video processing document security optical switching == See also == Optical computing Quantum neural network == References ==\nPhysics-informed neural networks (PINNs) are a type of universal function approximators that can embed the knowledge of any physical laws that govern a given data-set"": 'original_similarity': 0.8757791170814824: 'slide': 23}","{'chunk_text': 'of thousands of synapses. Each neuron sends and receives electrochemical signals called action potentials to its connected neighbors. A neuron can serve an excitatory role: amplifying and propagating signals it receives: or an inhibitory role: suppressing signals instead.Populations of interconnected neurons that are smaller than neural networks are called neural circuits. Very large interconnected networks are called large scale brain networks: and many of these together form brains and nervous systems. Signals generated by neural networks in the brain eventually travel through the nervous system and across neuromuscular junctions to muscle cells: where they cause contraction and thereby motion. == Artificial neural network == An artificial neural network is a mathematical model used to approximate nonlinear functions. While early artificial neural networks were physical machines: today they are almost always implemented in software. Neurons in an artificial neural network are usually arranged into layers: with information passing from the first layer (the input layer) through one or more intermediate layers (hidden layers) to the final layer (the output layer). The ""signal"" input to each neuron is a number: specifically a linear combination of the outputs of the connected neurons in the previous layer. The signal each neuron outputs is calculated from this number: according to its activation function. The behavior of the network depends on the strengths (or weights) of the connections between neurons. A network is trained by modifying these weights through empirical risk minimization or backpropagation in order to fit some preexisting dataset.Neural networks are used to solve problems in artificial intelligence: and have thereby found applications in many disciplines: including predictive modeling: adaptive control: facial recognition: handwriting recognition: general game playing: and generative AI. == History == The theoretical base for contemporary neural networks was independently proposed by Alexander Bain in 1873 and William James in 1890. Both posited that human thought emerged from interactions among large numbers of neurons inside the brain. In 1949: Donald Hebb described Hebbian learning: the idea that neural networks can change and learn over time by strengthening a synapse every time a signal travels along it.Artificial neural networks were originally used to model biological neural networks starting in the 1930s under the approach of connectionism. However: starting with the invention of the perceptron: a simple artificial neural network: by Warren McCulloch and Walter Pitts in 1943: followed by the implementation of one in hardware by Frank Rosenblatt in 1957: artificial neural networks became increasingly used for machine learning applications instead: and increasingly different from their biological counterparts. == See also == Emergence Biological cybernetics Biologically-inspired computing == References ==\nAn optical neural network is a physical implementation of an artificial neural network with optical components. Early optical neural networks used a photorefractive Volume hologram to interconnect arrays of input neurons to arrays of output with synaptic weights in proportion to the multiplexed hologram\'s strength. Volume holograms were further multiplexed using spectral hole burning to add one dimension of wavelength to space to achieve four dimensional interconnects of two dimensional arrays of neural inputs and outputs. This research led to extensive research on alternative methods using the strength of the optical interconnect for implementing neuronal communications.Some artificial neural networks that have been implemented as optical neural networks include the Hopfield neural network and the Kohonen self-organizing map with liquid crystal spatial light modulators Optical neural networks can also be based on the principles': 'original_similarity': 0.8537366917325314: 'slide': 131}","{'chunk_text': 'contemporary neural networks was independently proposed by Alexander Bain (1873) and William James (1890). In their work: both thoughts and body activity resulted from interactions among neurons within the brain. For Bain: every activity led to the firing of a certain set of neurons. When activities were repeated: the connections between those neurons strengthened. According to his theory: this repetition was what led to the formation of memory. The general scientific community at the time was skeptical of Bain\'s theory because it required what appeared to be an inordinate number of neural connections within the brain. It is now apparent that the brain is exceedingly complex and that the same brain “wiring” can handle multiple problems and inputs. James\' theory was similar to Bain\'s; however: he suggested that memories and actions resulted from electrical currents flowing among the neurons in the brain. His model: by focusing on the flow of electrical currents: did not require individual neural connections for each memory or action. C. S. Sherrington (1898) conducted experiments to test James\' theory. He ran electrical currents down the spinal cords of rats. However: instead of demonstrating an increase in electrical current as projected by James: Sherrington found that the electrical current strength decreased as the testing continued over time. Importantly: this work led to the discovery of the concept of habituation. McCulloch and Pitts (1943) also created a computational model for neural networks based on mathematics and algorithms. They called this model threshold logic. These early models paved the way for neural network research to split into two distinct approaches. One approach focused on biological processes in the brain and the other focused on the application of neural networks to artificial intelligence. The parallel distributed processing of the mid-1980s became popular under the name connectionism. The text by Rumelhart and McClelland (1986) provided a full exposition on the use of connectionism in computers to simulate neural processes. Artificial neural networks: as used in artificial intelligence: have traditionally been viewed as simplified models of neural processing in the brain: even though the relation between this model and brain biological architecture is debated: as it is not clear to what degree artificial neural networks mirror brain function. == Neuroscience == Theoretical and computational neuroscience is the field concerned with the analysis and computational modeling of biological neural systems. Since neural systems are intimately related to cognitive processes and behaviour: the field is closely related to cognitive and behavioural modeling. The aim of the field is to create models of biological neural systems in order to understand how biological systems work. To gain this understanding: neuroscientists strive to make a link between observed biological processes (data): biologically plausible mechanisms for neural processing and learning (neural network models) and theory (statistical learning theory and information theory). === Types of models === Many models are used; defined at different levels of abstraction: and modeling different aspects of neural systems. They range from models of the short-term behaviour of individual neurons: through models of the dynamics of neural circuitry arising from interactions between individual neurons: to models of behaviour arising from abstract neural modules that represent complete subsystems. These include models of the long-term and short-term plasticity of neural systems and their relation to learning and memory: from the individual neuron to the system level. === Connectivity === In August 2020 scientists reported that bi-directional connections: or added appropriate feedback connections: can accelerate and improve communication between and in modular neural networks of the brain\'s cerebral cortex and lower the threshold for their successful communication. They showed that adding feedback connections between a resonance pair can support successful propagation of a single pulse packet throughout the entire network. == Recent improvements == While initially research had been concerned mostly with the electrical characteristics of neurons: a particularly important part of the investigation in recent years has been the exploration of the role of neuromodulators such as dopamine: acetylcholine: and serotonin on behaviour and learning.Biophysical models: such as BCM theory: has been important in understanding mechanisms for synaptic plasticity: and have had applications in both computer science and neuroscience. == See also == Adaptive resonance theory Biological cybernetics Cognitive architecture Cognitive science Connectomics Cultured neuronal networks Parallel constraint satisfaction processes == References ==\nIn machine learning: an artificial neural network (also neural network or neural net: abbreviated ANN or NN) is a model inspired by the neuronal organization found in the biological neural networks in animal brains.An ANN is made of connected units or nodes called artificial neurons: which loosely model the neurons in a brain. These are connected by edges: which model the synapses in a brain. An artificial neuron receives signals from connected neurons: then processes them and sends a signal to other connected neurons. The ""signal"" is a real number: and the output of each neuron is computed by some non-linear function of the sum of its inputs: called the activation function. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Typically: neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer): possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least 2 hidden layers.Artificial neural networks are used for predictive modeling: adaptive control: and other applications where they can be trained via a dataset. They are also used to solve problems in artificial intelligence. Networks can learn from experience: and can derive conclusions from a complex and seemingly unrelated set of information. == Training == Neural networks are': 'original_similarity': 0.8497858327496093: 'slide': 245}"
0.4320900000166148,What are the two primary methods of optical neural computing currently under research?,"The two primary methods of optical neural computing under research are silicon photonics-based and free-space optics. Silicon photonics offer superior speed, while free-space optics deliver massive parallelism.","{'chunk_text': ""extensive research on alternative methods using the strength of the optical interconnect for implementing neuronal communications.Some artificial neural networks that have been implemented as optical neural networks include the Hopfield neural network and the Kohonen self-organizing map with liquid crystal spatial light modulators Optical neural networks can also be based on the principles of neuromorphic engineering: creating neuromorphic photonic systems. Typically: these systems encode information in the networks using spikes: mimicking the functionality of spiking neural networks in optical and photonic hardware. Photonic devices that have demonstrated neuromorphic functionalities include (among others) vertical-cavity surface-emitting lasers: integrated photonic modulators: optoelectronic systems based on superconducting Josephson junctions or systems based on resonant tunnelling diodes. == Electrochemical vs. optical neural networks == Biological neural networks function on an electrochemical basis: while optical neural networks use electromagnetic waves. Optical interfaces to biological neural networks can be created with optogenetics: but is not the same as an optical neural networks. In biological neural networks there exist a lot of different mechanisms for dynamically changing the state of the neurons: these include short-term and long-term synaptic plasticity. Synaptic plasticity is among the electrophysiological phenomena used to control the efficiency of synaptic transmission: long-term for learning and memory: and short-term for short transient changes in synaptic transmission efficiency. Implementing this with optical components is difficult: and ideally requires advanced photonic materials. Properties that might be desirable in photonic materials for optical neural networks include the ability to change their efficiency of transmitting light: based on the intensity of incoming light. == Rising Era of Optical Neural Networks == With the increasing significance of computer vision in various domains: the computational cost of these tasks has increased: making it more important to develop the new approaches of the processing acceleration. Optical computing has emerged as a potential alternative to GPU acceleration for modern neural networks: particularly considering the looming obsolescence of Moore's Law. Consequently: optical neural networks have garnered increased attention in the research community. Presently: two primary methods of optical neural computing are under research: silicon photonics-based and free-space optics. Each approach has its benefits and drawbacks; while silicon photonics may offer superior speed: it lacks the massive parallelism that free-space optics can deliver. Given the substantial parallelism capabilities of free-space optics: researchers have focused on taking advantage of it. One implementation: proposed by Lin et al.: involves the training and fabrication of phase masks for a handwritten digit classifier. By stacking 3D-printed phase masks: light passing through the fabricated network can be read by a photodetector array of ten detectors: each representing a digit class ranging from 1 to 10. Although this network can achieve terahertz-range classification: it lacks flexibility: as the phase masks are fabricated for a specific task and cannot be retrained. An alternative method for classification in free-space optics: introduced by Cahng et al.: employs a 4F system that is based on the convolution theorem to perform convolution operations. This system uses two lenses to execute the Fourier transforms of the convolution operation: enabling passive conversion into the Fourier domain without power consumption or latency. However: the convolution operation kernels in this implementation are also fabricated phase masks: limiting the device's functionality to specific convolutional layers of the network only. In contrast: Li et al. proposed a technique involving kernel tiling to use the parallelism of the 4F system while using a Digital Micromirror Device (DMD) instead of a phase mask. This approach allows users to upload various kernels into the 4F system and execute the entire network's inference on a single device. Unfortunately: modern neural networks are not designed for the 4F systems: as they were primarily developed during the CPU/GPU era. Mostly because they tend to use a lower resolution and a high number of channels in their feature maps. == Other Implementations == In 2007 there was one model of Optical Neural Network: the Programmable Optical Array/Analogic Computer (POAC). It had been implemented in the year 2000 and reported based on modified Joint Fourier Transform Correlator (JTC) and Bacteriorhodopsin (BR) as a holographic optical memory. Full parallelism: large array size and the speed of light are three promises offered by POAC to implement an optical CNN. They had been investigated during the last years with their practical limitations and considerations yielding the design of the first portable POAC version. The practical details – hardware (optical setups) and software (optical templates) – were published. However: POAC is a general purpose and programmable array computer that has a wide range of applications including: image processing pattern recognition target tracking real-time video processing document security optical switching == See also == Optical computing Quantum neural network == References ==\nPhysics-informed neural networks (PINNs) are a type of universal function approximators that can embed the knowledge of any physical laws that govern a given data-set in the learning process: and can be described by partial differential equations (PDEs). They overcome the low data availability of some biological and engineering systems that makes most state-of-the-art machine learning techniques lack robustness: rendering them ineffective in these scenarios. The prior knowledge of general physical laws acts in the training of neural networks (NNs) as"": 'original_similarity': 0.8597919691474242: 'slide': 79}","{'chunk_text': 'contests: approaching human level performance on various tasks: initially in pattern recognition and handwriting recognition. For example: the bi-directional and multi-dimensional long short-term memory (LSTM) of Graves et al. won three competitions in connected handwriting recognition in 2009 without any prior knowledge about the three languages to be learned.Ciresan and colleagues built the first pattern recognizers to achieve human-competitive/superhuman performance on benchmarks such as traffic sign recognition (IJCNN 2012). Radial basis function and wavelet networks were introduced in 2013. These can be shown to offer best approximation properties and have been applied in nonlinear system identification and classification applications.In 2014: the adversarial network principle was used in a generative adversarial network (GAN) by Ian Goodfellow et al. Here the adversarial network (discriminator) outputs a value between 1 and 0 depending on the likelihood of the first network\'s (generator) output is in a given set. This can be used to create realistic deepfakes. Excellent image quality is achieved by Nvidia\'s StyleGAN (2018) based on the Progressive GAN by Tero Karras: Timo Aila: Samuli Laine: and Jaakko Lehtinen. Here the GAN generator is grown from small to large scale in a pyramidal fashion. In 2015: Rupesh Kumar Srivastava: Klaus Greff: and Schmidhuber used the LSTM principle to create the Highway network: a feedforward neural network with hundreds of layers: much deeper than previous networks. 7 months later: Kaiming He: Xiangyu Zhang; Shaoqing Ren: and Jian Sun won the ImageNet 2015 competition with an open-gated or gateless Highway network variant called Residual neural network.In 2017: Ashish Vaswani et al. introduced the modern Transformer architecture in their paper ""Attention Is All You Need."" It combines this with a softmax operator and a projection matrix. Transformers have increasingly become the model of choice for natural language processing. Many modern large language models such as ChatGPT: GPT-4: and BERT use it. Transformers are also increasingly being used in computer vision.Ramenzanpour et al. showed in 2020 that analytical and computational techniques derived from statistical physics of disordered systems can be extended to large-scale problems: including machine learning: e.g.: to analyze the weight space of deep neural networks. == Models == ANNs began as an attempt to exploit the architecture of the human brain to perform tasks that conventional algorithms had little success with. They soon reoriented towards improving empirical results: abandoning attempts to remain true to their biological precursors. ANNs have the ability to learn and model non-linearities and complex relationships. This is achieved by neurons being connected in various patterns: allowing the output of some neurons to become the input of others. The network forms a directed: weighted graph.An artificial neural network consists of simulated neurons. Each neuron is connected to other nodes via links like a biological axon-synapse-dendrite connection. All the nodes connected by links take in some data and use it to perform specific operations and tasks on the data. Each link has a weight: determining the strength of one node\'s influence on another: allowing weights to choose the signal between neurons. === Artificial neurons === ANNs are composed of artificial neurons which are conceptually derived from biological neurons. Each artificial neuron has inputs and produces a single output which can be sent to multiple other neurons. The inputs can be the feature values of a sample of external data: such as images or documents: or they can be the outputs of other neurons. The outputs of the final output neurons of the neural net accomplish the task: such as recognizing an object in an image. To find the output of the neuron we take the weighted sum of all the inputs: weighted by the weights of the connections from the inputs to the neuron. We add a bias term to this sum. This weighted sum is sometimes called the activation. This weighted sum is then passed through a (usually nonlinear) activation function to produce the output. The initial inputs are external data: such as images and documents. The ultimate outputs accomplish the task: such as recognizing an object in an image. === Organization === The neurons are typically organized into multiple layers: especially in deep learning. Neurons of one layer connect only to neurons of the immediately preceding and immediately following layers. The layer that receives external data is the input layer. The layer that produces the ultimate result is the output layer. In between them are zero or more hidden layers. Single layer and unlayered networks are also used. Between two layers: multiple connection patterns are possible. They can be \'fully connected\': with every neuron in one layer connecting to every neuron in the next layer. They can be pooling: where a group of neurons in one layer connects to a single neuron in the next layer: thereby reducing the number of neurons in that layer. Neurons with only such connections form a directed acyclic graph and are known as feedforward networks. Alternatively: networks that allow connections between neurons in the same or previous layers are known as recurrent networks. === Hyperparameter === A hyperparameter is a constant parameter whose value is set before the learning process begins. The values of parameters are derived via learning. Examples of hyperparameters include learning rate: the number of hidden layers and batch size. The values of some hyperparameters can be dependent on those of other\nhyperparameters. For example: the size of some layers can depend on the overall number of layers. === Learning === Learning is the adaptation of the network to better handle a task by considering sample observations. Learning involves adjusting the weights (and optional thresholds) of the network to improve the accuracy of the result. This is done by minimizing the observed errors. Learning is complete when examining additional observations does not usefully reduce the error rate. Even after learning: the error rate typically does not reach 0. If after learning: the error rate is too high: the network typically must be redesigned. Practically this is done by defining a cost function that is evaluated periodically during learning. As long as its output': 'original_similarity': 0.8286402498413055: 'slide': 119}","{'chunk_text': ""critics say the chip isn't powerful enough: its supporters point out that this is only the first generation: and the capabilities of improved iterations will become clear. (IBM) == Benchmarks == Classification capabilities of spiking networks trained according to unsupervised learning methods have been tested on the common benchmark datasets: such as: Iris: Wisconsin Breast Cancer or Statlog Landsat dataset. Various approaches to information encoding and network design have been used. For example: a 2-layer feedforward network for data clustering and classification. Based on the idea proposed in Hopfield (1995) the authors implemented models of local receptive fields combining the properties of radial basis functions (RBF) and spiking neurons to convert input signals (classified data) having a floating-point representation into a spiking representation. == See also == == References ==\nThere are many types of artificial neural networks (ANN). Artificial neural networks are computational models inspired by biological neural networks: and are used to approximate functions that"": 'original_similarity': 0.828434996961603: 'slide': 25}"
0.39621139998780563,What was significant about the Programmable Optical Array/Analogic Computer (POAC)?,"The Programmable Optical Array/Analogic Computer (POAC) was a model of an optical neural network implemented in 2000. It utilized a Joint Fourier Transform Correlator (JTC) and Bacteriorhodopsin (BR) as a holographic optical memory. It promised full parallelism, large array size, and the speed of light for implementing an optical CNN.","{'chunk_text': ""neuromorphic photonic systems. Typically: these systems encode information in the networks using spikes: mimicking the functionality of spiking neural networks in optical and photonic hardware. Photonic devices that have demonstrated neuromorphic functionalities include (among others) vertical-cavity surface-emitting lasers: integrated photonic modulators: optoelectronic systems based on superconducting Josephson junctions or systems based on resonant tunnelling diodes. == Electrochemical vs. optical neural networks == Biological neural networks function on an electrochemical basis: while optical neural networks use electromagnetic waves. Optical interfaces to biological neural networks can be created with optogenetics: but is not the same as an optical neural networks. In biological neural networks there exist a lot of different mechanisms for dynamically changing the state of the neurons: these include short-term and long-term synaptic plasticity. Synaptic plasticity is among the electrophysiological phenomena used to control the efficiency of synaptic transmission: long-term for learning and memory: and short-term for short transient changes in synaptic transmission efficiency. Implementing this with optical components is difficult: and ideally requires advanced photonic materials. Properties that might be desirable in photonic materials for optical neural networks include the ability to change their efficiency of transmitting light: based on the intensity of incoming light. == Rising Era of Optical Neural Networks == With the increasing significance of computer vision in various domains: the computational cost of these tasks has increased: making it more important to develop the new approaches of the processing acceleration. Optical computing has emerged as a potential alternative to GPU acceleration for modern neural networks: particularly considering the looming obsolescence of Moore's Law. Consequently: optical neural networks have garnered increased attention in the research community. Presently: two primary methods of optical neural computing are under research: silicon photonics-based and free-space optics. Each approach has its benefits and drawbacks; while silicon photonics may offer superior speed: it lacks the massive parallelism that free-space optics can deliver. Given the substantial parallelism capabilities of free-space optics: researchers have focused on taking advantage of it. One implementation: proposed by Lin et al.: involves the training and fabrication of phase masks for a handwritten digit classifier. By stacking 3D-printed phase masks: light passing through the fabricated network can be read by a photodetector array of ten detectors: each representing a digit class ranging from 1 to 10. Although this network can achieve terahertz-range classification: it lacks flexibility: as the phase masks are fabricated for a specific task and cannot be retrained. An alternative method for classification in free-space optics: introduced by Cahng et al.: employs a 4F system that is based on the convolution theorem to perform convolution operations. This system uses two lenses to execute the Fourier transforms of the convolution operation: enabling passive conversion into the Fourier domain without power consumption or latency. However: the convolution operation kernels in this implementation are also fabricated phase masks: limiting the device's functionality to specific convolutional layers of the network only. In contrast: Li et al. proposed a technique involving kernel tiling to use the parallelism of the 4F system while using a Digital Micromirror Device (DMD) instead of a phase mask. This approach allows users to upload various kernels into the 4F system and execute the entire network's inference on a single device. Unfortunately: modern neural networks are not designed for the 4F systems: as they were primarily developed during the CPU/GPU era. Mostly because they tend to use a lower resolution and a high number of channels in their feature maps. == Other Implementations == In 2007 there was one model of Optical Neural Network: the Programmable Optical Array/Analogic Computer (POAC). It had been implemented in the year 2000 and reported based on modified Joint Fourier Transform Correlator (JTC) and Bacteriorhodopsin (BR) as a holographic optical memory. Full parallelism: large array size and the speed of light are three promises offered by POAC to implement an optical CNN. They had been investigated during the last years with their practical limitations and considerations yielding the design of the first portable POAC version. The practical details – hardware (optical setups) and software (optical templates) – were published. However: POAC is a general purpose and programmable array computer that has a wide range of applications including: image processing pattern recognition target tracking real-time video processing document security optical switching == See also == Optical computing Quantum neural network == References ==\nPhysics-informed neural networks (PINNs) are a type of universal function approximators that can embed the knowledge of any physical laws that govern a given data-set in the learning process: and can be described by partial differential equations (PDEs). They overcome the low data availability of some biological and engineering systems that makes most state-of-the-art machine learning techniques lack robustness: rendering them ineffective in these scenarios. The prior knowledge of general physical laws acts in the training of neural networks (NNs) as a regularization agent that limits the space of admissible solutions: increasing the correctness of the function approximation. This way: embedding this prior information into a neural network results in enhancing the information content of the available data: facilitating the learning algorithm to capture the right solution and to generalize well even with a low amount of training"": 'original_similarity': 0.8231415054149412: 'slide': 136}","{'chunk_text': 'where the same basic device structure is used for both logic operations and data storage. The authors used two-dimensional materials such as semiconducting molybdenum disulphide to precisely tune FGFETs as building blocks in which logic operations can be performed with the memory elements. === Integrated photonic tensor core === In 1988: Wei Zhang et al. discussed fast optical implementations of convolutional neural networks for alphabet recognition. In 2021: J. Feldmann et al. proposed an integrated photonic hardware accelerator for parallel convolutional processing. The authors identify two key advantages of integrated photonics over its electronic counterparts: (1) massively parallel data transfer through wavelength division multiplexing in conjunction with frequency combs: and (2) extremely high data modulation speeds. Their system can execute trillions of multiply-accumulate operations per second: indicating the potential of integrated photonics in data-heavy AI applications. Optical processors that can also perform backpropagation for artificial neural networks have been experimentally developed. == Nomenclature == As of 2016: the field is still in flux and vendors are pushing their own marketing term for what amounts to an ""AI accelerator"": in the hope that their designs and APIs will become the dominant design. There is no consensus on the boundary between these devices: nor the exact form they will take; however several examples clearly aim to fill this new space: with a fair amount of overlap in capabilities. In the past when consumer graphics accelerators emerged: the industry eventually adopted Nvidia\'s self-assigned term: ""the GPU"": as the collective noun for ""graphics accelerators"": which had taken many forms before settling on an overall pipeline implementing a model presented by Direct3D. All models of Intel Meteor Lake processors have a Versatile Processor Unit (VPU) built-in for accelerating inference for computer vision and deep learning. == Deep Learning Processors (DLP) == Inspired from the pioneer work of DianNao Family: many DLPs are proposed in both academia and industry with design optimized to leverage the features of deep neural networks for high efficiency. Only at ISCA 2016: three sessions: 15% (!) of the accepted papers: are all architecture designs about deep learning. Such efforts include Eyeriss (MIT): EIE (Stanford): Minerva (Harvard): Stripes (University of Toronto) in academia: TPU (Google): and MLU (Cambricon) in industry. We listed several representative works in Table 1. === Digital DLPs === The major components of DLPs architecture usually include a computation component: the on-chip memory hierarchy: and the control logic that manages the data communication and computing flows. Regarding the computation component: as most operations in deep learning can be aggregated into vector operations: the most common ways for building computation components in digital DLPs are the MAC-based (multiplier-accumulation) organization: either with vector MACs or scalar MACs. Rather than SIMD or SIMT in general processing devices: deep learning domain-specific parallelism is better explored on these MAC-based organizations. Regarding the memory hierarchy: as deep learning algorithms require high bandwidth to provide the computation component with sufficient data: DLPs usually employ a relatively larger size (tens of kilobytes or several megabytes) on-chip buffer but with dedicated on-chip data reuse strategy and data exchange strategy to alleviate the burden for memory bandwidth. For example: DianNao: 16 16-in vector MAC: requires 16 × 16 × 2 = 512 16-bit data: i.e.: almost 1024GB/s bandwidth requirements between computation components and buffers. With on-chip reuse: such bandwidth requirements are reduced drastically. Instead of the widely used cache in general processing devices: DLPs always use scratchpad memory as it could provide higher data reuse opportunities by leveraging the relatively regular data access pattern in deep learning algorithms. Regarding the control logic: as the deep learning algorithms keep evolving at a dramatic speed: DLPs start to leverage dedicated ISA (instruction set architecture) to support the deep learning domain flexibly. At first: DianNao used a VLIW-style instruction set where each instruction could finish a layer in a DNN. Cambricon introduces the first deep learning domain-specific ISA: which could support more than ten different deep learning algorithms. TPU also reveals five key instructions from the CISC-style ISA. === Hybrid DLPs === Hybrid DLPs emerge for DNN inference and training acceleration because of their high efficiency. Processing-in-memory (PIM) architectures are one most important type of hybrid DLP. The key design concept of PIM is to bridge the gap between computing and memory: with the following manners: 1) Moving computation components into memory cells: controllers: or memory chips to alleviate the memory wall issue. Such architectures significantly shorten data paths and leverage much higher internal bandwidth: hence resulting in attractive performance improvement. 2) Build high efficient DNN engines by adopting computational devices. In 2013: HP Lab demonstrated the astonishing capability of adopting ReRAM crossbar structure for computing. Inspiring by this work: tremendous work are proposed to explore the new architecture and system design based on ReRAM: phase change memory: etc. == Benchmarks == Benchmarks such as MLPerf and others may\nbe used to evaluate the performance of AI accelerators. Table 2 lists several typical benchmarks for AI accelerators. == Potential applications == Agricultural robots: for example: herbicide-free weed control. Autonomous vehicles: Nvidia has targeted their Drive PX-series boards at this application. Computer-aided diagnosis Industrial robots: increasing the range of tasks that can be automated: by adding adaptability to variable situations. Machine translation Military robots Natural language processing Search engines: increasing the energy efficiency of data centers and the ability to use increasingly advanced queries. Unmanned aerial vehicles: e.g. navigation systems: e.g. the Movidius Myriad 2 has been demonstrated successfully guiding autonomous drones. Voice user interface: e.g. in mobile phones: a target for Qualcomm Zeroth. == See also == Cognitive computer Neuromorphic engineering Optical neural network Physical neural network Cerebras Systems == References == == External links == Nvidia Puts The Accelerator To The Metal With Pascal.htm: The Next Platform Eyeriss Project: MIT https://alphaics.ai/': 'original_similarity': 0.8069631426156713: 'slide': 192}",{'chunk_text': 'benchmarks for AI accelerators. == Potential applications == Agricultural robots: for example: herbicide-free weed control. Autonomous vehicles: Nvidia has targeted their Drive PX-series boards at this application. Computer-aided diagnosis Industrial robots: increasing the range of tasks that can be automated: by adding adaptability to variable situations. Machine translation Military robots Natural language processing Search engines: increasing the energy efficiency of data centers and the ability to use increasingly advanced queries. Unmanned aerial vehicles: e.g. navigation systems: e.g. the Movidius Myriad 2 has been demonstrated successfully guiding autonomous drones. Voice user interface: e.g. in mobile phones: a target for Qualcomm Zeroth. == See also == Cognitive computer Neuromorphic engineering Optical neural network Physical neural network Cerebras Systems == References == == External links == Nvidia Puts The Accelerator To The Metal With Pascal.htm: The Next Platform Eyeriss Project: MIT https://alphaics.ai/\nBidirectional Encoder Representations from Transformers (BERT) is a language model based on the transformer architecture:': 'original_similarity': 0.7766430458753663: 'slide': 13}
0.38773229997605085,What are Physics-informed neural networks (PINNs)?,"Physics-informed neural networks (PINNs) are universal function approximators that can integrate the knowledge of any physical laws governing a given data-set in the learning process. They can be described by partial differential equations (PDEs) and help overcome the low data availability of some biological and engineering systems. They increase the correctness of the function approximation and enhance the information content of the available data, making learning algorithms capture the right solution and generalize well.",{'chunk_text': 'of the physical laws that govern the dynamics of a system can be described by partial differential equations. For example: the Navier–Stokes equations are a set of partial differential equations derived from the conservation laws (i.e.: conservation of mass: momentum: and energy) that govern fluid mechanics. The solution of the Navier–Stokes equations with appropriate initial and boundary conditions allows the quantification of flow dynamics in a precisely defined geometry. However: these equations cannot be solved exactly and therefore numerical methods must be used (such as finite differences: finite elements and finite volumes). In this setting: these governing equations must be solved while accounting for prior assumptions: linearization: and adequate time and space discretization. Recently: solving the governing partial differential equations of physical phenomena using deep learning has emerged as a new field of scientific machine learning (SciML): leveraging the universal approximation theorem and high expressivity of neural networks. In general: deep neural networks could approximate any high-dimensional function given that sufficient training data are supplied. However: such networks do not consider the physical characteristics underlying the problem: and the level of approximation accuracy provided by them is still heavily dependent on careful specifications of the problem geometry as well as the initial and boundary conditions. Without this preliminary information: the solution is not unique and may lose physical correctness. On the other hand: physics-informed neural networks (PINNs) leverage governing physical equations in neural network training. Namely: PINNs are designed to be trained to satisfy the given training data as well as the imposed governing equations. In this fashion: a neural network can be guided with training data that do not necessarily need to be large and complete. Potentially: an accurate solution of partial differential equations can be found without knowing the boundary conditions. Therefore: with some knowledge about the physical characteristics of the problem and some form of training data (even sparse and incomplete): PINN may be used for finding an optimal solution with high fidelity. PINNs allow for addressing a wide range of problems in computational science and represent a pioneering technology leading to the development of new classes of numerical solvers for PDEs. PINNs can be thought of as a meshfree alternative to traditional approaches (e.g.: CFD for fluid dynamics): and new data-driven approaches for model inversion and system identification. Notably: the trained PINN network can be used for predicting the values on simulation grids of different resolutions without the need to be retrained. In addition: they allow for exploiting automatic differentiation (AD) to compute the required derivatives in the partial differential equations: a new class of differentiation techniques widely used to derive neural networks assessed to be superior to numerical or symbolic differentiation. == Modeling and computation == A general nonlinear partial differential equation can be: ut+N[u;λ]=0:x∈Ω:t∈[0:T]{\\displaystyle u_{t}+N[u;\\lambda ]=0:\\quad x\\in \\Omega :\\quad t\\in [0:T]} where u(t:x){\\displaystyle u(t:x)} denotes the solution: N[⋅;λ]{\\displaystyle N[\\cdot ;\\lambda ]} is a nonlinear operator parametrized by λ{\\displaystyle \\lambda }: and Ω{\\displaystyle \\Omega } is a subset of RD{\\displaystyle \\mathbb {R} ^{D}}. This general form of governing equations summarizes a wide range of problems in mathematical physics: such as conservative laws: diffusion process: advection-diffusion systems: and kinetic equations. Given noisy measurements of a generic dynamic system described by the equation above: PINNs can be designed to solve two classes of problems: data-driven solution data-driven discoveryof partial differential equations. === Data-driven solution of partial differential equations === The data-driven solution of PDE computes the hidden state u(t:x){\\displaystyle u(t:x)} of the system given boundary data and/or measurements z{\\displaystyle z}: and fixed model parameters λ{\\displaystyle \\lambda }. We solve: ut+N[u]=0:x∈Ω:t∈[0:T]{\\displaystyle u_{t}+N[u]=0:\\quad x\\in \\Omega :\\quad t\\in [0:T]}. By defining the residual f(t:x){\\displaystyle f(t:x)} as f:=ut+N[u]=0{\\displaystyle f:=u_{t}+N[u]=0}: and approximating u(t:x){\\displaystyle u(t:x)} by a deep neural network. This network can be differentiated using automatic differentiation. The parameters of u(t:x){\\displaystyle u(t:x)} and f(t:x){\\displaystyle f(t:x)} can be then learned by minimizing the following loss function Ltot{\\displaystyle L_{tot}}: Ltot=Lu+Lf{\\displaystyle L_{tot}=L_{u}+L_{f}}. Where Lu=‖u−z‖Γ{\\displaystyle L_{u}=\\Vert u-z\\Vert _{\\Gamma }} is the error between the PINN u(t:x){\\displaystyle u(t:x)} and the set of boundary conditions and measured data on the set of points Γ{\\displaystyle \\Gamma } where the boundary conditions and data are defined: and Lf=‖f‖Γ{\\displaystyle L_{f}=\\Vert f\\Vert _{\\Gamma }} is the mean-squared error of the residual function. This second term encourages the PINN to learn the structural information expressed by the partial differential equation during the training process. This approach has been used to yield computationally efficient physics-informed surrogate models with applications in the forecasting of physical processes: model predictive control: multi-physics and multi-scale modeling: and simulation. It has been shown to converge to the solution of the PDE. === Data-driven discovery of partial differential equations === Given noisy and incomplete measurements z{\\displaystyle z} of the state of the system: the data-driven discovery of PDE results in computing the unknown state u(t:x){\\displaystyle u(t:x)} and learning model parameters λ{\\displaystyle \\lambda } that best describe the observed data and it reads as follows: ut+N[u;λ]=0:x∈Ω:t∈[0:T]{\\displaystyle u_{t}+N[u;\\lambda ]=0:\\quad x\\in \\Omega :\\quad t\\in [0:T]}. By defining f(t:x){\\displaystyle f(t:x)} as f:=ut+N[u;λ]=0{\\displaystyle f:=u_{t}+N[u;\\lambda ]=0}: and approximating u(t:x){\\displaystyle u(t:x)} by a deep neural network: f(t:x){\\displaystyle f(t:x)} results in a PINN. This network can be derived using automatic differentiation. The parameters of\nu(t:x){\\displaystyle u(t:x)} and f(t:x){\\displaystyle f(t:x)}: together with the parameter λ{\\displaystyle \\lambda } of the differential operator can be then learned by minimizing the following loss function Ltot{\\displaystyle L_{tot}}: Ltot=Lu+Lf{\\displaystyle L_{tot}=L_{u}+L_{f}}. Where Lu=‖u−z‖Γ{\\displaystyle L_{u}=\\Vert u-z\\Vert _{\\Gamma }}: with u{\\displaystyle u} and z{\\displaystyle z} state solutions and measurements at sparse location Γ{\\displaystyle \\Gamma }: respectively and Lf=‖f‖Γ{\\displaystyle L_{f}=\\Vert f\\Vert _{\\Gamma }} residual function. This second term requires the structured information represented by the partial differential equations to be satisfied in the training process. This strategy allows for discovering dynamic models described by nonlinear PDEs assembling computationally efficient and fully differentiable surrogate models that may find application in predictive forecasting: control: and data assimilation. == Physics-informed neural networks for piece-wise function approximation == PINN is unable to approximate PDEs that have strong non-linearity or sharp gradients that commonly occur in practical fluid flow problems. Piece-wise approximation has been': 'original_similarity': 0.9096441638039324: 'slide': 143},"{'chunk_text': ""nonlinear PDEs assembling computationally efficient and fully differentiable surrogate models that may find application in predictive forecasting: control: and data assimilation. == Physics-informed neural networks for piece-wise function approximation == PINN is unable to approximate PDEs that have strong non-linearity or sharp gradients that commonly occur in practical fluid flow problems. Piece-wise approximation has been an old practice in the field of numerical approximation. With the capability of approximating strong non-linearity extremely light weight PINNs are used to solve PDEs in much larger discrete subdomains that increases accuracy substantially and decreases computational load as well. DPINN (Distributed physics-informed neural networks) and DPIELM (Distributed physics-informed extreme learning machines) are generalizable space-time domain discretization for better approximation. DPIELM is an extremely fast and lightweight approximator with competitive accuracy. Domain scaling on the top has a special effect. Another school of thought is discretization for parallel computation to leverage usage of available computational resources. XPINNs is a generalized space-time domain decomposition approach for the physics-informed neural networks (PINNs) to solve nonlinear partial differential equations on arbitrary complex-geometry domains. The XPINNs further pushes the boundaries of both PINNs as well as Conservative PINNs (cPINNs): which is a spatial domain decomposition approach in the PINN framework tailored to conservation laws. Compared to PINN: the XPINN method has large representation and parallelization capacity due to the inherent property of deployment of multiple neural networks in the smaller subdomains. Unlike cPINN: XPINN can be extended to any type of PDEs. Moreover: the domain can be decomposed in any arbitrary way (in space and time): which is not possible in cPINN. Thus: XPINN offers both space and time parallelization: thereby reducing the training cost more effectively. The XPINN is particularly effective for the large-scale problems (involving large data set) as well as for the high-dimensional problems where single network based PINN is not adequate. The rigorous bounds on the errors resulting from the approximation of the nonlinear PDEs (incompressible Navier–Stokes equations) with PINNs and XPINNs are proved. == Physics-informed neural networks and functional interpolation == In the PINN framework: initial and boundary conditions are not analytically satisfied: thus they need to be included in the loss function of the network to be simultaneously learned with the differential equation (DE) unknown functions. Having competing objectives during the network's training can lead to unbalanced gradients while using gradient-based techniques: which causes PINNs to often struggle to accurately learn the underlying DE solution. This drawback is overcome by using functional interpolation techniques such as the Theory of Functional Connections (TFC)'s constrained expression: in the Deep-TFC framework: which reduces the solution search space of constrained problems to the subspace of neural network that analytically satisfies the constraints. A further improvement of PINN and functional interpolation approach is given by the Extreme Theory of Functional Connections (X-TFC) framework: where a single-layer Neural Network and the extreme learning machine training algorithm are employed. X-TFC allows to improve the accuracy and performance of regular PINNs: and its robustness and reliability are proved for stiff problems: optimal control: aerospace: and rarefied gas dynamics applications. == Physics-informed PointNet (PIPN) for multiple sets of irregular geometries == Regular PINNs are only able to obtain the solution of a forward or inverse problem on a single geometry. It means that for any new geometry (computational domain): one must retrain a PINN. This limitation of regular PINNs imposes high computational costs: specifically for a comprehensive investigation of geometric parameters in industrial designs. Physics-informed PointNet (PIPN) is fundamentally the result of a combination of PINN's loss function with PointNet. In fact: instead of using a simple fully connected neural network: PIPN uses PointNet as the core of its neural network. PointNet has been primarily designed for deep learning of 3D object classification and segmentation by the research group of Leonidas J. Guibas. PointNet extracts geometric features of input computational domains in PIPN. Thus: PIPN is able to solve governing equations on multiple computational domains (rather than only a single domain) with irregular geometries: simultaneously. The effectiveness of PIPN has been shown for incompressible flow: heat transfer and linear elasticity. == Physics-informed neural networks (PINNs) for inverse computations == Physics-informed neural networks (PINNs) have particularly proven effective in solving inverse problems within differential equations: demonstrating their applicability across science: engineering: and economics. They have shown useful for solving inverse problems in a variety of fields: including nano-optics: topology optimization/characterization: multiphase flow in porous media: and high-speed fluid flow. PINNs have demonstrated flexibility when dealing with noisy and uncertain observation datasets. They also demonstrated clear advantages in the inverse calculation of parameters for multi-fidelity datasets: meaning datasets with different quality: quantity: and types of observations. Uncertainties in calculations can be evaluated using ensemble-based or Bayesian-based calculations. == Limitations == Translation and discontinuous behavior are hard to approximate using PINNs. They fail when solving differential equations with slight advective dominance. The difficulty in training of PINNs in advection-dominated PDEs can be explained by Kolmogorov n–width of the solution. They also fail to solve a system of dynamical systems and hence have not been a success in solving chaotic equations. One of the reasons behind the failure of the regular PINNs is soft-constraining of Dirichlet and Neumann boundary conditions which pose multi-objective optimization problems. This requires the need for manually weighing the loss terms to be able to optimize. Another reason is getting optimization itself. Posing PDE solving as an optimization problem brings in all the problems that are faced in the world of optimization:\nthe major one being getting stuck at a local optimum pretty often. == References == == External links == PINN – repository to implement physics-informed neural network in Python XPINN – repository to implement extended physics-informed neural network (XPINN) in Python PIPN [2]– repository to implement physics-informed PointNet (PIPN) in Python"": 'original_similarity': 0.8680427758752978: 'slide': 89}",{'chunk_text': 'getting stuck at a local optimum pretty often. == References == == External links == PINN – repository to implement physics-informed neural network in Python XPINN – repository to implement extended physics-informed neural network (XPINN) in Python PIPN [2]– repository to implement physics-informed PointNet (PIPN) in Python\nQuantum neural networks are computational': 'original_similarity': 0.8669293694567906: 'slide': 3}
0.38714469998376444,How do PINNs work in terms of function approximation?,"PINNs work by leveraging governing physical equations in neural network training. They are designed to be trained to satisfy the given training data and the imposed governing equations. This means that a neural network can be guided with training data that do not necessarily need to be large and complete. Thus, even with sparse and incomplete data, PINN may be used for finding an optimal solution with high fidelity.",{'chunk_text': 'prior assumptions: linearization: and adequate time and space discretization. Recently: solving the governing partial differential equations of physical phenomena using deep learning has emerged as a new field of scientific machine learning (SciML): leveraging the universal approximation theorem and high expressivity of neural networks. In general: deep neural networks could approximate any high-dimensional function given that sufficient training data are supplied. However: such networks do not consider the physical characteristics underlying the problem: and the level of approximation accuracy provided by them is still heavily dependent on careful specifications of the problem geometry as well as the initial and boundary conditions. Without this preliminary information: the solution is not unique and may lose physical correctness. On the other hand: physics-informed neural networks (PINNs) leverage governing physical equations in neural network training. Namely: PINNs are designed to be trained to satisfy the given training data as well as the imposed governing equations. In this fashion: a neural network can be guided with training data that do not necessarily need to be large and complete. Potentially: an accurate solution of partial differential equations can be found without knowing the boundary conditions. Therefore: with some knowledge about the physical characteristics of the problem and some form of training data (even sparse and incomplete): PINN may be used for finding an optimal solution with high fidelity. PINNs allow for addressing a wide range of problems in computational science and represent a pioneering technology leading to the development of new classes of numerical solvers for PDEs. PINNs can be thought of as a meshfree alternative to traditional approaches (e.g.: CFD for fluid dynamics): and new data-driven approaches for model inversion and system identification. Notably: the trained PINN network can be used for predicting the values on simulation grids of different resolutions without the need to be retrained. In addition: they allow for exploiting automatic differentiation (AD) to compute the required derivatives in the partial differential equations: a new class of differentiation techniques widely used to derive neural networks assessed to be superior to numerical or symbolic differentiation. == Modeling and computation == A general nonlinear partial differential equation can be: ut+N[u;λ]=0:x∈Ω:t∈[0:T]{\\displaystyle u_{t}+N[u;\\lambda ]=0:\\quad x\\in \\Omega :\\quad t\\in [0:T]} where u(t:x){\\displaystyle u(t:x)} denotes the solution: N[⋅;λ]{\\displaystyle N[\\cdot ;\\lambda ]} is a nonlinear operator parametrized by λ{\\displaystyle \\lambda }: and Ω{\\displaystyle \\Omega } is a subset of RD{\\displaystyle \\mathbb {R} ^{D}}. This general form of governing equations summarizes a wide range of problems in mathematical physics: such as conservative laws: diffusion process: advection-diffusion systems: and kinetic equations. Given noisy measurements of a generic dynamic system described by the equation above: PINNs can be designed to solve two classes of problems: data-driven solution data-driven discoveryof partial differential equations. === Data-driven solution of partial differential equations === The data-driven solution of PDE computes the hidden state u(t:x){\\displaystyle u(t:x)} of the system given boundary data and/or measurements z{\\displaystyle z}: and fixed model parameters λ{\\displaystyle \\lambda }. We solve: ut+N[u]=0:x∈Ω:t∈[0:T]{\\displaystyle u_{t}+N[u]=0:\\quad x\\in \\Omega :\\quad t\\in [0:T]}. By defining the residual f(t:x){\\displaystyle f(t:x)} as f:=ut+N[u]=0{\\displaystyle f:=u_{t}+N[u]=0}: and approximating u(t:x){\\displaystyle u(t:x)} by a deep neural network. This network can be differentiated using automatic differentiation. The parameters of u(t:x){\\displaystyle u(t:x)} and f(t:x){\\displaystyle f(t:x)} can be then learned by minimizing the following loss function Ltot{\\displaystyle L_{tot}}: Ltot=Lu+Lf{\\displaystyle L_{tot}=L_{u}+L_{f}}. Where Lu=‖u−z‖Γ{\\displaystyle L_{u}=\\Vert u-z\\Vert _{\\Gamma }} is the error between the PINN u(t:x){\\displaystyle u(t:x)} and the set of boundary conditions and measured data on the set of points Γ{\\displaystyle \\Gamma } where the boundary conditions and data are defined: and Lf=‖f‖Γ{\\displaystyle L_{f}=\\Vert f\\Vert _{\\Gamma }} is the mean-squared error of the residual function. This second term encourages the PINN to learn the structural information expressed by the partial differential equation during the training process. This approach has been used to yield computationally efficient physics-informed surrogate models with applications in the forecasting of physical processes: model predictive control: multi-physics and multi-scale modeling: and simulation. It has been shown to converge to the solution of the PDE. === Data-driven discovery of partial differential equations === Given noisy and incomplete measurements z{\\displaystyle z} of the state of the system: the data-driven discovery of PDE results in computing the unknown state u(t:x){\\displaystyle u(t:x)} and learning model parameters λ{\\displaystyle \\lambda } that best describe the observed data and it reads as follows: ut+N[u;λ]=0:x∈Ω:t∈[0:T]{\\displaystyle u_{t}+N[u;\\lambda ]=0:\\quad x\\in \\Omega :\\quad t\\in [0:T]}. By defining f(t:x){\\displaystyle f(t:x)} as f:=ut+N[u;λ]=0{\\displaystyle f:=u_{t}+N[u;\\lambda ]=0}: and approximating u(t:x){\\displaystyle u(t:x)} by a deep neural network: f(t:x){\\displaystyle f(t:x)} results in a PINN. This network can be derived using automatic differentiation. The parameters of\nu(t:x){\\displaystyle u(t:x)} and f(t:x){\\displaystyle f(t:x)}: together with the parameter λ{\\displaystyle \\lambda } of the differential operator can be then learned by minimizing the following loss function Ltot{\\displaystyle L_{tot}}: Ltot=Lu+Lf{\\displaystyle L_{tot}=L_{u}+L_{f}}. Where Lu=‖u−z‖Γ{\\displaystyle L_{u}=\\Vert u-z\\Vert _{\\Gamma }}: with u{\\displaystyle u} and z{\\displaystyle z} state solutions and measurements at sparse location Γ{\\displaystyle \\Gamma }: respectively and Lf=‖f‖Γ{\\displaystyle L_{f}=\\Vert f\\Vert _{\\Gamma }} residual function. This second term requires the structured information represented by the partial differential equations to be satisfied in the training process. This strategy allows for discovering dynamic models described by nonlinear PDEs assembling computationally efficient and fully differentiable surrogate models that may find application in predictive forecasting: control: and data assimilation. == Physics-informed neural networks for piece-wise function approximation == PINN is unable to approximate PDEs that have strong non-linearity or sharp gradients that commonly occur in practical fluid flow problems. Piece-wise approximation has been an old practice in the field of numerical approximation. With the capability of approximating strong non-linearity extremely light weight PINNs are used to solve PDEs in much larger discrete subdomains that increases accuracy substantially and decreases computational load as well. DPINN (Distributed physics-informed neural networks) and DPIELM (Distributed physics-informed extreme learning machines) are generalizable space-time domain discretization for better approximation. DPIELM is an extremely fast and lightweight approximator with competitive accuracy. Domain scaling on the top has a special effect. Another school of thought is discretization for parallel computation to leverage usage of available computational resources. XPINNs is a generalized space-time domain decomposition approach': 'original_similarity': 0.8465222366673701: 'slide': 247},"{'chunk_text': ""gradients that commonly occur in practical fluid flow problems. Piece-wise approximation has been an old practice in the field of numerical approximation. With the capability of approximating strong non-linearity extremely light weight PINNs are used to solve PDEs in much larger discrete subdomains that increases accuracy substantially and decreases computational load as well. DPINN (Distributed physics-informed neural networks) and DPIELM (Distributed physics-informed extreme learning machines) are generalizable space-time domain discretization for better approximation. DPIELM is an extremely fast and lightweight approximator with competitive accuracy. Domain scaling on the top has a special effect. Another school of thought is discretization for parallel computation to leverage usage of available computational resources. XPINNs is a generalized space-time domain decomposition approach for the physics-informed neural networks (PINNs) to solve nonlinear partial differential equations on arbitrary complex-geometry domains. The XPINNs further pushes the boundaries of both PINNs as well as Conservative PINNs (cPINNs): which is a spatial domain decomposition approach in the PINN framework tailored to conservation laws. Compared to PINN: the XPINN method has large representation and parallelization capacity due to the inherent property of deployment of multiple neural networks in the smaller subdomains. Unlike cPINN: XPINN can be extended to any type of PDEs. Moreover: the domain can be decomposed in any arbitrary way (in space and time): which is not possible in cPINN. Thus: XPINN offers both space and time parallelization: thereby reducing the training cost more effectively. The XPINN is particularly effective for the large-scale problems (involving large data set) as well as for the high-dimensional problems where single network based PINN is not adequate. The rigorous bounds on the errors resulting from the approximation of the nonlinear PDEs (incompressible Navier–Stokes equations) with PINNs and XPINNs are proved. == Physics-informed neural networks and functional interpolation == In the PINN framework: initial and boundary conditions are not analytically satisfied: thus they need to be included in the loss function of the network to be simultaneously learned with the differential equation (DE) unknown functions. Having competing objectives during the network's training can lead to unbalanced gradients while using gradient-based techniques: which causes PINNs to often struggle to accurately learn the underlying DE solution. This drawback is overcome by using functional interpolation techniques such as the Theory of Functional Connections (TFC)'s constrained expression: in the Deep-TFC framework: which reduces the solution search space of constrained problems to the subspace of neural network that analytically satisfies the constraints. A further improvement of PINN and functional interpolation approach is given by the Extreme Theory of Functional Connections (X-TFC) framework: where a single-layer Neural Network and the extreme learning machine training algorithm are employed. X-TFC allows to improve the accuracy and performance of regular PINNs: and its robustness and reliability are proved for stiff problems: optimal control: aerospace: and rarefied gas dynamics applications. == Physics-informed PointNet (PIPN) for multiple sets of irregular geometries == Regular PINNs are only able to obtain the solution of a forward or inverse problem on a single geometry. It means that for any new geometry (computational domain): one must retrain a PINN. This limitation of regular PINNs imposes high computational costs: specifically for a comprehensive investigation of geometric parameters in industrial designs. Physics-informed PointNet (PIPN) is fundamentally the result of a combination of PINN's loss function with PointNet. In fact: instead of using a simple fully connected neural network: PIPN uses PointNet as the core of its neural network. PointNet has been primarily designed for deep learning of 3D object classification and segmentation by the research group of Leonidas J. Guibas. PointNet extracts geometric features of input computational domains in PIPN. Thus: PIPN is able to solve governing equations on multiple computational domains (rather than only a single domain) with irregular geometries: simultaneously. The effectiveness of PIPN has been shown for incompressible flow: heat transfer and linear elasticity. == Physics-informed neural networks (PINNs) for inverse computations == Physics-informed neural networks (PINNs) have particularly proven effective in solving inverse problems within differential equations: demonstrating their applicability across science: engineering: and economics. They have shown useful for solving inverse problems in a variety of fields: including nano-optics: topology optimization/characterization: multiphase flow in porous media: and high-speed fluid flow. PINNs have demonstrated flexibility when dealing with noisy and uncertain observation datasets. They also demonstrated clear advantages in the inverse calculation of parameters for multi-fidelity datasets: meaning datasets with different quality: quantity: and types of observations. Uncertainties in calculations can be evaluated using ensemble-based or Bayesian-based calculations. == Limitations == Translation and discontinuous behavior are hard to approximate using PINNs. They fail when solving differential equations with slight advective dominance. The difficulty in training of PINNs in advection-dominated PDEs can be explained by Kolmogorov n–width of the solution. They also fail to solve a system of dynamical systems and hence have not been a success in solving chaotic equations. One of the reasons behind the failure of the regular PINNs is soft-constraining of Dirichlet and Neumann boundary conditions which pose multi-objective optimization problems. This requires the need for manually weighing the loss terms to be able to optimize. Another reason is getting optimization itself. Posing PDE solving as an optimization problem brings in all the problems that are faced in the world of optimization:\nthe major one being getting stuck at a local optimum pretty often. == References == == External links == PINN – repository to implement physics-informed neural network in Python XPINN – repository to implement extended physics-informed neural network (XPINN) in Python PIPN [2]– repository to implement physics-informed PointNet (PIPN) in Python"": 'original_similarity': 0.8337571705958524: 'slide': 131}",{'chunk_text': 'getting stuck at a local optimum pretty often. == References == == External links == PINN – repository to implement physics-informed neural network in Python XPINN – repository to implement extended physics-informed neural network (XPINN) in Python PIPN [2]– repository to implement physics-informed PointNet (PIPN) in Python\nQuantum neural networks are computational': 'original_similarity': 0.8157808376177267: 'slide': 3}
0.39303549999021925,What is the significance of automatic differentiation (AD) in PINNs?,Automatic differentiation (AD) is exploited in PINNs to compute the required derivatives in the partial differential equations. It's a new class of differentiation techniques widely used to derive neural networks and is considered superior to numerical or symbolic differentiation.,{'chunk_text': 'and high expressivity of neural networks. In general: deep neural networks could approximate any high-dimensional function given that sufficient training data are supplied. However: such networks do not consider the physical characteristics underlying the problem: and the level of approximation accuracy provided by them is still heavily dependent on careful specifications of the problem geometry as well as the initial and boundary conditions. Without this preliminary information: the solution is not unique and may lose physical correctness. On the other hand: physics-informed neural networks (PINNs) leverage governing physical equations in neural network training. Namely: PINNs are designed to be trained to satisfy the given training data as well as the imposed governing equations. In this fashion: a neural network can be guided with training data that do not necessarily need to be large and complete. Potentially: an accurate solution of partial differential equations can be found without knowing the boundary conditions. Therefore: with some knowledge about the physical characteristics of the problem and some form of training data (even sparse and incomplete): PINN may be used for finding an optimal solution with high fidelity. PINNs allow for addressing a wide range of problems in computational science and represent a pioneering technology leading to the development of new classes of numerical solvers for PDEs. PINNs can be thought of as a meshfree alternative to traditional approaches (e.g.: CFD for fluid dynamics): and new data-driven approaches for model inversion and system identification. Notably: the trained PINN network can be used for predicting the values on simulation grids of different resolutions without the need to be retrained. In addition: they allow for exploiting automatic differentiation (AD) to compute the required derivatives in the partial differential equations: a new class of differentiation techniques widely used to derive neural networks assessed to be superior to numerical or symbolic differentiation. == Modeling and computation == A general nonlinear partial differential equation can be: ut+N[u;λ]=0:x∈Ω:t∈[0:T]{\\displaystyle u_{t}+N[u;\\lambda ]=0:\\quad x\\in \\Omega :\\quad t\\in [0:T]} where u(t:x){\\displaystyle u(t:x)} denotes the solution: N[⋅;λ]{\\displaystyle N[\\cdot ;\\lambda ]} is a nonlinear operator parametrized by λ{\\displaystyle \\lambda }: and Ω{\\displaystyle \\Omega } is a subset of RD{\\displaystyle \\mathbb {R} ^{D}}. This general form of governing equations summarizes a wide range of problems in mathematical physics: such as conservative laws: diffusion process: advection-diffusion systems: and kinetic equations. Given noisy measurements of a generic dynamic system described by the equation above: PINNs can be designed to solve two classes of problems: data-driven solution data-driven discoveryof partial differential equations. === Data-driven solution of partial differential equations === The data-driven solution of PDE computes the hidden state u(t:x){\\displaystyle u(t:x)} of the system given boundary data and/or measurements z{\\displaystyle z}: and fixed model parameters λ{\\displaystyle \\lambda }. We solve: ut+N[u]=0:x∈Ω:t∈[0:T]{\\displaystyle u_{t}+N[u]=0:\\quad x\\in \\Omega :\\quad t\\in [0:T]}. By defining the residual f(t:x){\\displaystyle f(t:x)} as f:=ut+N[u]=0{\\displaystyle f:=u_{t}+N[u]=0}: and approximating u(t:x){\\displaystyle u(t:x)} by a deep neural network. This network can be differentiated using automatic differentiation. The parameters of u(t:x){\\displaystyle u(t:x)} and f(t:x){\\displaystyle f(t:x)} can be then learned by minimizing the following loss function Ltot{\\displaystyle L_{tot}}: Ltot=Lu+Lf{\\displaystyle L_{tot}=L_{u}+L_{f}}. Where Lu=‖u−z‖Γ{\\displaystyle L_{u}=\\Vert u-z\\Vert _{\\Gamma }} is the error between the PINN u(t:x){\\displaystyle u(t:x)} and the set of boundary conditions and measured data on the set of points Γ{\\displaystyle \\Gamma } where the boundary conditions and data are defined: and Lf=‖f‖Γ{\\displaystyle L_{f}=\\Vert f\\Vert _{\\Gamma }} is the mean-squared error of the residual function. This second term encourages the PINN to learn the structural information expressed by the partial differential equation during the training process. This approach has been used to yield computationally efficient physics-informed surrogate models with applications in the forecasting of physical processes: model predictive control: multi-physics and multi-scale modeling: and simulation. It has been shown to converge to the solution of the PDE. === Data-driven discovery of partial differential equations === Given noisy and incomplete measurements z{\\displaystyle z} of the state of the system: the data-driven discovery of PDE results in computing the unknown state u(t:x){\\displaystyle u(t:x)} and learning model parameters λ{\\displaystyle \\lambda } that best describe the observed data and it reads as follows: ut+N[u;λ]=0:x∈Ω:t∈[0:T]{\\displaystyle u_{t}+N[u;\\lambda ]=0:\\quad x\\in \\Omega :\\quad t\\in [0:T]}. By defining f(t:x){\\displaystyle f(t:x)} as f:=ut+N[u;λ]=0{\\displaystyle f:=u_{t}+N[u;\\lambda ]=0}: and approximating u(t:x){\\displaystyle u(t:x)} by a deep neural network: f(t:x){\\displaystyle f(t:x)} results in a PINN. This network can be derived using automatic differentiation. The parameters of\nu(t:x){\\displaystyle u(t:x)} and f(t:x){\\displaystyle f(t:x)}: together with the parameter λ{\\displaystyle \\lambda } of the differential operator can be then learned by minimizing the following loss function Ltot{\\displaystyle L_{tot}}: Ltot=Lu+Lf{\\displaystyle L_{tot}=L_{u}+L_{f}}. Where Lu=‖u−z‖Γ{\\displaystyle L_{u}=\\Vert u-z\\Vert _{\\Gamma }}: with u{\\displaystyle u} and z{\\displaystyle z} state solutions and measurements at sparse location Γ{\\displaystyle \\Gamma }: respectively and Lf=‖f‖Γ{\\displaystyle L_{f}=\\Vert f\\Vert _{\\Gamma }} residual function. This second term requires the structured information represented by the partial differential equations to be satisfied in the training process. This strategy allows for discovering dynamic models described by nonlinear PDEs assembling computationally efficient and fully differentiable surrogate models that may find application in predictive forecasting: control: and data assimilation. == Physics-informed neural networks for piece-wise function approximation == PINN is unable to approximate PDEs that have strong non-linearity or sharp gradients that commonly occur in practical fluid flow problems. Piece-wise approximation has been an old practice in the field of numerical approximation. With the capability of approximating strong non-linearity extremely light weight PINNs are used to solve PDEs in much larger discrete subdomains that increases accuracy substantially and decreases computational load as well. DPINN (Distributed physics-informed neural networks) and DPIELM (Distributed physics-informed extreme learning machines) are generalizable space-time domain discretization for better approximation. DPIELM is an extremely fast and lightweight approximator with competitive accuracy. Domain scaling on the top has a special effect. Another school of thought is discretization for parallel computation to leverage usage of available computational resources. XPINNs is a generalized space-time domain decomposition approach for the physics-informed neural networks (PINNs) to solve nonlinear partial differential equations on arbitrary complex-geometry domains. The XPINNs further pushes the boundaries of both PINNs as well as Conservative PINNs (cPINNs): which is a spatial domain decomposition approach': 'original_similarity': 0.8332639315502834: 'slide': 285},"{'chunk_text': ""approximate PDEs that have strong non-linearity or sharp gradients that commonly occur in practical fluid flow problems. Piece-wise approximation has been an old practice in the field of numerical approximation. With the capability of approximating strong non-linearity extremely light weight PINNs are used to solve PDEs in much larger discrete subdomains that increases accuracy substantially and decreases computational load as well. DPINN (Distributed physics-informed neural networks) and DPIELM (Distributed physics-informed extreme learning machines) are generalizable space-time domain discretization for better approximation. DPIELM is an extremely fast and lightweight approximator with competitive accuracy. Domain scaling on the top has a special effect. Another school of thought is discretization for parallel computation to leverage usage of available computational resources. XPINNs is a generalized space-time domain decomposition approach for the physics-informed neural networks (PINNs) to solve nonlinear partial differential equations on arbitrary complex-geometry domains. The XPINNs further pushes the boundaries of both PINNs as well as Conservative PINNs (cPINNs): which is a spatial domain decomposition approach in the PINN framework tailored to conservation laws. Compared to PINN: the XPINN method has large representation and parallelization capacity due to the inherent property of deployment of multiple neural networks in the smaller subdomains. Unlike cPINN: XPINN can be extended to any type of PDEs. Moreover: the domain can be decomposed in any arbitrary way (in space and time): which is not possible in cPINN. Thus: XPINN offers both space and time parallelization: thereby reducing the training cost more effectively. The XPINN is particularly effective for the large-scale problems (involving large data set) as well as for the high-dimensional problems where single network based PINN is not adequate. The rigorous bounds on the errors resulting from the approximation of the nonlinear PDEs (incompressible Navier–Stokes equations) with PINNs and XPINNs are proved. == Physics-informed neural networks and functional interpolation == In the PINN framework: initial and boundary conditions are not analytically satisfied: thus they need to be included in the loss function of the network to be simultaneously learned with the differential equation (DE) unknown functions. Having competing objectives during the network's training can lead to unbalanced gradients while using gradient-based techniques: which causes PINNs to often struggle to accurately learn the underlying DE solution. This drawback is overcome by using functional interpolation techniques such as the Theory of Functional Connections (TFC)'s constrained expression: in the Deep-TFC framework: which reduces the solution search space of constrained problems to the subspace of neural network that analytically satisfies the constraints. A further improvement of PINN and functional interpolation approach is given by the Extreme Theory of Functional Connections (X-TFC) framework: where a single-layer Neural Network and the extreme learning machine training algorithm are employed. X-TFC allows to improve the accuracy and performance of regular PINNs: and its robustness and reliability are proved for stiff problems: optimal control: aerospace: and rarefied gas dynamics applications. == Physics-informed PointNet (PIPN) for multiple sets of irregular geometries == Regular PINNs are only able to obtain the solution of a forward or inverse problem on a single geometry. It means that for any new geometry (computational domain): one must retrain a PINN. This limitation of regular PINNs imposes high computational costs: specifically for a comprehensive investigation of geometric parameters in industrial designs. Physics-informed PointNet (PIPN) is fundamentally the result of a combination of PINN's loss function with PointNet. In fact: instead of using a simple fully connected neural network: PIPN uses PointNet as the core of its neural network. PointNet has been primarily designed for deep learning of 3D object classification and segmentation by the research group of Leonidas J. Guibas. PointNet extracts geometric features of input computational domains in PIPN. Thus: PIPN is able to solve governing equations on multiple computational domains (rather than only a single domain) with irregular geometries: simultaneously. The effectiveness of PIPN has been shown for incompressible flow: heat transfer and linear elasticity. == Physics-informed neural networks (PINNs) for inverse computations == Physics-informed neural networks (PINNs) have particularly proven effective in solving inverse problems within differential equations: demonstrating their applicability across science: engineering: and economics. They have shown useful for solving inverse problems in a variety of fields: including nano-optics: topology optimization/characterization: multiphase flow in porous media: and high-speed fluid flow. PINNs have demonstrated flexibility when dealing with noisy and uncertain observation datasets. They also demonstrated clear advantages in the inverse calculation of parameters for multi-fidelity datasets: meaning datasets with different quality: quantity: and types of observations. Uncertainties in calculations can be evaluated using ensemble-based or Bayesian-based calculations. == Limitations == Translation and discontinuous behavior are hard to approximate using PINNs. They fail when solving differential equations with slight advective dominance. The difficulty in training of PINNs in advection-dominated PDEs can be explained by Kolmogorov n–width of the solution. They also fail to solve a system of dynamical systems and hence have not been a success in solving chaotic equations. One of the reasons behind the failure of the regular PINNs is soft-constraining of Dirichlet and Neumann boundary conditions which pose multi-objective optimization problems. This requires the need for manually weighing the loss terms to be able to optimize. Another reason is getting optimization itself. Posing PDE solving as an optimization problem brings in all the problems that are faced in the world of optimization:\nthe major one being getting stuck at a local optimum pretty often. == References == == External links == PINN – repository to implement physics-informed neural network in Python XPINN – repository to implement extended physics-informed neural network (XPINN) in Python PIPN [2]– repository to implement physics-informed PointNet (PIPN) in Python"": 'original_similarity': 0.8314119344460769: 'slide': 123}",{'chunk_text': 'being getting stuck at a local optimum pretty often. == References == == External links == PINN – repository to implement physics-informed neural network in Python XPINN – repository to implement extended physics-informed neural network (XPINN) in Python PIPN [2]– repository to implement physics-informed PointNet (PIPN) in Python\nQuantum neural networks are': 'original_similarity': 0.7955900942555223: 'slide': 2}
0.4130412999948021,How can PINNs be applied for piece-wise function approximation?,"For problems with strong non-linearity or sharp gradients, lightweight PINNs are used for piece-wise approximation, which increases accuracy substantially and decreases computational load. Distributed physics-informed neural networks (DPINNs) and Distributed physics-informed extreme learning machines (DPIELMs) are used for better approximation, solving PDEs in much larger discrete subdomains.","{'chunk_text': ""piece-wise function approximation == PINN is unable to approximate PDEs that have strong non-linearity or sharp gradients that commonly occur in practical fluid flow problems. Piece-wise approximation has been an old practice in the field of numerical approximation. With the capability of approximating strong non-linearity extremely light weight PINNs are used to solve PDEs in much larger discrete subdomains that increases accuracy substantially and decreases computational load as well. DPINN (Distributed physics-informed neural networks) and DPIELM (Distributed physics-informed extreme learning machines) are generalizable space-time domain discretization for better approximation. DPIELM is an extremely fast and lightweight approximator with competitive accuracy. Domain scaling on the top has a special effect. Another school of thought is discretization for parallel computation to leverage usage of available computational resources. XPINNs is a generalized space-time domain decomposition approach for the physics-informed neural networks (PINNs) to solve nonlinear partial differential equations on arbitrary complex-geometry domains. The XPINNs further pushes the boundaries of both PINNs as well as Conservative PINNs (cPINNs): which is a spatial domain decomposition approach in the PINN framework tailored to conservation laws. Compared to PINN: the XPINN method has large representation and parallelization capacity due to the inherent property of deployment of multiple neural networks in the smaller subdomains. Unlike cPINN: XPINN can be extended to any type of PDEs. Moreover: the domain can be decomposed in any arbitrary way (in space and time): which is not possible in cPINN. Thus: XPINN offers both space and time parallelization: thereby reducing the training cost more effectively. The XPINN is particularly effective for the large-scale problems (involving large data set) as well as for the high-dimensional problems where single network based PINN is not adequate. The rigorous bounds on the errors resulting from the approximation of the nonlinear PDEs (incompressible Navier–Stokes equations) with PINNs and XPINNs are proved. == Physics-informed neural networks and functional interpolation == In the PINN framework: initial and boundary conditions are not analytically satisfied: thus they need to be included in the loss function of the network to be simultaneously learned with the differential equation (DE) unknown functions. Having competing objectives during the network's training can lead to unbalanced gradients while using gradient-based techniques: which causes PINNs to often struggle to accurately learn the underlying DE solution. This drawback is overcome by using functional interpolation techniques such as the Theory of Functional Connections (TFC)'s constrained expression: in the Deep-TFC framework: which reduces the solution search space of constrained problems to the subspace of neural network that analytically satisfies the constraints. A further improvement of PINN and functional interpolation approach is given by the Extreme Theory of Functional Connections (X-TFC) framework: where a single-layer Neural Network and the extreme learning machine training algorithm are employed. X-TFC allows to improve the accuracy and performance of regular PINNs: and its robustness and reliability are proved for stiff problems: optimal control: aerospace: and rarefied gas dynamics applications. == Physics-informed PointNet (PIPN) for multiple sets of irregular geometries == Regular PINNs are only able to obtain the solution of a forward or inverse problem on a single geometry. It means that for any new geometry (computational domain): one must retrain a PINN. This limitation of regular PINNs imposes high computational costs: specifically for a comprehensive investigation of geometric parameters in industrial designs. Physics-informed PointNet (PIPN) is fundamentally the result of a combination of PINN's loss function with PointNet. In fact: instead of using a simple fully connected neural network: PIPN uses PointNet as the core of its neural network. PointNet has been primarily designed for deep learning of 3D object classification and segmentation by the research group of Leonidas J. Guibas. PointNet extracts geometric features of input computational domains in PIPN. Thus: PIPN is able to solve governing equations on multiple computational domains (rather than only a single domain) with irregular geometries: simultaneously. The effectiveness of PIPN has been shown for incompressible flow: heat transfer and linear elasticity. == Physics-informed neural networks (PINNs) for inverse computations == Physics-informed neural networks (PINNs) have particularly proven effective in solving inverse problems within differential equations: demonstrating their applicability across science: engineering: and economics. They have shown useful for solving inverse problems in a variety of fields: including nano-optics: topology optimization/characterization: multiphase flow in porous media: and high-speed fluid flow. PINNs have demonstrated flexibility when dealing with noisy and uncertain observation datasets. They also demonstrated clear advantages in the inverse calculation of parameters for multi-fidelity datasets: meaning datasets with different quality: quantity: and types of observations. Uncertainties in calculations can be evaluated using ensemble-based or Bayesian-based calculations. == Limitations == Translation and discontinuous behavior are hard to approximate using PINNs. They fail when solving differential equations with slight advective dominance. The difficulty in training of PINNs in advection-dominated PDEs can be explained by Kolmogorov n–width of the solution. They also fail to solve a system of dynamical systems and hence have not been a success in solving chaotic equations. One of the reasons behind the failure of the regular PINNs is soft-constraining of Dirichlet and Neumann boundary conditions which pose multi-objective optimization problems. This requires the need for manually weighing the loss terms to be able to optimize. Another reason is getting optimization itself. Posing PDE solving as an optimization problem brings in all the problems that are faced in the world of optimization:\nthe major one being getting stuck at a local optimum pretty often. == References == == External links == PINN – repository to implement physics-informed neural network in Python XPINN – repository to implement extended physics-informed neural network (XPINN) in Python PIPN [2]– repository to implement physics-informed PointNet (PIPN) in Python"": 'original_similarity': 0.8464247699035212: 'slide': 115}",{'chunk_text': 'networks do not consider the physical characteristics underlying the problem: and the level of approximation accuracy provided by them is still heavily dependent on careful specifications of the problem geometry as well as the initial and boundary conditions. Without this preliminary information: the solution is not unique and may lose physical correctness. On the other hand: physics-informed neural networks (PINNs) leverage governing physical equations in neural network training. Namely: PINNs are designed to be trained to satisfy the given training data as well as the imposed governing equations. In this fashion: a neural network can be guided with training data that do not necessarily need to be large and complete. Potentially: an accurate solution of partial differential equations can be found without knowing the boundary conditions. Therefore: with some knowledge about the physical characteristics of the problem and some form of training data (even sparse and incomplete): PINN may be used for finding an optimal solution with high fidelity. PINNs allow for addressing a wide range of problems in computational science and represent a pioneering technology leading to the development of new classes of numerical solvers for PDEs. PINNs can be thought of as a meshfree alternative to traditional approaches (e.g.: CFD for fluid dynamics): and new data-driven approaches for model inversion and system identification. Notably: the trained PINN network can be used for predicting the values on simulation grids of different resolutions without the need to be retrained. In addition: they allow for exploiting automatic differentiation (AD) to compute the required derivatives in the partial differential equations: a new class of differentiation techniques widely used to derive neural networks assessed to be superior to numerical or symbolic differentiation. == Modeling and computation == A general nonlinear partial differential equation can be: ut+N[u;λ]=0:x∈Ω:t∈[0:T]{\\displaystyle u_{t}+N[u;\\lambda ]=0:\\quad x\\in \\Omega :\\quad t\\in [0:T]} where u(t:x){\\displaystyle u(t:x)} denotes the solution: N[⋅;λ]{\\displaystyle N[\\cdot ;\\lambda ]} is a nonlinear operator parametrized by λ{\\displaystyle \\lambda }: and Ω{\\displaystyle \\Omega } is a subset of RD{\\displaystyle \\mathbb {R} ^{D}}. This general form of governing equations summarizes a wide range of problems in mathematical physics: such as conservative laws: diffusion process: advection-diffusion systems: and kinetic equations. Given noisy measurements of a generic dynamic system described by the equation above: PINNs can be designed to solve two classes of problems: data-driven solution data-driven discoveryof partial differential equations. === Data-driven solution of partial differential equations === The data-driven solution of PDE computes the hidden state u(t:x){\\displaystyle u(t:x)} of the system given boundary data and/or measurements z{\\displaystyle z}: and fixed model parameters λ{\\displaystyle \\lambda }. We solve: ut+N[u]=0:x∈Ω:t∈[0:T]{\\displaystyle u_{t}+N[u]=0:\\quad x\\in \\Omega :\\quad t\\in [0:T]}. By defining the residual f(t:x){\\displaystyle f(t:x)} as f:=ut+N[u]=0{\\displaystyle f:=u_{t}+N[u]=0}: and approximating u(t:x){\\displaystyle u(t:x)} by a deep neural network. This network can be differentiated using automatic differentiation. The parameters of u(t:x){\\displaystyle u(t:x)} and f(t:x){\\displaystyle f(t:x)} can be then learned by minimizing the following loss function Ltot{\\displaystyle L_{tot}}: Ltot=Lu+Lf{\\displaystyle L_{tot}=L_{u}+L_{f}}. Where Lu=‖u−z‖Γ{\\displaystyle L_{u}=\\Vert u-z\\Vert _{\\Gamma }} is the error between the PINN u(t:x){\\displaystyle u(t:x)} and the set of boundary conditions and measured data on the set of points Γ{\\displaystyle \\Gamma } where the boundary conditions and data are defined: and Lf=‖f‖Γ{\\displaystyle L_{f}=\\Vert f\\Vert _{\\Gamma }} is the mean-squared error of the residual function. This second term encourages the PINN to learn the structural information expressed by the partial differential equation during the training process. This approach has been used to yield computationally efficient physics-informed surrogate models with applications in the forecasting of physical processes: model predictive control: multi-physics and multi-scale modeling: and simulation. It has been shown to converge to the solution of the PDE. === Data-driven discovery of partial differential equations === Given noisy and incomplete measurements z{\\displaystyle z} of the state of the system: the data-driven discovery of PDE results in computing the unknown state u(t:x){\\displaystyle u(t:x)} and learning model parameters λ{\\displaystyle \\lambda } that best describe the observed data and it reads as follows: ut+N[u;λ]=0:x∈Ω:t∈[0:T]{\\displaystyle u_{t}+N[u;\\lambda ]=0:\\quad x\\in \\Omega :\\quad t\\in [0:T]}. By defining f(t:x){\\displaystyle f(t:x)} as f:=ut+N[u;λ]=0{\\displaystyle f:=u_{t}+N[u;\\lambda ]=0}: and approximating u(t:x){\\displaystyle u(t:x)} by a deep neural network: f(t:x){\\displaystyle f(t:x)} results in a PINN. This network can be derived using automatic differentiation. The parameters of\nu(t:x){\\displaystyle u(t:x)} and f(t:x){\\displaystyle f(t:x)}: together with the parameter λ{\\displaystyle \\lambda } of the differential operator can be then learned by minimizing the following loss function Ltot{\\displaystyle L_{tot}}: Ltot=Lu+Lf{\\displaystyle L_{tot}=L_{u}+L_{f}}. Where Lu=‖u−z‖Γ{\\displaystyle L_{u}=\\Vert u-z\\Vert _{\\Gamma }}: with u{\\displaystyle u} and z{\\displaystyle z} state solutions and measurements at sparse location Γ{\\displaystyle \\Gamma }: respectively and Lf=‖f‖Γ{\\displaystyle L_{f}=\\Vert f\\Vert _{\\Gamma }} residual function. This second term requires the structured information represented by the partial differential equations to be satisfied in the training process. This strategy allows for discovering dynamic models described by nonlinear PDEs assembling computationally efficient and fully differentiable surrogate models that may find application in predictive forecasting: control: and data assimilation. == Physics-informed neural networks for piece-wise function approximation == PINN is unable to approximate PDEs that have strong non-linearity or sharp gradients that commonly occur in practical fluid flow problems. Piece-wise approximation has been an old practice in the field of numerical approximation. With the capability of approximating strong non-linearity extremely light weight PINNs are used to solve PDEs in much larger discrete subdomains that increases accuracy substantially and decreases computational load as well. DPINN (Distributed physics-informed neural networks) and DPIELM (Distributed physics-informed extreme learning machines) are generalizable space-time domain discretization for better approximation. DPIELM is an extremely fast and lightweight approximator with competitive accuracy. Domain scaling on the top has a special effect. Another school of thought is discretization for parallel computation to leverage usage of available computational resources. XPINNs is a generalized space-time domain decomposition approach for the physics-informed neural networks (PINNs) to solve nonlinear partial differential equations on arbitrary complex-geometry domains. The XPINNs further pushes the boundaries of both PINNs as well as Conservative PINNs (cPINNs): which is a spatial domain decomposition approach in the PINN framework tailored to conservation laws. Compared to PINN: the XPINN method has large representation and parallelization capacity due to the inherent property': 'original_similarity': 0.8344629403927505: 'slide': 310},{'chunk_text': 'one being getting stuck at a local optimum pretty often. == References == == External links == PINN – repository to implement physics-informed neural network in Python XPINN – repository to implement extended physics-informed neural network (XPINN) in Python PIPN [2]– repository to implement physics-informed PointNet (PIPN) in Python\nQuantum neural networks': 'original_similarity': 0.8038392461133683: 'slide': 1}
0.4267792000027839,What limitations do PINNs have?,"PINNs struggle to approximate translation and discontinuous behavior. They fail when solving differential equations with slight advective dominance and are not successful in solving chaotic equations. One of the reasons for this is the soft-constraining of Dirichlet and Neumann boundary conditions which pose multi-objective optimization problems. This necessitates the need for manually weighing the loss terms for optimization. Also, there is the risk of getting stuck at a local optimum often.",{'chunk_text': 'getting stuck at a local optimum pretty often. == References == == External links == PINN – repository to implement physics-informed neural network in Python XPINN – repository to implement extended physics-informed neural network (XPINN) in Python PIPN [2]– repository to implement physics-informed PointNet (PIPN) in Python\nQuantum neural networks are computational': 'original_similarity': 0.8066415017644506: 'slide': 3},{'chunk_text': 'the universal approximation theorem and high expressivity of neural networks. In general: deep neural networks could approximate any high-dimensional function given that sufficient training data are supplied. However: such networks do not consider the physical characteristics underlying the problem: and the level of approximation accuracy provided by them is still heavily dependent on careful specifications of the problem geometry as well as the initial and boundary conditions. Without this preliminary information: the solution is not unique and may lose physical correctness. On the other hand: physics-informed neural networks (PINNs) leverage governing physical equations in neural network training. Namely: PINNs are designed to be trained to satisfy the given training data as well as the imposed governing equations. In this fashion: a neural network can be guided with training data that do not necessarily need to be large and complete. Potentially: an accurate solution of partial differential equations can be found without knowing the boundary conditions. Therefore: with some knowledge about the physical characteristics of the problem and some form of training data (even sparse and incomplete): PINN may be used for finding an optimal solution with high fidelity. PINNs allow for addressing a wide range of problems in computational science and represent a pioneering technology leading to the development of new classes of numerical solvers for PDEs. PINNs can be thought of as a meshfree alternative to traditional approaches (e.g.: CFD for fluid dynamics): and new data-driven approaches for model inversion and system identification. Notably: the trained PINN network can be used for predicting the values on simulation grids of different resolutions without the need to be retrained. In addition: they allow for exploiting automatic differentiation (AD) to compute the required derivatives in the partial differential equations: a new class of differentiation techniques widely used to derive neural networks assessed to be superior to numerical or symbolic differentiation. == Modeling and computation == A general nonlinear partial differential equation can be: ut+N[u;λ]=0:x∈Ω:t∈[0:T]{\\displaystyle u_{t}+N[u;\\lambda ]=0:\\quad x\\in \\Omega :\\quad t\\in [0:T]} where u(t:x){\\displaystyle u(t:x)} denotes the solution: N[⋅;λ]{\\displaystyle N[\\cdot ;\\lambda ]} is a nonlinear operator parametrized by λ{\\displaystyle \\lambda }: and Ω{\\displaystyle \\Omega } is a subset of RD{\\displaystyle \\mathbb {R} ^{D}}. This general form of governing equations summarizes a wide range of problems in mathematical physics: such as conservative laws: diffusion process: advection-diffusion systems: and kinetic equations. Given noisy measurements of a generic dynamic system described by the equation above: PINNs can be designed to solve two classes of problems: data-driven solution data-driven discoveryof partial differential equations. === Data-driven solution of partial differential equations === The data-driven solution of PDE computes the hidden state u(t:x){\\displaystyle u(t:x)} of the system given boundary data and/or measurements z{\\displaystyle z}: and fixed model parameters λ{\\displaystyle \\lambda }. We solve: ut+N[u]=0:x∈Ω:t∈[0:T]{\\displaystyle u_{t}+N[u]=0:\\quad x\\in \\Omega :\\quad t\\in [0:T]}. By defining the residual f(t:x){\\displaystyle f(t:x)} as f:=ut+N[u]=0{\\displaystyle f:=u_{t}+N[u]=0}: and approximating u(t:x){\\displaystyle u(t:x)} by a deep neural network. This network can be differentiated using automatic differentiation. The parameters of u(t:x){\\displaystyle u(t:x)} and f(t:x){\\displaystyle f(t:x)} can be then learned by minimizing the following loss function Ltot{\\displaystyle L_{tot}}: Ltot=Lu+Lf{\\displaystyle L_{tot}=L_{u}+L_{f}}. Where Lu=‖u−z‖Γ{\\displaystyle L_{u}=\\Vert u-z\\Vert _{\\Gamma }} is the error between the PINN u(t:x){\\displaystyle u(t:x)} and the set of boundary conditions and measured data on the set of points Γ{\\displaystyle \\Gamma } where the boundary conditions and data are defined: and Lf=‖f‖Γ{\\displaystyle L_{f}=\\Vert f\\Vert _{\\Gamma }} is the mean-squared error of the residual function. This second term encourages the PINN to learn the structural information expressed by the partial differential equation during the training process. This approach has been used to yield computationally efficient physics-informed surrogate models with applications in the forecasting of physical processes: model predictive control: multi-physics and multi-scale modeling: and simulation. It has been shown to converge to the solution of the PDE. === Data-driven discovery of partial differential equations === Given noisy and incomplete measurements z{\\displaystyle z} of the state of the system: the data-driven discovery of PDE results in computing the unknown state u(t:x){\\displaystyle u(t:x)} and learning model parameters λ{\\displaystyle \\lambda } that best describe the observed data and it reads as follows: ut+N[u;λ]=0:x∈Ω:t∈[0:T]{\\displaystyle u_{t}+N[u;\\lambda ]=0:\\quad x\\in \\Omega :\\quad t\\in [0:T]}. By defining f(t:x){\\displaystyle f(t:x)} as f:=ut+N[u;λ]=0{\\displaystyle f:=u_{t}+N[u;\\lambda ]=0}: and approximating u(t:x){\\displaystyle u(t:x)} by a deep neural network: f(t:x){\\displaystyle f(t:x)} results in a PINN. This network can be derived using automatic differentiation. The parameters of\nu(t:x){\\displaystyle u(t:x)} and f(t:x){\\displaystyle f(t:x)}: together with the parameter λ{\\displaystyle \\lambda } of the differential operator can be then learned by minimizing the following loss function Ltot{\\displaystyle L_{tot}}: Ltot=Lu+Lf{\\displaystyle L_{tot}=L_{u}+L_{f}}. Where Lu=‖u−z‖Γ{\\displaystyle L_{u}=\\Vert u-z\\Vert _{\\Gamma }}: with u{\\displaystyle u} and z{\\displaystyle z} state solutions and measurements at sparse location Γ{\\displaystyle \\Gamma }: respectively and Lf=‖f‖Γ{\\displaystyle L_{f}=\\Vert f\\Vert _{\\Gamma }} residual function. This second term requires the structured information represented by the partial differential equations to be satisfied in the training process. This strategy allows for discovering dynamic models described by nonlinear PDEs assembling computationally efficient and fully differentiable surrogate models that may find application in predictive forecasting: control: and data assimilation. == Physics-informed neural networks for piece-wise function approximation == PINN is unable to approximate PDEs that have strong non-linearity or sharp gradients that commonly occur in practical fluid flow problems. Piece-wise approximation has been an old practice in the field of numerical approximation. With the capability of approximating strong non-linearity extremely light weight PINNs are used to solve PDEs in much larger discrete subdomains that increases accuracy substantially and decreases computational load as well. DPINN (Distributed physics-informed neural networks) and DPIELM (Distributed physics-informed extreme learning machines) are generalizable space-time domain discretization for better approximation. DPIELM is an extremely fast and lightweight approximator with competitive accuracy. Domain scaling on the top has a special effect. Another school of thought is discretization for parallel computation to leverage usage of available computational resources. XPINNs is a generalized space-time domain decomposition approach for the physics-informed neural networks (PINNs) to solve nonlinear partial differential equations on arbitrary complex-geometry domains. The XPINNs further pushes the boundaries of both PINNs as well as Conservative PINNs (cPINNs): which is a': 'original_similarity': 0.7991249461813221: 'slide': 281},"{'chunk_text': ""the top has a special effect. Another school of thought is discretization for parallel computation to leverage usage of available computational resources. XPINNs is a generalized space-time domain decomposition approach for the physics-informed neural networks (PINNs) to solve nonlinear partial differential equations on arbitrary complex-geometry domains. The XPINNs further pushes the boundaries of both PINNs as well as Conservative PINNs (cPINNs): which is a spatial domain decomposition approach in the PINN framework tailored to conservation laws. Compared to PINN: the XPINN method has large representation and parallelization capacity due to the inherent property of deployment of multiple neural networks in the smaller subdomains. Unlike cPINN: XPINN can be extended to any type of PDEs. Moreover: the domain can be decomposed in any arbitrary way (in space and time): which is not possible in cPINN. Thus: XPINN offers both space and time parallelization: thereby reducing the training cost more effectively. The XPINN is particularly effective for the large-scale problems (involving large data set) as well as for the high-dimensional problems where single network based PINN is not adequate. The rigorous bounds on the errors resulting from the approximation of the nonlinear PDEs (incompressible Navier–Stokes equations) with PINNs and XPINNs are proved. == Physics-informed neural networks and functional interpolation == In the PINN framework: initial and boundary conditions are not analytically satisfied: thus they need to be included in the loss function of the network to be simultaneously learned with the differential equation (DE) unknown functions. Having competing objectives during the network's training can lead to unbalanced gradients while using gradient-based techniques: which causes PINNs to often struggle to accurately learn the underlying DE solution. This drawback is overcome by using functional interpolation techniques such as the Theory of Functional Connections (TFC)'s constrained expression: in the Deep-TFC framework: which reduces the solution search space of constrained problems to the subspace of neural network that analytically satisfies the constraints. A further improvement of PINN and functional interpolation approach is given by the Extreme Theory of Functional Connections (X-TFC) framework: where a single-layer Neural Network and the extreme learning machine training algorithm are employed. X-TFC allows to improve the accuracy and performance of regular PINNs: and its robustness and reliability are proved for stiff problems: optimal control: aerospace: and rarefied gas dynamics applications. == Physics-informed PointNet (PIPN) for multiple sets of irregular geometries == Regular PINNs are only able to obtain the solution of a forward or inverse problem on a single geometry. It means that for any new geometry (computational domain): one must retrain a PINN. This limitation of regular PINNs imposes high computational costs: specifically for a comprehensive investigation of geometric parameters in industrial designs. Physics-informed PointNet (PIPN) is fundamentally the result of a combination of PINN's loss function with PointNet. In fact: instead of using a simple fully connected neural network: PIPN uses PointNet as the core of its neural network. PointNet has been primarily designed for deep learning of 3D object classification and segmentation by the research group of Leonidas J. Guibas. PointNet extracts geometric features of input computational domains in PIPN. Thus: PIPN is able to solve governing equations on multiple computational domains (rather than only a single domain) with irregular geometries: simultaneously. The effectiveness of PIPN has been shown for incompressible flow: heat transfer and linear elasticity. == Physics-informed neural networks (PINNs) for inverse computations == Physics-informed neural networks (PINNs) have particularly proven effective in solving inverse problems within differential equations: demonstrating their applicability across science: engineering: and economics. They have shown useful for solving inverse problems in a variety of fields: including nano-optics: topology optimization/characterization: multiphase flow in porous media: and high-speed fluid flow. PINNs have demonstrated flexibility when dealing with noisy and uncertain observation datasets. They also demonstrated clear advantages in the inverse calculation of parameters for multi-fidelity datasets: meaning datasets with different quality: quantity: and types of observations. Uncertainties in calculations can be evaluated using ensemble-based or Bayesian-based calculations. == Limitations == Translation and discontinuous behavior are hard to approximate using PINNs. They fail when solving differential equations with slight advective dominance. The difficulty in training of PINNs in advection-dominated PDEs can be explained by Kolmogorov n–width of the solution. They also fail to solve a system of dynamical systems and hence have not been a success in solving chaotic equations. One of the reasons behind the failure of the regular PINNs is soft-constraining of Dirichlet and Neumann boundary conditions which pose multi-objective optimization problems. This requires the need for manually weighing the loss terms to be able to optimize. Another reason is getting optimization itself. Posing PDE solving as an optimization problem brings in all the problems that are faced in the world of optimization:\nthe major one being getting stuck at a local optimum pretty often. == References == == External links == PINN – repository to implement physics-informed neural network in Python XPINN – repository to implement extended physics-informed neural network (XPINN) in Python PIPN [2]– repository to implement physics-informed PointNet (PIPN) in Python"": 'original_similarity': 0.7929773014891998: 'slide': 218}"
0.45061560001340695,Who were the first to publish ideas on quantum neural computation?,The first ideas on quantum neural computation were published independently in 1995 by Subhash Kak and Ron Chrisley.,"{'chunk_text': ""that input onto another layer of qubits. This layer of qubits evaluates this information and passes on the output to the next layer. Eventually the path leads to the final layer of qubits. The layers do not have to be of the same width: meaning they don't have to have the same number of qubits as the layer before or after it. This structure is trained on which path to take similar to classical artificial neural networks. This is discussed in a lower section. Quantum neural networks refer to three different categories: Quantum computer with classical data: classical computer with quantum data: and quantum computer with quantum data. == Examples == Quantum neural network research is still in its infancy: and a conglomeration of proposals and ideas of varying scope and mathematical rigor have been put forward. Most of them are based on the idea of replacing classical binary or McCulloch-Pitts neurons with a qubit (which can be called a “quron”): resulting in neural units that can be in a superposition of the state ‘firing’ and ‘resting’. === Quantum perceptrons === A lot of proposals attempt to find a quantum equivalent for the perceptron unit from which neural nets are constructed. A problem is that nonlinear activation functions do not immediately correspond to the mathematical structure of quantum theory: since a quantum evolution is described by linear operations and leads to probabilistic observation. Ideas to imitate the perceptron activation function with a quantum mechanical formalism reach from special measurements to postulating non-linear quantum operators (a mathematical framework that is disputed). A direct implementation of the activation function using the circuit-based model of quantum computation has recently been proposed by Schuld: Sinayskiy and Petruccione based on the quantum phase estimation algorithm. === Quantum networks === At a larger scale: researchers have attempted to generalize neural networks to the quantum setting. One way of constructing a quantum neuron is to first generalise classical neurons and then generalising them further to make unitary gates. Interactions between neurons can be controlled quantumly: with unitary gates: or classically: via measurement of the network states. This high-level theoretical technique can be applied broadly: by taking different types of networks and different implementations of quantum neurons: such as photonically implemented neurons and quantum reservoir processor (quantum version of reservoir computing). Most learning algorithms follow the classical model of training an artificial neural network to learn the input-output function of a given training set and use classical feedback loops to update parameters of the quantum system until they converge to an optimal configuration. Learning as a parameter optimisation problem has also been approached by adiabatic models of quantum computing.Quantum neural networks can be applied to algorithmic design: given qubits with tunable mutual interactions: one can attempt to learn interactions following the classical backpropagation rule from a training set of desired input-output relations: taken to be the desired output algorithm's behavior. The quantum network thus ‘learns’ an algorithm. === Quantum associative memory === The first quantum associative memory algorithm was introduced by Dan Ventura and Tony Martinez in 1999. The authors do not attempt to translate the structure of artificial neural network models into quantum theory: but propose an algorithm for a circuit-based quantum computer that simulates associative memory. The memory states (in Hopfield neural networks saved in the weights of the neural connections) are written into a superposition: and a Grover-like quantum search algorithm retrieves the memory state closest to a given input. As such: this is not a fully content-addressable memory: since only incomplete patterns can be retrieved. The first truly content-addressable quantum memory: which can retrieve patterns also from corrupted inputs: was proposed by Carlo A. Trugenberger. Both memories can store an exponential (in terms of n qubits) number of patterns but can be used only once due to the no-cloning theorem and their destruction upon measurement. Trugenberger: however: has shown that his proababilistic model of quantum associative memory can be efficiently implemented and re-used multiples times for any polynomial number of stored patterns: a large advantage with respect to classical associative memories. === Classical neural networks inspired by quantum theory === A substantial amount of interest has been given to a “quantum-inspired” model that uses ideas from quantum theory to implement a neural network based on fuzzy logic. == Training == Quantum Neural Networks can be theoretically trained similarly to training classical/artificial neural networks. A key difference lies in communication between the layers of a neural networks. For classical neural networks: at the end of a given operation: the current perceptron copies its output to the next layer of perceptron(s) in the network. However: in a quantum neural network: where each perceptron is a qubit: this would violate the no-cloning theorem. A proposed generalized solution to this is to replace the classical fan-out method with an arbitrary unitary that spreads out: but\ndoes not copy: the output of one qubit to the next layer of qubits. Using this fan-out Unitary (Uf{\\displaystyle U_{f}}) with a dummy state qubit in a known state (Ex. |0⟩{\\displaystyle |0\\rangle } in the computational basis): also known as an Ancilla bit: the information from the qubit can be transferred to the next layer of qubits. This process adheres to the quantum operation requirement of reversibility.Using this quantum feed-forward network: deep neural networks can be executed and trained efficiently. A deep neural network is essentially a network with many hidden-layers: as seen in the sample model neural network above. Since the Quantum neural network being discussed uses fan-out Unitary operators: and each operator only acts on its respective input: only two layers are used at any given time. In other words: no Unitary operator is acting on the entire network at any given time: meaning the number of qubits required for a given step depends on the number of inputs in a given layer. Since Quantum Computers are notorious for their ability to run multiple iterations in a short period of time: the efficiency of a quantum neural network is solely dependent on"": 'original_similarity': 0.840140296767265: 'slide': 192}","{'chunk_text': 'vanishing gradient problem: GPUs make backpropagation feasible for many-layered feedforward neural networks. Between 2009 and 2012: ANNs began winning prizes in image recognition contests: approaching human level performance on various tasks: initially in pattern recognition and handwriting recognition. For example: the bi-directional and multi-dimensional long short-term memory (LSTM) of Graves et al. won three competitions in connected handwriting recognition in 2009 without any prior knowledge about the three languages to be learned.Ciresan and colleagues built the first pattern recognizers to achieve human-competitive/superhuman performance on benchmarks such as traffic sign recognition (IJCNN 2012). Radial basis function and wavelet networks were introduced in 2013. These can be shown to offer best approximation properties and have been applied in nonlinear system identification and classification applications.In 2014: the adversarial network principle was used in a generative adversarial network (GAN) by Ian Goodfellow et al. Here the adversarial network (discriminator) outputs a value between 1 and 0 depending on the likelihood of the first network\'s (generator) output is in a given set. This can be used to create realistic deepfakes. Excellent image quality is achieved by Nvidia\'s StyleGAN (2018) based on the Progressive GAN by Tero Karras: Timo Aila: Samuli Laine: and Jaakko Lehtinen. Here the GAN generator is grown from small to large scale in a pyramidal fashion. In 2015: Rupesh Kumar Srivastava: Klaus Greff: and Schmidhuber used the LSTM principle to create the Highway network: a feedforward neural network with hundreds of layers: much deeper than previous networks. 7 months later: Kaiming He: Xiangyu Zhang; Shaoqing Ren: and Jian Sun won the ImageNet 2015 competition with an open-gated or gateless Highway network variant called Residual neural network.In 2017: Ashish Vaswani et al. introduced the modern Transformer architecture in their paper ""Attention Is All You Need."" It combines this with a softmax operator and a projection matrix. Transformers have increasingly become the model of choice for natural language processing. Many modern large language models such as ChatGPT: GPT-4: and BERT use it. Transformers are also increasingly being used in computer vision.Ramenzanpour et al. showed in 2020 that analytical and computational techniques derived from statistical physics of disordered systems can be extended to large-scale problems: including machine learning: e.g.: to analyze the weight space of deep neural networks. == Models == ANNs began as an attempt to exploit the architecture of the human brain to perform tasks that conventional algorithms had little success with. They soon reoriented towards improving empirical results: abandoning attempts to remain true to their biological precursors. ANNs have the ability to learn and model non-linearities and complex relationships. This is achieved by neurons being connected in various patterns: allowing the output of some neurons to become the input of others. The network forms a directed: weighted graph.An artificial neural network consists of simulated neurons. Each neuron is connected to other nodes via links like a biological axon-synapse-dendrite connection. All the nodes connected by links take in some data and use it to perform specific operations and tasks on the data. Each link has a weight: determining the strength of one node\'s influence on another: allowing weights to choose the signal between neurons. === Artificial neurons === ANNs are composed of artificial neurons which are conceptually derived from biological neurons. Each artificial neuron has inputs and produces a single output which can be sent to multiple other neurons. The inputs can be the feature values of a sample of external data: such as images or documents: or they can be the outputs of other neurons. The outputs of the final output neurons of the neural net accomplish the task: such as recognizing an object in an image. To find the output of the neuron we take the weighted sum of all the inputs: weighted by the weights of the connections from the inputs to the neuron. We add a bias term to this sum. This weighted sum is sometimes called the activation. This weighted sum is then passed through a (usually nonlinear) activation function to produce the output. The initial inputs are external data: such as images and documents. The ultimate outputs accomplish the task: such as recognizing an object in an image. === Organization === The neurons are typically organized into multiple layers: especially in deep learning. Neurons of one layer connect only to neurons of the immediately preceding and immediately following layers. The layer that receives external data is the input layer. The layer that produces the ultimate result is the output layer. In between them are zero or more hidden layers. Single layer and unlayered networks are also used. Between two layers: multiple connection patterns are possible. They can be \'fully connected\': with every neuron in one layer connecting to every neuron in the next layer. They can be pooling: where a group of neurons in one layer connects to a single neuron in the next layer: thereby reducing the number of neurons in that layer. Neurons with only such connections form a directed acyclic graph and are known as feedforward networks. Alternatively: networks that allow connections between neurons in the same or previous layers are known as recurrent networks. === Hyperparameter === A hyperparameter is a constant parameter whose value is set before the learning process begins. The values of parameters are derived via learning. Examples of hyperparameters include learning rate: the number of hidden layers and batch size. The values of some hyperparameters can be dependent on those of other\nhyperparameters. For example: the size of some layers can depend on the overall number of layers. === Learning === Learning is the adaptation of the network to better handle a task by considering sample observations. Learning involves adjusting the weights (and optional thresholds) of the network to improve the accuracy of the result. This is done by minimizing the observed errors. Learning is complete when examining additional observations does not usefully reduce the error rate. Even after learning: the error rate typically does not reach 0. If after learning: the error rate is too high: the network typically': 'original_similarity': 0.8072641370015439: 'slide': 96}","{'chunk_text': 'developing logic-in-memory devices and circuits based on floating-gate field-effect transistors (FGFETs). Such atomically thin semiconductors are considered promising for energy-efficient machine learning applications: where the same basic device structure is used for both logic operations and data storage. The authors used two-dimensional materials such as semiconducting molybdenum disulphide to precisely tune FGFETs as building blocks in which logic operations can be performed with the memory elements. === Integrated photonic tensor core === In 1988: Wei Zhang et al. discussed fast optical implementations of convolutional neural networks for alphabet recognition. In 2021: J. Feldmann et al. proposed an integrated photonic hardware accelerator for parallel convolutional processing. The authors identify two key advantages of integrated photonics over its electronic counterparts: (1) massively parallel data transfer through wavelength division multiplexing in conjunction with frequency combs: and (2) extremely high data modulation speeds. Their system can execute trillions of multiply-accumulate operations per second: indicating the potential of integrated photonics in data-heavy AI applications. Optical processors that can also perform backpropagation for artificial neural networks have been experimentally developed. == Nomenclature == As of 2016: the field is still in flux and vendors are pushing their own marketing term for what amounts to an ""AI accelerator"": in the hope that their designs and APIs will become the dominant design. There is no consensus on the boundary between these devices: nor the exact form they will take; however several examples clearly aim to fill this new space: with a fair amount of overlap in capabilities. In the past when consumer graphics accelerators emerged: the industry eventually adopted Nvidia\'s self-assigned term: ""the GPU"": as the collective noun for ""graphics accelerators"": which had taken many forms before settling on an overall pipeline implementing a model presented by Direct3D. All models of Intel Meteor Lake processors have a Versatile Processor Unit (VPU) built-in for accelerating inference for computer vision and deep learning. == Deep Learning Processors (DLP) == Inspired from the pioneer work of DianNao Family: many DLPs are proposed in both academia and industry with design optimized to leverage the features of deep neural networks for high efficiency. Only at ISCA 2016: three sessions: 15% (!) of the accepted papers: are all architecture designs about deep learning. Such efforts include Eyeriss (MIT): EIE (Stanford): Minerva (Harvard): Stripes (University of Toronto) in academia: TPU (Google): and MLU (Cambricon) in industry. We listed several representative works in Table 1. === Digital DLPs === The major components of DLPs architecture usually include a computation component: the on-chip memory hierarchy: and the control logic that manages the data communication and computing flows. Regarding the computation component: as most operations in deep learning can be aggregated into vector operations: the most common ways for building computation components in digital DLPs are the MAC-based (multiplier-accumulation) organization: either with vector MACs or scalar MACs. Rather than SIMD or SIMT in general processing devices: deep learning domain-specific parallelism is better explored on these MAC-based organizations. Regarding the memory hierarchy: as deep learning algorithms require high bandwidth to provide the computation component with sufficient data: DLPs usually employ a relatively larger size (tens of kilobytes or several megabytes) on-chip buffer but with dedicated on-chip data reuse strategy and data exchange strategy to alleviate the burden for memory bandwidth. For example: DianNao: 16 16-in vector MAC: requires 16 × 16 × 2 = 512 16-bit data: i.e.: almost 1024GB/s bandwidth requirements between computation components and buffers. With on-chip reuse: such bandwidth requirements are reduced drastically. Instead of the widely used cache in general processing devices: DLPs always use scratchpad memory as it could provide higher data reuse opportunities by leveraging the relatively regular data access pattern in deep learning algorithms. Regarding the control logic: as the deep learning algorithms keep evolving at a dramatic speed: DLPs start to leverage dedicated ISA (instruction set architecture) to support the deep learning domain flexibly. At first: DianNao used a VLIW-style instruction set where each instruction could finish a layer in a DNN. Cambricon introduces the first deep learning domain-specific ISA: which could support more than ten different deep learning algorithms. TPU also reveals five key instructions from the CISC-style ISA. === Hybrid DLPs === Hybrid DLPs emerge for DNN inference and training acceleration because of their high efficiency. Processing-in-memory (PIM) architectures are one most important type of hybrid DLP. The key design concept of PIM is to bridge the gap between computing and memory: with the following manners: 1) Moving computation components into memory cells: controllers: or memory chips to alleviate the memory wall issue. Such architectures significantly shorten data paths and leverage much higher internal bandwidth: hence resulting in attractive performance improvement. 2) Build high efficient DNN engines by adopting computational devices. In 2013: HP Lab demonstrated the astonishing capability of adopting ReRAM crossbar structure for computing. Inspiring by this work: tremendous work are proposed to explore the new architecture and system design based on ReRAM: phase change memory: etc. == Benchmarks == Benchmarks such as MLPerf and others may\nbe used to evaluate the performance of AI accelerators. Table 2 lists several typical benchmarks for AI accelerators. == Potential applications == Agricultural robots: for example: herbicide-free weed control. Autonomous vehicles: Nvidia has targeted their Drive PX-series boards at this application. Computer-aided diagnosis Industrial robots: increasing the range of tasks that can be automated: by adding adaptability to variable situations. Machine translation Military robots Natural language processing Search engines: increasing the energy efficiency of data centers and the ability to use increasingly advanced queries. Unmanned aerial vehicles: e.g. navigation systems: e.g. the Movidius Myriad 2 has been demonstrated successfully guiding autonomous drones. Voice user interface: e.g. in mobile phones: a target for Qualcomm Zeroth. == See also == Cognitive computer Neuromorphic engineering Optical neural network Physical neural network Cerebras Systems == References == == External links == Nvidia Puts The Accelerator To The Metal With Pascal.htm: The Next Platform Eyeriss Project: MIT https://alphaics.ai/': 'original_similarity': 0.8011092488724101: 'slide': 169}"
0.4083929000189528,What is one of the main motivations for investigating quantum neural networks?,"One important motivation for investigating quantum neural networks is the challenge of training classical neural networks, particularly in big data applications.","{'chunk_text': ""their classical counterparts: this structure intakes input from one layer of qubits: and passes that input onto another layer of qubits. This layer of qubits evaluates this information and passes on the output to the next layer. Eventually the path leads to the final layer of qubits. The layers do not have to be of the same width: meaning they don't have to have the same number of qubits as the layer before or after it. This structure is trained on which path to take similar to classical artificial neural networks. This is discussed in a lower section. Quantum neural networks refer to three different categories: Quantum computer with classical data: classical computer with quantum data: and quantum computer with quantum data. == Examples == Quantum neural network research is still in its infancy: and a conglomeration of proposals and ideas of varying scope and mathematical rigor have been put forward. Most of them are based on the idea of replacing classical binary or McCulloch-Pitts neurons with a qubit (which can be called a “quron”): resulting in neural units that can be in a superposition of the state ‘firing’ and ‘resting’. === Quantum perceptrons === A lot of proposals attempt to find a quantum equivalent for the perceptron unit from which neural nets are constructed. A problem is that nonlinear activation functions do not immediately correspond to the mathematical structure of quantum theory: since a quantum evolution is described by linear operations and leads to probabilistic observation. Ideas to imitate the perceptron activation function with a quantum mechanical formalism reach from special measurements to postulating non-linear quantum operators (a mathematical framework that is disputed). A direct implementation of the activation function using the circuit-based model of quantum computation has recently been proposed by Schuld: Sinayskiy and Petruccione based on the quantum phase estimation algorithm. === Quantum networks === At a larger scale: researchers have attempted to generalize neural networks to the quantum setting. One way of constructing a quantum neuron is to first generalise classical neurons and then generalising them further to make unitary gates. Interactions between neurons can be controlled quantumly: with unitary gates: or classically: via measurement of the network states. This high-level theoretical technique can be applied broadly: by taking different types of networks and different implementations of quantum neurons: such as photonically implemented neurons and quantum reservoir processor (quantum version of reservoir computing). Most learning algorithms follow the classical model of training an artificial neural network to learn the input-output function of a given training set and use classical feedback loops to update parameters of the quantum system until they converge to an optimal configuration. Learning as a parameter optimisation problem has also been approached by adiabatic models of quantum computing.Quantum neural networks can be applied to algorithmic design: given qubits with tunable mutual interactions: one can attempt to learn interactions following the classical backpropagation rule from a training set of desired input-output relations: taken to be the desired output algorithm's behavior. The quantum network thus ‘learns’ an algorithm. === Quantum associative memory === The first quantum associative memory algorithm was introduced by Dan Ventura and Tony Martinez in 1999. The authors do not attempt to translate the structure of artificial neural network models into quantum theory: but propose an algorithm for a circuit-based quantum computer that simulates associative memory. The memory states (in Hopfield neural networks saved in the weights of the neural connections) are written into a superposition: and a Grover-like quantum search algorithm retrieves the memory state closest to a given input. As such: this is not a fully content-addressable memory: since only incomplete patterns can be retrieved. The first truly content-addressable quantum memory: which can retrieve patterns also from corrupted inputs: was proposed by Carlo A. Trugenberger. Both memories can store an exponential (in terms of n qubits) number of patterns but can be used only once due to the no-cloning theorem and their destruction upon measurement. Trugenberger: however: has shown that his proababilistic model of quantum associative memory can be efficiently implemented and re-used multiples times for any polynomial number of stored patterns: a large advantage with respect to classical associative memories. === Classical neural networks inspired by quantum theory === A substantial amount of interest has been given to a “quantum-inspired” model that uses ideas from quantum theory to implement a neural network based on fuzzy logic. == Training == Quantum Neural Networks can be theoretically trained similarly to training classical/artificial neural networks. A key difference lies in communication between the layers of a neural networks. For classical neural networks: at the end of a given operation: the current perceptron copies its output to the next layer of perceptron(s) in the network. However: in a quantum neural network: where each perceptron is a qubit: this would violate the no-cloning theorem. A proposed generalized solution to this is to replace the classical fan-out method with an arbitrary unitary that spreads out: but\ndoes not copy: the output of one qubit to the next layer of qubits. Using this fan-out Unitary (Uf{\\displaystyle U_{f}}) with a dummy state qubit in a known state (Ex. |0⟩{\\displaystyle |0\\rangle } in the computational basis): also known as an Ancilla bit: the information from the qubit can be transferred to the next layer of qubits. This process adheres to the quantum operation requirement of reversibility.Using this quantum feed-forward network: deep neural networks can be executed and trained efficiently. A deep neural network is essentially a network with many hidden-layers: as seen in the sample model neural network above. Since the Quantum neural network being discussed uses fan-out Unitary operators: and each operator only acts on its respective input: only two layers are used at any given time. In other words: no Unitary operator is acting on the entire network at any given time: meaning the number of qubits required for a given step depends on the number of inputs in a given layer. Since Quantum Computers are notorious for their ability to run multiple iterations in a short"": 'original_similarity': 0.8642891102770764: 'slide': 178}","{'chunk_text': ""layer of qubits. Using this fan-out Unitary (Uf{\\displaystyle U_{f}}) with a dummy state qubit in a known state (Ex. |0⟩{\\displaystyle |0\\rangle } in the computational basis): also known as an Ancilla bit: the information from the qubit can be transferred to the next layer of qubits. This process adheres to the quantum operation requirement of reversibility.Using this quantum feed-forward network: deep neural networks can be executed and trained efficiently. A deep neural network is essentially a network with many hidden-layers: as seen in the sample model neural network above. Since the Quantum neural network being discussed uses fan-out Unitary operators: and each operator only acts on its respective input: only two layers are used at any given time. In other words: no Unitary operator is acting on the entire network at any given time: meaning the number of qubits required for a given step depends on the number of inputs in a given layer. Since Quantum Computers are notorious for their ability to run multiple iterations in a short period of time: the efficiency of a quantum neural network is solely dependent on the number of qubits in any given layer: and not on the depth of the network. === Cost functions === To determine the effectiveness of a neural network: a cost function is used: which essentially measures the proximity of the network's output to the expected or desired output. In a Classical Neural Network: the weights (w{\\displaystyle w}) and biases (b{\\displaystyle b}) at each step determine the outcome of the cost function C(w:b){\\displaystyle C(w:b)}. When training a Classical Neural network: the weights and biases are adjusted after each iteration: and given equation 1 below: where y(x){\\displaystyle y(x)} is the desired output and aout(x){\\displaystyle a^{\\text{out}}(x)} is the actual output: the cost function is optimized when C(w:b){\\displaystyle C(w:b)}= 0. For a quantum neural network: the cost function is determined by measuring the fidelity of the outcome state (ρout{\\displaystyle \\rho ^{\\text{out}}}) with the desired outcome state (ϕout{\\displaystyle \\phi ^{\\text{out}}}): seen in Equation 2 below. In this case: the Unitary operators are adjusted after each iteration: and the cost function is optimized when C = 1. Equation 1 C(w:b)=1N∑x||y(x)−aout(x)||2{\\displaystyle C(w:b)={1 \\over N}\\sum _{x}{||y(x)-a^{\\text{out}}(x)|| \\over 2}} Equation 2 C=1N∑xN⟨ϕout|ρout|ϕout⟩{\\displaystyle C={1 \\over N}\\sum _{x}^{N}{\\langle \\phi ^{\\text{out}}|\\rho ^{\\text{out}}|\\phi ^{\\text{out}}\\rangle }} == See also == Differentiable programming Optical neural network Holographic associative memory Quantum cognition Quantum machine learning == References == == External links == Recent review of quantum neural networks by M. Schuld: I. Sinayskiy and F. Petruccione Review of quantum neural networks by Wei Article by P. Gralewicz on the plausibility of quantum computing in biological neural networks Training a neural net to recognize images\nIn the context of artificial neural networks: the rectifier or ReLU (rectified"": 'original_similarity': 0.8391867892770644: 'slide': 10}","{'chunk_text': 'professional Go player. Google Translate uses a neural network to translate between more than 100 languages. In 2017: Covariant.ai was launched: which focuses on integrating deep learning into factories.As of 2008: researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement: or TAMER: which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor. First developed as TAMER: a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot with the ability to learn new tasks through observation. Using Deep TAMER: a robot learned a task with a human trainer: watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer: who provided feedback such as ""good job"" and ""bad job"". == Criticism and comment == Deep learning has attracted both criticism and comment: in some cases from outside the field of computer science. === Theory === A main criticism concerns the lack of theory surrounding some methods. Learning in the most common deep architectures is implemented using well-understood gradient descent. However: the theory surrounding other algorithms: such as contrastive divergence is less clear. (e.g.: Does it converge? If so: how fast? What is it approximating?) Deep learning methods are often looked at as a black box: with most confirmations done empirically: rather than theoretically.Others point out that deep learning should be looked at as a step towards realizing strong AI: not as an all-encompassing solution. Despite the power of deep learning methods: they still lack much of the functionality needed to realize this goal entirely. Research psychologist Gary Marcus noted: Realistically: deep learning is only part of the larger challenge of building intelligent machines. Such techniques lack ways of representing causal relationships (...) have no obvious ways of performing logical inferences: and they are also still a long way from integrating abstract knowledge: such as information about what objects are: what they are for: and how they are typically used. The most powerful A.I. systems: like Watson (...) use techniques like deep learning as just one element in a very complicated ensemble of techniques: ranging from the statistical technique of Bayesian inference to deductive reasoning. In further reference to the idea that artistic sensitivity might be inherent in relatively low levels of the cognitive hierarchy: a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained demonstrate a visual appeal: the original research notice received well over 1:000 comments: and was the subject of what was for a time the most frequently accessed article on The Guardian\'s website. === Errors === Some deep learning architectures display problematic behaviors: such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images (2014) and misclassifying minuscule perturbations of correctly classified images (2013). Goertzel hypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-component artificial general intelligence (AGI) architectures. These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar decompositions of observed entities and events. Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic goal of both human language acquisition and artificial intelligence (AI). === Cyber threat === As deep learning moves from the lab into the world: research and experience show that artificial neural networks are vulnerable to hacks and deception. By identifying patterns that these systems use to function: attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize. For example: an attacker can make subtle changes to an image such that the ANN finds a match even though the image looks to a human nothing like the search target. Such manipulation is termed an ""adversarial attack"".In 2016 researchers used one ANN to doctor images in trial and error fashion: identify another\'s focal points: and thereby generate images that deceived it. The modified images looked no different to human eyes. Another group showed that printouts of doctored images then photographed successfully tricked an image classification system. One defense is reverse image search: in which a possible fake image is submitted to a site such as TinEye that can then find other instances of it. A refinement is to search using only parts of the image: to identify images from which that piece may have been taken.Another group showed that certain psychedelic spectacles could fool a facial recognition system into thinking ordinary people were celebrities: potentially allowing one person to impersonate another. In 2017 researchers added stickers to stop signs and caused an ANN to misclassify them.ANNs\ncan however be further trained to detect attempts at deception: potentially leading attackers and defenders into an arms race similar to the kind that already defines the malware defense industry. ANNs have been trained to defeat ANN-based anti-malware software by repeatedly attacking a defense with malware that was continually altered by a genetic algorithm until it tricked the anti-malware while retaining its ability to damage the target.In 2016: another group demonstrated that certain sounds could make the Google Now voice command system open a particular web address: and hypothesized that this could ""serve as a stepping stone for further attacks (e.g.: opening a web page hosting drive-by malware)"".In ""data poisoning"": false data is continually smuggled into a machine learning system\'s training set to prevent it from achieving mastery. === Data collection ethics === Most Deep Learning systems rely on training and verification data that is generated and/or annotated by humans.': 'original_similarity': 0.8286635441518443: 'slide': 148}"
0.46420330001274124,How does the structure of a quantum neural network compare to that of a classical artificial neural network? ,"Most Quantum neural networks are developed as feed-forward networks similar to their classical counterparts. The structure intakes input from one layer of qubits and passes that input onto another layer after evaluation. However, the layers in quantum neural networks do not need to have the same number of qubits as the layer before or after it.","{'chunk_text': ""counterparts: this structure intakes input from one layer of qubits: and passes that input onto another layer of qubits. This layer of qubits evaluates this information and passes on the output to the next layer. Eventually the path leads to the final layer of qubits. The layers do not have to be of the same width: meaning they don't have to have the same number of qubits as the layer before or after it. This structure is trained on which path to take similar to classical artificial neural networks. This is discussed in a lower section. Quantum neural networks refer to three different categories: Quantum computer with classical data: classical computer with quantum data: and quantum computer with quantum data. == Examples == Quantum neural network research is still in its infancy: and a conglomeration of proposals and ideas of varying scope and mathematical rigor have been put forward. Most of them are based on the idea of replacing classical binary or McCulloch-Pitts neurons with a qubit (which can be called a “quron”): resulting in neural units that can be in a superposition of the state ‘firing’ and ‘resting’. === Quantum perceptrons === A lot of proposals attempt to find a quantum equivalent for the perceptron unit from which neural nets are constructed. A problem is that nonlinear activation functions do not immediately correspond to the mathematical structure of quantum theory: since a quantum evolution is described by linear operations and leads to probabilistic observation. Ideas to imitate the perceptron activation function with a quantum mechanical formalism reach from special measurements to postulating non-linear quantum operators (a mathematical framework that is disputed). A direct implementation of the activation function using the circuit-based model of quantum computation has recently been proposed by Schuld: Sinayskiy and Petruccione based on the quantum phase estimation algorithm. === Quantum networks === At a larger scale: researchers have attempted to generalize neural networks to the quantum setting. One way of constructing a quantum neuron is to first generalise classical neurons and then generalising them further to make unitary gates. Interactions between neurons can be controlled quantumly: with unitary gates: or classically: via measurement of the network states. This high-level theoretical technique can be applied broadly: by taking different types of networks and different implementations of quantum neurons: such as photonically implemented neurons and quantum reservoir processor (quantum version of reservoir computing). Most learning algorithms follow the classical model of training an artificial neural network to learn the input-output function of a given training set and use classical feedback loops to update parameters of the quantum system until they converge to an optimal configuration. Learning as a parameter optimisation problem has also been approached by adiabatic models of quantum computing.Quantum neural networks can be applied to algorithmic design: given qubits with tunable mutual interactions: one can attempt to learn interactions following the classical backpropagation rule from a training set of desired input-output relations: taken to be the desired output algorithm's behavior. The quantum network thus ‘learns’ an algorithm. === Quantum associative memory === The first quantum associative memory algorithm was introduced by Dan Ventura and Tony Martinez in 1999. The authors do not attempt to translate the structure of artificial neural network models into quantum theory: but propose an algorithm for a circuit-based quantum computer that simulates associative memory. The memory states (in Hopfield neural networks saved in the weights of the neural connections) are written into a superposition: and a Grover-like quantum search algorithm retrieves the memory state closest to a given input. As such: this is not a fully content-addressable memory: since only incomplete patterns can be retrieved. The first truly content-addressable quantum memory: which can retrieve patterns also from corrupted inputs: was proposed by Carlo A. Trugenberger. Both memories can store an exponential (in terms of n qubits) number of patterns but can be used only once due to the no-cloning theorem and their destruction upon measurement. Trugenberger: however: has shown that his proababilistic model of quantum associative memory can be efficiently implemented and re-used multiples times for any polynomial number of stored patterns: a large advantage with respect to classical associative memories. === Classical neural networks inspired by quantum theory === A substantial amount of interest has been given to a “quantum-inspired” model that uses ideas from quantum theory to implement a neural network based on fuzzy logic. == Training == Quantum Neural Networks can be theoretically trained similarly to training classical/artificial neural networks. A key difference lies in communication between the layers of a neural networks. For classical neural networks: at the end of a given operation: the current perceptron copies its output to the next layer of perceptron(s) in the network. However: in a quantum neural network: where each perceptron is a qubit: this would violate the no-cloning theorem. A proposed generalized solution to this is to replace the classical fan-out method with an arbitrary unitary that spreads out: but\ndoes not copy: the output of one qubit to the next layer of qubits. Using this fan-out Unitary (Uf{\\displaystyle U_{f}}) with a dummy state qubit in a known state (Ex. |0⟩{\\displaystyle |0\\rangle } in the computational basis): also known as an Ancilla bit: the information from the qubit can be transferred to the next layer of qubits. This process adheres to the quantum operation requirement of reversibility.Using this quantum feed-forward network: deep neural networks can be executed and trained efficiently. A deep neural network is essentially a network with many hidden-layers: as seen in the sample model neural network above. Since the Quantum neural network being discussed uses fan-out Unitary operators: and each operator only acts on its respective input: only two layers are used at any given time. In other words: no Unitary operator is acting on the entire network at any given time: meaning the number of qubits required for a given step depends on the number of inputs in a given layer. Since Quantum Computers are notorious for their ability to run multiple iterations in a short period of"": 'original_similarity': 0.8870908078774242: 'slide': 180}","{'chunk_text': ""spreads out: but\ndoes not copy: the output of one qubit to the next layer of qubits. Using this fan-out Unitary (Uf{\\displaystyle U_{f}}) with a dummy state qubit in a known state (Ex. |0⟩{\\displaystyle |0\\rangle } in the computational basis): also known as an Ancilla bit: the information from the qubit can be transferred to the next layer of qubits. This process adheres to the quantum operation requirement of reversibility.Using this quantum feed-forward network: deep neural networks can be executed and trained efficiently. A deep neural network is essentially a network with many hidden-layers: as seen in the sample model neural network above. Since the Quantum neural network being discussed uses fan-out Unitary operators: and each operator only acts on its respective input: only two layers are used at any given time. In other words: no Unitary operator is acting on the entire network at any given time: meaning the number of qubits required for a given step depends on the number of inputs in a given layer. Since Quantum Computers are notorious for their ability to run multiple iterations in a short period of time: the efficiency of a quantum neural network is solely dependent on the number of qubits in any given layer: and not on the depth of the network. === Cost functions === To determine the effectiveness of a neural network: a cost function is used: which essentially measures the proximity of the network's output to the expected or desired output. In a Classical Neural Network: the weights (w{\\displaystyle w}) and biases (b{\\displaystyle b}) at each step determine the outcome of the cost function C(w:b){\\displaystyle C(w:b)}. When training a Classical Neural network: the weights and biases are adjusted after each iteration: and given equation 1 below: where y(x){\\displaystyle y(x)} is the desired output and aout(x){\\displaystyle a^{\\text{out}}(x)} is the actual output: the cost function is optimized when C(w:b){\\displaystyle C(w:b)}= 0. For a quantum neural network: the cost function is determined by measuring the fidelity of the outcome state (ρout{\\displaystyle \\rho ^{\\text{out}}}) with the desired outcome state (ϕout{\\displaystyle \\phi ^{\\text{out}}}): seen in Equation 2 below. In this case: the Unitary operators are adjusted after each iteration: and the cost function is optimized when C = 1. Equation 1 C(w:b)=1N∑x||y(x)−aout(x)||2{\\displaystyle C(w:b)={1 \\over N}\\sum _{x}{||y(x)-a^{\\text{out}}(x)|| \\over 2}} Equation 2 C=1N∑xN⟨ϕout|ρout|ϕout⟩{\\displaystyle C={1 \\over N}\\sum _{x}^{N}{\\langle \\phi ^{\\text{out}}|\\rho ^{\\text{out}}|\\phi ^{\\text{out}}\\rangle }} == See also == Differentiable programming Optical neural network Holographic associative memory Quantum cognition Quantum machine learning == References == == External links == Recent review of quantum neural networks by M. Schuld: I. Sinayskiy and F. Petruccione Review of quantum neural networks by Wei Article by P. Gralewicz on the plausibility of quantum computing in biological neural networks Training a neural net to"": 'original_similarity': 0.8654519335893518: 'slide': -3}","{'chunk_text': 'how to play Atari video games using only pixels as data input. In 2015 they demonstrated their AlphaGo system: which learned the game of Go well enough to beat a professional Go player. Google Translate uses a neural network to translate between more than 100 languages. In 2017: Covariant.ai was launched: which focuses on integrating deep learning into factories.As of 2008: researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement: or TAMER: which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor. First developed as TAMER: a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot with the ability to learn new tasks through observation. Using Deep TAMER: a robot learned a task with a human trainer: watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer: who provided feedback such as ""good job"" and ""bad job"". == Criticism and comment == Deep learning has attracted both criticism and comment: in some cases from outside the field of computer science. === Theory === A main criticism concerns the lack of theory surrounding some methods. Learning in the most common deep architectures is implemented using well-understood gradient descent. However: the theory surrounding other algorithms: such as contrastive divergence is less clear. (e.g.: Does it converge? If so: how fast? What is it approximating?) Deep learning methods are often looked at as a black box: with most confirmations done empirically: rather than theoretically.Others point out that deep learning should be looked at as a step towards realizing strong AI: not as an all-encompassing solution. Despite the power of deep learning methods: they still lack much of the functionality needed to realize this goal entirely. Research psychologist Gary Marcus noted: Realistically: deep learning is only part of the larger challenge of building intelligent machines. Such techniques lack ways of representing causal relationships (...) have no obvious ways of performing logical inferences: and they are also still a long way from integrating abstract knowledge: such as information about what objects are: what they are for: and how they are typically used. The most powerful A.I. systems: like Watson (...) use techniques like deep learning as just one element in a very complicated ensemble of techniques: ranging from the statistical technique of Bayesian inference to deductive reasoning. In further reference to the idea that artistic sensitivity might be inherent in relatively low levels of the cognitive hierarchy: a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained demonstrate a visual appeal: the original research notice received well over 1:000 comments: and was the subject of what was for a time the most frequently accessed article on The Guardian\'s website. === Errors === Some deep learning architectures display problematic behaviors: such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images (2014) and misclassifying minuscule perturbations of correctly classified images (2013). Goertzel hypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-component artificial general intelligence (AGI) architectures. These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar decompositions of observed entities and events. Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic goal of both human language acquisition and artificial intelligence (AI). === Cyber threat === As deep learning moves from the lab into the world: research and experience show that artificial neural networks are vulnerable to hacks and deception. By identifying patterns that these systems use to function: attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize. For example: an attacker can make subtle changes to an image such that the ANN finds a match even though the image looks to a human nothing like the search target. Such manipulation is termed an ""adversarial attack"".In 2016 researchers used one ANN to doctor images in trial and error fashion: identify another\'s focal points: and thereby generate images that deceived it. The modified images looked no different to human eyes. Another group showed that printouts of doctored images then photographed successfully tricked an image classification system. One defense is reverse image search: in which a possible fake image is submitted to a site such as TinEye that can then find other instances of it. A refinement is to search using only parts of the image: to identify images from which that piece may have been taken.Another group showed that certain psychedelic spectacles could fool a facial recognition system into thinking ordinary people were celebrities: potentially allowing one person to impersonate another. In 2017 researchers added stickers to stop signs and caused an ANN to misclassify them.ANNs\ncan however be further trained to detect attempts at deception: potentially leading attackers and defenders into an arms race similar to the kind that already defines the malware defense industry. ANNs have been trained to defeat ANN-based anti-malware software by repeatedly attacking a defense with malware that was continually altered by a genetic algorithm until it tricked the anti-malware while retaining its ability to damage the target.In 2016: another group demonstrated that certain sounds could make the Google Now voice command system open a particular web address: and hypothesized that this could ""serve as a stepping stone for further attacks (e.g.: opening a web page hosting drive-by malware)"".In ""data poisoning"": false data is continually smuggled into a machine learning system\'s': 'original_similarity': 0.8461116918100376: 'slide': 118}"
0.446221099991817,What categories does the term 'quantum neural networks' refer to?,"The term 'quantum neural networks' refers to three different categories: Quantum computer with classical data, classical computer with quantum data, and quantum computer with quantum data.","{'chunk_text': ""counterparts: this structure intakes input from one layer of qubits: and passes that input onto another layer of qubits. This layer of qubits evaluates this information and passes on the output to the next layer. Eventually the path leads to the final layer of qubits. The layers do not have to be of the same width: meaning they don't have to have the same number of qubits as the layer before or after it. This structure is trained on which path to take similar to classical artificial neural networks. This is discussed in a lower section. Quantum neural networks refer to three different categories: Quantum computer with classical data: classical computer with quantum data: and quantum computer with quantum data. == Examples == Quantum neural network research is still in its infancy: and a conglomeration of proposals and ideas of varying scope and mathematical rigor have been put forward. Most of them are based on the idea of replacing classical binary or McCulloch-Pitts neurons with a qubit (which can be called a “quron”): resulting in neural units that can be in a superposition of the state ‘firing’ and ‘resting’. === Quantum perceptrons === A lot of proposals attempt to find a quantum equivalent for the perceptron unit from which neural nets are constructed. A problem is that nonlinear activation functions do not immediately correspond to the mathematical structure of quantum theory: since a quantum evolution is described by linear operations and leads to probabilistic observation. Ideas to imitate the perceptron activation function with a quantum mechanical formalism reach from special measurements to postulating non-linear quantum operators (a mathematical framework that is disputed). A direct implementation of the activation function using the circuit-based model of quantum computation has recently been proposed by Schuld: Sinayskiy and Petruccione based on the quantum phase estimation algorithm. === Quantum networks === At a larger scale: researchers have attempted to generalize neural networks to the quantum setting. One way of constructing a quantum neuron is to first generalise classical neurons and then generalising them further to make unitary gates. Interactions between neurons can be controlled quantumly: with unitary gates: or classically: via measurement of the network states. This high-level theoretical technique can be applied broadly: by taking different types of networks and different implementations of quantum neurons: such as photonically implemented neurons and quantum reservoir processor (quantum version of reservoir computing). Most learning algorithms follow the classical model of training an artificial neural network to learn the input-output function of a given training set and use classical feedback loops to update parameters of the quantum system until they converge to an optimal configuration. Learning as a parameter optimisation problem has also been approached by adiabatic models of quantum computing.Quantum neural networks can be applied to algorithmic design: given qubits with tunable mutual interactions: one can attempt to learn interactions following the classical backpropagation rule from a training set of desired input-output relations: taken to be the desired output algorithm's behavior. The quantum network thus ‘learns’ an algorithm. === Quantum associative memory === The first quantum associative memory algorithm was introduced by Dan Ventura and Tony Martinez in 1999. The authors do not attempt to translate the structure of artificial neural network models into quantum theory: but propose an algorithm for a circuit-based quantum computer that simulates associative memory. The memory states (in Hopfield neural networks saved in the weights of the neural connections) are written into a superposition: and a Grover-like quantum search algorithm retrieves the memory state closest to a given input. As such: this is not a fully content-addressable memory: since only incomplete patterns can be retrieved. The first truly content-addressable quantum memory: which can retrieve patterns also from corrupted inputs: was proposed by Carlo A. Trugenberger. Both memories can store an exponential (in terms of n qubits) number of patterns but can be used only once due to the no-cloning theorem and their destruction upon measurement. Trugenberger: however: has shown that his proababilistic model of quantum associative memory can be efficiently implemented and re-used multiples times for any polynomial number of stored patterns: a large advantage with respect to classical associative memories. === Classical neural networks inspired by quantum theory === A substantial amount of interest has been given to a “quantum-inspired” model that uses ideas from quantum theory to implement a neural network based on fuzzy logic. == Training == Quantum Neural Networks can be theoretically trained similarly to training classical/artificial neural networks. A key difference lies in communication between the layers of a neural networks. For classical neural networks: at the end of a given operation: the current perceptron copies its output to the next layer of perceptron(s) in the network. However: in a quantum neural network: where each perceptron is a qubit: this would violate the no-cloning theorem. A proposed generalized solution to this is to replace the classical fan-out method with an arbitrary unitary that spreads out: but\ndoes not copy: the output of one qubit to the next layer of qubits. Using this fan-out Unitary (Uf{\\displaystyle U_{f}}) with a dummy state qubit in a known state (Ex. |0⟩{\\displaystyle |0\\rangle } in the computational basis): also known as an Ancilla bit: the information from the qubit can be transferred to the next layer of qubits. This process adheres to the quantum operation requirement of reversibility.Using this quantum feed-forward network: deep neural networks can be executed and trained efficiently. A deep neural network is essentially a network with many hidden-layers: as seen in the sample model neural network above. Since the Quantum neural network being discussed uses fan-out Unitary operators: and each operator only acts on its respective input: only two layers are used at any given time. In other words: no Unitary operator is acting on the entire network at any given time: meaning the number of qubits required for a given step depends on the number of inputs in a given layer. Since Quantum Computers are notorious for their ability to run multiple iterations in a short period of"": 'original_similarity': 0.8792838453562324: 'slide': 180}","{'chunk_text': ""not copy: the output of one qubit to the next layer of qubits. Using this fan-out Unitary (Uf{\\displaystyle U_{f}}) with a dummy state qubit in a known state (Ex. |0⟩{\\displaystyle |0\\rangle } in the computational basis): also known as an Ancilla bit: the information from the qubit can be transferred to the next layer of qubits. This process adheres to the quantum operation requirement of reversibility.Using this quantum feed-forward network: deep neural networks can be executed and trained efficiently. A deep neural network is essentially a network with many hidden-layers: as seen in the sample model neural network above. Since the Quantum neural network being discussed uses fan-out Unitary operators: and each operator only acts on its respective input: only two layers are used at any given time. In other words: no Unitary operator is acting on the entire network at any given time: meaning the number of qubits required for a given step depends on the number of inputs in a given layer. Since Quantum Computers are notorious for their ability to run multiple iterations in a short period of time: the efficiency of a quantum neural network is solely dependent on the number of qubits in any given layer: and not on the depth of the network. === Cost functions === To determine the effectiveness of a neural network: a cost function is used: which essentially measures the proximity of the network's output to the expected or desired output. In a Classical Neural Network: the weights (w{\\displaystyle w}) and biases (b{\\displaystyle b}) at each step determine the outcome of the cost function C(w:b){\\displaystyle C(w:b)}. When training a Classical Neural network: the weights and biases are adjusted after each iteration: and given equation 1 below: where y(x){\\displaystyle y(x)} is the desired output and aout(x){\\displaystyle a^{\\text{out}}(x)} is the actual output: the cost function is optimized when C(w:b){\\displaystyle C(w:b)}= 0. For a quantum neural network: the cost function is determined by measuring the fidelity of the outcome state (ρout{\\displaystyle \\rho ^{\\text{out}}}) with the desired outcome state (ϕout{\\displaystyle \\phi ^{\\text{out}}}): seen in Equation 2 below. In this case: the Unitary operators are adjusted after each iteration: and the cost function is optimized when C = 1. Equation 1 C(w:b)=1N∑x||y(x)−aout(x)||2{\\displaystyle C(w:b)={1 \\over N}\\sum _{x}{||y(x)-a^{\\text{out}}(x)|| \\over 2}} Equation 2 C=1N∑xN⟨ϕout|ρout|ϕout⟩{\\displaystyle C={1 \\over N}\\sum _{x}^{N}{\\langle \\phi ^{\\text{out}}|\\rho ^{\\text{out}}|\\phi ^{\\text{out}}\\rangle }} == See also == Differentiable programming Optical neural network Holographic associative memory Quantum cognition Quantum machine learning == References == == External links == Recent review of quantum neural networks by M. Schuld: I. Sinayskiy and F. Petruccione Review of quantum neural networks by Wei Article by P. Gralewicz on the plausibility of quantum computing in biological neural networks Training a neural net to recognize images\nIn the"": 'original_similarity': 0.8431831929707924: 'slide': 0}","{'chunk_text': 'recognition and handwriting recognition. For example: the bi-directional and multi-dimensional long short-term memory (LSTM) of Graves et al. won three competitions in connected handwriting recognition in 2009 without any prior knowledge about the three languages to be learned.Ciresan and colleagues built the first pattern recognizers to achieve human-competitive/superhuman performance on benchmarks such as traffic sign recognition (IJCNN 2012). Radial basis function and wavelet networks were introduced in 2013. These can be shown to offer best approximation properties and have been applied in nonlinear system identification and classification applications.In 2014: the adversarial network principle was used in a generative adversarial network (GAN) by Ian Goodfellow et al. Here the adversarial network (discriminator) outputs a value between 1 and 0 depending on the likelihood of the first network\'s (generator) output is in a given set. This can be used to create realistic deepfakes. Excellent image quality is achieved by Nvidia\'s StyleGAN (2018) based on the Progressive GAN by Tero Karras: Timo Aila: Samuli Laine: and Jaakko Lehtinen. Here the GAN generator is grown from small to large scale in a pyramidal fashion. In 2015: Rupesh Kumar Srivastava: Klaus Greff: and Schmidhuber used the LSTM principle to create the Highway network: a feedforward neural network with hundreds of layers: much deeper than previous networks. 7 months later: Kaiming He: Xiangyu Zhang; Shaoqing Ren: and Jian Sun won the ImageNet 2015 competition with an open-gated or gateless Highway network variant called Residual neural network.In 2017: Ashish Vaswani et al. introduced the modern Transformer architecture in their paper ""Attention Is All You Need."" It combines this with a softmax operator and a projection matrix. Transformers have increasingly become the model of choice for natural language processing. Many modern large language models such as ChatGPT: GPT-4: and BERT use it. Transformers are also increasingly being used in computer vision.Ramenzanpour et al. showed in 2020 that analytical and computational techniques derived from statistical physics of disordered systems can be extended to large-scale problems: including machine learning: e.g.: to analyze the weight space of deep neural networks. == Models == ANNs began as an attempt to exploit the architecture of the human brain to perform tasks that conventional algorithms had little success with. They soon reoriented towards improving empirical results: abandoning attempts to remain true to their biological precursors. ANNs have the ability to learn and model non-linearities and complex relationships. This is achieved by neurons being connected in various patterns: allowing the output of some neurons to become the input of others. The network forms a directed: weighted graph.An artificial neural network consists of simulated neurons. Each neuron is connected to other nodes via links like a biological axon-synapse-dendrite connection. All the nodes connected by links take in some data and use it to perform specific operations and tasks on the data. Each link has a weight: determining the strength of one node\'s influence on another: allowing weights to choose the signal between neurons. === Artificial neurons === ANNs are composed of artificial neurons which are conceptually derived from biological neurons. Each artificial neuron has inputs and produces a single output which can be sent to multiple other neurons. The inputs can be the feature values of a sample of external data: such as images or documents: or they can be the outputs of other neurons. The outputs of the final output neurons of the neural net accomplish the task: such as recognizing an object in an image. To find the output of the neuron we take the weighted sum of all the inputs: weighted by the weights of the connections from the inputs to the neuron. We add a bias term to this sum. This weighted sum is sometimes called the activation. This weighted sum is then passed through a (usually nonlinear) activation function to produce the output. The initial inputs are external data: such as images and documents. The ultimate outputs accomplish the task: such as recognizing an object in an image. === Organization === The neurons are typically organized into multiple layers: especially in deep learning. Neurons of one layer connect only to neurons of the immediately preceding and immediately following layers. The layer that receives external data is the input layer. The layer that produces the ultimate result is the output layer. In between them are zero or more hidden layers. Single layer and unlayered networks are also used. Between two layers: multiple connection patterns are possible. They can be \'fully connected\': with every neuron in one layer connecting to every neuron in the next layer. They can be pooling: where a group of neurons in one layer connects to a single neuron in the next layer: thereby reducing the number of neurons in that layer. Neurons with only such connections form a directed acyclic graph and are known as feedforward networks. Alternatively: networks that allow connections between neurons in the same or previous layers are known as recurrent networks. === Hyperparameter === A hyperparameter is a constant parameter whose value is set before the learning process begins. The values of parameters are derived via learning. Examples of hyperparameters include learning rate: the number of hidden layers and batch size. The values of some hyperparameters can be dependent on those of other\nhyperparameters. For example: the size of some layers can depend on the overall number of layers. === Learning === Learning is the adaptation of the network to better handle a task by considering sample observations. Learning involves adjusting the weights (and optional thresholds) of the network to improve the accuracy of the result. This is done by minimizing the observed errors. Learning is complete when examining additional observations does not usefully reduce the error rate. Even after learning: the error rate typically does not reach 0. If after learning: the error rate is too high: the network typically must be redesigned. Practically this is done by defining a cost function that is evaluated periodically during learning. As long as its output continues to decline: learning continues. The cost is frequently defined as': 'original_similarity': 0.832797434854968: 'slide': 130}"
0.41108009999152273,How is the training method of classical and quantum neural networks different?,"Quantum Neural Networks can be theoretically trained similarly to training classical/artificial neural networks. The key difference lies in the communication between the layers of a neural network. In a quantum neural network, this is done by replacing the classical fan-out method with an arbitrary unitary that spreads out, but does not copy, the output of one qubit to the next layer of qubits. This process adheres to the quantum operation requirement of reversibility.","{'chunk_text': ""to the final layer of qubits. The layers do not have to be of the same width: meaning they don't have to have the same number of qubits as the layer before or after it. This structure is trained on which path to take similar to classical artificial neural networks. This is discussed in a lower section. Quantum neural networks refer to three different categories: Quantum computer with classical data: classical computer with quantum data: and quantum computer with quantum data. == Examples == Quantum neural network research is still in its infancy: and a conglomeration of proposals and ideas of varying scope and mathematical rigor have been put forward. Most of them are based on the idea of replacing classical binary or McCulloch-Pitts neurons with a qubit (which can be called a “quron”): resulting in neural units that can be in a superposition of the state ‘firing’ and ‘resting’. === Quantum perceptrons === A lot of proposals attempt to find a quantum equivalent for the perceptron unit from which neural nets are constructed. A problem is that nonlinear activation functions do not immediately correspond to the mathematical structure of quantum theory: since a quantum evolution is described by linear operations and leads to probabilistic observation. Ideas to imitate the perceptron activation function with a quantum mechanical formalism reach from special measurements to postulating non-linear quantum operators (a mathematical framework that is disputed). A direct implementation of the activation function using the circuit-based model of quantum computation has recently been proposed by Schuld: Sinayskiy and Petruccione based on the quantum phase estimation algorithm. === Quantum networks === At a larger scale: researchers have attempted to generalize neural networks to the quantum setting. One way of constructing a quantum neuron is to first generalise classical neurons and then generalising them further to make unitary gates. Interactions between neurons can be controlled quantumly: with unitary gates: or classically: via measurement of the network states. This high-level theoretical technique can be applied broadly: by taking different types of networks and different implementations of quantum neurons: such as photonically implemented neurons and quantum reservoir processor (quantum version of reservoir computing). Most learning algorithms follow the classical model of training an artificial neural network to learn the input-output function of a given training set and use classical feedback loops to update parameters of the quantum system until they converge to an optimal configuration. Learning as a parameter optimisation problem has also been approached by adiabatic models of quantum computing.Quantum neural networks can be applied to algorithmic design: given qubits with tunable mutual interactions: one can attempt to learn interactions following the classical backpropagation rule from a training set of desired input-output relations: taken to be the desired output algorithm's behavior. The quantum network thus ‘learns’ an algorithm. === Quantum associative memory === The first quantum associative memory algorithm was introduced by Dan Ventura and Tony Martinez in 1999. The authors do not attempt to translate the structure of artificial neural network models into quantum theory: but propose an algorithm for a circuit-based quantum computer that simulates associative memory. The memory states (in Hopfield neural networks saved in the weights of the neural connections) are written into a superposition: and a Grover-like quantum search algorithm retrieves the memory state closest to a given input. As such: this is not a fully content-addressable memory: since only incomplete patterns can be retrieved. The first truly content-addressable quantum memory: which can retrieve patterns also from corrupted inputs: was proposed by Carlo A. Trugenberger. Both memories can store an exponential (in terms of n qubits) number of patterns but can be used only once due to the no-cloning theorem and their destruction upon measurement. Trugenberger: however: has shown that his proababilistic model of quantum associative memory can be efficiently implemented and re-used multiples times for any polynomial number of stored patterns: a large advantage with respect to classical associative memories. === Classical neural networks inspired by quantum theory === A substantial amount of interest has been given to a “quantum-inspired” model that uses ideas from quantum theory to implement a neural network based on fuzzy logic. == Training == Quantum Neural Networks can be theoretically trained similarly to training classical/artificial neural networks. A key difference lies in communication between the layers of a neural networks. For classical neural networks: at the end of a given operation: the current perceptron copies its output to the next layer of perceptron(s) in the network. However: in a quantum neural network: where each perceptron is a qubit: this would violate the no-cloning theorem. A proposed generalized solution to this is to replace the classical fan-out method with an arbitrary unitary that spreads out: but\ndoes not copy: the output of one qubit to the next layer of qubits. Using this fan-out Unitary (Uf{\\displaystyle U_{f}}) with a dummy state qubit in a known state (Ex. |0⟩{\\displaystyle |0\\rangle } in the computational basis): also known as an Ancilla bit: the information from the qubit can be transferred to the next layer of qubits. This process adheres to the quantum operation requirement of reversibility.Using this quantum feed-forward network: deep neural networks can be executed and trained efficiently. A deep neural network is essentially a network with many hidden-layers: as seen in the sample model neural network above. Since the Quantum neural network being discussed uses fan-out Unitary operators: and each operator only acts on its respective input: only two layers are used at any given time. In other words: no Unitary operator is acting on the entire network at any given time: meaning the number of qubits required for a given step depends on the number of inputs in a given layer. Since Quantum Computers are notorious for their ability to run multiple iterations in a short period of time: the efficiency of a quantum neural network is solely dependent on the number of qubits in any given layer: and not on the depth of the network. === Cost functions === To determine the effectiveness of a neural"": 'original_similarity': 0.8599495590567985: 'slide': 219}","{'chunk_text': ""output of one qubit to the next layer of qubits. Using this fan-out Unitary (Uf{\\displaystyle U_{f}}) with a dummy state qubit in a known state (Ex. |0⟩{\\displaystyle |0\\rangle } in the computational basis): also known as an Ancilla bit: the information from the qubit can be transferred to the next layer of qubits. This process adheres to the quantum operation requirement of reversibility.Using this quantum feed-forward network: deep neural networks can be executed and trained efficiently. A deep neural network is essentially a network with many hidden-layers: as seen in the sample model neural network above. Since the Quantum neural network being discussed uses fan-out Unitary operators: and each operator only acts on its respective input: only two layers are used at any given time. In other words: no Unitary operator is acting on the entire network at any given time: meaning the number of qubits required for a given step depends on the number of inputs in a given layer. Since Quantum Computers are notorious for their ability to run multiple iterations in a short period of time: the efficiency of a quantum neural network is solely dependent on the number of qubits in any given layer: and not on the depth of the network. === Cost functions === To determine the effectiveness of a neural network: a cost function is used: which essentially measures the proximity of the network's output to the expected or desired output. In a Classical Neural Network: the weights (w{\\displaystyle w}) and biases (b{\\displaystyle b}) at each step determine the outcome of the cost function C(w:b){\\displaystyle C(w:b)}. When training a Classical Neural network: the weights and biases are adjusted after each iteration: and given equation 1 below: where y(x){\\displaystyle y(x)} is the desired output and aout(x){\\displaystyle a^{\\text{out}}(x)} is the actual output: the cost function is optimized when C(w:b){\\displaystyle C(w:b)}= 0. For a quantum neural network: the cost function is determined by measuring the fidelity of the outcome state (ρout{\\displaystyle \\rho ^{\\text{out}}}) with the desired outcome state (ϕout{\\displaystyle \\phi ^{\\text{out}}}): seen in Equation 2 below. In this case: the Unitary operators are adjusted after each iteration: and the cost function is optimized when C = 1. Equation 1 C(w:b)=1N∑x||y(x)−aout(x)||2{\\displaystyle C(w:b)={1 \\over N}\\sum _{x}{||y(x)-a^{\\text{out}}(x)|| \\over 2}} Equation 2 C=1N∑xN⟨ϕout|ρout|ϕout⟩{\\displaystyle C={1 \\over N}\\sum _{x}^{N}{\\langle \\phi ^{\\text{out}}|\\rho ^{\\text{out}}|\\phi ^{\\text{out}}\\rangle }} == See also == Differentiable programming Optical neural network Holographic associative memory Quantum cognition Quantum machine learning == References == == External links == Recent review of quantum neural networks by M. Schuld: I. Sinayskiy and F. Petruccione Review of quantum neural networks by Wei Article by P. Gralewicz on the plausibility of quantum computing in biological neural networks Training a neural net to recognize images\nIn the context of artificial"": 'original_similarity': 0.8416785138794977: 'slide': 3}","{'chunk_text': 'can then be used to calculate the confidence interval of network output: assuming a normal distribution. A confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified. By assigning a softmax activation function: a generalization of the logistic function: on the output layer of the neural network (or a softmax component in a component-based network) for categorical target variables: the outputs can be interpreted as posterior probabilities. This is useful in classification as it gives a certainty measure on classifications. The softmax activation function is: yi=exi∑j=1cexj{\\displaystyle y_{i}={\\frac {e^{x_{i}}}{\\sum _{j=1}^{c}e^{x_{j}}}}} == Criticism == === Training === A common criticism of neural networks: particularly in robotics: is that they require too many training samples for real-world operation. Any learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases. Potential solutions include randomly shuffling training examples: by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example: grouping examples in so-called mini-batches and/or introducing a recursive least squares algorithm for CMAC. Dean Pomerleau uses a neural network to train a robotic vehicle to drive on multiple types of roads (single lane: multi-lane: dirt: etc.): and a large amount of his research is devoted to extrapolating multiple training scenarios from a single training experience: and preserving past training diversity so that the system does not become overtrained (if: for example: it is presented with a series of right turns—it should not learn to always turn right). === Theory === A central claim of ANNs is that they embody new and powerful general principles for processing information. These principles are ill-defined. It is often claimed that they are emergent from the network itself. This allows simple statistical association (the basic function of artificial neural networks) to be described as learning or recognition. In 1997: Alexander Dewdney: a former Scientific American columnist: commented that as a result: artificial neural networks have a ""something-for-nothing quality: one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are. No human hand (or mind) intervenes; solutions are found as if by magic; and no one: it seems: has learned anything"". One response to Dewdney is that neural networks have been successfully used to handle many complex and diverse tasks: ranging from autonomously flying aircraft to detecting credit card fraud to mastering the game of Go. Technology writer Roger Bridgman commented: Neural networks: for instance: are in the dock not only because they have been hyped to high heaven: (what hasn\'t?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be ""an opaque: unreadable table...valueless as a scientific resource"". In spite of his emphatic declaration that science is not technology: Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having. Although it is true that analyzing what has been learned by an artificial neural network is difficult: it is much easier to do so than to analyze what has been learned by a biological neural network. Moreover: recent emphasis on the explainability of AI has contributed towards the development of methods: notably those based on attention mechanisms: for visualizing and explaining learned neural networks. Furthermore: researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles that allow a learning machine to be successful. For example: Bengio and LeCun (2007) wrote an article regarding local vs non-local learning: as well as shallow vs deep architecture.Biological brains use both shallow and deep circuits as reported by brain anatomy: displaying a wide variety of invariance. Weng argued that the brain self-wires largely according to signal statistics and therefore: a serial cascade cannot catch all major statistical dependencies. === Hardware === Large and effective neural networks require considerable computing resources. While the brain has hardware tailored to the task of processing signals through a graph of neurons: simulating even a simplified neuron on von Neumann architecture may consume vast amounts of memory and storage. Furthermore: the designer often needs to transmit signals through many of these connections and their associated neurons – which require enormous CPU power and time. Schmidhuber noted that the resurgence of neural networks in the twenty-first century is largely attributable to advances in hardware: from 1991 to 2015: computing power: especially as delivered by GPGPUs (on GPUs): has increased around a million-fold: making the standard backpropagation algorithm feasible for training networks that are several layers deeper than before. The use of accelerators such as FPGAs and GPUs can reduce training times from months to days.Neuromorphic engineering or a physical neural network addresses the hardware difficulty directly: by constructing non-von-Neumann chips to directly implement neural networks\nin circuitry. Another type of chip optimized for neural network processing is called a Tensor Processing Unit: or TPU. === Practical counterexamples === Analyzing what has been learned by an ANN is much easier than analyzing what has been learned by a biological neural network. Furthermore: researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a learning machine to be successful. For example: local vs. non-local learning and shallow vs. deep architecture. === Hybrid approaches === Advocates of hybrid models (combining neural networks and symbolic approaches) say that such a mixture can better capture the mechanisms of the human mind. === Dataset bias === Neural networks are dependent on the quality of the data they are trained on: thus low quality data with imbalanced representativeness can lead to the model learning and perpetuating societal biases. These inherited biases become especially critical when the ANNs are integrated into real-world scenarios where the training data may be': 'original_similarity': 0.82492654224299: 'slide': 160}"
0.48717489998671226,What is the ReLU activation function in artificial neural networks?,"The ReLU (rectified linear unit) activation function in artificial neural networks is defined as the positive part of its argument. It's also known as a ramp function, analogous to half-wave rectification in electrical engineering. This activation function was introduced by Kunihiko Fukushima in 1969. ","{'chunk_text': 'has strong biological motivations and mathematical justifications. In 2011 it was found to enable better training of deeper networks: compared to the widely used activation functions prior to 2011: e.g.: the logistic sigmoid (which is inspired by probability theory; see logistic regression) and its more practical counterpart: the hyperbolic tangent. The rectifier is: as of 2017: the most popular activation function for deep neural networks.Rectified linear units find applications in computer vision and speech recognition using deep neural nets and computational neuroscience. == Advantages == Sparse activation: For example: in a randomly initialized network: only about 50% of hidden units are activated (have a non-zero output). Better gradient propagation: Fewer vanishing gradient problems compared to sigmoidal activation functions that saturate in both directions. Efficient computation: Only comparison: addition and multiplication. Scale-invariant: max(0:ax)=amax(0:x) for a≥0{\\displaystyle \\max(0:ax)=a\\max(0:x){\\text{ for }}a\\geq 0}.Rectifying activation functions were used to separate specific excitation and unspecific inhibition in the neural abstraction pyramid: which was trained in a supervised way to learn several computer vision tasks. In 2011: the use of the rectifier as a non-linearity has been shown to enable training deep supervised neural networks without requiring unsupervised pre-training. Rectified linear units: compared to sigmoid function or similar activation functions: allow faster and effective training of deep neural architectures on large and complex datasets. == Potential problems == Non-differentiable at zero; however: it is differentiable anywhere else: and the value of the derivative at zero can be arbitrarily chosen to be 0 or 1. Not zero-centered. Unbounded. Dying ReLU problem: ReLU (rectified linear unit) neurons can sometimes be pushed into states in which they become inactive for essentially all inputs. In this state: no gradients flow backward through the neuron: and so the neuron becomes stuck in a perpetually inactive state and ""dies"". This is a form of the vanishing gradient problem. In some cases: large numbers of neurons in a network can become stuck in dead states: effectively decreasing the model capacity. This problem typically arises when the learning rate is set too high. It may be mitigated by using leaky ReLUs instead: which assign a small positive slope for x < 0; however: the performance is reduced. == Variants == === Piecewise-linear variants === ==== Leaky ReLU ==== Leaky ReLUs allow a small: positive gradient when the unit is not active: helping to mitigate the vanishing gradient problem. f(x)={xif x>0:0.01xotherwise.f′(x)={1if x>0:0.01otherwise.{\\displaystyle f(x)={\\begin{cases}x&{\\text{if }}x>0:\\\\0.01x&{\\text{otherwise}}.\\end{cases}}\\qquad \\qquad f\'(x)={\\begin{cases}1&{\\text{if }}x>0:\\\\0.01&{\\text{otherwise}}.\\end{cases}}} ==== Parametric ReLU ==== Parametric ReLUs (PReLUs) take this idea further by making the coefficient of leakage into a parameter that is learned along with the other neural-network parameters. f(x)={xif x>0:a⋅xotherwise.f′(x)={1if x>0:aotherwise.{\\displaystyle f(x)={\\begin{cases}x&{\\text{if }}x>0:\\\\a\\cdot x&{\\text{otherwise}}.\\end{cases}}\\qquad \\qquad \\qquad f\'(x)={\\begin{cases}1&{\\text{if }}x>0:\\\\a&{\\text{otherwise}}.\\end{cases}}}Note that for a ≤ 1: this is equivalent to f(x)=max(x:ax){\\displaystyle f(x)=\\max(x:ax)}and thus has a relation to ""maxout"" networks. === Other non-linear variants === ==== Gaussian-error linear unit (GELU) ==== GELU is a smooth approximation to the rectifier: f(x)=x⋅Φ(x):{\\displaystyle f(x)=x\\cdot \\Phi (x):}f′(x)=x⋅Φ′(x)+Φ(x):{\\displaystyle f\'(x)=x\\cdot \\Phi \'(x)+\\Phi (x):}where Φ(x)=P(X⩽x){\\displaystyle \\Phi (x)=P(X\\leqslant x)} is the cumulative distribution function of the standard normal distribution. This activation function is illustrated in the figure at the start of this article. It has a ""bump"" to the left of x < 0 and serves as the default activation for models such as BERT. ==== SiLU ==== The SiLU (sigmoid linear unit) or swish function is another smooth approximation: first coined in the GELU paper: f(x)=x⋅sigmoid\u2061(x):{\\displaystyle f(x)=x\\cdot \\operatorname {sigmoid} (x):}f′(x)=x⋅sigmoid′\u2061(x)+sigmoid\u2061(x):{\\displaystyle f\'(x)=x\\cdot \\operatorname {sigmoid} \'(x)+\\operatorname {sigmoid} (x):}where sigmoid\u2061(x){\\displaystyle \\operatorname {sigmoid} (x)} is the sigmoid function. ==== Softplus ==== A smooth approximation to the rectifier is the analytic function f(x)=ln\u2061(1+ex):f′(x)=ex1+ex=11+e−x:{\\displaystyle f(x)=\\ln(1+e^{x}):\\qquad \\qquad f\'(x)={\\frac {e^{x}}{1+e^{x}}}={\\frac {1}{1+e^{-x}}}:}which is called the softplus or SmoothReLU function. For large negative x{\\displaystyle x} it is roughly ln\u20611{\\displaystyle \\ln 1}: so just above 0: while for large positive x{\\displaystyle x} it is roughly ln\u2061(ex){\\displaystyle \\ln(e^{x})}: so just above x{\\displaystyle x}. This function can be approximated as: ln\u2061(1+ex)≈{ln\u20612:x=0:x1−e−x/ln\u20612:x≠0{\\displaystyle \\ln \\left(1+e^{x}\\right)\\approx {\\begin{cases}\\ln 2:&x=0:\\\\[6pt]{\\frac {x}{1-e^{-x/\\ln 2}}}:&x\\neq 0\\end{cases}}}By making the change of variables x=yln\u2061(2){\\displaystyle x=y\\ln(2)}: this is equivalent to log2\u2061(1+2y)≈{1:y=0:y1−e−y:y≠0.{\\displaystyle \\log _{2}(1+2^{y})\\approx {\\begin{cases}1:&y=0:\\\\[6pt]{\\frac {y}{1-e^{-y}}}:&y\\neq 0.\\end{cases}}}A sharpness parameter k{\\displaystyle k} may be included: f(x)=ln\u2061(1+ekx)k:f′(x)=ekx1+ekx=11+e−kx.{\\displaystyle f(x)={\\frac {\\ln(1+e^{kx})}{k}}:\\qquad \\qquad f\'(x)={\\frac {e^{kx}}{1+e^{kx}}}={\\frac {1}{1+e^{-kx}}}.}The derivative of softplus is the logistic function. The logistic sigmoid function is a smooth approximation of the derivative of the rectifier: the Heaviside step function. The multivariable generalization of single-variable softplus is the LogSumExp with the first argument set to zero: LSE0+\u2061(x1:…:xn):=LSE\u2061(0:x1:…:xn)=ln\u2061(1+ex1+⋯+exn).{\\displaystyle \\operatorname {LSE_{0}} ^{+}(x_{1}:\\dots :x_{n}):=\\operatorname {LSE} (0:x_{1}:\\dots :x_{n})=\\ln(1+e^{x_{1}}+\\cdots +e^{x_{n}}).}The LogSumExp function is LSE\u2061(x1:…:xn)=ln\u2061(ex1+⋯+exn):{\\displaystyle \\operatorname {LSE} (x_{1}:\\dots :x_{n})=\\ln(e^{x_{1}}+\\cdots +e^{x_{n}}):}and its gradient is the softmax; the softmax with the first argument set to zero is the multivariable generalization of the logistic function. Both LogSumExp and softmax are used in machine learning. ==== ELU ==== Exponential linear units try to make the mean activations closer to zero: which speeds up learning. It has been shown that ELUs can obtain higher classification accuracy than ReLUs. f(x)={xif x>0:a(ex−1)otherwise.f′(x)={1if x>0:a⋅exotherwise.{\\displaystyle f(x)={\\begin{cases}x&{\\text{if }}x>0:\\\\a\\left(e^{x}-1\\right)&{\\text{otherwise}}.\\end{cases}}\\qquad \\qquad f\'(x)={\\begin{cases}1&{\\text{if }}x>0:\\\\a\\cdot e^{x}&{\\text{otherwise}}.\\end{cases}}}In these formulas: a{\\displaystyle a} is a hyper-parameter to be tuned with the constraint a≥0{\\displaystyle a\\geq 0}. The ELU can be viewed as a smoothed version of a shifted ReLU (SReLU): which has the form f(x)=max(−a:x){\\displaystyle f(x)=\\max(-a:x)}: given the same interpretation of a{\\displaystyle a}. ==== Mish ==== The mish function can also be used as a smooth approximation of the rectifier. It is defined as f(x)=xtanh\u2061(softplus\u2061(x)):{\\displaystyle f(x)=x\\tanh {\\big (}\\operatorname {softplus} (x){\\big )}:}where tanh\u2061(x){\\displaystyle \\tanh(x)} is the hyperbolic tangent: and softplus\u2061(x){\\displaystyle \\operatorname {softplus} (x)} is the softplus function. Mish is non-monotonic and self-gated. It was inspired by Swish: itself a variant of ReLU. ==== Squareplus ==== Squareplus is the function squareplusb\u2061(x)=x+x2+b2{\\displaystyle\n\\operatorname {squareplus} _{b}(x)={\\frac {x+{\\sqrt {x^{2}+b}}}{2}}}where b≥0{\\displaystyle b\\geq 0} is a hyperparameter that determines the ""size"" of the curved region near x=0{\\displaystyle x=0}. (For example: letting b=0{\\displaystyle b=0} yields ReLU: and letting b=4{\\displaystyle b=4} yields the metallic mean function.) Squareplus shares many properties with softplus: It is monotonic: strictly positive: approaches 0 as x→−∞{\\displaystyle x\\to -\\infty }: approaches the identity as x→+∞{\\displaystyle x\\to +\\infty }: and is C∞{\\displaystyle C^{\\infty }} smooth. However: squareplus can be computed using only algebraic functions: making it well-suited for settings where': 'original_similarity': 0.8937131744537027: 'slide': 83}","{'chunk_text': 'done via fully connected layers. Neurons in a fully connected layer have connections to all activations in the previous layer: as seen in regular (non-convolutional) artificial neural networks. Their activations can thus be computed as an affine transformation: with matrix multiplication followed by a bias offset (vector addition of a learned or fixed bias term). === Loss layer === The ""loss layer"": or ""loss function"": specifies how training penalizes the deviation between the predicted output of the network: and the true data labels (during supervised learning). Various loss functions can be used: depending on the specific task. The Softmax loss function is used for predicting a single class of K mutually exclusive classes. Sigmoid cross-entropy loss is used for predicting K independent probability values in [0:1]{\\displaystyle [0:1]}. Euclidean loss is used for regressing to real-valued labels (−∞:∞){\\displaystyle (-\\infty :\\infty )}. == Hyperparameters == Hyperparameters are various settings that are used to control the learning process. CNNs use more hyperparameters than a standard multilayer perceptron (MLP). === Kernel size === The kernel is the number of pixels processed together. It is typically expressed as the kernel\'s dimensions: e.g.: 2x2: or 3x3. === Padding === Padding is the addition of (typically) 0-valued pixels on the borders of an image. This is done so that the border pixels are not undervalued (lost) from the output because they would ordinarily participate in only a single receptive field instance. The padding applied is typically one less than the corresponding kernel dimension. For example: a convolutional layer using 3x3 kernels would receive a 2-pixel pad: that is 1 pixel on each side of the image. === Stride === The stride is the number of pixels that the analysis window moves on each iteration. A stride of 2 means that each kernel is offset by 2 pixels from its predecessor. === Number of filters === Since feature map size decreases with depth: layers near the input layer tend to have fewer filters while higher layers can have more. To equalize computation at each layer: the product of feature values va with pixel position is kept roughly constant across layers. Preserving more information about the input would require keeping the total number of activations (number of feature maps times number of pixel positions) non-decreasing from one layer to the next. The number of feature maps directly controls the capacity and depends on the number of available examples and task complexity. === Filter size === Common filter sizes found in the literature vary greatly: and are usually chosen based on the data set. The challenge is to find the right level of granularity so as to create abstractions at the proper scale: given a particular data set: and without overfitting. === Pooling type and size === Max pooling is typically used: often with a 2x2 dimension. This implies that the input is drastically downsampled: reducing processing cost. Greater pooling reduces the dimension of the signal: and may result in unacceptable information loss. Often: non-overlapping pooling windows perform best. === Dilation === Dilation involves ignoring pixels within a kernel. This reduces processing/memory potentially without significant signal loss. A dilation of 2 on a 3x3 kernel expands the kernel to 5x5: while still processing 9 (evenly spaced) pixels. Accordingly: dilation of 4 expands the kernel to 7x7. == Translation equivariance and aliasing == It is commonly assumed that CNNs are invariant to shifts of the input. Convolution or pooling layers within a CNN that do not have a stride greater than one are indeed equivariant to translations of the input. However: layers with a stride greater than one ignore the Nyquist-Shannon sampling theorem and might lead to aliasing of the input signal While: in principle: CNNs are capable of implementing anti-aliasing filters: it has been observed that this does not happen in practice and yield models that are not equivariant to translations. Furthermore: if a CNN makes use of fully connected layers: translation equivariance does not imply translation invariance: as the fully connected layers are not invariant to shifts of the input. One solution for complete translation invariance is avoiding any down-sampling throughout the network and applying global average pooling at the last layer. Additionally: several other partial solutions have been proposed: such as anti-aliasing before downsampling operations: spatial transformer networks: data augmentation: subsampling combined with pooling: and capsule neural networks. == Evaluation == The accuracy of the final model is based on a sub-part of the dataset set apart at the start: often called a test-set. Other times methods such as k-fold cross-validation are applied. Other strategies include using conformal prediction. == Regularization methods == Regularization is a process of introducing additional information to solve an ill-posed problem or to prevent overfitting. CNNs use various types of regularization. === Empirical === ==== Dropout ==== Because a fully connected layer occupies most of the parameters: it is prone to overfitting. One method to reduce overfitting is dropout: introduced in 2014. At each training stage: individual nodes are either ""dropped out"" of the net (ignored) with probability 1−p{\\displaystyle 1-p} or kept with probability p{\\displaystyle p}: so that a reduced network is left; incoming and outgoing edges to a dropped-out node are also removed. Only the reduced network is trained on the data in that stage. The removed nodes are then reinserted into the network with their original weights. In the training stages: p{\\displaystyle p} is usually 0.5; for input nodes: it is typically much higher because information is directly lost when input nodes are ignored. At testing time after training has finished: we would ideally\nlike to find a sample average of all possible 2n{\\displaystyle 2^{n}} dropped-out networks; unfortunately this is unfeasible for large values of n{\\displaystyle n}. However: we can find an approximation by using the full network with each node\'s output weighted by a factor of p{\\displaystyle p}: so the expected value of the output of any node is the same as in the training stages. This is the biggest contribution of the dropout method: although it effectively generates 2n{\\displaystyle 2^{n}}': 'original_similarity': 0.8560136545052291: 'slide': 76}","{'chunk_text': 'utilizing weight sharing in combination with backpropagation training. Thus: while also using a pyramidal structure as in the neocognitron: it performed a global optimization of the weights instead of a local one.In 1988: Wei Zhang et al. applied backpropagation to a CNN (a simplified Neocognitron with convolutional interconnections between the image feature layers and the last fully connected layer) for alphabet recognition. They also proposed an implementation of the CNN with an optical computing system.In 1989: Yann LeCun et al. trained a CNN with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked: training required 3 days. Learning was fully automatic: performed better than manual coefficient design: and was suited to a broader range of image recognition problems and image types. Subsequently: Wei Zhang: et al. modified their model by removing the last fully connected layer and applied it for medical image object segmentation in 1991 and breast cancer detection in mammograms in 1994.In 1990 Yamaguchi et al. introduced max-pooling: a fixed filtering operation that calculates and propagates the maximum value of a given region. They combined TDNNs with max-pooling in order to realize a speaker independent isolated word recognition system. In a variant of the neocognitron called the cresceptron: instead of using Fukushima\'s spatial averaging: J. Weng et al. also used max-pooling where a downsampling unit computes the maximum of the activations of the units in its patch. Max-pooling is often used in modern CNNs.LeNet-5: a 7-level CNN by Yann LeCun et al. in 1998: that classifies digits: was applied by several banks to recognize hand-written numbers on checks (British English: cheques) digitized in 32x32 pixel images. The ability to process higher-resolution images requires larger and more layers of CNNs: so this technique is constrained by the availability of computing resources. In 2010: Backpropagation training through max-pooling was accelerated by GPUs and shown to perform better than other pooling variants. Behnke (2003) relied only on the sign of the gradient (Rprop) on problems such as image reconstruction and face localization. Rprop is a first-order optimization algorithm created by Martin Riedmiller and Heinrich Braun in 1992.In 2011: a deep GPU-based CNN called ""DanNet"" by Dan Ciresan: Ueli Meier: and Juergen Schmidhuber achieved human-competitive performance for the first time in computer vision contests. Subsequently: a similar GPU-based CNN by Alex Krizhevsky: Ilya Sutskever: and Geoffrey Hinton won the ImageNet Large Scale Visual Recognition Challenge 2012. A very deep CNN with over 100 layers by Kaiming He: Xiangyu Zhang: Shaoqing Ren: and Jian Sun of Microsoft won the ImageNet 2015 contest.ANNs were able to guarantee shift invariance to deal with small and large natural objects in large cluttered scenes: only when invariance extended beyond shift: to all ANN-learned concepts: such as location: type (object class label): scale: lighting and others. This was realized in Developmental Networks (DNs) whose embodiments are Where-What Networks: WWN-1 (2008) through WWN-7 (2013). == Artificial curiosity and generative adversarial networks == In 1991: Juergen Schmidhuber published adversarial neural networks that contest with each other in the form of a zero-sum game: where one network\'s gain is the other network\'s loss. The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called ""artificial curiosity."" Earlier adversarial machine learning systems ""neither involved unsupervised neural networks nor were about modeling data nor used gradient descent.""In 2014: this adversarial principle was used in a generative adversarial network (GAN) by Ian Goodfellow et al. Here the environmental reaction is 1 or 0 depending on whether the first network\'s output is in a given set. This can be used to create realistic deepfakes.In 1992: Schmidhuber also published another type of gradient-based adversarial neural networks where the goal of the zero-sum game is to create disentangled representations of input patterns. This was called predictability minimization.Nvidia\'s StyleGAN (2018) is based on the Progressive GAN by Tero Karras: Timo Aila: Samuli Laine: and Jaakko Lehtinen. Here the GAN generator is grown from small to large scale in a pyramidal fashion. StyleGANs improve consistency between fine and coarse details in the generator network. == Transformers and their variants == Many modern large language models such as ChatGPT: GPT-4: and BERT use a feedforward neural network called Transformer by Ashish Vaswani et. al. in their 2017 paper ""Attention Is All You Need."" Transformers have increasingly become the model of choice for natural language processing problems: replacing recurrent neural networks (RNNs) such as long short-term memory (LSTM).Basic ideas for this go back a long way: in 1992: Juergen Schmidhuber published the Transformer with ""linearized self-attention"" (save for a normalization operator): which is also called the ""linear Transformer."" He advertised it as an ""alternative to RNNs"" that can learn ""internal spotlights of attention:"" and experimentally applied it to problems of variable binding. Here a slow feedforward neural network learns by gradient descent to control the fast weights of another neural network through outer products of self-generated activation patterns called ""FROM"" and ""TO"" which in Transformer terminology are called ""key"" and ""value"" for ""self-attention."" This fast weight ""attention mapping"" is applied to queries. The 2017 Transformer combines this with a softmax operator and a projection matrix.Transformers are\nalso increasingly being used in computer vision. == Deep learning with unsupervised or self-supervised pre-training == In the 1980s: backpropagation did not work well for deep FNNs and RNNs. Here the word ""deep"" refers to the number of layers through which the data is transformed. More precisely: deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For an FNN: the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For RNNs: in which a signal may propagate through a layer more than once: the CAP depth is potentially unlimited. To': 'original_similarity': 0.8558093384457108: 'slide': 126}"
0.5349820999836084,How does ReLU help in neural networks?,"ReLU helps in neural networks by enabling better training of deeper networks, compared to the widely used activation functions prior to 2011 like the logistic sigmoid and the hyperbolic tangent. ReLU has found applications in computer vision and speech recognition using deep neural nets and computational neuroscience.","{'chunk_text': 'to the widely used activation functions prior to 2011: e.g.: the logistic sigmoid (which is inspired by probability theory; see logistic regression) and its more practical counterpart: the hyperbolic tangent. The rectifier is: as of 2017: the most popular activation function for deep neural networks.Rectified linear units find applications in computer vision and speech recognition using deep neural nets and computational neuroscience. == Advantages == Sparse activation: For example: in a randomly initialized network: only about 50% of hidden units are activated (have a non-zero output). Better gradient propagation: Fewer vanishing gradient problems compared to sigmoidal activation functions that saturate in both directions. Efficient computation: Only comparison: addition and multiplication. Scale-invariant: max(0:ax)=amax(0:x) for a≥0{\\displaystyle \\max(0:ax)=a\\max(0:x){\\text{ for }}a\\geq 0}.Rectifying activation functions were used to separate specific excitation and unspecific inhibition in the neural abstraction pyramid: which was trained in a supervised way to learn several computer vision tasks. In 2011: the use of the rectifier as a non-linearity has been shown to enable training deep supervised neural networks without requiring unsupervised pre-training. Rectified linear units: compared to sigmoid function or similar activation functions: allow faster and effective training of deep neural architectures on large and complex datasets. == Potential problems == Non-differentiable at zero; however: it is differentiable anywhere else: and the value of the derivative at zero can be arbitrarily chosen to be 0 or 1. Not zero-centered. Unbounded. Dying ReLU problem: ReLU (rectified linear unit) neurons can sometimes be pushed into states in which they become inactive for essentially all inputs. In this state: no gradients flow backward through the neuron: and so the neuron becomes stuck in a perpetually inactive state and ""dies"". This is a form of the vanishing gradient problem. In some cases: large numbers of neurons in a network can become stuck in dead states: effectively decreasing the model capacity. This problem typically arises when the learning rate is set too high. It may be mitigated by using leaky ReLUs instead: which assign a small positive slope for x < 0; however: the performance is reduced. == Variants == === Piecewise-linear variants === ==== Leaky ReLU ==== Leaky ReLUs allow a small: positive gradient when the unit is not active: helping to mitigate the vanishing gradient problem. f(x)={xif x>0:0.01xotherwise.f′(x)={1if x>0:0.01otherwise.{\\displaystyle f(x)={\\begin{cases}x&{\\text{if }}x>0:\\\\0.01x&{\\text{otherwise}}.\\end{cases}}\\qquad \\qquad f\'(x)={\\begin{cases}1&{\\text{if }}x>0:\\\\0.01&{\\text{otherwise}}.\\end{cases}}} ==== Parametric ReLU ==== Parametric ReLUs (PReLUs) take this idea further by making the coefficient of leakage into a parameter that is learned along with the other neural-network parameters. f(x)={xif x>0:a⋅xotherwise.f′(x)={1if x>0:aotherwise.{\\displaystyle f(x)={\\begin{cases}x&{\\text{if }}x>0:\\\\a\\cdot x&{\\text{otherwise}}.\\end{cases}}\\qquad \\qquad \\qquad f\'(x)={\\begin{cases}1&{\\text{if }}x>0:\\\\a&{\\text{otherwise}}.\\end{cases}}}Note that for a ≤ 1: this is equivalent to f(x)=max(x:ax){\\displaystyle f(x)=\\max(x:ax)}and thus has a relation to ""maxout"" networks. === Other non-linear variants === ==== Gaussian-error linear unit (GELU) ==== GELU is a smooth approximation to the rectifier: f(x)=x⋅Φ(x):{\\displaystyle f(x)=x\\cdot \\Phi (x):}f′(x)=x⋅Φ′(x)+Φ(x):{\\displaystyle f\'(x)=x\\cdot \\Phi \'(x)+\\Phi (x):}where Φ(x)=P(X⩽x){\\displaystyle \\Phi (x)=P(X\\leqslant x)} is the cumulative distribution function of the standard normal distribution. This activation function is illustrated in the figure at the start of this article. It has a ""bump"" to the left of x < 0 and serves as the default activation for models such as BERT. ==== SiLU ==== The SiLU (sigmoid linear unit) or swish function is another smooth approximation: first coined in the GELU paper: f(x)=x⋅sigmoid\u2061(x):{\\displaystyle f(x)=x\\cdot \\operatorname {sigmoid} (x):}f′(x)=x⋅sigmoid′\u2061(x)+sigmoid\u2061(x):{\\displaystyle f\'(x)=x\\cdot \\operatorname {sigmoid} \'(x)+\\operatorname {sigmoid} (x):}where sigmoid\u2061(x){\\displaystyle \\operatorname {sigmoid} (x)} is the sigmoid function. ==== Softplus ==== A smooth approximation to the rectifier is the analytic function f(x)=ln\u2061(1+ex):f′(x)=ex1+ex=11+e−x:{\\displaystyle f(x)=\\ln(1+e^{x}):\\qquad \\qquad f\'(x)={\\frac {e^{x}}{1+e^{x}}}={\\frac {1}{1+e^{-x}}}:}which is called the softplus or SmoothReLU function. For large negative x{\\displaystyle x} it is roughly ln\u20611{\\displaystyle \\ln 1}: so just above 0: while for large positive x{\\displaystyle x} it is roughly ln\u2061(ex){\\displaystyle \\ln(e^{x})}: so just above x{\\displaystyle x}. This function can be approximated as: ln\u2061(1+ex)≈{ln\u20612:x=0:x1−e−x/ln\u20612:x≠0{\\displaystyle \\ln \\left(1+e^{x}\\right)\\approx {\\begin{cases}\\ln 2:&x=0:\\\\[6pt]{\\frac {x}{1-e^{-x/\\ln 2}}}:&x\\neq 0\\end{cases}}}By making the change of variables x=yln\u2061(2){\\displaystyle x=y\\ln(2)}: this is equivalent to log2\u2061(1+2y)≈{1:y=0:y1−e−y:y≠0.{\\displaystyle \\log _{2}(1+2^{y})\\approx {\\begin{cases}1:&y=0:\\\\[6pt]{\\frac {y}{1-e^{-y}}}:&y\\neq 0.\\end{cases}}}A sharpness parameter k{\\displaystyle k} may be included: f(x)=ln\u2061(1+ekx)k:f′(x)=ekx1+ekx=11+e−kx.{\\displaystyle f(x)={\\frac {\\ln(1+e^{kx})}{k}}:\\qquad \\qquad f\'(x)={\\frac {e^{kx}}{1+e^{kx}}}={\\frac {1}{1+e^{-kx}}}.}The derivative of softplus is the logistic function. The logistic sigmoid function is a smooth approximation of the derivative of the rectifier: the Heaviside step function. The multivariable generalization of single-variable softplus is the LogSumExp with the first argument set to zero: LSE0+\u2061(x1:…:xn):=LSE\u2061(0:x1:…:xn)=ln\u2061(1+ex1+⋯+exn).{\\displaystyle \\operatorname {LSE_{0}} ^{+}(x_{1}:\\dots :x_{n}):=\\operatorname {LSE} (0:x_{1}:\\dots :x_{n})=\\ln(1+e^{x_{1}}+\\cdots +e^{x_{n}}).}The LogSumExp function is LSE\u2061(x1:…:xn)=ln\u2061(ex1+⋯+exn):{\\displaystyle \\operatorname {LSE} (x_{1}:\\dots :x_{n})=\\ln(e^{x_{1}}+\\cdots +e^{x_{n}}):}and its gradient is the softmax; the softmax with the first argument set to zero is the multivariable generalization of the logistic function. Both LogSumExp and softmax are used in machine learning. ==== ELU ==== Exponential linear units try to make the mean activations closer to zero: which speeds up learning. It has been shown that ELUs can obtain higher classification accuracy than ReLUs. f(x)={xif x>0:a(ex−1)otherwise.f′(x)={1if x>0:a⋅exotherwise.{\\displaystyle f(x)={\\begin{cases}x&{\\text{if }}x>0:\\\\a\\left(e^{x}-1\\right)&{\\text{otherwise}}.\\end{cases}}\\qquad \\qquad f\'(x)={\\begin{cases}1&{\\text{if }}x>0:\\\\a\\cdot e^{x}&{\\text{otherwise}}.\\end{cases}}}In these formulas: a{\\displaystyle a} is a hyper-parameter to be tuned with the constraint a≥0{\\displaystyle a\\geq 0}. The ELU can be viewed as a smoothed version of a shifted ReLU (SReLU): which has the form f(x)=max(−a:x){\\displaystyle f(x)=\\max(-a:x)}: given the same interpretation of a{\\displaystyle a}. ==== Mish ==== The mish function can also be used as a smooth approximation of the rectifier. It is defined as f(x)=xtanh\u2061(softplus\u2061(x)):{\\displaystyle f(x)=x\\tanh {\\big (}\\operatorname {softplus} (x){\\big )}:}where tanh\u2061(x){\\displaystyle \\tanh(x)} is the hyperbolic tangent: and softplus\u2061(x){\\displaystyle \\operatorname {softplus} (x)} is the softplus function. Mish is non-monotonic and self-gated. It was inspired by Swish: itself a variant of ReLU. ==== Squareplus ==== Squareplus is the function squareplusb\u2061(x)=x+x2+b2{\\displaystyle\n\\operatorname {squareplus} _{b}(x)={\\frac {x+{\\sqrt {x^{2}+b}}}{2}}}where b≥0{\\displaystyle b\\geq 0} is a hyperparameter that determines the ""size"" of the curved region near x=0{\\displaystyle x=0}. (For example: letting b=0{\\displaystyle b=0} yields ReLU: and letting b=4{\\displaystyle b=4} yields the metallic mean function.) Squareplus shares many properties with softplus: It is monotonic: strictly positive: approaches 0 as x→−∞{\\displaystyle x\\to -\\infty }: approaches the identity as x→+∞{\\displaystyle x\\to +\\infty }: and is C∞{\\displaystyle C^{\\infty }} smooth. However: squareplus can be computed using only algebraic functions: making it well-suited for settings where computational resources or instruction sets are limited. Additionally: squareplus requires no special consideration to ensure numerical stability when x{\\displaystyle x}': 'original_similarity': 0.8559724904248166: 'slide': 103}","{'chunk_text': '(non-convolutional) artificial neural networks. Their activations can thus be computed as an affine transformation: with matrix multiplication followed by a bias offset (vector addition of a learned or fixed bias term). === Loss layer === The ""loss layer"": or ""loss function"": specifies how training penalizes the deviation between the predicted output of the network: and the true data labels (during supervised learning). Various loss functions can be used: depending on the specific task. The Softmax loss function is used for predicting a single class of K mutually exclusive classes. Sigmoid cross-entropy loss is used for predicting K independent probability values in [0:1]{\\displaystyle [0:1]}. Euclidean loss is used for regressing to real-valued labels (−∞:∞){\\displaystyle (-\\infty :\\infty )}. == Hyperparameters == Hyperparameters are various settings that are used to control the learning process. CNNs use more hyperparameters than a standard multilayer perceptron (MLP). === Kernel size === The kernel is the number of pixels processed together. It is typically expressed as the kernel\'s dimensions: e.g.: 2x2: or 3x3. === Padding === Padding is the addition of (typically) 0-valued pixels on the borders of an image. This is done so that the border pixels are not undervalued (lost) from the output because they would ordinarily participate in only a single receptive field instance. The padding applied is typically one less than the corresponding kernel dimension. For example: a convolutional layer using 3x3 kernels would receive a 2-pixel pad: that is 1 pixel on each side of the image. === Stride === The stride is the number of pixels that the analysis window moves on each iteration. A stride of 2 means that each kernel is offset by 2 pixels from its predecessor. === Number of filters === Since feature map size decreases with depth: layers near the input layer tend to have fewer filters while higher layers can have more. To equalize computation at each layer: the product of feature values va with pixel position is kept roughly constant across layers. Preserving more information about the input would require keeping the total number of activations (number of feature maps times number of pixel positions) non-decreasing from one layer to the next. The number of feature maps directly controls the capacity and depends on the number of available examples and task complexity. === Filter size === Common filter sizes found in the literature vary greatly: and are usually chosen based on the data set. The challenge is to find the right level of granularity so as to create abstractions at the proper scale: given a particular data set: and without overfitting. === Pooling type and size === Max pooling is typically used: often with a 2x2 dimension. This implies that the input is drastically downsampled: reducing processing cost. Greater pooling reduces the dimension of the signal: and may result in unacceptable information loss. Often: non-overlapping pooling windows perform best. === Dilation === Dilation involves ignoring pixels within a kernel. This reduces processing/memory potentially without significant signal loss. A dilation of 2 on a 3x3 kernel expands the kernel to 5x5: while still processing 9 (evenly spaced) pixels. Accordingly: dilation of 4 expands the kernel to 7x7. == Translation equivariance and aliasing == It is commonly assumed that CNNs are invariant to shifts of the input. Convolution or pooling layers within a CNN that do not have a stride greater than one are indeed equivariant to translations of the input. However: layers with a stride greater than one ignore the Nyquist-Shannon sampling theorem and might lead to aliasing of the input signal While: in principle: CNNs are capable of implementing anti-aliasing filters: it has been observed that this does not happen in practice and yield models that are not equivariant to translations. Furthermore: if a CNN makes use of fully connected layers: translation equivariance does not imply translation invariance: as the fully connected layers are not invariant to shifts of the input. One solution for complete translation invariance is avoiding any down-sampling throughout the network and applying global average pooling at the last layer. Additionally: several other partial solutions have been proposed: such as anti-aliasing before downsampling operations: spatial transformer networks: data augmentation: subsampling combined with pooling: and capsule neural networks. == Evaluation == The accuracy of the final model is based on a sub-part of the dataset set apart at the start: often called a test-set. Other times methods such as k-fold cross-validation are applied. Other strategies include using conformal prediction. == Regularization methods == Regularization is a process of introducing additional information to solve an ill-posed problem or to prevent overfitting. CNNs use various types of regularization. === Empirical === ==== Dropout ==== Because a fully connected layer occupies most of the parameters: it is prone to overfitting. One method to reduce overfitting is dropout: introduced in 2014. At each training stage: individual nodes are either ""dropped out"" of the net (ignored) with probability 1−p{\\displaystyle 1-p} or kept with probability p{\\displaystyle p}: so that a reduced network is left; incoming and outgoing edges to a dropped-out node are also removed. Only the reduced network is trained on the data in that stage. The removed nodes are then reinserted into the network with their original weights. In the training stages: p{\\displaystyle p} is usually 0.5; for input nodes: it is typically much higher because information is directly lost when input nodes are ignored. At testing time after training has finished: we would ideally\nlike to find a sample average of all possible 2n{\\displaystyle 2^{n}} dropped-out networks; unfortunately this is unfeasible for large values of n{\\displaystyle n}. However: we can find an approximation by using the full network with each node\'s output weighted by a factor of p{\\displaystyle p}: so the expected value of the output of any node is the same as in the training stages. This is the biggest contribution of the dropout method: although it effectively generates 2n{\\displaystyle 2^{n}} neural nets: and as such allows for model combination: at test time only a single network needs to be tested. By avoiding training all': 'original_similarity': 0.8470214481682488: 'slide': 100}","{'chunk_text': 'et al. applied backpropagation to a CNN (a simplified Neocognitron with convolutional interconnections between the image feature layers and the last fully connected layer) for alphabet recognition. They also proposed an implementation of the CNN with an optical computing system.In 1989: Yann LeCun et al. trained a CNN with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked: training required 3 days. Learning was fully automatic: performed better than manual coefficient design: and was suited to a broader range of image recognition problems and image types. Subsequently: Wei Zhang: et al. modified their model by removing the last fully connected layer and applied it for medical image object segmentation in 1991 and breast cancer detection in mammograms in 1994.In 1990 Yamaguchi et al. introduced max-pooling: a fixed filtering operation that calculates and propagates the maximum value of a given region. They combined TDNNs with max-pooling in order to realize a speaker independent isolated word recognition system. In a variant of the neocognitron called the cresceptron: instead of using Fukushima\'s spatial averaging: J. Weng et al. also used max-pooling where a downsampling unit computes the maximum of the activations of the units in its patch. Max-pooling is often used in modern CNNs.LeNet-5: a 7-level CNN by Yann LeCun et al. in 1998: that classifies digits: was applied by several banks to recognize hand-written numbers on checks (British English: cheques) digitized in 32x32 pixel images. The ability to process higher-resolution images requires larger and more layers of CNNs: so this technique is constrained by the availability of computing resources. In 2010: Backpropagation training through max-pooling was accelerated by GPUs and shown to perform better than other pooling variants. Behnke (2003) relied only on the sign of the gradient (Rprop) on problems such as image reconstruction and face localization. Rprop is a first-order optimization algorithm created by Martin Riedmiller and Heinrich Braun in 1992.In 2011: a deep GPU-based CNN called ""DanNet"" by Dan Ciresan: Ueli Meier: and Juergen Schmidhuber achieved human-competitive performance for the first time in computer vision contests. Subsequently: a similar GPU-based CNN by Alex Krizhevsky: Ilya Sutskever: and Geoffrey Hinton won the ImageNet Large Scale Visual Recognition Challenge 2012. A very deep CNN with over 100 layers by Kaiming He: Xiangyu Zhang: Shaoqing Ren: and Jian Sun of Microsoft won the ImageNet 2015 contest.ANNs were able to guarantee shift invariance to deal with small and large natural objects in large cluttered scenes: only when invariance extended beyond shift: to all ANN-learned concepts: such as location: type (object class label): scale: lighting and others. This was realized in Developmental Networks (DNs) whose embodiments are Where-What Networks: WWN-1 (2008) through WWN-7 (2013). == Artificial curiosity and generative adversarial networks == In 1991: Juergen Schmidhuber published adversarial neural networks that contest with each other in the form of a zero-sum game: where one network\'s gain is the other network\'s loss. The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called ""artificial curiosity."" Earlier adversarial machine learning systems ""neither involved unsupervised neural networks nor were about modeling data nor used gradient descent.""In 2014: this adversarial principle was used in a generative adversarial network (GAN) by Ian Goodfellow et al. Here the environmental reaction is 1 or 0 depending on whether the first network\'s output is in a given set. This can be used to create realistic deepfakes.In 1992: Schmidhuber also published another type of gradient-based adversarial neural networks where the goal of the zero-sum game is to create disentangled representations of input patterns. This was called predictability minimization.Nvidia\'s StyleGAN (2018) is based on the Progressive GAN by Tero Karras: Timo Aila: Samuli Laine: and Jaakko Lehtinen. Here the GAN generator is grown from small to large scale in a pyramidal fashion. StyleGANs improve consistency between fine and coarse details in the generator network. == Transformers and their variants == Many modern large language models such as ChatGPT: GPT-4: and BERT use a feedforward neural network called Transformer by Ashish Vaswani et. al. in their 2017 paper ""Attention Is All You Need."" Transformers have increasingly become the model of choice for natural language processing problems: replacing recurrent neural networks (RNNs) such as long short-term memory (LSTM).Basic ideas for this go back a long way: in 1992: Juergen Schmidhuber published the Transformer with ""linearized self-attention"" (save for a normalization operator): which is also called the ""linear Transformer."" He advertised it as an ""alternative to RNNs"" that can learn ""internal spotlights of attention:"" and experimentally applied it to problems of variable binding. Here a slow feedforward neural network learns by gradient descent to control the fast weights of another neural network through outer products of self-generated activation patterns called ""FROM"" and ""TO"" which in Transformer terminology are called ""key"" and ""value"" for ""self-attention."" This fast weight ""attention mapping"" is applied to queries. The 2017 Transformer combines this with a softmax operator and a projection matrix.Transformers are\nalso increasingly being used in computer vision. == Deep learning with unsupervised or self-supervised pre-training == In the 1980s: backpropagation did not work well for deep FNNs and RNNs. Here the word ""deep"" refers to the number of layers through which the data is transformed. More precisely: deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For an FNN: the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For RNNs: in which a signal may propagate through a layer more than once: the CAP depth is potentially unlimited. To overcome this problem: Juergen Schmidhuber (1992) proposed a self-supervised hierarchy of RNNs pre-trained one level at a time by self-supervised learning. This ""neural history compressor"" uses predictive coding to learn internal representations at multiple self-organizing': 'original_similarity': 0.8452043949065007: 'slide': 161}"
0.44046679997700267,What are some advantages of the ReLU function?,The ReLU function offers the advantages of sparse activation where only about 50% of hidden units are activated. The feature helps in better gradient propagation and efficient computation. It is also scale-invariant.,"{'chunk_text': 'nets and computational neuroscience. == Advantages == Sparse activation: For example: in a randomly initialized network: only about 50% of hidden units are activated (have a non-zero output). Better gradient propagation: Fewer vanishing gradient problems compared to sigmoidal activation functions that saturate in both directions. Efficient computation: Only comparison: addition and multiplication. Scale-invariant: max(0:ax)=amax(0:x) for a≥0{\\displaystyle \\max(0:ax)=a\\max(0:x){\\text{ for }}a\\geq 0}.Rectifying activation functions were used to separate specific excitation and unspecific inhibition in the neural abstraction pyramid: which was trained in a supervised way to learn several computer vision tasks. In 2011: the use of the rectifier as a non-linearity has been shown to enable training deep supervised neural networks without requiring unsupervised pre-training. Rectified linear units: compared to sigmoid function or similar activation functions: allow faster and effective training of deep neural architectures on large and complex datasets. == Potential problems == Non-differentiable at zero; however: it is differentiable anywhere else: and the value of the derivative at zero can be arbitrarily chosen to be 0 or 1. Not zero-centered. Unbounded. Dying ReLU problem: ReLU (rectified linear unit) neurons can sometimes be pushed into states in which they become inactive for essentially all inputs. In this state: no gradients flow backward through the neuron: and so the neuron becomes stuck in a perpetually inactive state and ""dies"". This is a form of the vanishing gradient problem. In some cases: large numbers of neurons in a network can become stuck in dead states: effectively decreasing the model capacity. This problem typically arises when the learning rate is set too high. It may be mitigated by using leaky ReLUs instead: which assign a small positive slope for x < 0; however: the performance is reduced. == Variants == === Piecewise-linear variants === ==== Leaky ReLU ==== Leaky ReLUs allow a small: positive gradient when the unit is not active: helping to mitigate the vanishing gradient problem. f(x)={xif x>0:0.01xotherwise.f′(x)={1if x>0:0.01otherwise.{\\displaystyle f(x)={\\begin{cases}x&{\\text{if }}x>0:\\\\0.01x&{\\text{otherwise}}.\\end{cases}}\\qquad \\qquad f\'(x)={\\begin{cases}1&{\\text{if }}x>0:\\\\0.01&{\\text{otherwise}}.\\end{cases}}} ==== Parametric ReLU ==== Parametric ReLUs (PReLUs) take this idea further by making the coefficient of leakage into a parameter that is learned along with the other neural-network parameters. f(x)={xif x>0:a⋅xotherwise.f′(x)={1if x>0:aotherwise.{\\displaystyle f(x)={\\begin{cases}x&{\\text{if }}x>0:\\\\a\\cdot x&{\\text{otherwise}}.\\end{cases}}\\qquad \\qquad \\qquad f\'(x)={\\begin{cases}1&{\\text{if }}x>0:\\\\a&{\\text{otherwise}}.\\end{cases}}}Note that for a ≤ 1: this is equivalent to f(x)=max(x:ax){\\displaystyle f(x)=\\max(x:ax)}and thus has a relation to ""maxout"" networks. === Other non-linear variants === ==== Gaussian-error linear unit (GELU) ==== GELU is a smooth approximation to the rectifier: f(x)=x⋅Φ(x):{\\displaystyle f(x)=x\\cdot \\Phi (x):}f′(x)=x⋅Φ′(x)+Φ(x):{\\displaystyle f\'(x)=x\\cdot \\Phi \'(x)+\\Phi (x):}where Φ(x)=P(X⩽x){\\displaystyle \\Phi (x)=P(X\\leqslant x)} is the cumulative distribution function of the standard normal distribution. This activation function is illustrated in the figure at the start of this article. It has a ""bump"" to the left of x < 0 and serves as the default activation for models such as BERT. ==== SiLU ==== The SiLU (sigmoid linear unit) or swish function is another smooth approximation: first coined in the GELU paper: f(x)=x⋅sigmoid\u2061(x):{\\displaystyle f(x)=x\\cdot \\operatorname {sigmoid} (x):}f′(x)=x⋅sigmoid′\u2061(x)+sigmoid\u2061(x):{\\displaystyle f\'(x)=x\\cdot \\operatorname {sigmoid} \'(x)+\\operatorname {sigmoid} (x):}where sigmoid\u2061(x){\\displaystyle \\operatorname {sigmoid} (x)} is the sigmoid function. ==== Softplus ==== A smooth approximation to the rectifier is the analytic function f(x)=ln\u2061(1+ex):f′(x)=ex1+ex=11+e−x:{\\displaystyle f(x)=\\ln(1+e^{x}):\\qquad \\qquad f\'(x)={\\frac {e^{x}}{1+e^{x}}}={\\frac {1}{1+e^{-x}}}:}which is called the softplus or SmoothReLU function. For large negative x{\\displaystyle x} it is roughly ln\u20611{\\displaystyle \\ln 1}: so just above 0: while for large positive x{\\displaystyle x} it is roughly ln\u2061(ex){\\displaystyle \\ln(e^{x})}: so just above x{\\displaystyle x}. This function can be approximated as: ln\u2061(1+ex)≈{ln\u20612:x=0:x1−e−x/ln\u20612:x≠0{\\displaystyle \\ln \\left(1+e^{x}\\right)\\approx {\\begin{cases}\\ln 2:&x=0:\\\\[6pt]{\\frac {x}{1-e^{-x/\\ln 2}}}:&x\\neq 0\\end{cases}}}By making the change of variables x=yln\u2061(2){\\displaystyle x=y\\ln(2)}: this is equivalent to log2\u2061(1+2y)≈{1:y=0:y1−e−y:y≠0.{\\displaystyle \\log _{2}(1+2^{y})\\approx {\\begin{cases}1:&y=0:\\\\[6pt]{\\frac {y}{1-e^{-y}}}:&y\\neq 0.\\end{cases}}}A sharpness parameter k{\\displaystyle k} may be included: f(x)=ln\u2061(1+ekx)k:f′(x)=ekx1+ekx=11+e−kx.{\\displaystyle f(x)={\\frac {\\ln(1+e^{kx})}{k}}:\\qquad \\qquad f\'(x)={\\frac {e^{kx}}{1+e^{kx}}}={\\frac {1}{1+e^{-kx}}}.}The derivative of softplus is the logistic function. The logistic sigmoid function is a smooth approximation of the derivative of the rectifier: the Heaviside step function. The multivariable generalization of single-variable softplus is the LogSumExp with the first argument set to zero: LSE0+\u2061(x1:…:xn):=LSE\u2061(0:x1:…:xn)=ln\u2061(1+ex1+⋯+exn).{\\displaystyle \\operatorname {LSE_{0}} ^{+}(x_{1}:\\dots :x_{n}):=\\operatorname {LSE} (0:x_{1}:\\dots :x_{n})=\\ln(1+e^{x_{1}}+\\cdots +e^{x_{n}}).}The LogSumExp function is LSE\u2061(x1:…:xn)=ln\u2061(ex1+⋯+exn):{\\displaystyle \\operatorname {LSE} (x_{1}:\\dots :x_{n})=\\ln(e^{x_{1}}+\\cdots +e^{x_{n}}):}and its gradient is the softmax; the softmax with the first argument set to zero is the multivariable generalization of the logistic function. Both LogSumExp and softmax are used in machine learning. ==== ELU ==== Exponential linear units try to make the mean activations closer to zero: which speeds up learning. It has been shown that ELUs can obtain higher classification accuracy than ReLUs. f(x)={xif x>0:a(ex−1)otherwise.f′(x)={1if x>0:a⋅exotherwise.{\\displaystyle f(x)={\\begin{cases}x&{\\text{if }}x>0:\\\\a\\left(e^{x}-1\\right)&{\\text{otherwise}}.\\end{cases}}\\qquad \\qquad f\'(x)={\\begin{cases}1&{\\text{if }}x>0:\\\\a\\cdot e^{x}&{\\text{otherwise}}.\\end{cases}}}In these formulas: a{\\displaystyle a} is a hyper-parameter to be tuned with the constraint a≥0{\\displaystyle a\\geq 0}. The ELU can be viewed as a smoothed version of a shifted ReLU (SReLU): which has the form f(x)=max(−a:x){\\displaystyle f(x)=\\max(-a:x)}: given the same interpretation of a{\\displaystyle a}. ==== Mish ==== The mish function can also be used as a smooth approximation of the rectifier. It is defined as f(x)=xtanh\u2061(softplus\u2061(x)):{\\displaystyle f(x)=x\\tanh {\\big (}\\operatorname {softplus} (x){\\big )}:}where tanh\u2061(x){\\displaystyle \\tanh(x)} is the hyperbolic tangent: and softplus\u2061(x){\\displaystyle \\operatorname {softplus} (x)} is the softplus function. Mish is non-monotonic and self-gated. It was inspired by Swish: itself a variant of ReLU. ==== Squareplus ==== Squareplus is the function squareplusb\u2061(x)=x+x2+b2{\\displaystyle\n\\operatorname {squareplus} _{b}(x)={\\frac {x+{\\sqrt {x^{2}+b}}}{2}}}where b≥0{\\displaystyle b\\geq 0} is a hyperparameter that determines the ""size"" of the curved region near x=0{\\displaystyle x=0}. (For example: letting b=0{\\displaystyle b=0} yields ReLU: and letting b=4{\\displaystyle b=4} yields the metallic mean function.) Squareplus shares many properties with softplus: It is monotonic: strictly positive: approaches 0 as x→−∞{\\displaystyle x\\to -\\infty }: approaches the identity as x→+∞{\\displaystyle x\\to +\\infty }: and is C∞{\\displaystyle C^{\\infty }} smooth. However: squareplus can be computed using only algebraic functions: making it well-suited for settings where computational resources or instruction sets are limited. Additionally: squareplus requires no special consideration to ensure numerical stability when x{\\displaystyle x} is large. == See also == Softmax function Sigmoid function Tobit model Layer (deep learning) == References ==': 'original_similarity': 0.8625037968919356: 'slide': 161}","{'chunk_text': 'activations can thus be computed as an affine transformation: with matrix multiplication followed by a bias offset (vector addition of a learned or fixed bias term). === Loss layer === The ""loss layer"": or ""loss function"": specifies how training penalizes the deviation between the predicted output of the network: and the true data labels (during supervised learning). Various loss functions can be used: depending on the specific task. The Softmax loss function is used for predicting a single class of K mutually exclusive classes. Sigmoid cross-entropy loss is used for predicting K independent probability values in [0:1]{\\displaystyle [0:1]}. Euclidean loss is used for regressing to real-valued labels (−∞:∞){\\displaystyle (-\\infty :\\infty )}. == Hyperparameters == Hyperparameters are various settings that are used to control the learning process. CNNs use more hyperparameters than a standard multilayer perceptron (MLP). === Kernel size === The kernel is the number of pixels processed together. It is typically expressed as the kernel\'s dimensions: e.g.: 2x2: or 3x3. === Padding === Padding is the addition of (typically) 0-valued pixels on the borders of an image. This is done so that the border pixels are not undervalued (lost) from the output because they would ordinarily participate in only a single receptive field instance. The padding applied is typically one less than the corresponding kernel dimension. For example: a convolutional layer using 3x3 kernels would receive a 2-pixel pad: that is 1 pixel on each side of the image. === Stride === The stride is the number of pixels that the analysis window moves on each iteration. A stride of 2 means that each kernel is offset by 2 pixels from its predecessor. === Number of filters === Since feature map size decreases with depth: layers near the input layer tend to have fewer filters while higher layers can have more. To equalize computation at each layer: the product of feature values va with pixel position is kept roughly constant across layers. Preserving more information about the input would require keeping the total number of activations (number of feature maps times number of pixel positions) non-decreasing from one layer to the next. The number of feature maps directly controls the capacity and depends on the number of available examples and task complexity. === Filter size === Common filter sizes found in the literature vary greatly: and are usually chosen based on the data set. The challenge is to find the right level of granularity so as to create abstractions at the proper scale: given a particular data set: and without overfitting. === Pooling type and size === Max pooling is typically used: often with a 2x2 dimension. This implies that the input is drastically downsampled: reducing processing cost. Greater pooling reduces the dimension of the signal: and may result in unacceptable information loss. Often: non-overlapping pooling windows perform best. === Dilation === Dilation involves ignoring pixels within a kernel. This reduces processing/memory potentially without significant signal loss. A dilation of 2 on a 3x3 kernel expands the kernel to 5x5: while still processing 9 (evenly spaced) pixels. Accordingly: dilation of 4 expands the kernel to 7x7. == Translation equivariance and aliasing == It is commonly assumed that CNNs are invariant to shifts of the input. Convolution or pooling layers within a CNN that do not have a stride greater than one are indeed equivariant to translations of the input. However: layers with a stride greater than one ignore the Nyquist-Shannon sampling theorem and might lead to aliasing of the input signal While: in principle: CNNs are capable of implementing anti-aliasing filters: it has been observed that this does not happen in practice and yield models that are not equivariant to translations. Furthermore: if a CNN makes use of fully connected layers: translation equivariance does not imply translation invariance: as the fully connected layers are not invariant to shifts of the input. One solution for complete translation invariance is avoiding any down-sampling throughout the network and applying global average pooling at the last layer. Additionally: several other partial solutions have been proposed: such as anti-aliasing before downsampling operations: spatial transformer networks: data augmentation: subsampling combined with pooling: and capsule neural networks. == Evaluation == The accuracy of the final model is based on a sub-part of the dataset set apart at the start: often called a test-set. Other times methods such as k-fold cross-validation are applied. Other strategies include using conformal prediction. == Regularization methods == Regularization is a process of introducing additional information to solve an ill-posed problem or to prevent overfitting. CNNs use various types of regularization. === Empirical === ==== Dropout ==== Because a fully connected layer occupies most of the parameters: it is prone to overfitting. One method to reduce overfitting is dropout: introduced in 2014. At each training stage: individual nodes are either ""dropped out"" of the net (ignored) with probability 1−p{\\displaystyle 1-p} or kept with probability p{\\displaystyle p}: so that a reduced network is left; incoming and outgoing edges to a dropped-out node are also removed. Only the reduced network is trained on the data in that stage. The removed nodes are then reinserted into the network with their original weights. In the training stages: p{\\displaystyle p} is usually 0.5; for input nodes: it is typically much higher because information is directly lost when input nodes are ignored. At testing time after training has finished: we would ideally\nlike to find a sample average of all possible 2n{\\displaystyle 2^{n}} dropped-out networks; unfortunately this is unfeasible for large values of n{\\displaystyle n}. However: we can find an approximation by using the full network with each node\'s output weighted by a factor of p{\\displaystyle p}: so the expected value of the output of any node is the same as in the training stages. This is the biggest contribution of the dropout method: although it effectively generates 2n{\\displaystyle 2^{n}} neural nets: and as such allows for model combination: at test time only a single network needs to be tested. By avoiding training all nodes on all training data:': 'original_similarity': 0.8278498524788532: 'slide': 105}","{'chunk_text': 'input. The result of this convolution is an activation map: and the set of activation maps for each different filter are stacked together along the depth dimension to produce the output volume. Parameter sharing contributes to the translation invariance of the CNN architecture.Sometimes: the parameter sharing assumption may not make sense. This is especially the case when the input images to a CNN have some specific centered structure; for which we expect completely different features to be learned on different spatial locations. One practical example is when the inputs are faces that have been centered in the image: we might expect different eye-specific or hair-specific features to be learned in different parts of the image. In that case it is common to relax the parameter sharing scheme: and instead simply call the layer a ""locally connected layer"". === Pooling layer === Another important concept of CNNs is pooling: which is a form of non-linear down-sampling. There are several non-linear functions to implement pooling: where max pooling is the most common. It partitions the input image into a set of rectangles and: for each such sub-region: outputs the maximum. Intuitively: the exact location of a feature is less important than its rough location relative to other features. This is the idea behind the use of pooling in convolutional neural networks. The pooling layer serves to progressively reduce the spatial size of the representation: to reduce the number of parameters: memory footprint and amount of computation in the network: and hence to also control overfitting. This is known as down-sampling. It is common to periodically insert a pooling layer between successive convolutional layers (each one typically followed by an activation function: such as a ReLU layer) in a CNN architecture.: 460–461 While pooling layers contribute to local translation invariance: they do not provide global translation invariance in a CNN: unless a form of global pooling is used. The pooling layer commonly operates independently on every depth: or slice: of the input and resizes it spatially. A very common form of max pooling is a layer with filters of size 2×2: applied with a stride of 2: which subsamples every depth slice in the input by 2 along both width and height: discarding 75% of the activations: In this case: every max operation is over 4 numbers. The depth dimension remains unchanged (this is true for other forms of pooling as well). In addition to max pooling: pooling units can use other functions: such as average pooling or ℓ2-norm pooling. Average pooling was often used historically but has recently fallen out of favor compared to max pooling: which generally performs better in practice.Due to the effects of fast spatial reduction of the size of the representation: there is a recent trend towards using smaller filters or discarding pooling layers altogether. ""Region of Interest"" pooling (also known as RoI pooling) is a variant of max pooling: in which output size is fixed and input rectangle is a parameter.Pooling is a downsampling method and an important component of convolutional neural networks for object detection based on the Fast R-CNN architecture. === Channel Max Pooling === A CMP operation layer conducts the MP operation along the channel side among the corresponding positions of the consecutive feature maps for the purpose of redundant information elimination. The CMP makes the significant features gather together within fewer channels: which is important for fine-grained image classification that needs more discriminating features. Meanwhile: another advantage of the CMP operation is to make the channel number of feature maps smaller before it connects to the first fully connected (FC) layer. Similar to the MP operation: we denote the input feature maps and output feature maps of a CMP layer as F ∈ R(C×M×N) and C ∈ R(c×M×N): respectively: where C and c are the channel numbers of the input and output feature maps: M and N are the widths and the height of the feature maps: respectively. Note that the CMP operation only changes the channel number of the feature maps. The width and the height of the feature maps are not changed: which is different from the MP operation. === ReLU layer === ReLU is the abbreviation of rectified linear unit introduced by Kunihiko Fukushima in 1969. ReLU applies the non-saturating activation function f(x)=max(0:x){\\textstyle f(x)=\\max(0:x)}. It effectively removes negative values from an activation map by setting them to zero. It introduces nonlinearity to the decision function and in the overall network without affecting the receptive fields of the convolution layers. In 2011: Xavier Glorot: Antoine Bordes and Yoshua Bengio found that ReLU enables better training\nof deeper networks: compared to widely used activation functions prior to 2011. Other functions can also be used to increase nonlinearity: for example the saturating hyperbolic tangent f(x)=tanh\u2061(x){\\displaystyle f(x)=\\tanh(x)}: f(x)=|tanh\u2061(x)|{\\displaystyle f(x)=|\\tanh(x)|}: and the sigmoid function σ(x)=(1+e−x)−1{\\textstyle \\sigma (x)=(1+e^{-x})^{-1}}. ReLU is often preferred to other functions because it trains the neural network several times faster without a significant penalty to generalization accuracy. === Fully connected layer === After several convolutional and max pooling layers: the final classification is done via fully connected layers. Neurons in a fully connected layer have connections to all activations in the previous layer: as seen in regular (non-convolutional) artificial neural networks. Their activations can thus be computed as an affine transformation: with matrix multiplication followed by a bias offset (vector addition of a learned or fixed bias term). === Loss layer === The ""loss layer"": or ""loss function"": specifies how training penalizes the deviation between the predicted output of the network: and the true data labels (during supervised learning). Various loss functions can be used: depending on the specific task. The Softmax loss function is used for predicting a single class of K mutually exclusive classes. Sigmoid cross-entropy loss is used for predicting K independent probability values in [0:1]{\\displaystyle [0:1]}. Euclidean loss is used for regressing to real-valued labels (−∞:∞){\\displaystyle (-\\infty :\\infty )}. == Hyperparameters == Hyperparameters are various settings that are used to control the learning process. CNNs use more hyperparameters than': 'original_similarity': 0.8168749581203265: 'slide': 235}"
0.5014369999989867,What are some potential problems of ReLU?,"Some potential problems of ReLU include the fact that it is non-differentiable at zero. It's also not zero-centered, is unbounded, and may suffer from the dying ReLU problem where neurons become inactive for virtually all inputs, a form of the vanishing gradient problem.","{'chunk_text': 'a randomly initialized network: only about 50% of hidden units are activated (have a non-zero output). Better gradient propagation: Fewer vanishing gradient problems compared to sigmoidal activation functions that saturate in both directions. Efficient computation: Only comparison: addition and multiplication. Scale-invariant: max(0:ax)=amax(0:x) for a≥0{\\displaystyle \\max(0:ax)=a\\max(0:x){\\text{ for }}a\\geq 0}.Rectifying activation functions were used to separate specific excitation and unspecific inhibition in the neural abstraction pyramid: which was trained in a supervised way to learn several computer vision tasks. In 2011: the use of the rectifier as a non-linearity has been shown to enable training deep supervised neural networks without requiring unsupervised pre-training. Rectified linear units: compared to sigmoid function or similar activation functions: allow faster and effective training of deep neural architectures on large and complex datasets. == Potential problems == Non-differentiable at zero; however: it is differentiable anywhere else: and the value of the derivative at zero can be arbitrarily chosen to be 0 or 1. Not zero-centered. Unbounded. Dying ReLU problem: ReLU (rectified linear unit) neurons can sometimes be pushed into states in which they become inactive for essentially all inputs. In this state: no gradients flow backward through the neuron: and so the neuron becomes stuck in a perpetually inactive state and ""dies"". This is a form of the vanishing gradient problem. In some cases: large numbers of neurons in a network can become stuck in dead states: effectively decreasing the model capacity. This problem typically arises when the learning rate is set too high. It may be mitigated by using leaky ReLUs instead: which assign a small positive slope for x < 0; however: the performance is reduced. == Variants == === Piecewise-linear variants === ==== Leaky ReLU ==== Leaky ReLUs allow a small: positive gradient when the unit is not active: helping to mitigate the vanishing gradient problem. f(x)={xif x>0:0.01xotherwise.f′(x)={1if x>0:0.01otherwise.{\\displaystyle f(x)={\\begin{cases}x&{\\text{if }}x>0:\\\\0.01x&{\\text{otherwise}}.\\end{cases}}\\qquad \\qquad f\'(x)={\\begin{cases}1&{\\text{if }}x>0:\\\\0.01&{\\text{otherwise}}.\\end{cases}}} ==== Parametric ReLU ==== Parametric ReLUs (PReLUs) take this idea further by making the coefficient of leakage into a parameter that is learned along with the other neural-network parameters. f(x)={xif x>0:a⋅xotherwise.f′(x)={1if x>0:aotherwise.{\\displaystyle f(x)={\\begin{cases}x&{\\text{if }}x>0:\\\\a\\cdot x&{\\text{otherwise}}.\\end{cases}}\\qquad \\qquad \\qquad f\'(x)={\\begin{cases}1&{\\text{if }}x>0:\\\\a&{\\text{otherwise}}.\\end{cases}}}Note that for a ≤ 1: this is equivalent to f(x)=max(x:ax){\\displaystyle f(x)=\\max(x:ax)}and thus has a relation to ""maxout"" networks. === Other non-linear variants === ==== Gaussian-error linear unit (GELU) ==== GELU is a smooth approximation to the rectifier: f(x)=x⋅Φ(x):{\\displaystyle f(x)=x\\cdot \\Phi (x):}f′(x)=x⋅Φ′(x)+Φ(x):{\\displaystyle f\'(x)=x\\cdot \\Phi \'(x)+\\Phi (x):}where Φ(x)=P(X⩽x){\\displaystyle \\Phi (x)=P(X\\leqslant x)} is the cumulative distribution function of the standard normal distribution. This activation function is illustrated in the figure at the start of this article. It has a ""bump"" to the left of x < 0 and serves as the default activation for models such as BERT. ==== SiLU ==== The SiLU (sigmoid linear unit) or swish function is another smooth approximation: first coined in the GELU paper: f(x)=x⋅sigmoid\u2061(x):{\\displaystyle f(x)=x\\cdot \\operatorname {sigmoid} (x):}f′(x)=x⋅sigmoid′\u2061(x)+sigmoid\u2061(x):{\\displaystyle f\'(x)=x\\cdot \\operatorname {sigmoid} \'(x)+\\operatorname {sigmoid} (x):}where sigmoid\u2061(x){\\displaystyle \\operatorname {sigmoid} (x)} is the sigmoid function. ==== Softplus ==== A smooth approximation to the rectifier is the analytic function f(x)=ln\u2061(1+ex):f′(x)=ex1+ex=11+e−x:{\\displaystyle f(x)=\\ln(1+e^{x}):\\qquad \\qquad f\'(x)={\\frac {e^{x}}{1+e^{x}}}={\\frac {1}{1+e^{-x}}}:}which is called the softplus or SmoothReLU function. For large negative x{\\displaystyle x} it is roughly ln\u20611{\\displaystyle \\ln 1}: so just above 0: while for large positive x{\\displaystyle x} it is roughly ln\u2061(ex){\\displaystyle \\ln(e^{x})}: so just above x{\\displaystyle x}. This function can be approximated as: ln\u2061(1+ex)≈{ln\u20612:x=0:x1−e−x/ln\u20612:x≠0{\\displaystyle \\ln \\left(1+e^{x}\\right)\\approx {\\begin{cases}\\ln 2:&x=0:\\\\[6pt]{\\frac {x}{1-e^{-x/\\ln 2}}}:&x\\neq 0\\end{cases}}}By making the change of variables x=yln\u2061(2){\\displaystyle x=y\\ln(2)}: this is equivalent to log2\u2061(1+2y)≈{1:y=0:y1−e−y:y≠0.{\\displaystyle \\log _{2}(1+2^{y})\\approx {\\begin{cases}1:&y=0:\\\\[6pt]{\\frac {y}{1-e^{-y}}}:&y\\neq 0.\\end{cases}}}A sharpness parameter k{\\displaystyle k} may be included: f(x)=ln\u2061(1+ekx)k:f′(x)=ekx1+ekx=11+e−kx.{\\displaystyle f(x)={\\frac {\\ln(1+e^{kx})}{k}}:\\qquad \\qquad f\'(x)={\\frac {e^{kx}}{1+e^{kx}}}={\\frac {1}{1+e^{-kx}}}.}The derivative of softplus is the logistic function. The logistic sigmoid function is a smooth approximation of the derivative of the rectifier: the Heaviside step function. The multivariable generalization of single-variable softplus is the LogSumExp with the first argument set to zero: LSE0+\u2061(x1:…:xn):=LSE\u2061(0:x1:…:xn)=ln\u2061(1+ex1+⋯+exn).{\\displaystyle \\operatorname {LSE_{0}} ^{+}(x_{1}:\\dots :x_{n}):=\\operatorname {LSE} (0:x_{1}:\\dots :x_{n})=\\ln(1+e^{x_{1}}+\\cdots +e^{x_{n}}).}The LogSumExp function is LSE\u2061(x1:…:xn)=ln\u2061(ex1+⋯+exn):{\\displaystyle \\operatorname {LSE} (x_{1}:\\dots :x_{n})=\\ln(e^{x_{1}}+\\cdots +e^{x_{n}}):}and its gradient is the softmax; the softmax with the first argument set to zero is the multivariable generalization of the logistic function. Both LogSumExp and softmax are used in machine learning. ==== ELU ==== Exponential linear units try to make the mean activations closer to zero: which speeds up learning. It has been shown that ELUs can obtain higher classification accuracy than ReLUs. f(x)={xif x>0:a(ex−1)otherwise.f′(x)={1if x>0:a⋅exotherwise.{\\displaystyle f(x)={\\begin{cases}x&{\\text{if }}x>0:\\\\a\\left(e^{x}-1\\right)&{\\text{otherwise}}.\\end{cases}}\\qquad \\qquad f\'(x)={\\begin{cases}1&{\\text{if }}x>0:\\\\a\\cdot e^{x}&{\\text{otherwise}}.\\end{cases}}}In these formulas: a{\\displaystyle a} is a hyper-parameter to be tuned with the constraint a≥0{\\displaystyle a\\geq 0}. The ELU can be viewed as a smoothed version of a shifted ReLU (SReLU): which has the form f(x)=max(−a:x){\\displaystyle f(x)=\\max(-a:x)}: given the same interpretation of a{\\displaystyle a}. ==== Mish ==== The mish function can also be used as a smooth approximation of the rectifier. It is defined as f(x)=xtanh\u2061(softplus\u2061(x)):{\\displaystyle f(x)=x\\tanh {\\big (}\\operatorname {softplus} (x){\\big )}:}where tanh\u2061(x){\\displaystyle \\tanh(x)} is the hyperbolic tangent: and softplus\u2061(x){\\displaystyle \\operatorname {softplus} (x)} is the softplus function. Mish is non-monotonic and self-gated. It was inspired by Swish: itself a variant of ReLU. ==== Squareplus ==== Squareplus is the function squareplusb\u2061(x)=x+x2+b2{\\displaystyle\n\\operatorname {squareplus} _{b}(x)={\\frac {x+{\\sqrt {x^{2}+b}}}{2}}}where b≥0{\\displaystyle b\\geq 0} is a hyperparameter that determines the ""size"" of the curved region near x=0{\\displaystyle x=0}. (For example: letting b=0{\\displaystyle b=0} yields ReLU: and letting b=4{\\displaystyle b=4} yields the metallic mean function.) Squareplus shares many properties with softplus: It is monotonic: strictly positive: approaches 0 as x→−∞{\\displaystyle x\\to -\\infty }: approaches the identity as x→+∞{\\displaystyle x\\to +\\infty }: and is C∞{\\displaystyle C^{\\infty }} smooth. However: squareplus can be computed using only algebraic functions: making it well-suited for settings where computational resources or instruction sets are limited. Additionally: squareplus requires no special consideration to ensure numerical stability when x{\\displaystyle x} is large. == See also == Softmax function Sigmoid function Tobit model Layer (deep learning) == References ==': 'original_similarity': 0.8256819240821662: 'slide': 173}","{'chunk_text': 'with the input. The result of this convolution is an activation map: and the set of activation maps for each different filter are stacked together along the depth dimension to produce the output volume. Parameter sharing contributes to the translation invariance of the CNN architecture.Sometimes: the parameter sharing assumption may not make sense. This is especially the case when the input images to a CNN have some specific centered structure; for which we expect completely different features to be learned on different spatial locations. One practical example is when the inputs are faces that have been centered in the image: we might expect different eye-specific or hair-specific features to be learned in different parts of the image. In that case it is common to relax the parameter sharing scheme: and instead simply call the layer a ""locally connected layer"". === Pooling layer === Another important concept of CNNs is pooling: which is a form of non-linear down-sampling. There are several non-linear functions to implement pooling: where max pooling is the most common. It partitions the input image into a set of rectangles and: for each such sub-region: outputs the maximum. Intuitively: the exact location of a feature is less important than its rough location relative to other features. This is the idea behind the use of pooling in convolutional neural networks. The pooling layer serves to progressively reduce the spatial size of the representation: to reduce the number of parameters: memory footprint and amount of computation in the network: and hence to also control overfitting. This is known as down-sampling. It is common to periodically insert a pooling layer between successive convolutional layers (each one typically followed by an activation function: such as a ReLU layer) in a CNN architecture.: 460–461 While pooling layers contribute to local translation invariance: they do not provide global translation invariance in a CNN: unless a form of global pooling is used. The pooling layer commonly operates independently on every depth: or slice: of the input and resizes it spatially. A very common form of max pooling is a layer with filters of size 2×2: applied with a stride of 2: which subsamples every depth slice in the input by 2 along both width and height: discarding 75% of the activations: In this case: every max operation is over 4 numbers. The depth dimension remains unchanged (this is true for other forms of pooling as well). In addition to max pooling: pooling units can use other functions: such as average pooling or ℓ2-norm pooling. Average pooling was often used historically but has recently fallen out of favor compared to max pooling: which generally performs better in practice.Due to the effects of fast spatial reduction of the size of the representation: there is a recent trend towards using smaller filters or discarding pooling layers altogether. ""Region of Interest"" pooling (also known as RoI pooling) is a variant of max pooling: in which output size is fixed and input rectangle is a parameter.Pooling is a downsampling method and an important component of convolutional neural networks for object detection based on the Fast R-CNN architecture. === Channel Max Pooling === A CMP operation layer conducts the MP operation along the channel side among the corresponding positions of the consecutive feature maps for the purpose of redundant information elimination. The CMP makes the significant features gather together within fewer channels: which is important for fine-grained image classification that needs more discriminating features. Meanwhile: another advantage of the CMP operation is to make the channel number of feature maps smaller before it connects to the first fully connected (FC) layer. Similar to the MP operation: we denote the input feature maps and output feature maps of a CMP layer as F ∈ R(C×M×N) and C ∈ R(c×M×N): respectively: where C and c are the channel numbers of the input and output feature maps: M and N are the widths and the height of the feature maps: respectively. Note that the CMP operation only changes the channel number of the feature maps. The width and the height of the feature maps are not changed: which is different from the MP operation. === ReLU layer === ReLU is the abbreviation of rectified linear unit introduced by Kunihiko Fukushima in 1969. ReLU applies the non-saturating activation function f(x)=max(0:x){\\textstyle f(x)=\\max(0:x)}. It effectively removes negative values from an activation map by setting them to zero. It introduces nonlinearity to the decision function and in the overall network without affecting the receptive fields of the convolution layers. In 2011: Xavier Glorot: Antoine Bordes and Yoshua Bengio found that ReLU enables better training\nof deeper networks: compared to widely used activation functions prior to 2011. Other functions can also be used to increase nonlinearity: for example the saturating hyperbolic tangent f(x)=tanh\u2061(x){\\displaystyle f(x)=\\tanh(x)}: f(x)=|tanh\u2061(x)|{\\displaystyle f(x)=|\\tanh(x)|}: and the sigmoid function σ(x)=(1+e−x)−1{\\textstyle \\sigma (x)=(1+e^{-x})^{-1}}. ReLU is often preferred to other functions because it trains the neural network several times faster without a significant penalty to generalization accuracy. === Fully connected layer === After several convolutional and max pooling layers: the final classification is done via fully connected layers. Neurons in a fully connected layer have connections to all activations in the previous layer: as seen in regular (non-convolutional) artificial neural networks. Their activations can thus be computed as an affine transformation: with matrix multiplication followed by a bias offset (vector addition of a learned or fixed bias term). === Loss layer === The ""loss layer"": or ""loss function"": specifies how training penalizes the deviation between the predicted output of the network: and the true data labels (during supervised learning). Various loss functions can be used: depending on the specific task. The Softmax loss function is used for predicting a single class of K mutually exclusive classes. Sigmoid cross-entropy loss is used for predicting K independent probability values in [0:1]{\\displaystyle [0:1]}. Euclidean loss is used for regressing to real-valued labels (−∞:∞){\\displaystyle (-\\infty :\\infty )}. == Hyperparameters == Hyperparameters are various settings that are used to control the learning process. CNNs use more': 'original_similarity': 0.8049787205643828: 'slide': 233}","{'chunk_text': 'predicted output of the network: and the true data labels (during supervised learning). Various loss functions can be used: depending on the specific task. The Softmax loss function is used for predicting a single class of K mutually exclusive classes. Sigmoid cross-entropy loss is used for predicting K independent probability values in [0:1]{\\displaystyle [0:1]}. Euclidean loss is used for regressing to real-valued labels (−∞:∞){\\displaystyle (-\\infty :\\infty )}. == Hyperparameters == Hyperparameters are various settings that are used to control the learning process. CNNs use more hyperparameters than a standard multilayer perceptron (MLP). === Kernel size === The kernel is the number of pixels processed together. It is typically expressed as the kernel\'s dimensions: e.g.: 2x2: or 3x3. === Padding === Padding is the addition of (typically) 0-valued pixels on the borders of an image. This is done so that the border pixels are not undervalued (lost) from the output because they would ordinarily participate in only a single receptive field instance. The padding applied is typically one less than the corresponding kernel dimension. For example: a convolutional layer using 3x3 kernels would receive a 2-pixel pad: that is 1 pixel on each side of the image. === Stride === The stride is the number of pixels that the analysis window moves on each iteration. A stride of 2 means that each kernel is offset by 2 pixels from its predecessor. === Number of filters === Since feature map size decreases with depth: layers near the input layer tend to have fewer filters while higher layers can have more. To equalize computation at each layer: the product of feature values va with pixel position is kept roughly constant across layers. Preserving more information about the input would require keeping the total number of activations (number of feature maps times number of pixel positions) non-decreasing from one layer to the next. The number of feature maps directly controls the capacity and depends on the number of available examples and task complexity. === Filter size === Common filter sizes found in the literature vary greatly: and are usually chosen based on the data set. The challenge is to find the right level of granularity so as to create abstractions at the proper scale: given a particular data set: and without overfitting. === Pooling type and size === Max pooling is typically used: often with a 2x2 dimension. This implies that the input is drastically downsampled: reducing processing cost. Greater pooling reduces the dimension of the signal: and may result in unacceptable information loss. Often: non-overlapping pooling windows perform best. === Dilation === Dilation involves ignoring pixels within a kernel. This reduces processing/memory potentially without significant signal loss. A dilation of 2 on a 3x3 kernel expands the kernel to 5x5: while still processing 9 (evenly spaced) pixels. Accordingly: dilation of 4 expands the kernel to 7x7. == Translation equivariance and aliasing == It is commonly assumed that CNNs are invariant to shifts of the input. Convolution or pooling layers within a CNN that do not have a stride greater than one are indeed equivariant to translations of the input. However: layers with a stride greater than one ignore the Nyquist-Shannon sampling theorem and might lead to aliasing of the input signal While: in principle: CNNs are capable of implementing anti-aliasing filters: it has been observed that this does not happen in practice and yield models that are not equivariant to translations. Furthermore: if a CNN makes use of fully connected layers: translation equivariance does not imply translation invariance: as the fully connected layers are not invariant to shifts of the input. One solution for complete translation invariance is avoiding any down-sampling throughout the network and applying global average pooling at the last layer. Additionally: several other partial solutions have been proposed: such as anti-aliasing before downsampling operations: spatial transformer networks: data augmentation: subsampling combined with pooling: and capsule neural networks. == Evaluation == The accuracy of the final model is based on a sub-part of the dataset set apart at the start: often called a test-set. Other times methods such as k-fold cross-validation are applied. Other strategies include using conformal prediction. == Regularization methods == Regularization is a process of introducing additional information to solve an ill-posed problem or to prevent overfitting. CNNs use various types of regularization. === Empirical === ==== Dropout ==== Because a fully connected layer occupies most of the parameters: it is prone to overfitting. One method to reduce overfitting is dropout: introduced in 2014. At each training stage: individual nodes are either ""dropped out"" of the net (ignored) with probability 1−p{\\displaystyle 1-p} or kept with probability p{\\displaystyle p}: so that a reduced network is left; incoming and outgoing edges to a dropped-out node are also removed. Only the reduced network is trained on the data in that stage. The removed nodes are then reinserted into the network with their original weights. In the training stages: p{\\displaystyle p} is usually 0.5; for input nodes: it is typically much higher because information is directly lost when input nodes are ignored. At testing time after training has finished: we would ideally\nlike to find a sample average of all possible 2n{\\displaystyle 2^{n}} dropped-out networks; unfortunately this is unfeasible for large values of n{\\displaystyle n}. However: we can find an approximation by using the full network with each node\'s output weighted by a factor of p{\\displaystyle p}: so the expected value of the output of any node is the same as in the training stages. This is the biggest contribution of the dropout method: although it effectively generates 2n{\\displaystyle 2^{n}} neural nets: and as such allows for model combination: at test time only a single network needs to be tested. By avoiding training all nodes on all training data: dropout decreases overfitting. The method also significantly improves training speed. This makes the model combination practical: even for deep neural networks. The technique seems to reduce node interactions: leading them to learn more robust features that better generalize to new data. ==== DropConnect ====': 'original_similarity': 0.8043811757058906: 'slide': 149}"
0.7228235000220593,Can you name and explain an variant of ReLU?,"A variant of ReLU is the Leaky ReLU. It allows a small, positive gradient when the unit is not active, helping to mitigate the vanishing gradient problem.","{'chunk_text': '50% of hidden units are activated (have a non-zero output). Better gradient propagation: Fewer vanishing gradient problems compared to sigmoidal activation functions that saturate in both directions. Efficient computation: Only comparison: addition and multiplication. Scale-invariant: max(0:ax)=amax(0:x) for a≥0{\\displaystyle \\max(0:ax)=a\\max(0:x){\\text{ for }}a\\geq 0}.Rectifying activation functions were used to separate specific excitation and unspecific inhibition in the neural abstraction pyramid: which was trained in a supervised way to learn several computer vision tasks. In 2011: the use of the rectifier as a non-linearity has been shown to enable training deep supervised neural networks without requiring unsupervised pre-training. Rectified linear units: compared to sigmoid function or similar activation functions: allow faster and effective training of deep neural architectures on large and complex datasets. == Potential problems == Non-differentiable at zero; however: it is differentiable anywhere else: and the value of the derivative at zero can be arbitrarily chosen to be 0 or 1. Not zero-centered. Unbounded. Dying ReLU problem: ReLU (rectified linear unit) neurons can sometimes be pushed into states in which they become inactive for essentially all inputs. In this state: no gradients flow backward through the neuron: and so the neuron becomes stuck in a perpetually inactive state and ""dies"". This is a form of the vanishing gradient problem. In some cases: large numbers of neurons in a network can become stuck in dead states: effectively decreasing the model capacity. This problem typically arises when the learning rate is set too high. It may be mitigated by using leaky ReLUs instead: which assign a small positive slope for x < 0; however: the performance is reduced. == Variants == === Piecewise-linear variants === ==== Leaky ReLU ==== Leaky ReLUs allow a small: positive gradient when the unit is not active: helping to mitigate the vanishing gradient problem. f(x)={xif x>0:0.01xotherwise.f′(x)={1if x>0:0.01otherwise.{\\displaystyle f(x)={\\begin{cases}x&{\\text{if }}x>0:\\\\0.01x&{\\text{otherwise}}.\\end{cases}}\\qquad \\qquad f\'(x)={\\begin{cases}1&{\\text{if }}x>0:\\\\0.01&{\\text{otherwise}}.\\end{cases}}} ==== Parametric ReLU ==== Parametric ReLUs (PReLUs) take this idea further by making the coefficient of leakage into a parameter that is learned along with the other neural-network parameters. f(x)={xif x>0:a⋅xotherwise.f′(x)={1if x>0:aotherwise.{\\displaystyle f(x)={\\begin{cases}x&{\\text{if }}x>0:\\\\a\\cdot x&{\\text{otherwise}}.\\end{cases}}\\qquad \\qquad \\qquad f\'(x)={\\begin{cases}1&{\\text{if }}x>0:\\\\a&{\\text{otherwise}}.\\end{cases}}}Note that for a ≤ 1: this is equivalent to f(x)=max(x:ax){\\displaystyle f(x)=\\max(x:ax)}and thus has a relation to ""maxout"" networks. === Other non-linear variants === ==== Gaussian-error linear unit (GELU) ==== GELU is a smooth approximation to the rectifier: f(x)=x⋅Φ(x):{\\displaystyle f(x)=x\\cdot \\Phi (x):}f′(x)=x⋅Φ′(x)+Φ(x):{\\displaystyle f\'(x)=x\\cdot \\Phi \'(x)+\\Phi (x):}where Φ(x)=P(X⩽x){\\displaystyle \\Phi (x)=P(X\\leqslant x)} is the cumulative distribution function of the standard normal distribution. This activation function is illustrated in the figure at the start of this article. It has a ""bump"" to the left of x < 0 and serves as the default activation for models such as BERT. ==== SiLU ==== The SiLU (sigmoid linear unit) or swish function is another smooth approximation: first coined in the GELU paper: f(x)=x⋅sigmoid\u2061(x):{\\displaystyle f(x)=x\\cdot \\operatorname {sigmoid} (x):}f′(x)=x⋅sigmoid′\u2061(x)+sigmoid\u2061(x):{\\displaystyle f\'(x)=x\\cdot \\operatorname {sigmoid} \'(x)+\\operatorname {sigmoid} (x):}where sigmoid\u2061(x){\\displaystyle \\operatorname {sigmoid} (x)} is the sigmoid function. ==== Softplus ==== A smooth approximation to the rectifier is the analytic function f(x)=ln\u2061(1+ex):f′(x)=ex1+ex=11+e−x:{\\displaystyle f(x)=\\ln(1+e^{x}):\\qquad \\qquad f\'(x)={\\frac {e^{x}}{1+e^{x}}}={\\frac {1}{1+e^{-x}}}:}which is called the softplus or SmoothReLU function. For large negative x{\\displaystyle x} it is roughly ln\u20611{\\displaystyle \\ln 1}: so just above 0: while for large positive x{\\displaystyle x} it is roughly ln\u2061(ex){\\displaystyle \\ln(e^{x})}: so just above x{\\displaystyle x}. This function can be approximated as: ln\u2061(1+ex)≈{ln\u20612:x=0:x1−e−x/ln\u20612:x≠0{\\displaystyle \\ln \\left(1+e^{x}\\right)\\approx {\\begin{cases}\\ln 2:&x=0:\\\\[6pt]{\\frac {x}{1-e^{-x/\\ln 2}}}:&x\\neq 0\\end{cases}}}By making the change of variables x=yln\u2061(2){\\displaystyle x=y\\ln(2)}: this is equivalent to log2\u2061(1+2y)≈{1:y=0:y1−e−y:y≠0.{\\displaystyle \\log _{2}(1+2^{y})\\approx {\\begin{cases}1:&y=0:\\\\[6pt]{\\frac {y}{1-e^{-y}}}:&y\\neq 0.\\end{cases}}}A sharpness parameter k{\\displaystyle k} may be included: f(x)=ln\u2061(1+ekx)k:f′(x)=ekx1+ekx=11+e−kx.{\\displaystyle f(x)={\\frac {\\ln(1+e^{kx})}{k}}:\\qquad \\qquad f\'(x)={\\frac {e^{kx}}{1+e^{kx}}}={\\frac {1}{1+e^{-kx}}}.}The derivative of softplus is the logistic function. The logistic sigmoid function is a smooth approximation of the derivative of the rectifier: the Heaviside step function. The multivariable generalization of single-variable softplus is the LogSumExp with the first argument set to zero: LSE0+\u2061(x1:…:xn):=LSE\u2061(0:x1:…:xn)=ln\u2061(1+ex1+⋯+exn).{\\displaystyle \\operatorname {LSE_{0}} ^{+}(x_{1}:\\dots :x_{n}):=\\operatorname {LSE} (0:x_{1}:\\dots :x_{n})=\\ln(1+e^{x_{1}}+\\cdots +e^{x_{n}}).}The LogSumExp function is LSE\u2061(x1:…:xn)=ln\u2061(ex1+⋯+exn):{\\displaystyle \\operatorname {LSE} (x_{1}:\\dots :x_{n})=\\ln(e^{x_{1}}+\\cdots +e^{x_{n}}):}and its gradient is the softmax; the softmax with the first argument set to zero is the multivariable generalization of the logistic function. Both LogSumExp and softmax are used in machine learning. ==== ELU ==== Exponential linear units try to make the mean activations closer to zero: which speeds up learning. It has been shown that ELUs can obtain higher classification accuracy than ReLUs. f(x)={xif x>0:a(ex−1)otherwise.f′(x)={1if x>0:a⋅exotherwise.{\\displaystyle f(x)={\\begin{cases}x&{\\text{if }}x>0:\\\\a\\left(e^{x}-1\\right)&{\\text{otherwise}}.\\end{cases}}\\qquad \\qquad f\'(x)={\\begin{cases}1&{\\text{if }}x>0:\\\\a\\cdot e^{x}&{\\text{otherwise}}.\\end{cases}}}In these formulas: a{\\displaystyle a} is a hyper-parameter to be tuned with the constraint a≥0{\\displaystyle a\\geq 0}. The ELU can be viewed as a smoothed version of a shifted ReLU (SReLU): which has the form f(x)=max(−a:x){\\displaystyle f(x)=\\max(-a:x)}: given the same interpretation of a{\\displaystyle a}. ==== Mish ==== The mish function can also be used as a smooth approximation of the rectifier. It is defined as f(x)=xtanh\u2061(softplus\u2061(x)):{\\displaystyle f(x)=x\\tanh {\\big (}\\operatorname {softplus} (x){\\big )}:}where tanh\u2061(x){\\displaystyle \\tanh(x)} is the hyperbolic tangent: and softplus\u2061(x){\\displaystyle \\operatorname {softplus} (x)} is the softplus function. Mish is non-monotonic and self-gated. It was inspired by Swish: itself a variant of ReLU. ==== Squareplus ==== Squareplus is the function squareplusb\u2061(x)=x+x2+b2{\\displaystyle\n\\operatorname {squareplus} _{b}(x)={\\frac {x+{\\sqrt {x^{2}+b}}}{2}}}where b≥0{\\displaystyle b\\geq 0} is a hyperparameter that determines the ""size"" of the curved region near x=0{\\displaystyle x=0}. (For example: letting b=0{\\displaystyle b=0} yields ReLU: and letting b=4{\\displaystyle b=4} yields the metallic mean function.) Squareplus shares many properties with softplus: It is monotonic: strictly positive: approaches 0 as x→−∞{\\displaystyle x\\to -\\infty }: approaches the identity as x→+∞{\\displaystyle x\\to +\\infty }: and is C∞{\\displaystyle C^{\\infty }} smooth. However: squareplus can be computed using only algebraic functions: making it well-suited for settings where computational resources or instruction sets are limited. Additionally: squareplus requires no special consideration to ensure numerical stability when x{\\displaystyle x} is large. == See also == Softmax function Sigmoid function Tobit model Layer (deep learning) == References ==': 'original_similarity': 0.8300311165974136: 'slide': 179}","{'chunk_text': 'with an optical computing system.In 1989: Yann LeCun et al. trained a CNN with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked: training required 3 days. Learning was fully automatic: performed better than manual coefficient design: and was suited to a broader range of image recognition problems and image types. Subsequently: Wei Zhang: et al. modified their model by removing the last fully connected layer and applied it for medical image object segmentation in 1991 and breast cancer detection in mammograms in 1994.In 1990 Yamaguchi et al. introduced max-pooling: a fixed filtering operation that calculates and propagates the maximum value of a given region. They combined TDNNs with max-pooling in order to realize a speaker independent isolated word recognition system. In a variant of the neocognitron called the cresceptron: instead of using Fukushima\'s spatial averaging: J. Weng et al. also used max-pooling where a downsampling unit computes the maximum of the activations of the units in its patch. Max-pooling is often used in modern CNNs.LeNet-5: a 7-level CNN by Yann LeCun et al. in 1998: that classifies digits: was applied by several banks to recognize hand-written numbers on checks (British English: cheques) digitized in 32x32 pixel images. The ability to process higher-resolution images requires larger and more layers of CNNs: so this technique is constrained by the availability of computing resources. In 2010: Backpropagation training through max-pooling was accelerated by GPUs and shown to perform better than other pooling variants. Behnke (2003) relied only on the sign of the gradient (Rprop) on problems such as image reconstruction and face localization. Rprop is a first-order optimization algorithm created by Martin Riedmiller and Heinrich Braun in 1992.In 2011: a deep GPU-based CNN called ""DanNet"" by Dan Ciresan: Ueli Meier: and Juergen Schmidhuber achieved human-competitive performance for the first time in computer vision contests. Subsequently: a similar GPU-based CNN by Alex Krizhevsky: Ilya Sutskever: and Geoffrey Hinton won the ImageNet Large Scale Visual Recognition Challenge 2012. A very deep CNN with over 100 layers by Kaiming He: Xiangyu Zhang: Shaoqing Ren: and Jian Sun of Microsoft won the ImageNet 2015 contest.ANNs were able to guarantee shift invariance to deal with small and large natural objects in large cluttered scenes: only when invariance extended beyond shift: to all ANN-learned concepts: such as location: type (object class label): scale: lighting and others. This was realized in Developmental Networks (DNs) whose embodiments are Where-What Networks: WWN-1 (2008) through WWN-7 (2013). == Artificial curiosity and generative adversarial networks == In 1991: Juergen Schmidhuber published adversarial neural networks that contest with each other in the form of a zero-sum game: where one network\'s gain is the other network\'s loss. The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called ""artificial curiosity."" Earlier adversarial machine learning systems ""neither involved unsupervised neural networks nor were about modeling data nor used gradient descent.""In 2014: this adversarial principle was used in a generative adversarial network (GAN) by Ian Goodfellow et al. Here the environmental reaction is 1 or 0 depending on whether the first network\'s output is in a given set. This can be used to create realistic deepfakes.In 1992: Schmidhuber also published another type of gradient-based adversarial neural networks where the goal of the zero-sum game is to create disentangled representations of input patterns. This was called predictability minimization.Nvidia\'s StyleGAN (2018) is based on the Progressive GAN by Tero Karras: Timo Aila: Samuli Laine: and Jaakko Lehtinen. Here the GAN generator is grown from small to large scale in a pyramidal fashion. StyleGANs improve consistency between fine and coarse details in the generator network. == Transformers and their variants == Many modern large language models such as ChatGPT: GPT-4: and BERT use a feedforward neural network called Transformer by Ashish Vaswani et. al. in their 2017 paper ""Attention Is All You Need."" Transformers have increasingly become the model of choice for natural language processing problems: replacing recurrent neural networks (RNNs) such as long short-term memory (LSTM).Basic ideas for this go back a long way: in 1992: Juergen Schmidhuber published the Transformer with ""linearized self-attention"" (save for a normalization operator): which is also called the ""linear Transformer."" He advertised it as an ""alternative to RNNs"" that can learn ""internal spotlights of attention:"" and experimentally applied it to problems of variable binding. Here a slow feedforward neural network learns by gradient descent to control the fast weights of another neural network through outer products of self-generated activation patterns called ""FROM"" and ""TO"" which in Transformer terminology are called ""key"" and ""value"" for ""self-attention."" This fast weight ""attention mapping"" is applied to queries. The 2017 Transformer combines this with a softmax operator and a projection matrix.Transformers are\nalso increasingly being used in computer vision. == Deep learning with unsupervised or self-supervised pre-training == In the 1980s: backpropagation did not work well for deep FNNs and RNNs. Here the word ""deep"" refers to the number of layers through which the data is transformed. More precisely: deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For an FNN: the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For RNNs: in which a signal may propagate through a layer more than once: the CAP depth is potentially unlimited. To overcome this problem: Juergen Schmidhuber (1992) proposed a self-supervised hierarchy of RNNs pre-trained one level at a time by self-supervised learning. This ""neural history compressor"" uses predictive coding to learn internal representations at multiple self-organizing time scales. The deep architecture may be used to reproduce the original data from the top level feature activations. The RNN hierarchy can be ""collapsed"" into a single RNN: by ""distilling"" a higher level ""chunker""': 'original_similarity': 0.8143860073989796: 'slide': 196}","{'chunk_text': 'volume. Therefore: it is common to refer to the sets of weights as a filter (or a kernel): which is convolved with the input. The result of this convolution is an activation map: and the set of activation maps for each different filter are stacked together along the depth dimension to produce the output volume. Parameter sharing contributes to the translation invariance of the CNN architecture.Sometimes: the parameter sharing assumption may not make sense. This is especially the case when the input images to a CNN have some specific centered structure; for which we expect completely different features to be learned on different spatial locations. One practical example is when the inputs are faces that have been centered in the image: we might expect different eye-specific or hair-specific features to be learned in different parts of the image. In that case it is common to relax the parameter sharing scheme: and instead simply call the layer a ""locally connected layer"". === Pooling layer === Another important concept of CNNs is pooling: which is a form of non-linear down-sampling. There are several non-linear functions to implement pooling: where max pooling is the most common. It partitions the input image into a set of rectangles and: for each such sub-region: outputs the maximum. Intuitively: the exact location of a feature is less important than its rough location relative to other features. This is the idea behind the use of pooling in convolutional neural networks. The pooling layer serves to progressively reduce the spatial size of the representation: to reduce the number of parameters: memory footprint and amount of computation in the network: and hence to also control overfitting. This is known as down-sampling. It is common to periodically insert a pooling layer between successive convolutional layers (each one typically followed by an activation function: such as a ReLU layer) in a CNN architecture.: 460–461 While pooling layers contribute to local translation invariance: they do not provide global translation invariance in a CNN: unless a form of global pooling is used. The pooling layer commonly operates independently on every depth: or slice: of the input and resizes it spatially. A very common form of max pooling is a layer with filters of size 2×2: applied with a stride of 2: which subsamples every depth slice in the input by 2 along both width and height: discarding 75% of the activations: In this case: every max operation is over 4 numbers. The depth dimension remains unchanged (this is true for other forms of pooling as well). In addition to max pooling: pooling units can use other functions: such as average pooling or ℓ2-norm pooling. Average pooling was often used historically but has recently fallen out of favor compared to max pooling: which generally performs better in practice.Due to the effects of fast spatial reduction of the size of the representation: there is a recent trend towards using smaller filters or discarding pooling layers altogether. ""Region of Interest"" pooling (also known as RoI pooling) is a variant of max pooling: in which output size is fixed and input rectangle is a parameter.Pooling is a downsampling method and an important component of convolutional neural networks for object detection based on the Fast R-CNN architecture. === Channel Max Pooling === A CMP operation layer conducts the MP operation along the channel side among the corresponding positions of the consecutive feature maps for the purpose of redundant information elimination. The CMP makes the significant features gather together within fewer channels: which is important for fine-grained image classification that needs more discriminating features. Meanwhile: another advantage of the CMP operation is to make the channel number of feature maps smaller before it connects to the first fully connected (FC) layer. Similar to the MP operation: we denote the input feature maps and output feature maps of a CMP layer as F ∈ R(C×M×N) and C ∈ R(c×M×N): respectively: where C and c are the channel numbers of the input and output feature maps: M and N are the widths and the height of the feature maps: respectively. Note that the CMP operation only changes the channel number of the feature maps. The width and the height of the feature maps are not changed: which is different from the MP operation. === ReLU layer === ReLU is the abbreviation of rectified linear unit introduced by Kunihiko Fukushima in 1969. ReLU applies the non-saturating activation function f(x)=max(0:x){\\textstyle f(x)=\\max(0:x)}. It effectively removes negative values from an activation map by setting them to zero. It introduces nonlinearity to the decision function and in the overall network without affecting the receptive fields of the convolution layers. In 2011: Xavier Glorot: Antoine Bordes and Yoshua Bengio found that ReLU enables better training\nof deeper networks: compared to widely used activation functions prior to 2011. Other functions can also be used to increase nonlinearity: for example the saturating hyperbolic tangent f(x)=tanh\u2061(x){\\displaystyle f(x)=\\tanh(x)}: f(x)=|tanh\u2061(x)|{\\displaystyle f(x)=|\\tanh(x)|}: and the sigmoid function σ(x)=(1+e−x)−1{\\textstyle \\sigma (x)=(1+e^{-x})^{-1}}. ReLU is often preferred to other functions because it trains the neural network several times faster without a significant penalty to generalization accuracy. === Fully connected layer === After several convolutional and max pooling layers: the final classification is done via fully connected layers. Neurons in a fully connected layer have connections to all activations in the previous layer: as seen in regular (non-convolutional) artificial neural networks. Their activations can thus be computed as an affine transformation: with matrix multiplication followed by a bias offset (vector addition of a learned or fixed bias term). === Loss layer === The ""loss layer"": or ""loss function"": specifies how training penalizes the deviation between the predicted output of the network: and the true data labels (during supervised learning). Various loss functions can be used: depending on the specific task. The Softmax loss function is used for predicting a single class of K mutually exclusive classes. Sigmoid cross-entropy loss is used for predicting K independent probability values in [0:1]{\\displaystyle [0:1]}. Euclidean loss is used for regressing to real-valued labels (−∞:∞){\\displaystyle': 'original_similarity': 0.8127232689883315: 'slide': 212}"
0.41368170001078397,What is a recurrent neural network (RNN)?,"A recurrent neural network (RNN) is one of the two broad types of artificial neural network, characterized by the flow of information between its layers. It is a bi-directional artificial neural network, meaning it allows the output from some nodes to affect subsequent input to the same nodes. ","{'chunk_text': 'to process arbitrary sequences of inputs. == History == The Ising model (1925) by Wilhelm Lenz and Ernst Ising was the first RNN architecture that did not learn. Shun\'ichi Amari made it adaptive in 1972. This was also called the Hopfield network (1982). See also David Rumelhart\'s work in 1986. In 1993: a neural history compressor system solved a ""Very Deep Learning"" task that required more than 1000 subsequent layers in an RNN unfolded in time. === LSTM === Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1997 and set accuracy records in multiple applications domains.Around 2007: LSTM started to revolutionize speech recognition: outperforming traditional models in certain speech applications. In 2009: a Connectionist Temporal Classification (CTC)-trained LSTM network was the first RNN to win pattern recognition contests when it won several competitions in connected handwriting recognition. In 2014: the Chinese company Baidu used CTC-trained RNNs to break the 2S09 Switchboard Hub5\'00 speech recognition dataset benchmark without using any traditional speech processing methods.LSTM also improved large-vocabulary speech recognition and text-to-speech synthesis and was used in Google Android. In 2015: Google\'s speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM.LSTM broke records for improved machine translation: Language Modeling and Multilingual Language Processing. LSTM combined with convolutional neural networks (CNNs) improved automatic image captioning. == Architectures == RNNs come in many variants. === Fully recurrent === Fully recurrent neural networks (FRNN) connect the outputs of all neurons to the inputs of all neurons. This is the most general neural network topology because all other topologies can be represented by setting some connection weights to zero to simulate the lack of connections between those neurons. The illustration to the right may be misleading to many because practical neural network topologies are frequently organized in ""layers"" and the drawing gives that appearance. However: what appears to be layers are: in fact: different steps in time of the same fully recurrent neural network. The left-most item in the illustration shows the recurrent connections as the arc labeled \'v\'. It is ""unfolded"" in time to produce the appearance of layers. === Elman networks and Jordan networks === An Elman network is a three-layer network (arranged horizontally as x: y: and z in the illustration) with the addition of a set of context units (u in the illustration). The middle (hidden) layer is connected to these context units fixed with a weight of one. At each time step: the input is fed forward and a learning rule is applied. The fixed back-connections save a copy of the previous values of the hidden units in the context units (since they propagate over the connections before the learning rule is applied). Thus the network can maintain a sort of state: allowing it to perform such tasks as sequence-prediction that are beyond the power of a standard multilayer perceptron. Jordan networks are similar to Elman networks. The context units are fed from the output layer instead of the hidden layer. The context units in a Jordan network are also called the state layer. They have a recurrent connection to themselves.Elman and Jordan networks are also known as ""Simple recurrent networks"" (SRN). Elman network ht=σh(Whxt+Uhht−1+bh)yt=σy(Wyht+by){\\displaystyle {\\begin{aligned}h_{t}&=\\sigma _{h}(W_{h}x_{t}+U_{h}h_{t-1}+b_{h})\\\\y_{t}&=\\sigma _{y}(W_{y}h_{t}+b_{y})\\end{aligned}}} Jordan network ht=σh(Whxt+Uhyt−1+bh)yt=σy(Wyht+by){\\displaystyle {\\begin{aligned}h_{t}&=\\sigma _{h}(W_{h}x_{t}+U_{h}y_{t-1}+b_{h})\\\\y_{t}&=\\sigma _{y}(W_{y}h_{t}+b_{y})\\end{aligned}}}Variables and functions xt{\\displaystyle x_{t}}: input vector ht{\\displaystyle h_{t}}: hidden layer vector yt{\\displaystyle y_{t}}: output vector W{\\displaystyle W}: U{\\displaystyle U} and b{\\displaystyle b}: parameter matrices and vector σh{\\displaystyle \\sigma _{h}} and σy{\\displaystyle \\sigma _{y}}: Activation functions === Hopfield === The Hopfield network is an RNN in which all connections across layers are equally sized. It requires stationary inputs and is thus not a general RNN: as it does not process sequences of patterns. However: it guarantees that it will converge. If the connections are trained using Hebbian learning: then the Hopfield network can perform as robust content-addressable memory: resistant to connection alteration. ==== Bidirectional associative memory ==== Introduced by Bart Kosko: a bidirectional associative memory (BAM) network is a variant of a Hopfield network that stores associative data as a vector. The bi-directionality comes from passing information through a matrix and its transpose. Typically: bipolar encoding is preferred to binary encoding of the associative pairs. Recently: stochastic BAM models using Markov stepping were optimized for increased network stability and relevance to real-world applications.A BAM network has two layers: either of which can be driven as an input to recall an association and produce an output on the other layer. === Echo state === Echo state networks (ESN) have a sparsely connected random hidden\nlayer. The weights of output neurons are the only part of the network that can change (be trained). ESNs are good at reproducing certain time series. A variant for spiking neurons is known as a liquid state machine. === Independently RNN (IndRNN) === The independently recurrent neural network (IndRNN) addresses the gradient vanishing and exploding problems in the traditional fully connected RNN. Each neuron in one layer only receives its own past state as context information (instead of full connectivity to all other neurons in this layer) and thus neurons are independent of each other\'s history. The gradient backpropagation can be regulated to avoid gradient vanishing and exploding in order to keep long or short-term memory. The cross-neuron information is explored in the next layers. IndRNN can be robustly trained with non-saturated nonlinear functions such as ReLU. Deep networks can be trained using skip connections. === Recursive === A recursive neural network is created by applying the same set of weights recursively over a differentiable graph-like structure by traversing the structure in topological order. Such networks are typically also trained by the reverse mode of automatic differentiation. They can process distributed representations of structure: such as logical terms. A special case of recursive neural networks is the RNN whose structure corresponds to a linear chain. Recursive neural networks have been applied to natural language processing. The Recursive Neural Tensor Network uses a tensor-based composition function for all nodes in the tree. === Neural history compressor === The neural': 'original_similarity': 0.9022969122783122: 'slide': 246}","{'chunk_text': 'neurons are independent of each other\'s history. The gradient backpropagation can be regulated to avoid gradient vanishing and exploding in order to keep long or short-term memory. The cross-neuron information is explored in the next layers. IndRNN can be robustly trained with non-saturated nonlinear functions such as ReLU. Deep networks can be trained using skip connections. === Recursive === A recursive neural network is created by applying the same set of weights recursively over a differentiable graph-like structure by traversing the structure in topological order. Such networks are typically also trained by the reverse mode of automatic differentiation. They can process distributed representations of structure: such as logical terms. A special case of recursive neural networks is the RNN whose structure corresponds to a linear chain. Recursive neural networks have been applied to natural language processing. The Recursive Neural Tensor Network uses a tensor-based composition function for all nodes in the tree. === Neural history compressor === The neural history compressor is an unsupervised stack of RNNs. At the input level: it learns to predict its next input from the previous inputs. Only unpredictable inputs of some RNN in the hierarchy become inputs to the next higher level RNN: which therefore recomputes its internal state only rarely. Each higher level RNN thus studies a compressed representation of the information in the RNN below. This is done such that the input sequence can be precisely reconstructed from the representation at the highest level. The system effectively minimizes the description length or the negative logarithm of the probability of the data. Given a lot of learnable predictability in the incoming data sequence: the highest level RNN can use supervised learning to easily classify even deep sequences with long intervals between important events. It is possible to distill the RNN hierarchy into two RNNs: the ""conscious"" chunker (higher level) and the ""subconscious"" automatizer (lower level). Once the chunker has learned to predict and compress inputs that are unpredictable by the automatizer: then the automatizer can be forced in the next learning phase to predict or imitate through additional units the hidden units of the more slowly changing chunker. This makes it easy for the automatizer to learn appropriate: rarely changing memories across long intervals. In turn: this helps the automatizer to make many of its once unpredictable inputs predictable: such that the chunker can focus on the remaining unpredictable events.A generative model partially overcame the vanishing gradient problem of automatic differentiation or backpropagation in neural networks in 1992. In 1993: such a system solved a ""Very Deep Learning"" task that required more than 1000 subsequent layers in an RNN unfolded in time. === Second order RNNs === Second-order RNNs use higher order weights wijk{\\displaystyle w{}_{ijk}} instead of the standard wij{\\displaystyle w{}_{ij}} weights: and states can be a product. This allows a direct mapping to a finite-state machine both in training: stability: and representation. Long short-term memory is an example of this but has no such formal mappings or proof of stability. === Long short-term memory === Long short-term memory (LSTM) is a deep learning system that avoids the vanishing gradient problem. LSTM is normally augmented by recurrent gates called ""forget gates"". LSTM prevents backpropagated errors from vanishing or exploding. Instead: errors can flow backward through unlimited numbers of virtual layers unfolded in space. That is: LSTM can learn tasks that require memories of events that happened thousands or even millions of discrete time steps earlier. Problem-specific LSTM-like topologies can be evolved. LSTM works even given long delays between significant events and can handle signals that mix low and high-frequency components. Many applications use stacks of LSTM RNNs and train them by connectionist temporal classification (CTC) to find an RNN weight matrix that maximizes the probability of the label sequences in a training set: given the corresponding input sequences. CTC achieves both alignment and recognition. LSTM can learn to recognize context-sensitive languages unlike previous models based on hidden Markov models (HMM) and similar concepts. === Gated recurrent unit === Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks introduced in 2014. They are used in the full form and several simplified variants. Their performance on polyphonic music modeling and speech signal modeling was found to be similar to that of long short-term memory. They have fewer parameters than LSTM: as they lack an output gate. === Bi-directional === Bi-directional RNNs use a finite sequence to predict or label each element of the sequence based on the element\'s past and future contexts. This is done by concatenating the outputs of two RNNs: one processing the sequence from left to right: and the other one from right to left. The combined outputs are the predictions of the teacher-given target signals. This technique has been proven to be especially useful when combined with LSTM RNNs. === Continuous-time === A continuous-time recurrent neural network (CTRNN) uses a system of ordinary differential equations to model the effects on a neuron of the incoming inputs. For a neuron i{\\displaystyle i} in the network with activation yi{\\displaystyle y_{i}}: the rate of change of activation is given by: τiy˙i=−yi+∑j=1nwjiσ(yj−Θj)+Ii(t){\\displaystyle \\tau _{i}{\\dot {y}}_{i}=-y_{i}+\\sum _{j=1}^{n}w_{ji}\\sigma (y_{j}-\\Theta _{j})+I_{i}(t)}Where: τi{\\displaystyle \\tau _{i}} : Time constant of postsynaptic node yi{\\displaystyle y_{i}} : Activation of postsynaptic node y˙i{\\displaystyle {\\dot {y}}_{i}} : Rate of change of activation of postsynaptic node wji{\\displaystyle w{}_{ji}} : Weight of connection from pre to postsynaptic node σ(x){\\displaystyle \\sigma (x)} : Sigmoid of x e.g. σ(x)=1/(1+e−x){\\displaystyle \\sigma (x)=1/(1+e^{-x})}. yj{\\displaystyle y_{j}} : Activation of\npresynaptic node Θj{\\displaystyle \\Theta _{j}} : Bias of presynaptic node Ii(t){\\displaystyle I_{i}(t)} : Input (if any) to nodeCTRNNs have been applied to evolutionary robotics where they have been used to address vision: co-operation: and minimal cognitive behaviour.Note that: by the Shannon sampling theorem: discrete-time recurrent neural networks can be viewed as continuous-time recurrent neural networks where the differential equations have transformed into equivalent difference equations. This transformation can be thought of as occurring after the post-synaptic node activation functions yi(t){\\displaystyle y_{i}(t)} have been low-pass filtered but prior to sampling. ===': 'original_similarity': 0.864081192730425: 'slide': 88}","{'chunk_text': ""reading == Mandic: Danilo P.; Chambers: Jonathon A. (2001). Recurrent Neural Networks for Prediction: Learning Algorithms: Architectures and Stability. Wiley. ISBN 978-0-471-49517-8. == External links == Recurrent Neural Networks with over 60 RNN papers by Jürgen Schmidhuber's group at Dalle Molle Institute for Artificial Intelligence Research Elman Neural Network implementation for WEKA\nA residual neural network (also referred to as"": 'original_similarity': 0.8612490942316833: 'slide': 6}"
0.4184774000023026,What is the difference between recurrent neural networks and convolutional neural networks in terms of impulse response?,"The term ""recurrent neural network"" is used to refer to the class of networks with an infinite impulse response, whereas ""convolutional neural network"" refers to the class of finite impulse response. Both classes of networks exhibit temporal dynamic behavior.","{'chunk_text': 'History == The Ising model (1925) by Wilhelm Lenz and Ernst Ising was the first RNN architecture that did not learn. Shun\'ichi Amari made it adaptive in 1972. This was also called the Hopfield network (1982). See also David Rumelhart\'s work in 1986. In 1993: a neural history compressor system solved a ""Very Deep Learning"" task that required more than 1000 subsequent layers in an RNN unfolded in time. === LSTM === Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1997 and set accuracy records in multiple applications domains.Around 2007: LSTM started to revolutionize speech recognition: outperforming traditional models in certain speech applications. In 2009: a Connectionist Temporal Classification (CTC)-trained LSTM network was the first RNN to win pattern recognition contests when it won several competitions in connected handwriting recognition. In 2014: the Chinese company Baidu used CTC-trained RNNs to break the 2S09 Switchboard Hub5\'00 speech recognition dataset benchmark without using any traditional speech processing methods.LSTM also improved large-vocabulary speech recognition and text-to-speech synthesis and was used in Google Android. In 2015: Google\'s speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM.LSTM broke records for improved machine translation: Language Modeling and Multilingual Language Processing. LSTM combined with convolutional neural networks (CNNs) improved automatic image captioning. == Architectures == RNNs come in many variants. === Fully recurrent === Fully recurrent neural networks (FRNN) connect the outputs of all neurons to the inputs of all neurons. This is the most general neural network topology because all other topologies can be represented by setting some connection weights to zero to simulate the lack of connections between those neurons. The illustration to the right may be misleading to many because practical neural network topologies are frequently organized in ""layers"" and the drawing gives that appearance. However: what appears to be layers are: in fact: different steps in time of the same fully recurrent neural network. The left-most item in the illustration shows the recurrent connections as the arc labeled \'v\'. It is ""unfolded"" in time to produce the appearance of layers. === Elman networks and Jordan networks === An Elman network is a three-layer network (arranged horizontally as x: y: and z in the illustration) with the addition of a set of context units (u in the illustration). The middle (hidden) layer is connected to these context units fixed with a weight of one. At each time step: the input is fed forward and a learning rule is applied. The fixed back-connections save a copy of the previous values of the hidden units in the context units (since they propagate over the connections before the learning rule is applied). Thus the network can maintain a sort of state: allowing it to perform such tasks as sequence-prediction that are beyond the power of a standard multilayer perceptron. Jordan networks are similar to Elman networks. The context units are fed from the output layer instead of the hidden layer. The context units in a Jordan network are also called the state layer. They have a recurrent connection to themselves.Elman and Jordan networks are also known as ""Simple recurrent networks"" (SRN). Elman network ht=σh(Whxt+Uhht−1+bh)yt=σy(Wyht+by){\\displaystyle {\\begin{aligned}h_{t}&=\\sigma _{h}(W_{h}x_{t}+U_{h}h_{t-1}+b_{h})\\\\y_{t}&=\\sigma _{y}(W_{y}h_{t}+b_{y})\\end{aligned}}} Jordan network ht=σh(Whxt+Uhyt−1+bh)yt=σy(Wyht+by){\\displaystyle {\\begin{aligned}h_{t}&=\\sigma _{h}(W_{h}x_{t}+U_{h}y_{t-1}+b_{h})\\\\y_{t}&=\\sigma _{y}(W_{y}h_{t}+b_{y})\\end{aligned}}}Variables and functions xt{\\displaystyle x_{t}}: input vector ht{\\displaystyle h_{t}}: hidden layer vector yt{\\displaystyle y_{t}}: output vector W{\\displaystyle W}: U{\\displaystyle U} and b{\\displaystyle b}: parameter matrices and vector σh{\\displaystyle \\sigma _{h}} and σy{\\displaystyle \\sigma _{y}}: Activation functions === Hopfield === The Hopfield network is an RNN in which all connections across layers are equally sized. It requires stationary inputs and is thus not a general RNN: as it does not process sequences of patterns. However: it guarantees that it will converge. If the connections are trained using Hebbian learning: then the Hopfield network can perform as robust content-addressable memory: resistant to connection alteration. ==== Bidirectional associative memory ==== Introduced by Bart Kosko: a bidirectional associative memory (BAM) network is a variant of a Hopfield network that stores associative data as a vector. The bi-directionality comes from passing information through a matrix and its transpose. Typically: bipolar encoding is preferred to binary encoding of the associative pairs. Recently: stochastic BAM models using Markov stepping were optimized for increased network stability and relevance to real-world applications.A BAM network has two layers: either of which can be driven as an input to recall an association and produce an output on the other layer. === Echo state === Echo state networks (ESN) have a sparsely connected random hidden\nlayer. The weights of output neurons are the only part of the network that can change (be trained). ESNs are good at reproducing certain time series. A variant for spiking neurons is known as a liquid state machine. === Independently RNN (IndRNN) === The independently recurrent neural network (IndRNN) addresses the gradient vanishing and exploding problems in the traditional fully connected RNN. Each neuron in one layer only receives its own past state as context information (instead of full connectivity to all other neurons in this layer) and thus neurons are independent of each other\'s history. The gradient backpropagation can be regulated to avoid gradient vanishing and exploding in order to keep long or short-term memory. The cross-neuron information is explored in the next layers. IndRNN can be robustly trained with non-saturated nonlinear functions such as ReLU. Deep networks can be trained using skip connections. === Recursive === A recursive neural network is created by applying the same set of weights recursively over a differentiable graph-like structure by traversing the structure in topological order. Such networks are typically also trained by the reverse mode of automatic differentiation. They can process distributed representations of structure: such as logical terms. A special case of recursive neural networks is the RNN whose structure corresponds to a linear chain. Recursive neural networks have been applied to natural language processing. The Recursive Neural Tensor Network uses a tensor-based composition function for all nodes in the tree. === Neural history compressor === The neural history compressor is an unsupervised stack of': 'original_similarity': 0.8545575735145693: 'slide': 253}","{'chunk_text': ""poorly-populated set.Convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field. CNNs use relatively little pre-processing compared to other image classification algorithms. This means that the network learns to optimize the filters (or kernels) through automated learning: whereas in traditional algorithms these filters are hand-engineered. This independence from prior knowledge and human intervention in feature extraction is a major advantage. == Architecture == A convolutional neural network consists of an input layer: hidden layers and an output layer. In a convolutional neural network: the hidden layers include one or more layers that perform convolutions. Typically this includes a layer that performs a dot product of the convolution kernel with the layer's input matrix. This product is usually the Frobenius inner product: and its activation function is commonly ReLU. As the convolution kernel slides along the input matrix for the layer: the convolution operation generates a feature map: which in turn contributes to the input of the next layer. This is followed by other layers such as pooling layers: fully connected layers: and normalization layers. Here it should be noted how close a convolutional neural network is to a matched filter. === Convolutional layers === In a CNN: the input is a tensor with shape: (number of inputs) × (input height) × (input width) × (input channels) After passing through a convolutional layer: the image becomes abstracted to a feature map: also called an activation map: with shape: (number of inputs) × (feature map height) × (feature map width) × (feature map channels). Convolutional layers convolve the input and pass its result to the next layer. This is similar to the response of a neuron in the visual cortex to a specific stimulus. Each convolutional neuron processes data only for its receptive field. Although fully connected feedforward neural networks can be used to learn features and classify data: this architecture is generally impractical for larger inputs (e.g.: high-resolution images): which would require massive numbers of neurons because each pixel is a relevant input feature. A fully connected layer for an image of size 100 × 100 has 10:000 weights for each neuron in the second layer. Convolution reduces the number of free parameters: allowing the network to be deeper. For example: using a 5 × 5 tiling region: each with the same shared weights: requires only 25 neurons. Using regularized weights over fewer parameters avoids the vanishing gradients and exploding gradients problems seen during backpropagation in earlier neural networks.To speed processing: standard convolutional layers can be replaced by depthwise separable convolutional layers: which are based on a depthwise convolution followed by a pointwise convolution. The depthwise convolution is a spatial convolution applied independently over each channel of the input tensor: while the pointwise convolution is a standard convolution restricted to the use of 1×1{\\displaystyle 1\\times 1} kernels. === Pooling layers === Convolutional networks may include local and/or global pooling layers along with traditional convolutional layers. Pooling layers reduce the dimensions of data by combining the outputs of neuron clusters at one layer into a single neuron in the next layer. Local pooling combines small clusters: tiling sizes such as 2 × 2 are commonly used. Global pooling acts on all the neurons of the feature map. There are two common types of pooling in popular use: max and average. Max pooling uses the maximum value of each local cluster of neurons in the feature map: while average pooling takes the average value. === Fully connected layers === Fully connected layers connect every neuron in one layer to every neuron in another layer. It is the same as a traditional multilayer perceptron neural network (MLP). The flattened matrix goes through a fully connected layer to classify the images. === Receptive field === In neural networks: each neuron receives input from some number of locations in the previous layer. In a convolutional layer: each neuron receives input from only a restricted area of the previous layer called the neuron's receptive field. Typically the area is a square (e.g. 5 by 5 neurons). Whereas: in a fully connected layer: the receptive field is the entire previous layer. Thus: in each convolutional layer: each neuron takes input\nfrom a larger area in the input than previous layers. This is due to applying the convolution over and over: which takes the value of a pixel into account: as well as its surrounding pixels. When using dilated layers: the number of pixels in the receptive field remains constant: but the field is more sparsely populated as its dimensions grow when combining the effect of several layers. To manipulate the receptive field size as desired: there are some alternatives to the standard convolutional layer. For example: atrous or dilated convolution expands the receptive field size without increasing the number of parameters by interleaving visible and blind regions. Moreover: a single dilated convolutional layer can comprise filters with multiple dilation ratios: thus having a variable receptive field size. === Weights === Each neuron in a neural network computes an output value by applying a specific function to the input values received from the receptive field in the previous layer. The function that is applied to the input values is determined by a vector of weights and a bias (typically real numbers). Learning consists of iteratively adjusting these biases and weights. The vectors of weights and biases are called filters and represent particular features of the input (e.g.: a particular shape). A distinguishing feature of CNNs is that many neurons can share the same filter. This reduces the memory footprint because a single bias and a single vector of weights are used across all receptive fields that share that filter: as opposed to each receptive field having its own bias and vector weighting."": 'original_similarity': 0.8289761578617018: 'slide': 259}","{'chunk_text': 'to the input values received from the receptive field in the previous layer. The function that is applied to the input values is determined by a vector of weights and a bias (typically real numbers). Learning consists of iteratively adjusting these biases and weights. The vectors of weights and biases are called filters and represent particular features of the input (e.g.: a particular shape). A distinguishing feature of CNNs is that many neurons can share the same filter. This reduces the memory footprint because a single bias and a single vector of weights are used across all receptive fields that share that filter: as opposed to each receptive field having its own bias and vector weighting. == History == CNN are often compared to the way the brain achieves vision processing in living organisms. === Receptive fields in the visual cortex === Work by Hubel and Wiesel in the 1950s and 1960s showed that cat visual cortices contain neurons that individually respond to small regions of the visual field. Provided the eyes are not moving: the region of visual space within which visual stimuli affect the firing of a single neuron is known as its receptive field. Neighboring cells have similar and overlapping receptive fields. Receptive field size and location varies systematically across the cortex to form a complete map of visual space. The cortex in each hemisphere represents the contralateral visual field.Their 1968 paper identified two basic visual cell types in the brain: simple cells: whose output is maximized by straight edges having particular orientations within their receptive field complex cells: which have larger receptive fields: whose output is insensitive to the exact position of the edges in the field.Hubel and Wiesel also proposed a cascading model of these two types of cells for use in pattern recognition tasks. === Neocognitron: origin of the CNN architecture === The ""neocognitron"" was introduced by Kunihiko Fukushima in 1980. It was inspired by the above-mentioned work of Hubel and Wiesel. The neocognitron introduced the two basic types of layers in CNNs: A convolutional layer which contains units whose receptive fields cover a patch of the previous layer. The weight vector (the set of adaptive parameters) of such a unit is often called a filter. Units can share filters. Downsampling layers which contain units whose receptive fields cover patches of previous convolutional layers. Such a unit typically computes the average of the activations of the units in its patch. This downsampling helps to correctly classify objects in visual scenes even when the objects are shifted.In 1969: Kunihiko Fukushima also introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for CNNs and deep neural networks in general.In a variant of the neocognitron called the cresceptron: instead of using Fukushima\'s spatial averaging: J. Weng et al. in 1993 introduced a method called max-pooling where a downsampling unit computes the maximum of the activations of the units in its patch. Max-pooling is often used in modern CNNs.Several supervised and unsupervised learning algorithms have been proposed over the decades to train the weights of a neocognitron. Today: however: the CNN architecture is usually trained through backpropagation. The neocognitron is the first CNN which requires units located at multiple network positions to have shared weights. Convolutional neural networks were presented at the Neural Information Processing Workshop in 1987: automatically analyzing time-varying signals by replacing learned multiplication with convolution in time: and demonstrated for speech recognition. === Time delay neural networks === The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel et al. for phoneme recognition and was one of the first convolutional networks: as it achieved shift-invariance. A TDNN is a 1-D convolutional neural net where the convolution is performed along the time axis of the data. It is the first CNN utilizing weight sharing in combination with a training by gradient descent: using backpropagation. Thus: while also using a pyramidal structure as in the neocognitron: it performed a global optimization of the weights instead of a local one.. TDNNs are convolutional networks that share weights along the temporal dimension. They allow speech signals to be processed time-invariantly. In 1990 Hampshire and Waibel introduced a variant that performs a two-dimensional convolution. Since these TDNNs operated on spectrograms: the resulting phoneme recognition system was invariant to both time and frequency shifts. This inspired translation invariance in image processing with CNNs. The tiling of neuron outputs can cover timed stages.TDNNs now achieve the best performance in far-distance speech recognition. ==== Max pooling ==== In 1990 Yamaguchi et al. introduced the concept of max pooling: a fixed filtering operation that calculates and propagates the maximum value of a given region. They did so by combining TDNNs with max pooling to realize a speaker-independent isolated word recognition system. In their system they used several TDNNs per word: one for each syllable. The results of each TDNN over the input signal were combined using max pooling and the outputs of the pooling layers were then passed on to networks performing the actual word classification. === Image recognition with CNNs trained by gradient descent === Denker et\nal. (1989) designed a 2-D CNN system to recognize hand-written ZIP Code numbers. However: the lack of an efficient training method to determine the kernel coefficients of the involved convolutions meant that all the coefficients had to be laboriously hand-designed.Following the advances in the training of 1-D CNNs by Waibel et al. (1987): Yann LeCun et al. (1989) used back-propagation to learn the convolution kernel coefficients directly from images of hand-written numbers. Learning was thus fully automatic: performed better than manual coefficient design: and was suited to a broader range of image recognition problems and image types. Wei Zhang et al. (1988) used back-propagation to train the convolution kernels of a CNN for alphabets recognition. The model was called Shift-Invariant Artificial Neural Network (SIANN) before the name CNN was coined later in the early 1990s. Wei Zhang et al. also applied the same CNN without the': 'original_similarity': 0.8240162073635026: 'slide': 144}"
0.39503099999274127,What is a significant feature of LSTM networks related to states?,"It can add additional stored states and the storage under direct control by the network, referred to as gated states or gated memory.","{'chunk_text': 'is explored in the next layers. IndRNN can be robustly trained with non-saturated nonlinear functions such as ReLU. Deep networks can be trained using skip connections. === Recursive === A recursive neural network is created by applying the same set of weights recursively over a differentiable graph-like structure by traversing the structure in topological order. Such networks are typically also trained by the reverse mode of automatic differentiation. They can process distributed representations of structure: such as logical terms. A special case of recursive neural networks is the RNN whose structure corresponds to a linear chain. Recursive neural networks have been applied to natural language processing. The Recursive Neural Tensor Network uses a tensor-based composition function for all nodes in the tree. === Neural history compressor === The neural history compressor is an unsupervised stack of RNNs. At the input level: it learns to predict its next input from the previous inputs. Only unpredictable inputs of some RNN in the hierarchy become inputs to the next higher level RNN: which therefore recomputes its internal state only rarely. Each higher level RNN thus studies a compressed representation of the information in the RNN below. This is done such that the input sequence can be precisely reconstructed from the representation at the highest level. The system effectively minimizes the description length or the negative logarithm of the probability of the data. Given a lot of learnable predictability in the incoming data sequence: the highest level RNN can use supervised learning to easily classify even deep sequences with long intervals between important events. It is possible to distill the RNN hierarchy into two RNNs: the ""conscious"" chunker (higher level) and the ""subconscious"" automatizer (lower level). Once the chunker has learned to predict and compress inputs that are unpredictable by the automatizer: then the automatizer can be forced in the next learning phase to predict or imitate through additional units the hidden units of the more slowly changing chunker. This makes it easy for the automatizer to learn appropriate: rarely changing memories across long intervals. In turn: this helps the automatizer to make many of its once unpredictable inputs predictable: such that the chunker can focus on the remaining unpredictable events.A generative model partially overcame the vanishing gradient problem of automatic differentiation or backpropagation in neural networks in 1992. In 1993: such a system solved a ""Very Deep Learning"" task that required more than 1000 subsequent layers in an RNN unfolded in time. === Second order RNNs === Second-order RNNs use higher order weights wijk{\\displaystyle w{}_{ijk}} instead of the standard wij{\\displaystyle w{}_{ij}} weights: and states can be a product. This allows a direct mapping to a finite-state machine both in training: stability: and representation. Long short-term memory is an example of this but has no such formal mappings or proof of stability. === Long short-term memory === Long short-term memory (LSTM) is a deep learning system that avoids the vanishing gradient problem. LSTM is normally augmented by recurrent gates called ""forget gates"". LSTM prevents backpropagated errors from vanishing or exploding. Instead: errors can flow backward through unlimited numbers of virtual layers unfolded in space. That is: LSTM can learn tasks that require memories of events that happened thousands or even millions of discrete time steps earlier. Problem-specific LSTM-like topologies can be evolved. LSTM works even given long delays between significant events and can handle signals that mix low and high-frequency components. Many applications use stacks of LSTM RNNs and train them by connectionist temporal classification (CTC) to find an RNN weight matrix that maximizes the probability of the label sequences in a training set: given the corresponding input sequences. CTC achieves both alignment and recognition. LSTM can learn to recognize context-sensitive languages unlike previous models based on hidden Markov models (HMM) and similar concepts. === Gated recurrent unit === Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks introduced in 2014. They are used in the full form and several simplified variants. Their performance on polyphonic music modeling and speech signal modeling was found to be similar to that of long short-term memory. They have fewer parameters than LSTM: as they lack an output gate. === Bi-directional === Bi-directional RNNs use a finite sequence to predict or label each element of the sequence based on the element\'s past and future contexts. This is done by concatenating the outputs of two RNNs: one processing the sequence from left to right: and the other one from right to left. The combined outputs are the predictions of the teacher-given target signals. This technique has been proven to be especially useful when combined with LSTM RNNs. === Continuous-time === A continuous-time recurrent neural network (CTRNN) uses a system of ordinary differential equations to model the effects on a neuron of the incoming inputs. For a neuron i{\\displaystyle i} in the network with activation yi{\\displaystyle y_{i}}: the rate of change of activation is given by: τiy˙i=−yi+∑j=1nwjiσ(yj−Θj)+Ii(t){\\displaystyle \\tau _{i}{\\dot {y}}_{i}=-y_{i}+\\sum _{j=1}^{n}w_{ji}\\sigma (y_{j}-\\Theta _{j})+I_{i}(t)}Where: τi{\\displaystyle \\tau _{i}} : Time constant of postsynaptic node yi{\\displaystyle y_{i}} : Activation of postsynaptic node y˙i{\\displaystyle {\\dot {y}}_{i}} : Rate of change of activation of postsynaptic node wji{\\displaystyle w{}_{ji}} : Weight of connection from pre to postsynaptic node σ(x){\\displaystyle \\sigma (x)} : Sigmoid of x e.g. σ(x)=1/(1+e−x){\\displaystyle \\sigma (x)=1/(1+e^{-x})}. yj{\\displaystyle y_{j}} : Activation of\npresynaptic node Θj{\\displaystyle \\Theta _{j}} : Bias of presynaptic node Ii(t){\\displaystyle I_{i}(t)} : Input (if any) to nodeCTRNNs have been applied to evolutionary robotics where they have been used to address vision: co-operation: and minimal cognitive behaviour.Note that: by the Shannon sampling theorem: discrete-time recurrent neural networks can be viewed as continuous-time recurrent neural networks where the differential equations have transformed into equivalent difference equations. This transformation can be thought of as occurring after the post-synaptic node activation functions yi(t){\\displaystyle y_{i}(t)} have been low-pass filtered but prior to sampling. === Hierarchical recurrent neural network === Hierarchical recurrent neural networks (HRNN) connect their neurons in various ways to decompose hierarchical behavior into useful subprograms. Such hierarchical structures of cognition are present': 'original_similarity': 0.8345278167195526: 'slide': 118}","{'chunk_text': '(hidden units). Boltzmann machine learning was at first slow to simulate: but the contrastive divergence algorithm speeds up training for Boltzmann machines and Products of Experts. ==== Self-organizing map ==== The self-organizing map (SOM) uses unsupervised learning. A set of neurons learn to map points in an input space to coordinates in an output space. The input space can have different dimensions and topology from the output space: and SOM attempts to preserve these. ==== Learning vector quantization ==== Learning vector quantization (LVQ) can be interpreted as a neural network architecture. Prototypical representatives of the classes parameterize: together with an appropriate distance measure: in a distance-based classification scheme. === Simple recurrent === Simple recurrent networks have three layers: with the addition of a set of ""context units"" in the input layer. These units connect from the hidden layer or the output layer with a fixed weight of one. At each time step: the input is propagated in a standard feedforward fashion: and then a backpropagation-like learning rule is applied (not performing gradient descent). The fixed back connections leave a copy of the previous values of the hidden units in the context units (since they propagate over the connections before the learning rule is applied). === Reservoir computing === Reservoir computing is a computation framework that may be viewed as an extension of neural networks. Typically an input signal is fed into a fixed (random) dynamical system called a reservoir whose dynamics map the input to a higher dimension. A readout mechanism is trained to map the reservoir to the desired output. Training is performed only at the readout stage. Liquid-state machines are a type of reservoir computing. ==== Echo state ==== The echo state network (ESN) employs a sparsely connected random hidden layer. The weights of output neurons are the only part of the network that are trained. ESN are good at reproducing certain time series. === Long short-term memory === The long short-term memory (LSTM) avoids the vanishing gradient problem. It works even when with long delays between inputs and can handle signals that mix low and high frequency components. LSTM RNN outperformed other RNN and other sequence learning methods such as HMM in applications such as language learning and connected handwriting recognition. === Bi-directional === Bi-directional RNN: or BRNN: use a finite sequence to predict or label each element of a sequence based on both the past and future context of the element. This is done by adding the outputs of two RNNs: one processing the sequence from left to right: the other one from right to left. The combined outputs are the predictions of the teacher-given target signals. This technique proved to be especially useful when combined with LSTM. === Hierarchical === Hierarchical RNN connects elements in various ways to decompose hierarchical behavior into useful subprograms. === Stochastic === A district from conventional neural networks: stochastic artificial neural network used as an approximation to random functions. === Genetic Scale === A RNN (often a LSTM) where a series is decomposed into a number of scales where every scale informs the primary length between two consecutive points. A first order scale consists of a normal RNN: a second order consists of all points separated by two indices and so on. The Nth order RNN connects the first and last node. The outputs from all the various scales are treated as a Committee of Machines and the associated scores are used genetically for the next iteration. == Modular == Biological studies have shown that the human brain operates as a collection of small networks. This realization gave birth to the concept of modular neural networks: in which several small networks cooperate or compete to solve problems. === Committee of machines === A committee of machines (CoM) is a collection of different neural networks that together ""vote"" on a given example. This generally gives a much better result than individual networks. Because neural networks suffer from local minima: starting with the same architecture and training but using randomly different initial weights often gives vastly different results. A CoM tends to stabilize the result. The CoM is similar to the general machine learning bagging method: except that the necessary variety of machines in the committee is obtained by training from different starting weights rather than training on different randomly selected subsets of the training data. === Associative === The associative neural network (ASNN) is an extension of committee of machines that combines multiple feedforward neural networks and the k-nearest neighbor technique. It uses the correlation between ensemble responses as a measure of distance amid the analyzed cases for the kNN. This corrects the Bias of the neural network ensemble. An associative neural network has a memory that can coincide with the training set. If new data become available: the network instantly improves its predictive ability and provides data approximation (self-learns) without\nretraining. Another important feature of ASNN is the possibility to interpret neural network results by analysis of correlations between data cases in the space of models. == Physical == A physical neural network includes electrically adjustable resistance material to simulate artificial synapses. Examples include the ADALINE memristor-based neural network. An optical neural network is a physical implementation of an artificial neural network with optical components. == Dynamic == Dynamic neural networks address nonlinear multivariate behaviour and include (learning of) time-dependent behaviour: such as transient phenomena and delay effects. Techniques to estimate a system process from observed data fall under the general category of system identification. === Cascading === Cascade correlation is an architecture and supervised learning algorithm. Instead of just adjusting the weights in a network of fixed topology: Cascade-Correlation begins with a minimal network: then automatically trains and adds new hidden units one by one: creating a multi-layer structure. Once a new hidden unit has been added to the network: its input-side weights are frozen. This unit then becomes a permanent feature-detector in the network: available for producing outputs or for creating other: more complex feature detectors. The Cascade-Correlation architecture': 'original_similarity': 0.8294686751200087: 'slide': 189}","{'chunk_text': 'history compressor system solved a ""Very Deep Learning"" task that required more than 1000 subsequent layers in an RNN unfolded in time. === LSTM === Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1997 and set accuracy records in multiple applications domains.Around 2007: LSTM started to revolutionize speech recognition: outperforming traditional models in certain speech applications. In 2009: a Connectionist Temporal Classification (CTC)-trained LSTM network was the first RNN to win pattern recognition contests when it won several competitions in connected handwriting recognition. In 2014: the Chinese company Baidu used CTC-trained RNNs to break the 2S09 Switchboard Hub5\'00 speech recognition dataset benchmark without using any traditional speech processing methods.LSTM also improved large-vocabulary speech recognition and text-to-speech synthesis and was used in Google Android. In 2015: Google\'s speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM.LSTM broke records for improved machine translation: Language Modeling and Multilingual Language Processing. LSTM combined with convolutional neural networks (CNNs) improved automatic image captioning. == Architectures == RNNs come in many variants. === Fully recurrent === Fully recurrent neural networks (FRNN) connect the outputs of all neurons to the inputs of all neurons. This is the most general neural network topology because all other topologies can be represented by setting some connection weights to zero to simulate the lack of connections between those neurons. The illustration to the right may be misleading to many because practical neural network topologies are frequently organized in ""layers"" and the drawing gives that appearance. However: what appears to be layers are: in fact: different steps in time of the same fully recurrent neural network. The left-most item in the illustration shows the recurrent connections as the arc labeled \'v\'. It is ""unfolded"" in time to produce the appearance of layers. === Elman networks and Jordan networks === An Elman network is a three-layer network (arranged horizontally as x: y: and z in the illustration) with the addition of a set of context units (u in the illustration). The middle (hidden) layer is connected to these context units fixed with a weight of one. At each time step: the input is fed forward and a learning rule is applied. The fixed back-connections save a copy of the previous values of the hidden units in the context units (since they propagate over the connections before the learning rule is applied). Thus the network can maintain a sort of state: allowing it to perform such tasks as sequence-prediction that are beyond the power of a standard multilayer perceptron. Jordan networks are similar to Elman networks. The context units are fed from the output layer instead of the hidden layer. The context units in a Jordan network are also called the state layer. They have a recurrent connection to themselves.Elman and Jordan networks are also known as ""Simple recurrent networks"" (SRN). Elman network ht=σh(Whxt+Uhht−1+bh)yt=σy(Wyht+by){\\displaystyle {\\begin{aligned}h_{t}&=\\sigma _{h}(W_{h}x_{t}+U_{h}h_{t-1}+b_{h})\\\\y_{t}&=\\sigma _{y}(W_{y}h_{t}+b_{y})\\end{aligned}}} Jordan network ht=σh(Whxt+Uhyt−1+bh)yt=σy(Wyht+by){\\displaystyle {\\begin{aligned}h_{t}&=\\sigma _{h}(W_{h}x_{t}+U_{h}y_{t-1}+b_{h})\\\\y_{t}&=\\sigma _{y}(W_{y}h_{t}+b_{y})\\end{aligned}}}Variables and functions xt{\\displaystyle x_{t}}: input vector ht{\\displaystyle h_{t}}: hidden layer vector yt{\\displaystyle y_{t}}: output vector W{\\displaystyle W}: U{\\displaystyle U} and b{\\displaystyle b}: parameter matrices and vector σh{\\displaystyle \\sigma _{h}} and σy{\\displaystyle \\sigma _{y}}: Activation functions === Hopfield === The Hopfield network is an RNN in which all connections across layers are equally sized. It requires stationary inputs and is thus not a general RNN: as it does not process sequences of patterns. However: it guarantees that it will converge. If the connections are trained using Hebbian learning: then the Hopfield network can perform as robust content-addressable memory: resistant to connection alteration. ==== Bidirectional associative memory ==== Introduced by Bart Kosko: a bidirectional associative memory (BAM) network is a variant of a Hopfield network that stores associative data as a vector. The bi-directionality comes from passing information through a matrix and its transpose. Typically: bipolar encoding is preferred to binary encoding of the associative pairs. Recently: stochastic BAM models using Markov stepping were optimized for increased network stability and relevance to real-world applications.A BAM network has two layers: either of which can be driven as an input to recall an association and produce an output on the other layer. === Echo state === Echo state networks (ESN) have a sparsely connected random hidden\nlayer. The weights of output neurons are the only part of the network that can change (be trained). ESNs are good at reproducing certain time series. A variant for spiking neurons is known as a liquid state machine. === Independently RNN (IndRNN) === The independently recurrent neural network (IndRNN) addresses the gradient vanishing and exploding problems in the traditional fully connected RNN. Each neuron in one layer only receives its own past state as context information (instead of full connectivity to all other neurons in this layer) and thus neurons are independent of each other\'s history. The gradient backpropagation can be regulated to avoid gradient vanishing and exploding in order to keep long or short-term memory. The cross-neuron information is explored in the next layers. IndRNN can be robustly trained with non-saturated nonlinear functions such as ReLU. Deep networks can be trained using skip connections. === Recursive === A recursive neural network is created by applying the same set of weights recursively over a differentiable graph-like structure by traversing the structure in topological order. Such networks are typically also trained by the reverse mode of automatic differentiation. They can process distributed representations of structure: such as logical terms. A special case of recursive neural networks is the RNN whose structure corresponds to a linear chain. Recursive neural networks have been applied to natural language processing. The Recursive Neural Tensor Network uses a tensor-based composition function for all nodes in the tree. === Neural history compressor === The neural history compressor is an unsupervised stack of RNNs. At the input level: it learns to predict its next input from the previous inputs. Only unpredictable inputs of some RNN in the hierarchy become inputs to the next higher level RNN: which therefore recomputes its internal state only rarely. Each higher level RNN thus studies': 'original_similarity': 0.8221434748931253: 'slide': 300}"
0.4286156000162009,What was the first RNN architecture that did not learn and who made it adaptive?,"The Ising model by Wilhelm Lenz and Ernst Ising, made in 1925, was the first RNN architecture that did not learn. It was made adaptive by Shun'ichi Amari in 1972.","{'chunk_text': 'a deep network with eight layers trained by the group method of data handling.The first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun\'ichi Amari. In computer experiments conducted by Amari\'s student Saito: a five layer MLP with two modifiable layers learned internal representations to classify non-linearily separable pattern classes. In 1987 Matthew Brand reported that wide 12-layer nonlinear perceptrons could be fully end-to-end trained to reproduce logic functions of nontrivial circuit depth via gradient descent on small batches of random input/output samples: but concluded that training time on contemporary hardware (sub-megaflop computers) made the technique impractical: and proposed using fixed random early layers as an input hash for a single modifiable layer. Instead: subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique. In 1970: Seppo Linnainmaa published the reverse mode of automatic differentiation of discrete connected networks of nested differentiable functions. This became known as backpropagation. It is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes. The terminology ""back-propagating errors"" was actually introduced in 1962 by Rosenblatt: but he did not know how to implement this: although Henry J. Kelley had a continuous precursor of backpropagation already in 1960 in the context of control theory. In 1982: Paul Werbos applied backpropagation to MLPs in the way that has become standard. In 1985: David E. Rumelhart et al. published an experimental analysis of the technique.Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers began with the Neocognitron introduced by Kunihiko Fukushima in 1980. In 1969: he also introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for CNNs and deep learning in general. CNNs have become an essential tool for computer vision. The term Deep Learning was introduced to the machine learning community by Rina Dechter in 1986: and to artificial neural networks by Igor Aizenberg and colleagues in 2000: in the context of Boolean threshold neurons.In 1988: Wei Zhang et al. applied the backpropagation algorithm to a convolutional neural network (a simplified Neocognitron with convolutional interconnections between the image feature layers and the last fully connected layer) for alphabet recognition. They also proposed an implementation of the CNN with an optical computing system. In 1989: Yann LeCun et al. applied backpropagation to a CNN with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked: training required 3 days. Subsequently: Wei Zhang: et al. modified their model by removing the last fully connected layer and applied it for medical image object segmentation in 1991 and breast cancer detection in mammograms in 1994. LeNet-5 (1998): a 7-level CNN by Yann LeCun et al.: that classifies digits: was applied by several banks to recognize hand-written numbers on checks digitized in 32x32 pixel images. In the 1980s: backpropagation did not work well for deep learning with long credit assignment paths. To overcome this problem: Jürgen Schmidhuber (1992) proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning. It uses predictive coding to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN: by distilling a higher level chunker network into a lower level automatizer network. In 1993: a chunker solved a deep learning task whose depth exceeded 1000.In 1992: Jürgen Schmidhuber also published an alternative to RNNs which is now called a linear Transformer or a Transformer with linearized self-attention (save for a normalization operator). It learns internal spotlights of attention: a slow feedforward neural network learns by gradient descent to control the fast weights of another neural network through outer products of self-generated activation patterns FROM and TO (which are now called key and value for self-attention). This fast weight attention mapping is applied to a query pattern. The modern Transformer was introduced by Ashish Vaswani et al. in their 2017 paper ""Attention Is All You Need"". It combines this with a softmax operator and a projection matrix. Transformers have increasingly become the model of choice for natural language processing. Many modern large language models such as ChatGPT: GPT-4: and BERT use it. Transformers are also increasingly being used in computer vision.In 1991: Jürgen Schmidhuber also published adversarial neural networks that contest with each other in the form of a zero-sum game: where one network\'s gain is the other network\'s loss. The first network is a\ngenerative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called ""artificial curiosity"". In 2014: this principle was used in a generative adversarial network (GAN) by Ian Goodfellow et al. Here the environmental reaction is 1 or 0 depending on whether the first network\'s output is in a given set. This can be used to create realistic deepfakes. Excellent image quality is achieved by Nvidia\'s StyleGAN (2018) based on the Progressive GAN by Tero Karras et al. Here the GAN generator is grown from small to large scale in a pyramidal fashion. Sepp Hochreiter\'s diploma thesis (1991) was called ""one of the most important documents in the history of machine learning"" by his supervisor Schmidhuber. It not only tested the neural history compressor: but also identified and analyzed the vanishing gradient problem. Hochreiter proposed recurrent residual connections to solve this problem. This led to the deep learning method called long short-term memory (LSTM): published in 1997. LSTM recurrent neural networks can learn ""very deep learning"" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. The ""vanilla LSTM"" with forget gate was introduced in 1999 by Felix Gers: Schmidhuber and Fred Cummins. LSTM has become the most cited neural network of the 20th century. In 2015: Rupesh Kumar Srivastava: Klaus Greff: and Schmidhuber used LSTM principles to': 'original_similarity': 0.8161304583606056: 'slide': 243}","{'chunk_text': 'this problem. This led to the deep learning method called long short-term memory (LSTM): published in 1997. LSTM recurrent neural networks can learn ""very deep learning"" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. The ""vanilla LSTM"" with forget gate was introduced in 1999 by Felix Gers: Schmidhuber and Fred Cummins. LSTM has become the most cited neural network of the 20th century. In 2015: Rupesh Kumar Srivastava: Klaus Greff: and Schmidhuber used LSTM principles to create the Highway network: a feedforward neural network with hundreds of layers: much deeper than previous networks. 7 months later: Kaiming He: Xiangyu Zhang; Shaoqing Ren: and Jian Sun won the ImageNet 2015 competition with an open-gated or gateless Highway network variant called Residual neural network. This has become the most cited neural network of the 21st century.In 1994: André de Carvalho: together with Mike Fairhurst and David Bisset: published experimental results of a multi-layer boolean neural network: also known as a weightless neural network: composed of a 3-layers self-organising feature extraction neural network module (SOFT) followed by a multi-layer classification neural network module (GSN): which were independently trained. Each layer in the feature extraction module extracted features with growing complexity regarding the previous layer.In 1995: Brendan Frey demonstrated that it was possible to train (over two days) a network containing six fully connected layers and several hundred hidden units using the wake-sleep algorithm: co-developed with Peter Dayan and Hinton.Since 1997: Sven Behnke extended the feed-forward hierarchical convolutional approach in the Neural Abstraction Pyramid by lateral and backward connections in order to flexibly incorporate context into decisions and iteratively resolve local ambiguities. Simpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) were a popular choice in the 1990s and 2000s: because of artificial neural networks\' computational cost and a lack of understanding of how the brain wires its biological networks. Both shallow and deep learning (e.g.: recurrent nets) of ANNs for speech recognition have been explored for many years. These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively. Key difficulties have been analyzed: including gradient diminishing and weak temporal correlation structure in neural predictive models. Additional difficulties were the lack of training data and limited computing power. Most speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government\'s NSA and DARPA: SRI studied deep neural networks (DNNs) in speech and speaker recognition. The speaker recognition team led by Larry Heck reported significant success with deep neural networks in speech processing in the 1998 National Institute of Standards and Technology Speaker Recognition evaluation. The SRI deep neural network was then deployed in the Nuance Verifier: representing the first major industrial application of deep learning. The principle of elevating ""raw"" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the ""raw"" spectrogram or linear filter-bank features in the late 1990s: showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech: waveforms: later produced excellent larger-scale results.Speech recognition was taken over by LSTM. In 2003: LSTM started to become competitive with traditional speech recognizers on certain tasks. In 2006: Alex Graves: Santiago Fernández: Faustino Gomez: and Schmidhuber combined it with connectionist temporal classification (CTC) in stacks of LSTM RNNs. In 2015: Google\'s speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM: which they made available through Google Voice Search.The impact of deep learning in industry began in the early 2000s: when CNNs already processed an estimated 10% to 20% of all the checks written in the US: according to Yann LeCun. Industrial applications of deep learning to large-scale speech recognition started around 2010. In 2006: publications by Geoff Hinton: Ruslan Salakhutdinov: Osindero and Teh showed how a many-layered feedforward neural network could be effectively pre-trained one layer at a time: treating each layer in turn as an unsupervised restricted Boltzmann machine: then fine-tuning it using supervised backpropagation. The papers referred to learning for deep belief nets. The 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech: and the possibility that given more capable hardware and large-scale data sets that deep neural nets might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets. However: it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large: context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems. The nature of the recognition errors produced by the two types of systems was characteristically different: offering technical insights into how to integrate deep learning into the existing highly\nefficient: run-time speech decoding system deployed by all major speech recognition systems. Analysis around 2009–2010: contrasting the GMM (and other generative speech models) vs. DNN models: stimulated early industrial investment in deep learning for speech recognition. That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models. In 2010: researchers extended deep learning from TIMIT to large vocabulary speech recognition: by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees.Deep learning is part of state-of-the-art systems in various disciplines: particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification): as well as a range of large-vocabulary speech recognition tasks have steadily improved. Convolutional neural networks were superseded for ASR by CTC for LSTM. but are more successful in computer vision. Advances in hardware have driven renewed interest in deep learning. In 2009:': 'original_similarity': 0.8021462804437107: 'slide': 157}","{'chunk_text': 'a ""Very Deep Learning"" task that required more than 1000 subsequent layers in an RNN unfolded in time. === LSTM === Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1997 and set accuracy records in multiple applications domains.Around 2007: LSTM started to revolutionize speech recognition: outperforming traditional models in certain speech applications. In 2009: a Connectionist Temporal Classification (CTC)-trained LSTM network was the first RNN to win pattern recognition contests when it won several competitions in connected handwriting recognition. In 2014: the Chinese company Baidu used CTC-trained RNNs to break the 2S09 Switchboard Hub5\'00 speech recognition dataset benchmark without using any traditional speech processing methods.LSTM also improved large-vocabulary speech recognition and text-to-speech synthesis and was used in Google Android. In 2015: Google\'s speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM.LSTM broke records for improved machine translation: Language Modeling and Multilingual Language Processing. LSTM combined with convolutional neural networks (CNNs) improved automatic image captioning. == Architectures == RNNs come in many variants. === Fully recurrent === Fully recurrent neural networks (FRNN) connect the outputs of all neurons to the inputs of all neurons. This is the most general neural network topology because all other topologies can be represented by setting some connection weights to zero to simulate the lack of connections between those neurons. The illustration to the right may be misleading to many because practical neural network topologies are frequently organized in ""layers"" and the drawing gives that appearance. However: what appears to be layers are: in fact: different steps in time of the same fully recurrent neural network. The left-most item in the illustration shows the recurrent connections as the arc labeled \'v\'. It is ""unfolded"" in time to produce the appearance of layers. === Elman networks and Jordan networks === An Elman network is a three-layer network (arranged horizontally as x: y: and z in the illustration) with the addition of a set of context units (u in the illustration). The middle (hidden) layer is connected to these context units fixed with a weight of one. At each time step: the input is fed forward and a learning rule is applied. The fixed back-connections save a copy of the previous values of the hidden units in the context units (since they propagate over the connections before the learning rule is applied). Thus the network can maintain a sort of state: allowing it to perform such tasks as sequence-prediction that are beyond the power of a standard multilayer perceptron. Jordan networks are similar to Elman networks. The context units are fed from the output layer instead of the hidden layer. The context units in a Jordan network are also called the state layer. They have a recurrent connection to themselves.Elman and Jordan networks are also known as ""Simple recurrent networks"" (SRN). Elman network ht=σh(Whxt+Uhht−1+bh)yt=σy(Wyht+by){\\displaystyle {\\begin{aligned}h_{t}&=\\sigma _{h}(W_{h}x_{t}+U_{h}h_{t-1}+b_{h})\\\\y_{t}&=\\sigma _{y}(W_{y}h_{t}+b_{y})\\end{aligned}}} Jordan network ht=σh(Whxt+Uhyt−1+bh)yt=σy(Wyht+by){\\displaystyle {\\begin{aligned}h_{t}&=\\sigma _{h}(W_{h}x_{t}+U_{h}y_{t-1}+b_{h})\\\\y_{t}&=\\sigma _{y}(W_{y}h_{t}+b_{y})\\end{aligned}}}Variables and functions xt{\\displaystyle x_{t}}: input vector ht{\\displaystyle h_{t}}: hidden layer vector yt{\\displaystyle y_{t}}: output vector W{\\displaystyle W}: U{\\displaystyle U} and b{\\displaystyle b}: parameter matrices and vector σh{\\displaystyle \\sigma _{h}} and σy{\\displaystyle \\sigma _{y}}: Activation functions === Hopfield === The Hopfield network is an RNN in which all connections across layers are equally sized. It requires stationary inputs and is thus not a general RNN: as it does not process sequences of patterns. However: it guarantees that it will converge. If the connections are trained using Hebbian learning: then the Hopfield network can perform as robust content-addressable memory: resistant to connection alteration. ==== Bidirectional associative memory ==== Introduced by Bart Kosko: a bidirectional associative memory (BAM) network is a variant of a Hopfield network that stores associative data as a vector. The bi-directionality comes from passing information through a matrix and its transpose. Typically: bipolar encoding is preferred to binary encoding of the associative pairs. Recently: stochastic BAM models using Markov stepping were optimized for increased network stability and relevance to real-world applications.A BAM network has two layers: either of which can be driven as an input to recall an association and produce an output on the other layer. === Echo state === Echo state networks (ESN) have a sparsely connected random hidden\nlayer. The weights of output neurons are the only part of the network that can change (be trained). ESNs are good at reproducing certain time series. A variant for spiking neurons is known as a liquid state machine. === Independently RNN (IndRNN) === The independently recurrent neural network (IndRNN) addresses the gradient vanishing and exploding problems in the traditional fully connected RNN. Each neuron in one layer only receives its own past state as context information (instead of full connectivity to all other neurons in this layer) and thus neurons are independent of each other\'s history. The gradient backpropagation can be regulated to avoid gradient vanishing and exploding in order to keep long or short-term memory. The cross-neuron information is explored in the next layers. IndRNN can be robustly trained with non-saturated nonlinear functions such as ReLU. Deep networks can be trained using skip connections. === Recursive === A recursive neural network is created by applying the same set of weights recursively over a differentiable graph-like structure by traversing the structure in topological order. Such networks are typically also trained by the reverse mode of automatic differentiation. They can process distributed representations of structure: such as logical terms. A special case of recursive neural networks is the RNN whose structure corresponds to a linear chain. Recursive neural networks have been applied to natural language processing. The Recursive Neural Tensor Network uses a tensor-based composition function for all nodes in the tree. === Neural history compressor === The neural history compressor is an unsupervised stack of RNNs. At the input level: it learns to predict its next input from the previous inputs. Only unpredictable inputs of some RNN in the hierarchy become inputs to the next higher level RNN: which therefore recomputes its internal state only rarely. Each higher level RNN thus studies a compressed representation of': 'original_similarity': 0.7994149546253367: 'slide': 304}"
0.47893630000180565,What is the function of an Elman network in a recurrent neural network?,"An Elman network is a three-layer network with the addition of a set of context units. It can maintain a sort of state, allowing it to perform tasks such as sequence prediction that are beyond the power of a standard multilayer perceptron.","{'chunk_text': 'was also called the Hopfield network (1982). See also David Rumelhart\'s work in 1986. In 1993: a neural history compressor system solved a ""Very Deep Learning"" task that required more than 1000 subsequent layers in an RNN unfolded in time. === LSTM === Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1997 and set accuracy records in multiple applications domains.Around 2007: LSTM started to revolutionize speech recognition: outperforming traditional models in certain speech applications. In 2009: a Connectionist Temporal Classification (CTC)-trained LSTM network was the first RNN to win pattern recognition contests when it won several competitions in connected handwriting recognition. In 2014: the Chinese company Baidu used CTC-trained RNNs to break the 2S09 Switchboard Hub5\'00 speech recognition dataset benchmark without using any traditional speech processing methods.LSTM also improved large-vocabulary speech recognition and text-to-speech synthesis and was used in Google Android. In 2015: Google\'s speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM.LSTM broke records for improved machine translation: Language Modeling and Multilingual Language Processing. LSTM combined with convolutional neural networks (CNNs) improved automatic image captioning. == Architectures == RNNs come in many variants. === Fully recurrent === Fully recurrent neural networks (FRNN) connect the outputs of all neurons to the inputs of all neurons. This is the most general neural network topology because all other topologies can be represented by setting some connection weights to zero to simulate the lack of connections between those neurons. The illustration to the right may be misleading to many because practical neural network topologies are frequently organized in ""layers"" and the drawing gives that appearance. However: what appears to be layers are: in fact: different steps in time of the same fully recurrent neural network. The left-most item in the illustration shows the recurrent connections as the arc labeled \'v\'. It is ""unfolded"" in time to produce the appearance of layers. === Elman networks and Jordan networks === An Elman network is a three-layer network (arranged horizontally as x: y: and z in the illustration) with the addition of a set of context units (u in the illustration). The middle (hidden) layer is connected to these context units fixed with a weight of one. At each time step: the input is fed forward and a learning rule is applied. The fixed back-connections save a copy of the previous values of the hidden units in the context units (since they propagate over the connections before the learning rule is applied). Thus the network can maintain a sort of state: allowing it to perform such tasks as sequence-prediction that are beyond the power of a standard multilayer perceptron. Jordan networks are similar to Elman networks. The context units are fed from the output layer instead of the hidden layer. The context units in a Jordan network are also called the state layer. They have a recurrent connection to themselves.Elman and Jordan networks are also known as ""Simple recurrent networks"" (SRN). Elman network ht=σh(Whxt+Uhht−1+bh)yt=σy(Wyht+by){\\displaystyle {\\begin{aligned}h_{t}&=\\sigma _{h}(W_{h}x_{t}+U_{h}h_{t-1}+b_{h})\\\\y_{t}&=\\sigma _{y}(W_{y}h_{t}+b_{y})\\end{aligned}}} Jordan network ht=σh(Whxt+Uhyt−1+bh)yt=σy(Wyht+by){\\displaystyle {\\begin{aligned}h_{t}&=\\sigma _{h}(W_{h}x_{t}+U_{h}y_{t-1}+b_{h})\\\\y_{t}&=\\sigma _{y}(W_{y}h_{t}+b_{y})\\end{aligned}}}Variables and functions xt{\\displaystyle x_{t}}: input vector ht{\\displaystyle h_{t}}: hidden layer vector yt{\\displaystyle y_{t}}: output vector W{\\displaystyle W}: U{\\displaystyle U} and b{\\displaystyle b}: parameter matrices and vector σh{\\displaystyle \\sigma _{h}} and σy{\\displaystyle \\sigma _{y}}: Activation functions === Hopfield === The Hopfield network is an RNN in which all connections across layers are equally sized. It requires stationary inputs and is thus not a general RNN: as it does not process sequences of patterns. However: it guarantees that it will converge. If the connections are trained using Hebbian learning: then the Hopfield network can perform as robust content-addressable memory: resistant to connection alteration. ==== Bidirectional associative memory ==== Introduced by Bart Kosko: a bidirectional associative memory (BAM) network is a variant of a Hopfield network that stores associative data as a vector. The bi-directionality comes from passing information through a matrix and its transpose. Typically: bipolar encoding is preferred to binary encoding of the associative pairs. Recently: stochastic BAM models using Markov stepping were optimized for increased network stability and relevance to real-world applications.A BAM network has two layers: either of which can be driven as an input to recall an association and produce an output on the other layer. === Echo state === Echo state networks (ESN) have a sparsely connected random hidden\nlayer. The weights of output neurons are the only part of the network that can change (be trained). ESNs are good at reproducing certain time series. A variant for spiking neurons is known as a liquid state machine. === Independently RNN (IndRNN) === The independently recurrent neural network (IndRNN) addresses the gradient vanishing and exploding problems in the traditional fully connected RNN. Each neuron in one layer only receives its own past state as context information (instead of full connectivity to all other neurons in this layer) and thus neurons are independent of each other\'s history. The gradient backpropagation can be regulated to avoid gradient vanishing and exploding in order to keep long or short-term memory. The cross-neuron information is explored in the next layers. IndRNN can be robustly trained with non-saturated nonlinear functions such as ReLU. Deep networks can be trained using skip connections. === Recursive === A recursive neural network is created by applying the same set of weights recursively over a differentiable graph-like structure by traversing the structure in topological order. Such networks are typically also trained by the reverse mode of automatic differentiation. They can process distributed representations of structure: such as logical terms. A special case of recursive neural networks is the RNN whose structure corresponds to a linear chain. Recursive neural networks have been applied to natural language processing. The Recursive Neural Tensor Network uses a tensor-based composition function for all nodes in the tree. === Neural history compressor === The neural history compressor is an unsupervised stack of RNNs. At the input level: it learns to predict its next input from the previous inputs. Only unpredictable inputs of some RNN in the hierarchy become inputs to the': 'original_similarity': 0.8320454821123658: 'slide': 282}","{'chunk_text': 'recursive neural network is created by applying the same set of weights recursively over a differentiable graph-like structure by traversing the structure in topological order. Such networks are typically also trained by the reverse mode of automatic differentiation. They can process distributed representations of structure: such as logical terms. A special case of recursive neural networks is the RNN whose structure corresponds to a linear chain. Recursive neural networks have been applied to natural language processing. The Recursive Neural Tensor Network uses a tensor-based composition function for all nodes in the tree. === Neural history compressor === The neural history compressor is an unsupervised stack of RNNs. At the input level: it learns to predict its next input from the previous inputs. Only unpredictable inputs of some RNN in the hierarchy become inputs to the next higher level RNN: which therefore recomputes its internal state only rarely. Each higher level RNN thus studies a compressed representation of the information in the RNN below. This is done such that the input sequence can be precisely reconstructed from the representation at the highest level. The system effectively minimizes the description length or the negative logarithm of the probability of the data. Given a lot of learnable predictability in the incoming data sequence: the highest level RNN can use supervised learning to easily classify even deep sequences with long intervals between important events. It is possible to distill the RNN hierarchy into two RNNs: the ""conscious"" chunker (higher level) and the ""subconscious"" automatizer (lower level). Once the chunker has learned to predict and compress inputs that are unpredictable by the automatizer: then the automatizer can be forced in the next learning phase to predict or imitate through additional units the hidden units of the more slowly changing chunker. This makes it easy for the automatizer to learn appropriate: rarely changing memories across long intervals. In turn: this helps the automatizer to make many of its once unpredictable inputs predictable: such that the chunker can focus on the remaining unpredictable events.A generative model partially overcame the vanishing gradient problem of automatic differentiation or backpropagation in neural networks in 1992. In 1993: such a system solved a ""Very Deep Learning"" task that required more than 1000 subsequent layers in an RNN unfolded in time. === Second order RNNs === Second-order RNNs use higher order weights wijk{\\displaystyle w{}_{ijk}} instead of the standard wij{\\displaystyle w{}_{ij}} weights: and states can be a product. This allows a direct mapping to a finite-state machine both in training: stability: and representation. Long short-term memory is an example of this but has no such formal mappings or proof of stability. === Long short-term memory === Long short-term memory (LSTM) is a deep learning system that avoids the vanishing gradient problem. LSTM is normally augmented by recurrent gates called ""forget gates"". LSTM prevents backpropagated errors from vanishing or exploding. Instead: errors can flow backward through unlimited numbers of virtual layers unfolded in space. That is: LSTM can learn tasks that require memories of events that happened thousands or even millions of discrete time steps earlier. Problem-specific LSTM-like topologies can be evolved. LSTM works even given long delays between significant events and can handle signals that mix low and high-frequency components. Many applications use stacks of LSTM RNNs and train them by connectionist temporal classification (CTC) to find an RNN weight matrix that maximizes the probability of the label sequences in a training set: given the corresponding input sequences. CTC achieves both alignment and recognition. LSTM can learn to recognize context-sensitive languages unlike previous models based on hidden Markov models (HMM) and similar concepts. === Gated recurrent unit === Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks introduced in 2014. They are used in the full form and several simplified variants. Their performance on polyphonic music modeling and speech signal modeling was found to be similar to that of long short-term memory. They have fewer parameters than LSTM: as they lack an output gate. === Bi-directional === Bi-directional RNNs use a finite sequence to predict or label each element of the sequence based on the element\'s past and future contexts. This is done by concatenating the outputs of two RNNs: one processing the sequence from left to right: and the other one from right to left. The combined outputs are the predictions of the teacher-given target signals. This technique has been proven to be especially useful when combined with LSTM RNNs. === Continuous-time === A continuous-time recurrent neural network (CTRNN) uses a system of ordinary differential equations to model the effects on a neuron of the incoming inputs. For a neuron i{\\displaystyle i} in the network with activation yi{\\displaystyle y_{i}}: the rate of change of activation is given by: τiy˙i=−yi+∑j=1nwjiσ(yj−Θj)+Ii(t){\\displaystyle \\tau _{i}{\\dot {y}}_{i}=-y_{i}+\\sum _{j=1}^{n}w_{ji}\\sigma (y_{j}-\\Theta _{j})+I_{i}(t)}Where: τi{\\displaystyle \\tau _{i}} : Time constant of postsynaptic node yi{\\displaystyle y_{i}} : Activation of postsynaptic node y˙i{\\displaystyle {\\dot {y}}_{i}} : Rate of change of activation of postsynaptic node wji{\\displaystyle w{}_{ji}} : Weight of connection from pre to postsynaptic node σ(x){\\displaystyle \\sigma (x)} : Sigmoid of x e.g. σ(x)=1/(1+e−x){\\displaystyle \\sigma (x)=1/(1+e^{-x})}. yj{\\displaystyle y_{j}} : Activation of\npresynaptic node Θj{\\displaystyle \\Theta _{j}} : Bias of presynaptic node Ii(t){\\displaystyle I_{i}(t)} : Input (if any) to nodeCTRNNs have been applied to evolutionary robotics where they have been used to address vision: co-operation: and minimal cognitive behaviour.Note that: by the Shannon sampling theorem: discrete-time recurrent neural networks can be viewed as continuous-time recurrent neural networks where the differential equations have transformed into equivalent difference equations. This transformation can be thought of as occurring after the post-synaptic node activation functions yi(t){\\displaystyle y_{i}(t)} have been low-pass filtered but prior to sampling. === Hierarchical recurrent neural network === Hierarchical recurrent neural networks (HRNN) connect their neurons in various ways to decompose hierarchical behavior into useful subprograms. Such hierarchical structures of cognition are present in theories of memory presented by philosopher Henri Bergson: whose philosophical views have inspired hierarchical models.Hierarchical recurrent neural networks are useful in forecasting: helping to predict disaggregated inflation components of': 'original_similarity': 0.8233066696644414: 'slide': 148}","{'chunk_text': 'of as a noisy Hopfield network. It is one of the first neural networks to demonstrate learning of latent variables (hidden units). Boltzmann machine learning was at first slow to simulate: but the contrastive divergence algorithm speeds up training for Boltzmann machines and Products of Experts. ==== Self-organizing map ==== The self-organizing map (SOM) uses unsupervised learning. A set of neurons learn to map points in an input space to coordinates in an output space. The input space can have different dimensions and topology from the output space: and SOM attempts to preserve these. ==== Learning vector quantization ==== Learning vector quantization (LVQ) can be interpreted as a neural network architecture. Prototypical representatives of the classes parameterize: together with an appropriate distance measure: in a distance-based classification scheme. === Simple recurrent === Simple recurrent networks have three layers: with the addition of a set of ""context units"" in the input layer. These units connect from the hidden layer or the output layer with a fixed weight of one. At each time step: the input is propagated in a standard feedforward fashion: and then a backpropagation-like learning rule is applied (not performing gradient descent). The fixed back connections leave a copy of the previous values of the hidden units in the context units (since they propagate over the connections before the learning rule is applied). === Reservoir computing === Reservoir computing is a computation framework that may be viewed as an extension of neural networks. Typically an input signal is fed into a fixed (random) dynamical system called a reservoir whose dynamics map the input to a higher dimension. A readout mechanism is trained to map the reservoir to the desired output. Training is performed only at the readout stage. Liquid-state machines are a type of reservoir computing. ==== Echo state ==== The echo state network (ESN) employs a sparsely connected random hidden layer. The weights of output neurons are the only part of the network that are trained. ESN are good at reproducing certain time series. === Long short-term memory === The long short-term memory (LSTM) avoids the vanishing gradient problem. It works even when with long delays between inputs and can handle signals that mix low and high frequency components. LSTM RNN outperformed other RNN and other sequence learning methods such as HMM in applications such as language learning and connected handwriting recognition. === Bi-directional === Bi-directional RNN: or BRNN: use a finite sequence to predict or label each element of a sequence based on both the past and future context of the element. This is done by adding the outputs of two RNNs: one processing the sequence from left to right: the other one from right to left. The combined outputs are the predictions of the teacher-given target signals. This technique proved to be especially useful when combined with LSTM. === Hierarchical === Hierarchical RNN connects elements in various ways to decompose hierarchical behavior into useful subprograms. === Stochastic === A district from conventional neural networks: stochastic artificial neural network used as an approximation to random functions. === Genetic Scale === A RNN (often a LSTM) where a series is decomposed into a number of scales where every scale informs the primary length between two consecutive points. A first order scale consists of a normal RNN: a second order consists of all points separated by two indices and so on. The Nth order RNN connects the first and last node. The outputs from all the various scales are treated as a Committee of Machines and the associated scores are used genetically for the next iteration. == Modular == Biological studies have shown that the human brain operates as a collection of small networks. This realization gave birth to the concept of modular neural networks: in which several small networks cooperate or compete to solve problems. === Committee of machines === A committee of machines (CoM) is a collection of different neural networks that together ""vote"" on a given example. This generally gives a much better result than individual networks. Because neural networks suffer from local minima: starting with the same architecture and training but using randomly different initial weights often gives vastly different results. A CoM tends to stabilize the result. The CoM is similar to the general machine learning bagging method: except that the necessary variety of machines in the committee is obtained by training from different starting weights rather than training on different randomly selected subsets of the training data. === Associative === The associative neural network (ASNN) is an extension of committee of machines that combines multiple feedforward neural networks and the k-nearest neighbor technique. It uses the correlation between ensemble responses as a measure of distance amid the analyzed cases for the kNN. This corrects the Bias of the neural network ensemble. An associative neural network has a memory that can coincide with the training set. If new data become available: the network instantly improves its predictive ability and provides data approximation (self-learns) without\nretraining. Another important feature of ASNN is the possibility to interpret neural network results by analysis of correlations between data cases in the space of models. == Physical == A physical neural network includes electrically adjustable resistance material to simulate artificial synapses. Examples include the ADALINE memristor-based neural network. An optical neural network is a physical implementation of an artificial neural network with optical components. == Dynamic == Dynamic neural networks address nonlinear multivariate behaviour and include (learning of) time-dependent behaviour: such as transient phenomena and delay effects. Techniques to estimate a system process from observed data fall under the general category of system identification. === Cascading === Cascade correlation is an architecture and supervised learning algorithm. Instead of just adjusting the weights in a network of fixed topology: Cascade-Correlation begins with a minimal network: then automatically trains and adds new hidden units one by one: creating a multi-layer structure. Once a new hidden unit has been added to the network: its input-side weights are frozen. This unit then becomes a': 'original_similarity': 0.8167150563336142: 'slide': 169}"
0.4479198000044562,What is a residual neural network?,"A residual neural network, also known as a residual network or ResNet, is a deep learning model in which the weight layers learn residual functions in relation to the layer inputs. It behaves like a highway network, with gates that are opened through strongly positive bias weights. This system allows deep learning models with numerous layers to train more easily and achieve better accuracy. ","{'chunk_text': 'neural network. The neural networks developed in 2014 by the Visual Geometry Group (VGG) at the University of Oxford approached a depth of 19 layers by stacking 3-by-3 convolutional layers. However: stacking more layers led to a steep reduction in training accuracy: which is referred to as the ""degradation"" problem.A deeper network should not produce a higher training loss than its shallower counterpart: if this deeper network can be constructed by its shallower counterpart stacked with extra layers. If the extra layers can be set as identity mappings: the deeper network would represent the same function as the shallower counterpart. It is hypothesized that the optimizer is not able to approach identity mappings for the parameterized layers. === Residual learning === In a multi-layer neural network model: consider a subnetwork with a certain number (e.g.: 2 or 3) of stacked layers. Denote the underlying function performed by this subnetwork as H(x){\\textstyle H(x)}: where x{\\textstyle x} is the input to this subnetwork. The idea of ""Residual Learning"" re-parameterizes this subnetwork and lets the parameter layers represent a residual function F(x):=H(x)−x{\\textstyle F(x):=H(x)-x}. The output y{\\textstyle y} of this subnetwork is represented as: y=F(x)+x{\\displaystyle {\\begin{aligned}y&=F(x)+x\\end{aligned}}}This is also the principle of the 1997 LSTM cell computing yt+1=F(xt)+xt{\\textstyle y_{t+1}=F(x_{t})+x_{t}}: which becomes y=F(x)+x{\\textstyle y=F(x)+x} during backpropagation through time. The function F(x){\\textstyle F(x)} is often represented by matrix multiplication interlaced with activation functions and normalization operations (e.g.: Batch Normalization or Layer Normalization). This subnetwork is referred to as a ""Residual Block"". A deep residual network is constructed by stacking a series of residual blocks. The operation of ""+ x{\\textstyle +\\ x}"" in ""y=F(x)+x{\\textstyle y=F(x)+x}"" is approached by a skip connection that performs identity mapping and connects the input of a residual block with its output. This connection is often referred to as a ""Residual Connection"" in later work. === Signal propagation === The introduction of identity mappings facilitates signal propagation in both forward and backward paths. ==== Forward propagation ==== If the output of the ℓ{\\textstyle \\ell }-th residual block is the input to the (ℓ+1){\\textstyle (\\ell +1)}-th residual block (i.e.: assuming no activation function between blocks): we have: xℓ+1=F(xℓ)+xℓ{\\displaystyle {\\begin{aligned}x_{\\ell +1}&=F(x_{\\ell })+x_{\\ell }\\end{aligned}}}Applying this formulation recursively: e.g.: xℓ+2=F(xℓ+1)+xℓ+1=F(xℓ+1)+F(xℓ)+xℓ{\\displaystyle {\\begin{aligned}x_{\\ell +2}=F(x_{\\ell +1})+x_{\\ell +1}=F(x_{\\ell +1})+F(x_{\\ell })+x_{\\ell }\\end{aligned}}}: we have: xL=xℓ+∑i=lL−1F(xi){\\displaystyle {\\begin{aligned}x_{L}&=x_{\\ell }+\\sum _{i=l}^{L-1}F(x_{i})\\\\\\end{aligned}}}where L{\\textstyle L} is the index of any later residual block (e.g.: the last block) and ℓ{\\textstyle \\ell } is the index of any earlier block. This formulation suggests that there is always a signal that is directly sent from a shallower block ℓ{\\textstyle \\ell } to a deeper block L{\\textstyle L}. ==== Backward propagation ==== The Residual Learning formulation provides the added benefit of addressing the vanishing gradient problem to some extent. However: it is crucial to acknowledge that the vanishing gradient issue is not the root cause of the degradation problem: as it has already been tackled through the use of normalization layers. Taking the derivative w.r.t. xℓ{\\textstyle x_{\\ell }} according to the above forward propagation: we have: ∂E∂xℓ=∂E∂xL∂xL∂xℓ=∂E∂xL(1+∂∂xℓ∑i=lL−1F(xi))=∂E∂xL+∂E∂xL∂∂xℓ∑i=lL−1F(xi){\\displaystyle {\\begin{aligned}{\\frac {\\partial {\\mathcal {E}}}{\\partial x_{\\ell }}}&={\\frac {\\partial {\\mathcal {E}}}{\\partial x_{L}}}{\\frac {\\partial x_{L}}{\\partial x_{\\ell }}}\\\\&={\\frac {\\partial {\\mathcal {E}}}{\\partial x_{L}}}\\left(1+{\\frac {\\partial }{\\partial x_{\\ell }}}\\sum _{i=l}^{L-1}F(x_{i})\\right)\\\\&={\\frac {\\partial {\\mathcal {E}}}{\\partial x_{L}}}+{\\frac {\\partial {\\mathcal {E}}}{\\partial x_{L}}}{\\frac {\\partial }{\\partial x_{\\ell }}}\\sum _{i=l}^{L-1}F(x_{i})\\\\\\end{aligned}}}Here E{\\textstyle {\\mathcal {E}}} is the loss function to be minimized. This formulation suggests that the gradient computation of a shallower layer ∂E∂xℓ{\\textstyle {\\frac {\\partial {\\mathcal {E}}}{\\partial x_{\\ell }}}} always has a term ∂E∂xL{\\textstyle {\\frac {\\partial {\\mathcal {E}}}{\\partial x_{L}}}} that is directly added. Even if the gradients of the F(xi){\\textstyle F(x_{i})} terms are small: the total gradient ∂E∂xℓ{\\textstyle {\\frac {\\partial {\\mathcal {E}}}{\\partial x_{\\ell }}}} is not vanishing thanks to the added term ∂E∂xL{\\textstyle {\\frac {\\partial {\\mathcal {E}}}{\\partial x_{L}}}}. == Variants of residual blocks == === Basic block === A Basic Block is the simplest building block studied in the original ResNet. This block consists of two sequential 3x3 convolutional layers and a residual connection. The input and output dimensions of both layers are equal. === Bottleneck block === A Bottleneck Block consists of three sequential convolutional layers and a residual connection. The first layer in this block is a 1x1 convolution for dimension reduction: e.g.: to 1/4 of the input dimension; the second layer performs a 3x3 convolution; the last layer is another 1x1 convolution for dimension restoration. The models of ResNet-50: ResNet-101: and ResNet-152 in are all based on Bottleneck Blocks. === Pre-activation block === The Pre-activation Residual Block applies the activation functions (e.g.: non-linearity and normalization) before applying the residual function F{\\textstyle F}. Formally: the computation of a Pre-activation Residual Block can be written as: xℓ+1=F(ϕ(xℓ))+xℓ{\\displaystyle {\\begin{aligned}x_{\\ell +1}&=F(\\phi (x_{\\ell }))+x_{\\ell }\\end{aligned}}}where ϕ{\\textstyle \\phi } can be any non-linearity activation (e.g.: ReLU) or normalization (e.g.: LayerNorm) operation. This design reduces the number of non-identity mappings between Residual Blocks. This design was used to train models with 200 to over 1000 layers.Since GPT-2: the Transformer Blocks have been dominantly implemented as Pre-activation Blocks. This is often referred to as ""pre-normalization"" in the literature of Transformer models. === Transformer block === A Transformer Block is a stack of two Residual Blocks. Each Residual Block has a Residual Connection. The first Residual Block is a Multi-Head Attention Block:\nwhich performs (self-)attention computation followed by a linear projection. The second Residual Block is a feed-forward Multi-Layer Perceptron (MLP) Block. This block is analogous to an ""inverse"" bottleneck block: it has a linear projection layer (which is equivalent to a 1x1 convolution in the context of Convolutional Neural Networks) that increases the dimension: and another linear projection that reduces the dimension. A Transformer Block has a depth of 4 layers (linear projections). The GPT-3 model has 96 Transformer Blocks (in the literature of Transformers: a Transformer Block is often referred to as a ""Transformer Layer""). This model has a depth of about 400 projection layers: including 96x4 layers in Transformer Blocks and a few extra layers for input embedding and output prediction. Very deep Transformer models cannot be successfully trained without Residual Connections. == Related Work == In 1961: Frank Rosenblatt described a three-layer multilayer': 'original_similarity': 0.8892190640056439: 'slide': 143}","{'chunk_text': 'another linear projection that reduces the dimension. A Transformer Block has a depth of 4 layers (linear projections). The GPT-3 model has 96 Transformer Blocks (in the literature of Transformers: a Transformer Block is often referred to as a ""Transformer Layer""). This model has a depth of about 400 projection layers: including 96x4 layers in Transformer Blocks and a few extra layers for input embedding and output prediction. Very deep Transformer models cannot be successfully trained without Residual Connections. == Related Work == In 1961: Frank Rosenblatt described a three-layer multilayer perceptron (MLP) model with skip connections. The model was referred to as a ""cross-coupled system"": and the skip connections were forms of cross-coupled connections. In two books published in 1994 and 1996: ""skip-layer"" connections were presented in feed-forward MLP models: ""The general definition [of MLP] allows more than one hidden layer: and it also allows \'skip-layer\' connections from input to output"" (p261 in: p144 in ): ""... which allows the non-linear units to perturb a linear functional form"" (p262 in ). This description suggests that the non-linear MLP performs like a residual function (perturbation) added to a linear function. Sepp Hochreiter analyzed the vanishing gradient problem in 1991 and attributed to it the reason why deep learning did not work well. To overcome this problem: long short-term memory (LSTM) recurrent neural networks had skip connections or residual connections with a weight of 1.0 in every LSTM cell (called the constant error carrousel) to compute yt+1=F(xt)+xt{\\textstyle y_{t+1}=F(x_{t})+x_{t}}. During backpropagation through time: this becomes the above-mentioned residual formula y=F(x)+x{\\textstyle y=F(x)+x} for feedforward neural networks. This enables training very deep recurrent neural networks with a very long time span t. A later LSTM version published in 2000 modulates the identity LSTM connections by so-called forget gates such that their weights are not fixed to 1.0 but can be learned. In experiments: the forget gates were initialized with positive bias weights: thus being opened: addressing the vanishing gradient problem. The highway network of May 2015 applies these principles to feedforward neural networks. It was reported to be ""the first very deep feedforward network with hundreds of layers"". It is like an LSTM with forget gates unfolded in time: while the later Residual Nets have no equivalent of forget gates and are like the unfolded original LSTM. If the skip connections in Highway Networks are ""without gates"": or if their gates are kept open (activation 1.0) through strong positive bias weights: they become the identity skip connections in Residual Networks. The original Highway Network paper not only introduced the basic principle for very deep feedforward networks: but also included experimental results with 20: 50: and 100 layers networks: and mentioned ongoing experiments with up to 900 layers. Networks with 50 or 100 layers had lower training error than their plain network counterparts: but no lower training error than their 20 layers counterpart (on the MNIST dataset: Figure 1 in ). No improvement on test accuracy was reported with networks deeper than 19 layers (on the CIFAR-10 dataset; Table 1 in ). The ResNet paper: however: provided strong experimental evidence of the benefits of going deeper than 20 layers. It argued that the identity mapping without modulation is crucial and mentioned that modulation in the skip connection can still lead to vanishing signals in forward and backward propagation (Section 3 in ). This is also why the forget gates of the 2000 LSTM were initially opened through positive bias weights: as long as the gates are open: it behaves like the 1997 LSTM. Similarly: a Highway Net whose gates are opened through strongly positive bias weights behaves like a ResNet. The skip connections used in modern neural networks (e.g.: Transformers) are dominantly identity mappings. DenseNets in 2016 were designed as deep neural networks that attempt to connect each layer to every other layer. DenseNets approached this goal by using identity mappings as skip connections. Unlike ResNets: DenseNets merge the layer output with skip connections by concatenation: not addition. Neural networks with Stochastic Depth were made possible given the Residual Network architectures. This training procedure randomly drops a subset of layers and lets the signal propagate through the identity skip connection. Also known as ""DropPath"": this is an effective regularization method for training large and deep models: such as the Vision Transformer (ViT). == Biological relation == The original Residual Network paper made no claim on being inspired by biological systems. But research later on has related Residual Networks to biologically-plausible algorithms. A study published in Science in 2023 disclosed the complete connectome of an insect brain (of a fruit fly larva). This study discovered ""multilayer shortcuts"" that resemble the skip connections in artificial neural networks: including ResNets. == References ==\nA Siamese neural network (sometimes called a twin neural network) is an artificial neural network that uses the same weights while working in tandem on two different input vectors to compute comparable output vectors. Often one of the output vectors is precomputed: thus forming a baseline against which the other output vector is compared. This': 'original_similarity': 0.8429843188137807: 'slide': 53}","{'chunk_text': 'this problem. This led to the deep learning method called long short-term memory (LSTM): published in 1997. LSTM recurrent neural networks can learn ""very deep learning"" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. The ""vanilla LSTM"" with forget gate was introduced in 1999 by Felix Gers: Schmidhuber and Fred Cummins. LSTM has become the most cited neural network of the 20th century. In 2015: Rupesh Kumar Srivastava: Klaus Greff: and Schmidhuber used LSTM principles to create the Highway network: a feedforward neural network with hundreds of layers: much deeper than previous networks. 7 months later: Kaiming He: Xiangyu Zhang; Shaoqing Ren: and Jian Sun won the ImageNet 2015 competition with an open-gated or gateless Highway network variant called Residual neural network. This has become the most cited neural network of the 21st century.In 1994: André de Carvalho: together with Mike Fairhurst and David Bisset: published experimental results of a multi-layer boolean neural network: also known as a weightless neural network: composed of a 3-layers self-organising feature extraction neural network module (SOFT) followed by a multi-layer classification neural network module (GSN): which were independently trained. Each layer in the feature extraction module extracted features with growing complexity regarding the previous layer.In 1995: Brendan Frey demonstrated that it was possible to train (over two days) a network containing six fully connected layers and several hundred hidden units using the wake-sleep algorithm: co-developed with Peter Dayan and Hinton.Since 1997: Sven Behnke extended the feed-forward hierarchical convolutional approach in the Neural Abstraction Pyramid by lateral and backward connections in order to flexibly incorporate context into decisions and iteratively resolve local ambiguities. Simpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) were a popular choice in the 1990s and 2000s: because of artificial neural networks\' computational cost and a lack of understanding of how the brain wires its biological networks. Both shallow and deep learning (e.g.: recurrent nets) of ANNs for speech recognition have been explored for many years. These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively. Key difficulties have been analyzed: including gradient diminishing and weak temporal correlation structure in neural predictive models. Additional difficulties were the lack of training data and limited computing power. Most speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government\'s NSA and DARPA: SRI studied deep neural networks (DNNs) in speech and speaker recognition. The speaker recognition team led by Larry Heck reported significant success with deep neural networks in speech processing in the 1998 National Institute of Standards and Technology Speaker Recognition evaluation. The SRI deep neural network was then deployed in the Nuance Verifier: representing the first major industrial application of deep learning. The principle of elevating ""raw"" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the ""raw"" spectrogram or linear filter-bank features in the late 1990s: showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech: waveforms: later produced excellent larger-scale results.Speech recognition was taken over by LSTM. In 2003: LSTM started to become competitive with traditional speech recognizers on certain tasks. In 2006: Alex Graves: Santiago Fernández: Faustino Gomez: and Schmidhuber combined it with connectionist temporal classification (CTC) in stacks of LSTM RNNs. In 2015: Google\'s speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM: which they made available through Google Voice Search.The impact of deep learning in industry began in the early 2000s: when CNNs already processed an estimated 10% to 20% of all the checks written in the US: according to Yann LeCun. Industrial applications of deep learning to large-scale speech recognition started around 2010. In 2006: publications by Geoff Hinton: Ruslan Salakhutdinov: Osindero and Teh showed how a many-layered feedforward neural network could be effectively pre-trained one layer at a time: treating each layer in turn as an unsupervised restricted Boltzmann machine: then fine-tuning it using supervised backpropagation. The papers referred to learning for deep belief nets. The 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech: and the possibility that given more capable hardware and large-scale data sets that deep neural nets might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets. However: it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large: context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems. The nature of the recognition errors produced by the two types of systems was characteristically different: offering technical insights into how to integrate deep learning into the existing highly\nefficient: run-time speech decoding system deployed by all major speech recognition systems. Analysis around 2009–2010: contrasting the GMM (and other generative speech models) vs. DNN models: stimulated early industrial investment in deep learning for speech recognition. That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models. In 2010: researchers extended deep learning from TIMIT to large vocabulary speech recognition: by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees.Deep learning is part of state-of-the-art systems in various disciplines: particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification): as well as a range of large-vocabulary speech recognition tasks have steadily improved. Convolutional neural networks were superseded for ASR by CTC for LSTM. but are more successful in computer vision. Advances in hardware have driven renewed interest in deep learning. In 2009:': 'original_similarity': 0.8313253657339844: 'slide': 157}"
0.4872761000005994,Who are the people behind the development of residual networks?,"Residual networks were developed by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. They were the winners of the 2015 ImageNet competition.","{'chunk_text': 'shallower counterpart stacked with extra layers. If the extra layers can be set as identity mappings: the deeper network would represent the same function as the shallower counterpart. It is hypothesized that the optimizer is not able to approach identity mappings for the parameterized layers. === Residual learning === In a multi-layer neural network model: consider a subnetwork with a certain number (e.g.: 2 or 3) of stacked layers. Denote the underlying function performed by this subnetwork as H(x){\\textstyle H(x)}: where x{\\textstyle x} is the input to this subnetwork. The idea of ""Residual Learning"" re-parameterizes this subnetwork and lets the parameter layers represent a residual function F(x):=H(x)−x{\\textstyle F(x):=H(x)-x}. The output y{\\textstyle y} of this subnetwork is represented as: y=F(x)+x{\\displaystyle {\\begin{aligned}y&=F(x)+x\\end{aligned}}}This is also the principle of the 1997 LSTM cell computing yt+1=F(xt)+xt{\\textstyle y_{t+1}=F(x_{t})+x_{t}}: which becomes y=F(x)+x{\\textstyle y=F(x)+x} during backpropagation through time. The function F(x){\\textstyle F(x)} is often represented by matrix multiplication interlaced with activation functions and normalization operations (e.g.: Batch Normalization or Layer Normalization). This subnetwork is referred to as a ""Residual Block"". A deep residual network is constructed by stacking a series of residual blocks. The operation of ""+ x{\\textstyle +\\ x}"" in ""y=F(x)+x{\\textstyle y=F(x)+x}"" is approached by a skip connection that performs identity mapping and connects the input of a residual block with its output. This connection is often referred to as a ""Residual Connection"" in later work. === Signal propagation === The introduction of identity mappings facilitates signal propagation in both forward and backward paths. ==== Forward propagation ==== If the output of the ℓ{\\textstyle \\ell }-th residual block is the input to the (ℓ+1){\\textstyle (\\ell +1)}-th residual block (i.e.: assuming no activation function between blocks): we have: xℓ+1=F(xℓ)+xℓ{\\displaystyle {\\begin{aligned}x_{\\ell +1}&=F(x_{\\ell })+x_{\\ell }\\end{aligned}}}Applying this formulation recursively: e.g.: xℓ+2=F(xℓ+1)+xℓ+1=F(xℓ+1)+F(xℓ)+xℓ{\\displaystyle {\\begin{aligned}x_{\\ell +2}=F(x_{\\ell +1})+x_{\\ell +1}=F(x_{\\ell +1})+F(x_{\\ell })+x_{\\ell }\\end{aligned}}}: we have: xL=xℓ+∑i=lL−1F(xi){\\displaystyle {\\begin{aligned}x_{L}&=x_{\\ell }+\\sum _{i=l}^{L-1}F(x_{i})\\\\\\end{aligned}}}where L{\\textstyle L} is the index of any later residual block (e.g.: the last block) and ℓ{\\textstyle \\ell } is the index of any earlier block. This formulation suggests that there is always a signal that is directly sent from a shallower block ℓ{\\textstyle \\ell } to a deeper block L{\\textstyle L}. ==== Backward propagation ==== The Residual Learning formulation provides the added benefit of addressing the vanishing gradient problem to some extent. However: it is crucial to acknowledge that the vanishing gradient issue is not the root cause of the degradation problem: as it has already been tackled through the use of normalization layers. Taking the derivative w.r.t. xℓ{\\textstyle x_{\\ell }} according to the above forward propagation: we have: ∂E∂xℓ=∂E∂xL∂xL∂xℓ=∂E∂xL(1+∂∂xℓ∑i=lL−1F(xi))=∂E∂xL+∂E∂xL∂∂xℓ∑i=lL−1F(xi){\\displaystyle {\\begin{aligned}{\\frac {\\partial {\\mathcal {E}}}{\\partial x_{\\ell }}}&={\\frac {\\partial {\\mathcal {E}}}{\\partial x_{L}}}{\\frac {\\partial x_{L}}{\\partial x_{\\ell }}}\\\\&={\\frac {\\partial {\\mathcal {E}}}{\\partial x_{L}}}\\left(1+{\\frac {\\partial }{\\partial x_{\\ell }}}\\sum _{i=l}^{L-1}F(x_{i})\\right)\\\\&={\\frac {\\partial {\\mathcal {E}}}{\\partial x_{L}}}+{\\frac {\\partial {\\mathcal {E}}}{\\partial x_{L}}}{\\frac {\\partial }{\\partial x_{\\ell }}}\\sum _{i=l}^{L-1}F(x_{i})\\\\\\end{aligned}}}Here E{\\textstyle {\\mathcal {E}}} is the loss function to be minimized. This formulation suggests that the gradient computation of a shallower layer ∂E∂xℓ{\\textstyle {\\frac {\\partial {\\mathcal {E}}}{\\partial x_{\\ell }}}} always has a term ∂E∂xL{\\textstyle {\\frac {\\partial {\\mathcal {E}}}{\\partial x_{L}}}} that is directly added. Even if the gradients of the F(xi){\\textstyle F(x_{i})} terms are small: the total gradient ∂E∂xℓ{\\textstyle {\\frac {\\partial {\\mathcal {E}}}{\\partial x_{\\ell }}}} is not vanishing thanks to the added term ∂E∂xL{\\textstyle {\\frac {\\partial {\\mathcal {E}}}{\\partial x_{L}}}}. == Variants of residual blocks == === Basic block === A Basic Block is the simplest building block studied in the original ResNet. This block consists of two sequential 3x3 convolutional layers and a residual connection. The input and output dimensions of both layers are equal. === Bottleneck block === A Bottleneck Block consists of three sequential convolutional layers and a residual connection. The first layer in this block is a 1x1 convolution for dimension reduction: e.g.: to 1/4 of the input dimension; the second layer performs a 3x3 convolution; the last layer is another 1x1 convolution for dimension restoration. The models of ResNet-50: ResNet-101: and ResNet-152 in are all based on Bottleneck Blocks. === Pre-activation block === The Pre-activation Residual Block applies the activation functions (e.g.: non-linearity and normalization) before applying the residual function F{\\textstyle F}. Formally: the computation of a Pre-activation Residual Block can be written as: xℓ+1=F(ϕ(xℓ))+xℓ{\\displaystyle {\\begin{aligned}x_{\\ell +1}&=F(\\phi (x_{\\ell }))+x_{\\ell }\\end{aligned}}}where ϕ{\\textstyle \\phi } can be any non-linearity activation (e.g.: ReLU) or normalization (e.g.: LayerNorm) operation. This design reduces the number of non-identity mappings between Residual Blocks. This design was used to train models with 200 to over 1000 layers.Since GPT-2: the Transformer Blocks have been dominantly implemented as Pre-activation Blocks. This is often referred to as ""pre-normalization"" in the literature of Transformer models. === Transformer block === A Transformer Block is a stack of two Residual Blocks. Each Residual Block has a Residual Connection. The first Residual Block is a Multi-Head Attention Block:\nwhich performs (self-)attention computation followed by a linear projection. The second Residual Block is a feed-forward Multi-Layer Perceptron (MLP) Block. This block is analogous to an ""inverse"" bottleneck block: it has a linear projection layer (which is equivalent to a 1x1 convolution in the context of Convolutional Neural Networks) that increases the dimension: and another linear projection that reduces the dimension. A Transformer Block has a depth of 4 layers (linear projections). The GPT-3 model has 96 Transformer Blocks (in the literature of Transformers: a Transformer Block is often referred to as a ""Transformer Layer""). This model has a depth of about 400 projection layers: including 96x4 layers in Transformer Blocks and a few extra layers for input embedding and output prediction. Very deep Transformer models cannot be successfully trained without Residual Connections. == Related Work == In 1961: Frank Rosenblatt described a three-layer multilayer perceptron (MLP) model with skip connections. The model was referred to as a ""cross-coupled system"": and the skip connections were forms of cross-coupled connections. In two books published in 1994 and 1996: ""skip-layer"" connections were presented in feed-forward MLP models: ""The general definition [of MLP] allows more than one hidden layer: and it also allows \'skip-layer\' connections from input to output"" (p261 in: p144 in ): ""... which allows the non-linear units': 'original_similarity': 0.7968819431718291: 'slide': 215}","{'chunk_text': 'with forget gate was introduced in 1999 by Felix Gers: Schmidhuber and Fred Cummins. LSTM has become the most cited neural network of the 20th century. In 2015: Rupesh Kumar Srivastava: Klaus Greff: and Schmidhuber used LSTM principles to create the Highway network: a feedforward neural network with hundreds of layers: much deeper than previous networks. 7 months later: Kaiming He: Xiangyu Zhang; Shaoqing Ren: and Jian Sun won the ImageNet 2015 competition with an open-gated or gateless Highway network variant called Residual neural network. This has become the most cited neural network of the 21st century.In 1994: André de Carvalho: together with Mike Fairhurst and David Bisset: published experimental results of a multi-layer boolean neural network: also known as a weightless neural network: composed of a 3-layers self-organising feature extraction neural network module (SOFT) followed by a multi-layer classification neural network module (GSN): which were independently trained. Each layer in the feature extraction module extracted features with growing complexity regarding the previous layer.In 1995: Brendan Frey demonstrated that it was possible to train (over two days) a network containing six fully connected layers and several hundred hidden units using the wake-sleep algorithm: co-developed with Peter Dayan and Hinton.Since 1997: Sven Behnke extended the feed-forward hierarchical convolutional approach in the Neural Abstraction Pyramid by lateral and backward connections in order to flexibly incorporate context into decisions and iteratively resolve local ambiguities. Simpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) were a popular choice in the 1990s and 2000s: because of artificial neural networks\' computational cost and a lack of understanding of how the brain wires its biological networks. Both shallow and deep learning (e.g.: recurrent nets) of ANNs for speech recognition have been explored for many years. These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively. Key difficulties have been analyzed: including gradient diminishing and weak temporal correlation structure in neural predictive models. Additional difficulties were the lack of training data and limited computing power. Most speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government\'s NSA and DARPA: SRI studied deep neural networks (DNNs) in speech and speaker recognition. The speaker recognition team led by Larry Heck reported significant success with deep neural networks in speech processing in the 1998 National Institute of Standards and Technology Speaker Recognition evaluation. The SRI deep neural network was then deployed in the Nuance Verifier: representing the first major industrial application of deep learning. The principle of elevating ""raw"" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the ""raw"" spectrogram or linear filter-bank features in the late 1990s: showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech: waveforms: later produced excellent larger-scale results.Speech recognition was taken over by LSTM. In 2003: LSTM started to become competitive with traditional speech recognizers on certain tasks. In 2006: Alex Graves: Santiago Fernández: Faustino Gomez: and Schmidhuber combined it with connectionist temporal classification (CTC) in stacks of LSTM RNNs. In 2015: Google\'s speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM: which they made available through Google Voice Search.The impact of deep learning in industry began in the early 2000s: when CNNs already processed an estimated 10% to 20% of all the checks written in the US: according to Yann LeCun. Industrial applications of deep learning to large-scale speech recognition started around 2010. In 2006: publications by Geoff Hinton: Ruslan Salakhutdinov: Osindero and Teh showed how a many-layered feedforward neural network could be effectively pre-trained one layer at a time: treating each layer in turn as an unsupervised restricted Boltzmann machine: then fine-tuning it using supervised backpropagation. The papers referred to learning for deep belief nets. The 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech: and the possibility that given more capable hardware and large-scale data sets that deep neural nets might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets. However: it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large: context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems. The nature of the recognition errors produced by the two types of systems was characteristically different: offering technical insights into how to integrate deep learning into the existing highly\nefficient: run-time speech decoding system deployed by all major speech recognition systems. Analysis around 2009–2010: contrasting the GMM (and other generative speech models) vs. DNN models: stimulated early industrial investment in deep learning for speech recognition. That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models. In 2010: researchers extended deep learning from TIMIT to large vocabulary speech recognition: by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees.Deep learning is part of state-of-the-art systems in various disciplines: particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification): as well as a range of large-vocabulary speech recognition tasks have steadily improved. Convolutional neural networks were superseded for ASR by CTC for LSTM. but are more successful in computer vision. Advances in hardware have driven renewed interest in deep learning. In 2009: Nvidia was involved in what was called the ""big bang"" of deep learning: ""as deep-learning neural networks were trained with Nvidia graphics processing units (GPUs)"". That year: Andrew Ng determined that GPUs could increase the speed of deep-learning systems by about 100 times. In particular: GPUs are well-suited': 'original_similarity': 0.7804592419549695: 'slide': 205}","{'chunk_text': 'for input embedding and output prediction. Very deep Transformer models cannot be successfully trained without Residual Connections. == Related Work == In 1961: Frank Rosenblatt described a three-layer multilayer perceptron (MLP) model with skip connections. The model was referred to as a ""cross-coupled system"": and the skip connections were forms of cross-coupled connections. In two books published in 1994 and 1996: ""skip-layer"" connections were presented in feed-forward MLP models: ""The general definition [of MLP] allows more than one hidden layer: and it also allows \'skip-layer\' connections from input to output"" (p261 in: p144 in ): ""... which allows the non-linear units to perturb a linear functional form"" (p262 in ). This description suggests that the non-linear MLP performs like a residual function (perturbation) added to a linear function. Sepp Hochreiter analyzed the vanishing gradient problem in 1991 and attributed to it the reason why deep learning did not work well. To overcome this problem: long short-term memory (LSTM) recurrent neural networks had skip connections or residual connections with a weight of 1.0 in every LSTM cell (called the constant error carrousel) to compute yt+1=F(xt)+xt{\\textstyle y_{t+1}=F(x_{t})+x_{t}}. During backpropagation through time: this becomes the above-mentioned residual formula y=F(x)+x{\\textstyle y=F(x)+x} for feedforward neural networks. This enables training very deep recurrent neural networks with a very long time span t. A later LSTM version published in 2000 modulates the identity LSTM connections by so-called forget gates such that their weights are not fixed to 1.0 but can be learned. In experiments: the forget gates were initialized with positive bias weights: thus being opened: addressing the vanishing gradient problem. The highway network of May 2015 applies these principles to feedforward neural networks. It was reported to be ""the first very deep feedforward network with hundreds of layers"". It is like an LSTM with forget gates unfolded in time: while the later Residual Nets have no equivalent of forget gates and are like the unfolded original LSTM. If the skip connections in Highway Networks are ""without gates"": or if their gates are kept open (activation 1.0) through strong positive bias weights: they become the identity skip connections in Residual Networks. The original Highway Network paper not only introduced the basic principle for very deep feedforward networks: but also included experimental results with 20: 50: and 100 layers networks: and mentioned ongoing experiments with up to 900 layers. Networks with 50 or 100 layers had lower training error than their plain network counterparts: but no lower training error than their 20 layers counterpart (on the MNIST dataset: Figure 1 in ). No improvement on test accuracy was reported with networks deeper than 19 layers (on the CIFAR-10 dataset; Table 1 in ). The ResNet paper: however: provided strong experimental evidence of the benefits of going deeper than 20 layers. It argued that the identity mapping without modulation is crucial and mentioned that modulation in the skip connection can still lead to vanishing signals in forward and backward propagation (Section 3 in ). This is also why the forget gates of the 2000 LSTM were initially opened through positive bias weights: as long as the gates are open: it behaves like the 1997 LSTM. Similarly: a Highway Net whose gates are opened through strongly positive bias weights behaves like a ResNet. The skip connections used in modern neural networks (e.g.: Transformers) are dominantly identity mappings. DenseNets in 2016 were designed as deep neural networks that attempt to connect each layer to every other layer. DenseNets approached this goal by using identity mappings as skip connections. Unlike ResNets: DenseNets merge the layer output with skip connections by concatenation: not addition. Neural networks with Stochastic Depth were made possible given the Residual Network architectures. This training procedure randomly drops a subset of layers and lets the signal propagate through the identity skip connection. Also known as ""DropPath"": this is an effective regularization method for training large and deep models: such as the Vision Transformer (ViT). == Biological relation == The original Residual Network paper made no claim on being inspired by biological systems. But research later on has related Residual Networks to biologically-plausible algorithms. A study published in Science in 2023 disclosed the complete connectome of an insect brain (of a fruit fly larva). This study discovered ""multilayer shortcuts"" that resemble the skip connections in artificial neural networks: including ResNets. == References ==\nA Siamese neural network (sometimes called a twin neural network) is an artificial neural network that uses the same weights while working in tandem on two different input vectors to compute comparable output vectors. Often one of the output vectors is precomputed: thus forming a baseline against which the other output vector is compared. This is similar to comparing fingerprints but can be described more technically as a distance function for locality-sensitive hashing.It is possible to build an architecture that is functionally similar to a siamese network but implements a slightly different function. This is typically used for comparing similar instances in different type sets.Uses of similarity measures where a twin network might be used are such': 'original_similarity': 0.7799590333186486: 'slide': 115}"
0.5670091999927536,What is the degradation problem in neural networks?,"The degradation problem refers to the steep reduction in training accuracy that occurs when more layers are stacked in a neural network. Despite the assumption that a deeper network should not produce a higher training loss than its shallower counterpart, the addition of extra layers often contributes to this issue.","{'chunk_text': 'an estimate for variance. This value can then be used to calculate the confidence interval of network output: assuming a normal distribution. A confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified. By assigning a softmax activation function: a generalization of the logistic function: on the output layer of the neural network (or a softmax component in a component-based network) for categorical target variables: the outputs can be interpreted as posterior probabilities. This is useful in classification as it gives a certainty measure on classifications. The softmax activation function is: yi=exi∑j=1cexj{\\displaystyle y_{i}={\\frac {e^{x_{i}}}{\\sum _{j=1}^{c}e^{x_{j}}}}} == Criticism == === Training === A common criticism of neural networks: particularly in robotics: is that they require too many training samples for real-world operation. Any learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases. Potential solutions include randomly shuffling training examples: by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example: grouping examples in so-called mini-batches and/or introducing a recursive least squares algorithm for CMAC. Dean Pomerleau uses a neural network to train a robotic vehicle to drive on multiple types of roads (single lane: multi-lane: dirt: etc.): and a large amount of his research is devoted to extrapolating multiple training scenarios from a single training experience: and preserving past training diversity so that the system does not become overtrained (if: for example: it is presented with a series of right turns—it should not learn to always turn right). === Theory === A central claim of ANNs is that they embody new and powerful general principles for processing information. These principles are ill-defined. It is often claimed that they are emergent from the network itself. This allows simple statistical association (the basic function of artificial neural networks) to be described as learning or recognition. In 1997: Alexander Dewdney: a former Scientific American columnist: commented that as a result: artificial neural networks have a ""something-for-nothing quality: one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are. No human hand (or mind) intervenes; solutions are found as if by magic; and no one: it seems: has learned anything"". One response to Dewdney is that neural networks have been successfully used to handle many complex and diverse tasks: ranging from autonomously flying aircraft to detecting credit card fraud to mastering the game of Go. Technology writer Roger Bridgman commented: Neural networks: for instance: are in the dock not only because they have been hyped to high heaven: (what hasn\'t?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be ""an opaque: unreadable table...valueless as a scientific resource"". In spite of his emphatic declaration that science is not technology: Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having. Although it is true that analyzing what has been learned by an artificial neural network is difficult: it is much easier to do so than to analyze what has been learned by a biological neural network. Moreover: recent emphasis on the explainability of AI has contributed towards the development of methods: notably those based on attention mechanisms: for visualizing and explaining learned neural networks. Furthermore: researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles that allow a learning machine to be successful. For example: Bengio and LeCun (2007) wrote an article regarding local vs non-local learning: as well as shallow vs deep architecture.Biological brains use both shallow and deep circuits as reported by brain anatomy: displaying a wide variety of invariance. Weng argued that the brain self-wires largely according to signal statistics and therefore: a serial cascade cannot catch all major statistical dependencies. === Hardware === Large and effective neural networks require considerable computing resources. While the brain has hardware tailored to the task of processing signals through a graph of neurons: simulating even a simplified neuron on von Neumann architecture may consume vast amounts of memory and storage. Furthermore: the designer often needs to transmit signals through many of these connections and their associated neurons – which require enormous CPU power and time. Schmidhuber noted that the resurgence of neural networks in the twenty-first century is largely attributable to advances in hardware: from 1991 to 2015: computing power: especially as delivered by GPGPUs (on GPUs): has increased around a million-fold: making the standard backpropagation algorithm feasible for training networks that are several layers deeper than before. The use of accelerators such as FPGAs and GPUs can reduce training times from months to days.Neuromorphic engineering or a physical neural network addresses the hardware difficulty directly: by constructing non-von-Neumann chips to directly implement neural networks\nin circuitry. Another type of chip optimized for neural network processing is called a Tensor Processing Unit: or TPU. === Practical counterexamples === Analyzing what has been learned by an ANN is much easier than analyzing what has been learned by a biological neural network. Furthermore: researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a learning machine to be successful. For example: local vs. non-local learning and shallow vs. deep architecture. === Hybrid approaches === Advocates of hybrid models (combining neural networks and symbolic approaches) say that such a mixture can better capture the mechanisms of the human mind. === Dataset bias === Neural networks are dependent on the quality of the data they are trained on: thus low quality data with imbalanced representativeness can lead to the model learning and perpetuating societal biases. These inherited biases become especially critical when the ANNs are integrated into real-world scenarios': 'original_similarity': 0.836999957453136: 'slide': 154}","{'chunk_text': 'neural networks can learn ""very deep learning"" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. The ""vanilla LSTM"" with forget gate was introduced in 1999 by Felix Gers: Schmidhuber and Fred Cummins. LSTM has become the most cited neural network of the 20th century. In 2015: Rupesh Kumar Srivastava: Klaus Greff: and Schmidhuber used LSTM principles to create the Highway network: a feedforward neural network with hundreds of layers: much deeper than previous networks. 7 months later: Kaiming He: Xiangyu Zhang; Shaoqing Ren: and Jian Sun won the ImageNet 2015 competition with an open-gated or gateless Highway network variant called Residual neural network. This has become the most cited neural network of the 21st century.In 1994: André de Carvalho: together with Mike Fairhurst and David Bisset: published experimental results of a multi-layer boolean neural network: also known as a weightless neural network: composed of a 3-layers self-organising feature extraction neural network module (SOFT) followed by a multi-layer classification neural network module (GSN): which were independently trained. Each layer in the feature extraction module extracted features with growing complexity regarding the previous layer.In 1995: Brendan Frey demonstrated that it was possible to train (over two days) a network containing six fully connected layers and several hundred hidden units using the wake-sleep algorithm: co-developed with Peter Dayan and Hinton.Since 1997: Sven Behnke extended the feed-forward hierarchical convolutional approach in the Neural Abstraction Pyramid by lateral and backward connections in order to flexibly incorporate context into decisions and iteratively resolve local ambiguities. Simpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) were a popular choice in the 1990s and 2000s: because of artificial neural networks\' computational cost and a lack of understanding of how the brain wires its biological networks. Both shallow and deep learning (e.g.: recurrent nets) of ANNs for speech recognition have been explored for many years. These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively. Key difficulties have been analyzed: including gradient diminishing and weak temporal correlation structure in neural predictive models. Additional difficulties were the lack of training data and limited computing power. Most speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government\'s NSA and DARPA: SRI studied deep neural networks (DNNs) in speech and speaker recognition. The speaker recognition team led by Larry Heck reported significant success with deep neural networks in speech processing in the 1998 National Institute of Standards and Technology Speaker Recognition evaluation. The SRI deep neural network was then deployed in the Nuance Verifier: representing the first major industrial application of deep learning. The principle of elevating ""raw"" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the ""raw"" spectrogram or linear filter-bank features in the late 1990s: showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech: waveforms: later produced excellent larger-scale results.Speech recognition was taken over by LSTM. In 2003: LSTM started to become competitive with traditional speech recognizers on certain tasks. In 2006: Alex Graves: Santiago Fernández: Faustino Gomez: and Schmidhuber combined it with connectionist temporal classification (CTC) in stacks of LSTM RNNs. In 2015: Google\'s speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM: which they made available through Google Voice Search.The impact of deep learning in industry began in the early 2000s: when CNNs already processed an estimated 10% to 20% of all the checks written in the US: according to Yann LeCun. Industrial applications of deep learning to large-scale speech recognition started around 2010. In 2006: publications by Geoff Hinton: Ruslan Salakhutdinov: Osindero and Teh showed how a many-layered feedforward neural network could be effectively pre-trained one layer at a time: treating each layer in turn as an unsupervised restricted Boltzmann machine: then fine-tuning it using supervised backpropagation. The papers referred to learning for deep belief nets. The 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech: and the possibility that given more capable hardware and large-scale data sets that deep neural nets might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets. However: it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large: context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems. The nature of the recognition errors produced by the two types of systems was characteristically different: offering technical insights into how to integrate deep learning into the existing highly\nefficient: run-time speech decoding system deployed by all major speech recognition systems. Analysis around 2009–2010: contrasting the GMM (and other generative speech models) vs. DNN models: stimulated early industrial investment in deep learning for speech recognition. That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models. In 2010: researchers extended deep learning from TIMIT to large vocabulary speech recognition: by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees.Deep learning is part of state-of-the-art systems in various disciplines: particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification): as well as a range of large-vocabulary speech recognition tasks have steadily improved. Convolutional neural networks were superseded for ASR by CTC for LSTM. but are more successful in computer vision. Advances in hardware have driven renewed interest in deep learning. In 2009: Nvidia was involved in what was called the ""big bang"" of deep learning: ""as deep-learning neural networks were trained': 'original_similarity': 0.8182353882898595: 'slide': 176}","{'chunk_text': '1986 by Rina Dechter although the history of its appearance is apparently more complicated.The first general: working learning algorithm for supervised: deep: feedforward: multilayer perceptrons was published by Alexey Ivakhnenko and Lapa in 1967. A 1971 paper described a deep network with eight layers trained by the group method of data handling.The first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun\'ichi Amari. In computer experiments conducted by Amari\'s student Saito: a five layer MLP with two modifiable layers learned internal representations to classify non-linearily separable pattern classes. In 1987 Matthew Brand reported that wide 12-layer nonlinear perceptrons could be fully end-to-end trained to reproduce logic functions of nontrivial circuit depth via gradient descent on small batches of random input/output samples: but concluded that training time on contemporary hardware (sub-megaflop computers) made the technique impractical: and proposed using fixed random early layers as an input hash for a single modifiable layer. Instead: subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique. In 1970: Seppo Linnainmaa published the reverse mode of automatic differentiation of discrete connected networks of nested differentiable functions. This became known as backpropagation. It is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes. The terminology ""back-propagating errors"" was actually introduced in 1962 by Rosenblatt: but he did not know how to implement this: although Henry J. Kelley had a continuous precursor of backpropagation already in 1960 in the context of control theory. In 1982: Paul Werbos applied backpropagation to MLPs in the way that has become standard. In 1985: David E. Rumelhart et al. published an experimental analysis of the technique.Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers began with the Neocognitron introduced by Kunihiko Fukushima in 1980. In 1969: he also introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for CNNs and deep learning in general. CNNs have become an essential tool for computer vision. The term Deep Learning was introduced to the machine learning community by Rina Dechter in 1986: and to artificial neural networks by Igor Aizenberg and colleagues in 2000: in the context of Boolean threshold neurons.In 1988: Wei Zhang et al. applied the backpropagation algorithm to a convolutional neural network (a simplified Neocognitron with convolutional interconnections between the image feature layers and the last fully connected layer) for alphabet recognition. They also proposed an implementation of the CNN with an optical computing system. In 1989: Yann LeCun et al. applied backpropagation to a CNN with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked: training required 3 days. Subsequently: Wei Zhang: et al. modified their model by removing the last fully connected layer and applied it for medical image object segmentation in 1991 and breast cancer detection in mammograms in 1994. LeNet-5 (1998): a 7-level CNN by Yann LeCun et al.: that classifies digits: was applied by several banks to recognize hand-written numbers on checks digitized in 32x32 pixel images. In the 1980s: backpropagation did not work well for deep learning with long credit assignment paths. To overcome this problem: Jürgen Schmidhuber (1992) proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning. It uses predictive coding to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN: by distilling a higher level chunker network into a lower level automatizer network. In 1993: a chunker solved a deep learning task whose depth exceeded 1000.In 1992: Jürgen Schmidhuber also published an alternative to RNNs which is now called a linear Transformer or a Transformer with linearized self-attention (save for a normalization operator). It learns internal spotlights of attention: a slow feedforward neural network learns by gradient descent to control the fast weights of another neural network through outer products of self-generated activation patterns FROM and TO (which are now called key and value for self-attention). This fast weight attention mapping is applied to a query pattern. The modern Transformer was introduced by Ashish Vaswani et al. in their 2017 paper ""Attention Is All You Need"". It combines this with a softmax operator and a projection matrix. Transformers have increasingly become the model of choice for natural language processing. Many modern large language models such as ChatGPT: GPT-4: and BERT use it. Transformers are also increasingly being used in computer vision.In 1991: Jürgen Schmidhuber also published adversarial neural networks that contest with each other in the form of a zero-sum game: where one network\'s gain is the other network\'s loss. The first network is a\ngenerative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called ""artificial curiosity"". In 2014: this principle was used in a generative adversarial network (GAN) by Ian Goodfellow et al. Here the environmental reaction is 1 or 0 depending on whether the first network\'s output is in a given set. This can be used to create realistic deepfakes. Excellent image quality is achieved by Nvidia\'s StyleGAN (2018) based on the Progressive GAN by Tero Karras et al. Here the GAN generator is grown from small to large scale in a pyramidal fashion. Sepp Hochreiter\'s diploma thesis (1991) was called ""one of the most important documents in the history of machine learning"" by his supervisor Schmidhuber. It not only tested the neural history compressor: but also identified and analyzed the vanishing gradient problem. Hochreiter proposed recurrent residual connections to solve this problem. This led to the deep learning method called long short-term memory (LSTM): published in 1997. LSTM recurrent neural networks can learn ""very deep learning"" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. The ""vanilla LSTM"" with': 'original_similarity': 0.8156106199812478: 'slide': 205}"
0.5217103000031784,Can you explain 'Residual Learning' in a neural network?,"In a residual learning, a residual function is represented by the parameter layers in a multi-layer neural network, re-parameterizing the subnetwork. The residual function is often denoted F(x):=H(x)-x, where H(x) is the underlying function performed by the network and x is the input to the subnetwork. The output y of this network is given by y=F(x)+x. This concept is also applied in the LSTM cell computing.","{'chunk_text': 'depth of 19 layers by stacking 3-by-3 convolutional layers. However: stacking more layers led to a steep reduction in training accuracy: which is referred to as the ""degradation"" problem.A deeper network should not produce a higher training loss than its shallower counterpart: if this deeper network can be constructed by its shallower counterpart stacked with extra layers. If the extra layers can be set as identity mappings: the deeper network would represent the same function as the shallower counterpart. It is hypothesized that the optimizer is not able to approach identity mappings for the parameterized layers. === Residual learning === In a multi-layer neural network model: consider a subnetwork with a certain number (e.g.: 2 or 3) of stacked layers. Denote the underlying function performed by this subnetwork as H(x){\\textstyle H(x)}: where x{\\textstyle x} is the input to this subnetwork. The idea of ""Residual Learning"" re-parameterizes this subnetwork and lets the parameter layers represent a residual function F(x):=H(x)−x{\\textstyle F(x):=H(x)-x}. The output y{\\textstyle y} of this subnetwork is represented as: y=F(x)+x{\\displaystyle {\\begin{aligned}y&=F(x)+x\\end{aligned}}}This is also the principle of the 1997 LSTM cell computing yt+1=F(xt)+xt{\\textstyle y_{t+1}=F(x_{t})+x_{t}}: which becomes y=F(x)+x{\\textstyle y=F(x)+x} during backpropagation through time. The function F(x){\\textstyle F(x)} is often represented by matrix multiplication interlaced with activation functions and normalization operations (e.g.: Batch Normalization or Layer Normalization). This subnetwork is referred to as a ""Residual Block"". A deep residual network is constructed by stacking a series of residual blocks. The operation of ""+ x{\\textstyle +\\ x}"" in ""y=F(x)+x{\\textstyle y=F(x)+x}"" is approached by a skip connection that performs identity mapping and connects the input of a residual block with its output. This connection is often referred to as a ""Residual Connection"" in later work. === Signal propagation === The introduction of identity mappings facilitates signal propagation in both forward and backward paths. ==== Forward propagation ==== If the output of the ℓ{\\textstyle \\ell }-th residual block is the input to the (ℓ+1){\\textstyle (\\ell +1)}-th residual block (i.e.: assuming no activation function between blocks): we have: xℓ+1=F(xℓ)+xℓ{\\displaystyle {\\begin{aligned}x_{\\ell +1}&=F(x_{\\ell })+x_{\\ell }\\end{aligned}}}Applying this formulation recursively: e.g.: xℓ+2=F(xℓ+1)+xℓ+1=F(xℓ+1)+F(xℓ)+xℓ{\\displaystyle {\\begin{aligned}x_{\\ell +2}=F(x_{\\ell +1})+x_{\\ell +1}=F(x_{\\ell +1})+F(x_{\\ell })+x_{\\ell }\\end{aligned}}}: we have: xL=xℓ+∑i=lL−1F(xi){\\displaystyle {\\begin{aligned}x_{L}&=x_{\\ell }+\\sum _{i=l}^{L-1}F(x_{i})\\\\\\end{aligned}}}where L{\\textstyle L} is the index of any later residual block (e.g.: the last block) and ℓ{\\textstyle \\ell } is the index of any earlier block. This formulation suggests that there is always a signal that is directly sent from a shallower block ℓ{\\textstyle \\ell } to a deeper block L{\\textstyle L}. ==== Backward propagation ==== The Residual Learning formulation provides the added benefit of addressing the vanishing gradient problem to some extent. However: it is crucial to acknowledge that the vanishing gradient issue is not the root cause of the degradation problem: as it has already been tackled through the use of normalization layers. Taking the derivative w.r.t. xℓ{\\textstyle x_{\\ell }} according to the above forward propagation: we have: ∂E∂xℓ=∂E∂xL∂xL∂xℓ=∂E∂xL(1+∂∂xℓ∑i=lL−1F(xi))=∂E∂xL+∂E∂xL∂∂xℓ∑i=lL−1F(xi){\\displaystyle {\\begin{aligned}{\\frac {\\partial {\\mathcal {E}}}{\\partial x_{\\ell }}}&={\\frac {\\partial {\\mathcal {E}}}{\\partial x_{L}}}{\\frac {\\partial x_{L}}{\\partial x_{\\ell }}}\\\\&={\\frac {\\partial {\\mathcal {E}}}{\\partial x_{L}}}\\left(1+{\\frac {\\partial }{\\partial x_{\\ell }}}\\sum _{i=l}^{L-1}F(x_{i})\\right)\\\\&={\\frac {\\partial {\\mathcal {E}}}{\\partial x_{L}}}+{\\frac {\\partial {\\mathcal {E}}}{\\partial x_{L}}}{\\frac {\\partial }{\\partial x_{\\ell }}}\\sum _{i=l}^{L-1}F(x_{i})\\\\\\end{aligned}}}Here E{\\textstyle {\\mathcal {E}}} is the loss function to be minimized. This formulation suggests that the gradient computation of a shallower layer ∂E∂xℓ{\\textstyle {\\frac {\\partial {\\mathcal {E}}}{\\partial x_{\\ell }}}} always has a term ∂E∂xL{\\textstyle {\\frac {\\partial {\\mathcal {E}}}{\\partial x_{L}}}} that is directly added. Even if the gradients of the F(xi){\\textstyle F(x_{i})} terms are small: the total gradient ∂E∂xℓ{\\textstyle {\\frac {\\partial {\\mathcal {E}}}{\\partial x_{\\ell }}}} is not vanishing thanks to the added term ∂E∂xL{\\textstyle {\\frac {\\partial {\\mathcal {E}}}{\\partial x_{L}}}}. == Variants of residual blocks == === Basic block === A Basic Block is the simplest building block studied in the original ResNet. This block consists of two sequential 3x3 convolutional layers and a residual connection. The input and output dimensions of both layers are equal. === Bottleneck block === A Bottleneck Block consists of three sequential convolutional layers and a residual connection. The first layer in this block is a 1x1 convolution for dimension reduction: e.g.: to 1/4 of the input dimension; the second layer performs a 3x3 convolution; the last layer is another 1x1 convolution for dimension restoration. The models of ResNet-50: ResNet-101: and ResNet-152 in are all based on Bottleneck Blocks. === Pre-activation block === The Pre-activation Residual Block applies the activation functions (e.g.: non-linearity and normalization) before applying the residual function F{\\textstyle F}. Formally: the computation of a Pre-activation Residual Block can be written as: xℓ+1=F(ϕ(xℓ))+xℓ{\\displaystyle {\\begin{aligned}x_{\\ell +1}&=F(\\phi (x_{\\ell }))+x_{\\ell }\\end{aligned}}}where ϕ{\\textstyle \\phi } can be any non-linearity activation (e.g.: ReLU) or normalization (e.g.: LayerNorm) operation. This design reduces the number of non-identity mappings between Residual Blocks. This design was used to train models with 200 to over 1000 layers.Since GPT-2: the Transformer Blocks have been dominantly implemented as Pre-activation Blocks. This is often referred to as ""pre-normalization"" in the literature of Transformer models. === Transformer block === A Transformer Block is a stack of two Residual Blocks. Each Residual Block has a Residual Connection. The first Residual Block is a Multi-Head Attention Block:\nwhich performs (self-)attention computation followed by a linear projection. The second Residual Block is a feed-forward Multi-Layer Perceptron (MLP) Block. This block is analogous to an ""inverse"" bottleneck block: it has a linear projection layer (which is equivalent to a 1x1 convolution in the context of Convolutional Neural Networks) that increases the dimension: and another linear projection that reduces the dimension. A Transformer Block has a depth of 4 layers (linear projections). The GPT-3 model has 96 Transformer Blocks (in the literature of Transformers: a Transformer Block is often referred to as a ""Transformer Layer""). This model has a depth of about 400 projection layers: including 96x4 layers in Transformer Blocks and a few extra layers for input embedding and output prediction. Very deep Transformer models cannot be successfully trained without Residual Connections. == Related Work == In 1961: Frank Rosenblatt described a three-layer multilayer perceptron (MLP) model with skip connections. The model was referred to as a ""cross-coupled system"": and the skip connections were forms': 'original_similarity': 0.8748974324883338: 'slide': 164}","{'chunk_text': 'Networks) that increases the dimension: and another linear projection that reduces the dimension. A Transformer Block has a depth of 4 layers (linear projections). The GPT-3 model has 96 Transformer Blocks (in the literature of Transformers: a Transformer Block is often referred to as a ""Transformer Layer""). This model has a depth of about 400 projection layers: including 96x4 layers in Transformer Blocks and a few extra layers for input embedding and output prediction. Very deep Transformer models cannot be successfully trained without Residual Connections. == Related Work == In 1961: Frank Rosenblatt described a three-layer multilayer perceptron (MLP) model with skip connections. The model was referred to as a ""cross-coupled system"": and the skip connections were forms of cross-coupled connections. In two books published in 1994 and 1996: ""skip-layer"" connections were presented in feed-forward MLP models: ""The general definition [of MLP] allows more than one hidden layer: and it also allows \'skip-layer\' connections from input to output"" (p261 in: p144 in ): ""... which allows the non-linear units to perturb a linear functional form"" (p262 in ). This description suggests that the non-linear MLP performs like a residual function (perturbation) added to a linear function. Sepp Hochreiter analyzed the vanishing gradient problem in 1991 and attributed to it the reason why deep learning did not work well. To overcome this problem: long short-term memory (LSTM) recurrent neural networks had skip connections or residual connections with a weight of 1.0 in every LSTM cell (called the constant error carrousel) to compute yt+1=F(xt)+xt{\\textstyle y_{t+1}=F(x_{t})+x_{t}}. During backpropagation through time: this becomes the above-mentioned residual formula y=F(x)+x{\\textstyle y=F(x)+x} for feedforward neural networks. This enables training very deep recurrent neural networks with a very long time span t. A later LSTM version published in 2000 modulates the identity LSTM connections by so-called forget gates such that their weights are not fixed to 1.0 but can be learned. In experiments: the forget gates were initialized with positive bias weights: thus being opened: addressing the vanishing gradient problem. The highway network of May 2015 applies these principles to feedforward neural networks. It was reported to be ""the first very deep feedforward network with hundreds of layers"". It is like an LSTM with forget gates unfolded in time: while the later Residual Nets have no equivalent of forget gates and are like the unfolded original LSTM. If the skip connections in Highway Networks are ""without gates"": or if their gates are kept open (activation 1.0) through strong positive bias weights: they become the identity skip connections in Residual Networks. The original Highway Network paper not only introduced the basic principle for very deep feedforward networks: but also included experimental results with 20: 50: and 100 layers networks: and mentioned ongoing experiments with up to 900 layers. Networks with 50 or 100 layers had lower training error than their plain network counterparts: but no lower training error than their 20 layers counterpart (on the MNIST dataset: Figure 1 in ). No improvement on test accuracy was reported with networks deeper than 19 layers (on the CIFAR-10 dataset; Table 1 in ). The ResNet paper: however: provided strong experimental evidence of the benefits of going deeper than 20 layers. It argued that the identity mapping without modulation is crucial and mentioned that modulation in the skip connection can still lead to vanishing signals in forward and backward propagation (Section 3 in ). This is also why the forget gates of the 2000 LSTM were initially opened through positive bias weights: as long as the gates are open: it behaves like the 1997 LSTM. Similarly: a Highway Net whose gates are opened through strongly positive bias weights behaves like a ResNet. The skip connections used in modern neural networks (e.g.: Transformers) are dominantly identity mappings. DenseNets in 2016 were designed as deep neural networks that attempt to connect each layer to every other layer. DenseNets approached this goal by using identity mappings as skip connections. Unlike ResNets: DenseNets merge the layer output with skip connections by concatenation: not addition. Neural networks with Stochastic Depth were made possible given the Residual Network architectures. This training procedure randomly drops a subset of layers and lets the signal propagate through the identity skip connection. Also known as ""DropPath"": this is an effective regularization method for training large and deep models: such as the Vision Transformer (ViT). == Biological relation == The original Residual Network paper made no claim on being inspired by biological systems. But research later on has related Residual Networks to biologically-plausible algorithms. A study published in Science in 2023 disclosed the complete connectome of an insect brain (of a fruit fly larva). This study discovered ""multilayer shortcuts"" that resemble the skip connections in artificial neural networks: including ResNets. == References ==\nA Siamese neural network (sometimes called a twin neural network) is an artificial neural network that uses the same weights while working in tandem on two different input vectors to compute comparable output vectors. Often one of the output vectors is precomputed: thus forming a baseline against which the': 'original_similarity': 0.8370485886476204: 'slide': 47}","{'chunk_text': 'set can be used as an estimate for variance. This value can then be used to calculate the confidence interval of network output: assuming a normal distribution. A confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified. By assigning a softmax activation function: a generalization of the logistic function: on the output layer of the neural network (or a softmax component in a component-based network) for categorical target variables: the outputs can be interpreted as posterior probabilities. This is useful in classification as it gives a certainty measure on classifications. The softmax activation function is: yi=exi∑j=1cexj{\\displaystyle y_{i}={\\frac {e^{x_{i}}}{\\sum _{j=1}^{c}e^{x_{j}}}}} == Criticism == === Training === A common criticism of neural networks: particularly in robotics: is that they require too many training samples for real-world operation. Any learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases. Potential solutions include randomly shuffling training examples: by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example: grouping examples in so-called mini-batches and/or introducing a recursive least squares algorithm for CMAC. Dean Pomerleau uses a neural network to train a robotic vehicle to drive on multiple types of roads (single lane: multi-lane: dirt: etc.): and a large amount of his research is devoted to extrapolating multiple training scenarios from a single training experience: and preserving past training diversity so that the system does not become overtrained (if: for example: it is presented with a series of right turns—it should not learn to always turn right). === Theory === A central claim of ANNs is that they embody new and powerful general principles for processing information. These principles are ill-defined. It is often claimed that they are emergent from the network itself. This allows simple statistical association (the basic function of artificial neural networks) to be described as learning or recognition. In 1997: Alexander Dewdney: a former Scientific American columnist: commented that as a result: artificial neural networks have a ""something-for-nothing quality: one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are. No human hand (or mind) intervenes; solutions are found as if by magic; and no one: it seems: has learned anything"". One response to Dewdney is that neural networks have been successfully used to handle many complex and diverse tasks: ranging from autonomously flying aircraft to detecting credit card fraud to mastering the game of Go. Technology writer Roger Bridgman commented: Neural networks: for instance: are in the dock not only because they have been hyped to high heaven: (what hasn\'t?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be ""an opaque: unreadable table...valueless as a scientific resource"". In spite of his emphatic declaration that science is not technology: Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having. Although it is true that analyzing what has been learned by an artificial neural network is difficult: it is much easier to do so than to analyze what has been learned by a biological neural network. Moreover: recent emphasis on the explainability of AI has contributed towards the development of methods: notably those based on attention mechanisms: for visualizing and explaining learned neural networks. Furthermore: researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles that allow a learning machine to be successful. For example: Bengio and LeCun (2007) wrote an article regarding local vs non-local learning: as well as shallow vs deep architecture.Biological brains use both shallow and deep circuits as reported by brain anatomy: displaying a wide variety of invariance. Weng argued that the brain self-wires largely according to signal statistics and therefore: a serial cascade cannot catch all major statistical dependencies. === Hardware === Large and effective neural networks require considerable computing resources. While the brain has hardware tailored to the task of processing signals through a graph of neurons: simulating even a simplified neuron on von Neumann architecture may consume vast amounts of memory and storage. Furthermore: the designer often needs to transmit signals through many of these connections and their associated neurons – which require enormous CPU power and time. Schmidhuber noted that the resurgence of neural networks in the twenty-first century is largely attributable to advances in hardware: from 1991 to 2015: computing power: especially as delivered by GPGPUs (on GPUs): has increased around a million-fold: making the standard backpropagation algorithm feasible for training networks that are several layers deeper than before. The use of accelerators such as FPGAs and GPUs can reduce training times from months to days.Neuromorphic engineering or a physical neural network addresses the hardware difficulty directly: by constructing non-von-Neumann chips to directly implement neural networks\nin circuitry. Another type of chip optimized for neural network processing is called a Tensor Processing Unit: or TPU. === Practical counterexamples === Analyzing what has been learned by an ANN is much easier than analyzing what has been learned by a biological neural network. Furthermore: researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a learning machine to be successful. For example: local vs. non-local learning and shallow vs. deep architecture. === Hybrid approaches === Advocates of hybrid models (combining neural networks and symbolic approaches) say that such a mixture can better capture the mechanisms of the human mind. === Dataset bias === Neural networks are dependent on the quality of the data they are trained on: thus low quality data with imbalanced representativeness can lead to the model learning and perpetuating societal biases. These inherited biases become especially critical when the ANNs': 'original_similarity': 0.8329020558653123: 'slide': 149}"
0.45620810001855716,What is a Transformer Block?,"A Transformer Block is a stack of two Residual Blocks, each with a Residual Connection. It consists first of a Multi-Head Attention Block, which performs (self-)attention computation followed by a linear projection. The second block is a feed-forward Multi-Layer Perceptron (MLP) Block, which increases and then reduces the dimension through linear projections. The GPT-3 model, for instance, has 96 Transformer Blocks, amounting to a depth of about 400 projection layers. Very deep Transformer models cannot be successfully trained without Residual Connections.","{'chunk_text': 'literature of Transformers: a Transformer Block is often referred to as a ""Transformer Layer""). This model has a depth of about 400 projection layers: including 96x4 layers in Transformer Blocks and a few extra layers for input embedding and output prediction. Very deep Transformer models cannot be successfully trained without Residual Connections. == Related Work == In 1961: Frank Rosenblatt described a three-layer multilayer perceptron (MLP) model with skip connections. The model was referred to as a ""cross-coupled system"": and the skip connections were forms of cross-coupled connections. In two books published in 1994 and 1996: ""skip-layer"" connections were presented in feed-forward MLP models: ""The general definition [of MLP] allows more than one hidden layer: and it also allows \'skip-layer\' connections from input to output"" (p261 in: p144 in ): ""... which allows the non-linear units to perturb a linear functional form"" (p262 in ). This description suggests that the non-linear MLP performs like a residual function (perturbation) added to a linear function. Sepp Hochreiter analyzed the vanishing gradient problem in 1991 and attributed to it the reason why deep learning did not work well. To overcome this problem: long short-term memory (LSTM) recurrent neural networks had skip connections or residual connections with a weight of 1.0 in every LSTM cell (called the constant error carrousel) to compute yt+1=F(xt)+xt{\\textstyle y_{t+1}=F(x_{t})+x_{t}}. During backpropagation through time: this becomes the above-mentioned residual formula y=F(x)+x{\\textstyle y=F(x)+x} for feedforward neural networks. This enables training very deep recurrent neural networks with a very long time span t. A later LSTM version published in 2000 modulates the identity LSTM connections by so-called forget gates such that their weights are not fixed to 1.0 but can be learned. In experiments: the forget gates were initialized with positive bias weights: thus being opened: addressing the vanishing gradient problem. The highway network of May 2015 applies these principles to feedforward neural networks. It was reported to be ""the first very deep feedforward network with hundreds of layers"". It is like an LSTM with forget gates unfolded in time: while the later Residual Nets have no equivalent of forget gates and are like the unfolded original LSTM. If the skip connections in Highway Networks are ""without gates"": or if their gates are kept open (activation 1.0) through strong positive bias weights: they become the identity skip connections in Residual Networks. The original Highway Network paper not only introduced the basic principle for very deep feedforward networks: but also included experimental results with 20: 50: and 100 layers networks: and mentioned ongoing experiments with up to 900 layers. Networks with 50 or 100 layers had lower training error than their plain network counterparts: but no lower training error than their 20 layers counterpart (on the MNIST dataset: Figure 1 in ). No improvement on test accuracy was reported with networks deeper than 19 layers (on the CIFAR-10 dataset; Table 1 in ). The ResNet paper: however: provided strong experimental evidence of the benefits of going deeper than 20 layers. It argued that the identity mapping without modulation is crucial and mentioned that modulation in the skip connection can still lead to vanishing signals in forward and backward propagation (Section 3 in ). This is also why the forget gates of the 2000 LSTM were initially opened through positive bias weights: as long as the gates are open: it behaves like the 1997 LSTM. Similarly: a Highway Net whose gates are opened through strongly positive bias weights behaves like a ResNet. The skip connections used in modern neural networks (e.g.: Transformers) are dominantly identity mappings. DenseNets in 2016 were designed as deep neural networks that attempt to connect each layer to every other layer. DenseNets approached this goal by using identity mappings as skip connections. Unlike ResNets: DenseNets merge the layer output with skip connections by concatenation: not addition. Neural networks with Stochastic Depth were made possible given the Residual Network architectures. This training procedure randomly drops a subset of layers and lets the signal propagate through the identity skip connection. Also known as ""DropPath"": this is an effective regularization method for training large and deep models: such as the Vision Transformer (ViT). == Biological relation == The original Residual Network paper made no claim on being inspired by biological systems. But research later on has related Residual Networks to biologically-plausible algorithms. A study published in Science in 2023 disclosed the complete connectome of an insect brain (of a fruit fly larva). This study discovered ""multilayer shortcuts"" that resemble the skip connections in artificial neural networks: including ResNets. == References ==\nA Siamese neural network (sometimes called a twin neural network) is an artificial neural network that uses the same weights while working in tandem on two different input vectors to compute comparable output vectors. Often one of the output vectors is precomputed: thus forming a baseline against which the other output vector is compared. This is similar to comparing fingerprints but can be described more technically as a distance function for locality-sensitive hashing.It is possible to build an architecture that is functionally': 'original_similarity': 0.8012405049694928: 'slide': 80}","{'chunk_text': 'is also the principle of the 1997 LSTM cell computing yt+1=F(xt)+xt{\\textstyle y_{t+1}=F(x_{t})+x_{t}}: which becomes y=F(x)+x{\\textstyle y=F(x)+x} during backpropagation through time. The function F(x){\\textstyle F(x)} is often represented by matrix multiplication interlaced with activation functions and normalization operations (e.g.: Batch Normalization or Layer Normalization). This subnetwork is referred to as a ""Residual Block"". A deep residual network is constructed by stacking a series of residual blocks. The operation of ""+ x{\\textstyle +\\ x}"" in ""y=F(x)+x{\\textstyle y=F(x)+x}"" is approached by a skip connection that performs identity mapping and connects the input of a residual block with its output. This connection is often referred to as a ""Residual Connection"" in later work. === Signal propagation === The introduction of identity mappings facilitates signal propagation in both forward and backward paths. ==== Forward propagation ==== If the output of the ℓ{\\textstyle \\ell }-th residual block is the input to the (ℓ+1){\\textstyle (\\ell +1)}-th residual block (i.e.: assuming no activation function between blocks): we have: xℓ+1=F(xℓ)+xℓ{\\displaystyle {\\begin{aligned}x_{\\ell +1}&=F(x_{\\ell })+x_{\\ell }\\end{aligned}}}Applying this formulation recursively: e.g.: xℓ+2=F(xℓ+1)+xℓ+1=F(xℓ+1)+F(xℓ)+xℓ{\\displaystyle {\\begin{aligned}x_{\\ell +2}=F(x_{\\ell +1})+x_{\\ell +1}=F(x_{\\ell +1})+F(x_{\\ell })+x_{\\ell }\\end{aligned}}}: we have: xL=xℓ+∑i=lL−1F(xi){\\displaystyle {\\begin{aligned}x_{L}&=x_{\\ell }+\\sum _{i=l}^{L-1}F(x_{i})\\\\\\end{aligned}}}where L{\\textstyle L} is the index of any later residual block (e.g.: the last block) and ℓ{\\textstyle \\ell } is the index of any earlier block. This formulation suggests that there is always a signal that is directly sent from a shallower block ℓ{\\textstyle \\ell } to a deeper block L{\\textstyle L}. ==== Backward propagation ==== The Residual Learning formulation provides the added benefit of addressing the vanishing gradient problem to some extent. However: it is crucial to acknowledge that the vanishing gradient issue is not the root cause of the degradation problem: as it has already been tackled through the use of normalization layers. Taking the derivative w.r.t. xℓ{\\textstyle x_{\\ell }} according to the above forward propagation: we have: ∂E∂xℓ=∂E∂xL∂xL∂xℓ=∂E∂xL(1+∂∂xℓ∑i=lL−1F(xi))=∂E∂xL+∂E∂xL∂∂xℓ∑i=lL−1F(xi){\\displaystyle {\\begin{aligned}{\\frac {\\partial {\\mathcal {E}}}{\\partial x_{\\ell }}}&={\\frac {\\partial {\\mathcal {E}}}{\\partial x_{L}}}{\\frac {\\partial x_{L}}{\\partial x_{\\ell }}}\\\\&={\\frac {\\partial {\\mathcal {E}}}{\\partial x_{L}}}\\left(1+{\\frac {\\partial }{\\partial x_{\\ell }}}\\sum _{i=l}^{L-1}F(x_{i})\\right)\\\\&={\\frac {\\partial {\\mathcal {E}}}{\\partial x_{L}}}+{\\frac {\\partial {\\mathcal {E}}}{\\partial x_{L}}}{\\frac {\\partial }{\\partial x_{\\ell }}}\\sum _{i=l}^{L-1}F(x_{i})\\\\\\end{aligned}}}Here E{\\textstyle {\\mathcal {E}}} is the loss function to be minimized. This formulation suggests that the gradient computation of a shallower layer ∂E∂xℓ{\\textstyle {\\frac {\\partial {\\mathcal {E}}}{\\partial x_{\\ell }}}} always has a term ∂E∂xL{\\textstyle {\\frac {\\partial {\\mathcal {E}}}{\\partial x_{L}}}} that is directly added. Even if the gradients of the F(xi){\\textstyle F(x_{i})} terms are small: the total gradient ∂E∂xℓ{\\textstyle {\\frac {\\partial {\\mathcal {E}}}{\\partial x_{\\ell }}}} is not vanishing thanks to the added term ∂E∂xL{\\textstyle {\\frac {\\partial {\\mathcal {E}}}{\\partial x_{L}}}}. == Variants of residual blocks == === Basic block === A Basic Block is the simplest building block studied in the original ResNet. This block consists of two sequential 3x3 convolutional layers and a residual connection. The input and output dimensions of both layers are equal. === Bottleneck block === A Bottleneck Block consists of three sequential convolutional layers and a residual connection. The first layer in this block is a 1x1 convolution for dimension reduction: e.g.: to 1/4 of the input dimension; the second layer performs a 3x3 convolution; the last layer is another 1x1 convolution for dimension restoration. The models of ResNet-50: ResNet-101: and ResNet-152 in are all based on Bottleneck Blocks. === Pre-activation block === The Pre-activation Residual Block applies the activation functions (e.g.: non-linearity and normalization) before applying the residual function F{\\textstyle F}. Formally: the computation of a Pre-activation Residual Block can be written as: xℓ+1=F(ϕ(xℓ))+xℓ{\\displaystyle {\\begin{aligned}x_{\\ell +1}&=F(\\phi (x_{\\ell }))+x_{\\ell }\\end{aligned}}}where ϕ{\\textstyle \\phi } can be any non-linearity activation (e.g.: ReLU) or normalization (e.g.: LayerNorm) operation. This design reduces the number of non-identity mappings between Residual Blocks. This design was used to train models with 200 to over 1000 layers.Since GPT-2: the Transformer Blocks have been dominantly implemented as Pre-activation Blocks. This is often referred to as ""pre-normalization"" in the literature of Transformer models. === Transformer block === A Transformer Block is a stack of two Residual Blocks. Each Residual Block has a Residual Connection. The first Residual Block is a Multi-Head Attention Block:\nwhich performs (self-)attention computation followed by a linear projection. The second Residual Block is a feed-forward Multi-Layer Perceptron (MLP) Block. This block is analogous to an ""inverse"" bottleneck block: it has a linear projection layer (which is equivalent to a 1x1 convolution in the context of Convolutional Neural Networks) that increases the dimension: and another linear projection that reduces the dimension. A Transformer Block has a depth of 4 layers (linear projections). The GPT-3 model has 96 Transformer Blocks (in the literature of Transformers: a Transformer Block is often referred to as a ""Transformer Layer""). This model has a depth of about 400 projection layers: including 96x4 layers in Transformer Blocks and a few extra layers for input embedding and output prediction. Very deep Transformer models cannot be successfully trained without Residual Connections. == Related Work == In 1961: Frank Rosenblatt described a three-layer multilayer perceptron (MLP) model with skip connections. The model was referred to as a ""cross-coupled system"": and the skip connections were forms of cross-coupled connections. In two books published in 1994 and 1996: ""skip-layer"" connections were presented in feed-forward MLP models: ""The general definition [of MLP] allows more than one hidden layer: and it also allows \'skip-layer\' connections from input to output"" (p261 in: p144 in ): ""... which allows the non-linear units to perturb a linear functional form"" (p262 in ). This description suggests that the non-linear MLP performs like a residual function (perturbation) added to a linear function. Sepp Hochreiter analyzed the vanishing gradient problem in 1991 and attributed to it the reason why deep learning did not work well. To overcome this problem: long short-term memory (LSTM) recurrent neural networks had skip connections or residual connections with a weight of 1.0 in every LSTM cell (called the constant error carrousel) to compute yt+1=F(xt)+xt{\\textstyle y_{t+1}=F(x_{t})+x_{t}}. During backpropagation through time: this becomes the above-mentioned residual formula y=F(x)+x{\\textstyle y=F(x)+x} for feedforward neural networks. This enables training very deep recurrent neural networks with a very long time span t. A later LSTM version published': 'original_similarity': 0.7841908172515344: 'slide': 335}","{'chunk_text': 'as a form of semi-supervised learning: as the model is trained first on an unlabelled dataset (pretraining step) by learning to generate datapoints in the dataset: and then it is trained to classify a labelled dataset.While the unnormalized linear transformer dates back to 1992: the modern transformer architecture was not available until 2017 when it was published by researchers at Google in a paper ""Attention Is All You Need"". That development led to the emergence of large language models such as BERT in 2018 which was a pre-trained transformer (PT) but not designed to be generative (BERT was an ""encoder-only"" model). Also around that time: in 2018: OpenAI published its article entitled ""Improving Language Understanding by Generative Pre-Training:"" in which it introduced the first generative pre-trained transformer (GPT) system (""GPT-1"").Prior to transformer-based architectures: the best-performing neural NLP (natural language processing) models commonly employed supervised learning from large amounts of manually-labeled data. The reliance on supervised learning limited their use on datasets that were not well-annotated: and also made it prohibitively expensive and time-consuming to train extremely large language models.The semi-supervised approach OpenAI employed to make a large-scale generative system—and was first to do with a transformer model—involved two stages: an unsupervised generative ""pretraining"" stage to set initial parameters using a language modeling objective: and a supervised discriminative ""fine-tuning"" stage to adapt these parameters to a target task. === Later developments === Regarding more recent GPT foundation models: OpenAI published its first versions of GPT-3 in July 2020. There were three models: with 1B: 6.7B: 175B parameters: respectively named babbage: curie: and davinci (giving initials B: C: and D).In July 2021: OpenAI published Codex: a task-specific GPT model targeted for programming applications. This was developed by fine-tuning a 12B parameter version of GPT-3 (different from previous GPT-3 models) using code from GitHub.In March 2022: OpenAI published two versions of GPT-3 that were fine-tuned for instruction-following (instruction-tuned): named davinci-instruct-beta (175B) and text-davinci-001: and then started beta testing code-davinci-002. text-davinci-002 was instruction-tuned from code-davinci-002. Both text-davinci-003 and ChatGPT were released in November 2022: with both building upon text-davinci-002 via reinforcement learning from human feedback (RLHF). text-davinci-003 is trained for following instructions (like its predecessors): whereas ChatGPT is further trained for conversational interaction with a human user.OpenAI\'s most recent GPT foundation model: GPT-4: was released on March 14: 2023. It can be accessed directly by users via a premium version of ChatGPT: and is available to developers for incorporation into other products and services via OpenAI\'s API. Other producers of GPT foundation models include EleutherAI (with a series of models starting in March 2021) and Cerebras (with seven models released in March 2023). == Foundational models == A foundational model is an AI model trained on broad data at scale such that it can be adapted to a wide range of downstream tasks.Thus far: the most notable GPT foundation models have been from OpenAI\'s GPT-n series. The most recent from that is GPT-4: for which OpenAI declined to publish the size or training details (citing ""the competitive landscape and the safety implications of large-scale models""). Other such models include Google\'s PaLM: a broad foundation model that has been compared to GPT-3 and has recently been made available to developers via an API: and Together\'s GPT-JT: which has been reported as the closest-performing open-source alternative to GPT-3 (and is derived from earlier open-source GPTs). Meta AI (formerly Facebook) also has a generative transformer-based foundational large language model: known as LLaMA.Foundational GPTs can also employ modalities other than text: for input and/or output. GPT-4 is a multi-modal LLM that is capable of processing text and image input (though its output is limited to text). Regarding multimodal output: some generative transformer-based models are used for text-to-image technologies such as diffusion and parallel decoding. Such kinds of models can serve as visual foundation models (VFMs) for developing downstream systems that can work with images. == Task-specific models == A foundational GPT model can be further adapted to produce more targeted systems directed to specific tasks and/or subject-matter domains. Methods for such adaptation can include additional fine-tuning (beyond that done for the foundation model) as well as certain forms of prompt engineering.An important example of this is fine-tuning models to follow instructions: which is of course a fairly broad task but more targeted than a foundation model. In January 2022: OpenAI introduced ""InstructGPT""—a series of models which were fine-tuned to follow instructions using a combination of supervised training and reinforcement learning from human feedback (RLHF) on base GPT-3 language models. Advantages this had over the bare foundational models included higher accuracy: less\nnegative/toxic sentiment: and generally better alignment with user needs. Hence: OpenAI began using this as the basis for its API service offerings. Other instruction-tuned models have been released by others: including a fully open version.Another (related) kind of task-specific models are chatbots: which engage in human-like conversation. In November 2022: OpenAI launched ChatGPT—an online chat interface powered by an instruction-tuned language model trained in a similar fashion to InstructGPT. They trained this model using RLHF: with human AI trainers providing conversations in which they played both the user and the AI: and mixed this new dialogue dataset with the InstructGPT dataset for a conversational format suitable for a chatbot. Other major chatbots currently include Microsoft\'s Bing Chat: which uses OpenAI\'s GPT-4 (as part of a broader close collaboration between OpenAI and Microsoft): and Google\'s competing chatbot Bard (initially based on their LaMDA family of conversation-trained language models: with plans to switch to PaLM).Yet another kind of task that a GPT can be used for is the meta-task of generating its own instructions: like developing a series of prompts for \'itself\' to be able to effectuate a more general goal given by a human user. This is known as an AI agent: and more specifically a recursive one because it uses results from its previous self-instructions to help it form its subsequent prompts; the first major example of this was Auto-GPT (which uses OpenAI\'s GPT models): and others have': 'original_similarity': 0.7741279571448412: 'slide': 236}"
0.48795399998198263,What is a Siamese neural network?,"A Siamese neural network, also known as a twin neural network, is an artificial neural network that uses the same weights while operating on two different input vectors to compute comparable output vectors. It is often used for comparing similar instances in different type sets and in applications such as face recognition and matching queries with indexed documents.","{'chunk_text': 'well-known application of twin networks are face recognition: where known images of people are precomputed and compared to an image from a turnstile or similar. It is not obvious at first: but there are two slightly different problems. One is recognizing a person among a large number of other persons: that is the facial recognition problem. DeepFace is an example of such a system. In its most extreme form this is recognizing a single person at a train station or airport. The other is face verification: that is to verify whether the photo in a pass is the same as the person claiming he or she is the same person. The twin network might be the same: but the implementation can be quite different. == Learning == Learning in twin networks can be done with triplet loss or contrastive loss. For learning by triplet loss a baseline vector (anchor image) is compared against a positive vector (truthy image) and a negative vector (falsy image). The negative vector will force learning in the network: while the positive vector will act like a regularizer. For learning by contrastive loss there must be a weight decay to regularize the weights: or some similar operation like a normalization. A distance metric for a loss function may have the following properties Non-negativity: δ(x:y)≥0{\\displaystyle \\delta (x:y)\\geq 0} Identity of Non-discernibles: δ(x:y)=0⟺x=y{\\displaystyle \\delta (x:y)=0\\iff x=y} Commutativity: δ(x:y)=δ(y:x){\\displaystyle \\delta (x:y)=\\delta (y:x)} Triangle inequality: δ(x:z)≤δ(x:y)+δ(y:z){\\displaystyle \\delta (x:z)\\leq \\delta (x:y)+\\delta (y:z)}In particular: the triplet loss algorithm is often defined with squared Euclidean (which unlike Euclidean: does not have triangle inequality) distance at its core. === Predefined metrics: Euclidean distance metric === The common learning goal is to minimize a distance metric for similar objects and maximize for distinct ones. This gives a loss function like δ(x(i):x(j))={min ‖f\u2061(x(i))−f\u2061(x(j))‖:i=jmax ‖f\u2061(x(i))−f\u2061(x(j))‖:i≠j{\\displaystyle {\\begin{aligned}\\delta (x^{(i)}:x^{(j)})={\\begin{cases}\\min \\ \\|\\operatorname {f} \\left(x^{(i)}\\right)-\\operatorname {f} \\left(x^{(j)}\\right)\\|\\::i=j\\\\\\max \\ \\|\\operatorname {f} \\left(x^{(i)}\\right)-\\operatorname {f} \\left(x^{(j)}\\right)\\|\\::i\\neq j\\end{cases}}\\end{aligned}}} i:j{\\displaystyle i:j} are indexes into a set of vectors f\u2061(⋅){\\displaystyle \\operatorname {f} (\\cdot )} function implemented by the twin networkThe most common distance metric used is Euclidean distance: in case of which the loss function can be rewritten in matrix form as δ\u2061(x(i):x(j))≈(x(i)−x(j))T(x(i)−x(j)){\\displaystyle \\operatorname {\\delta } (\\mathbf {x} ^{(i)}:\\mathbf {x} ^{(j)})\\approx (\\mathbf {x} ^{(i)}-\\mathbf {x} ^{(j)})^{T}(\\mathbf {x} ^{(i)}-\\mathbf {x} ^{(j)})} === Learned metrics: nonlinear distance metric === A more general case is where the output vector from the twin network is passed through additional network layers implementing non-linear distance metrics. ifi=jthenδ\u2061[f\u2061(x(i)):f\u2061(x(j))]is smallotherwiseδ\u2061[f\u2061(x(i)):f\u2061(x(j))]is large{\\displaystyle {\\begin{aligned}{\\text{if}}\\:i=j\\:{\\text{then}}&\\:\\operatorname {\\delta } \\left[\\operatorname {f} \\left(x^{(i)}\\right):\\:\\operatorname {f} \\left(x^{(j)}\\right)\\right]\\:{\\text{is small}}\\\\{\\text{otherwise}}&\\:\\operatorname {\\delta } \\left[\\operatorname {f} \\left(x^{(i)}\\right):\\:\\operatorname {f} \\left(x^{(j)}\\right)\\right]\\:{\\text{is large}}\\end{aligned}}} i:j{\\displaystyle i:j} are indexes into a set of vectors f\u2061(⋅){\\displaystyle \\operatorname {f} (\\cdot )}function implemented by the twin network δ\u2061(⋅){\\displaystyle \\operatorname {\\delta } (\\cdot )}function implemented by the network joining outputs from the twin networkOn a matrix form the previous is often approximated as a Mahalanobis distance for a linear space as δ\u2061(x(i):x(j))≈(x(i)−x(j))TM(x(i)−x(j)){\\displaystyle \\operatorname {\\delta } (\\mathbf {x} ^{(i)}:\\mathbf {x} ^{(j)})\\approx (\\mathbf {x} ^{(i)}-\\mathbf {x} ^{(j)})^{T}\\mathbf {M} (\\mathbf {x} ^{(i)}-\\mathbf {x} ^{(j)})}This can be further subdivided in at least Unsupervised learning and Supervised learning. === Learned metrics: half-twin networks === This form also allows the twin network to be more of a half-twin: implementing a slightly different functions ifi=jthenδ\u2061[f\u2061(x(i)):g\u2061(x(j))]is smallotherwiseδ\u2061[f\u2061(x(i)):g\u2061(x(j))]is large{\\displaystyle {\\begin{aligned}{\\text{if}}\\:i=j\\:{\\text{then}}&\\:\\operatorname {\\delta } \\left[\\operatorname {f} \\left(x^{(i)}\\right):\\:\\operatorname {g} \\left(x^{(j)}\\right)\\right]\\:{\\text{is small}}\\\\{\\text{otherwise}}&\\:\\operatorname {\\delta } \\left[\\operatorname {f} \\left(x^{(i)}\\right):\\:\\operatorname {g} \\left(x^{(j)}\\right)\\right]\\:{\\text{is large}}\\end{aligned}}} i:j{\\displaystyle i:j} are indexes into a set of vectors f\u2061(⋅):g\u2061(⋅){\\displaystyle \\operatorname {f} (\\cdot ):\\operatorname {g} (\\cdot )}function implemented by the half-twin network δ\u2061(⋅){\\displaystyle \\operatorname {\\delta } (\\cdot )}function implemented by the network joining outputs from the twin network == Twin networks for object tracking == Twin networks have been used in object tracking because of its unique two tandem inputs and similarity measurement. In object tracking: one input of the twin network is user pre-selected exemplar image: the other input is a larger search image: which twin network\'s job is to locate exemplar inside of search image. By measuring the similarity between exemplar and each part of the search image: a map of similarity score can be given by the twin network. Furthermore: using a Fully Convolutional Network: the process of computing each sector\'s similarity score can be replaced with only one cross correlation layer.After being first introduced in 2016: Twin fully convolutional network has been used in many High-performance Real-time Object Tracking Neural Networks. Like CFnet: StructSiam: SiamFC-tri: DSiam: SA-Siam: SiamRPN: DaSiamRPN: Cascaded SiamRPN: SiamMask: SiamRPN++: Deeper and Wider SiamRPN. == See also == Artificial neural network Triplet loss == Further reading == Chicco: Davide (2020): ""Siamese neural networks: an overview"": Artificial Neural Networks: Methods in Molecular Biology: vol. 2190 (3rd ed.): New York City: New York: USA: Springer Protocols: Humana Press: pp. 73–94: doi:10.1007/978-1-0716-0826-5_3: ISBN 978-1-0716-0826-5: PMID 32804361: S2CID 221144012 == References ==\nSpiking neural networks (SNNs) are artificial neural networks (ANN) that more closely mimic natural neural networks. In addition to neuronal and synaptic state: SNNs incorporate the concept of time into their operating model. The idea is that neurons in the SNN do not transmit information at each propagation cycle (as it happens with typical multi-layer perceptron networks): but rather transmit information only when a membrane potential—an intrinsic quality of the neuron related to its membrane electrical charge—reaches a specific value: called the threshold. When the membrane potential reaches the threshold: the neuron fires: and generates a signal that travels to other neurons which: in turn: increase or decrease their potentials in response to this signal. A neuron model that fires at the moment of threshold crossing is also called a spiking neuron model.Although it was previously believed that the': 'original_similarity': 0.8879246493537558: 'slide': 137}","{'chunk_text': ""level: which in turn: refines the accuracy and efficiency of information processing. Recently: This phenomenon is achieved mostly achieved using Compartmental neuron models. The simpler versions are of neuron models with adaptive thresholds: indirect way of achieving SFA: equips SNNs with improved learning capabilities: even with constrained synaptic plasticity: and elevates computational efficiency. This feature lessens the demand on network layers by decreasing the need for spike processing: thus cutting down on computational load and memory access time—essential aspects of neural computation. Moreover: SNNs utilizing neurons capable of SFA achieve levels of accuracy that rival those of conventional artificial neural networks: including those based on long short-term memory models: while also requiring fewer neurons for comparable computational tasks. This efficiency not only streamlines the computational workflow but also conserves space and energy: offering a pragmatic step forward in the practical application of SNNs for complex computing tasks: all while maintaining a commitment to technical integrity. == Applications == SNNs can in principle apply to the same applications as traditional ANNs. In addition: SNNs can model the central nervous system of biological organisms: such as an insect seeking food without prior knowledge of the environment. Due to their relative realism: they can be used to study the operation of biological neural circuits. Starting with a hypothesis about the topology of a biological neuronal circuit and its function: recordings of this circuit can be compared to the output of the corresponding SNN: evaluating the plausibility of the hypothesis. However: there is a lack of effective training mechanisms for SNNs: which can be inhibitory for some applications: including computer vision tasks. As of 2019 SNNs lag behind ANNs in terms of accuracy: but the gap is decreasing: and has vanished on some tasks.When using SNNs for image based data we need to convert static images into binary spike trains coding. Types of encodings: Temporal coding generates one spike per neuron in which spike latency is inversely proportional to the pixel intensity. Rate coding converts pixel intensity into a spike train where the number of spikes is proportional to the pixel intensity. Direct coding uses a trainable layer to generate float value for each time-step. We have a learnable layer which converts each pixel at certain time step in float number and then threshold is used on the generated floating numbers to see if they will be 1 or 0. Phase coding encodes temporal information into spike patterns based on a global oscillator. Burst coding transmits the burst of spikes in a small-time duration: increasing the reliability of synaptic communication between neurons. == Software == A diverse range of application software can simulate SNNs. This software can be classified according to its uses: === SNN simulation === These simulate complex neural models with a high level of detail and accuracy. Large networks usually require lengthy processing. Candidates include:Brian – developed by Romain Brette and Dan Goodman at the École Normale Supérieure; GENESIS (the GEneral NEural SImulation System) – developed in James Bower's laboratory at Caltech; NEST – developed by the NEST Initiative; NEURON – mainly developed by Michael Hines: John W. Moore and Ted Carnevale in Yale University and Duke University; RAVSim (Runtime Tool) – mainly developed by Sanaullah in Bielefeld University of Applied Sciences and Arts; == Hardware == Future neuromorphic architectures will comprise billions of such nanosynapses: which require a clear understanding of the physical mechanisms responsible for plasticity. Experimental systems based on ferroelectric tunnel junctions have been used to show that STDP can be harnessed from heterogeneous polarization switching. Through combined scanning probe imaging: electrical transport and atomic-scale molecular dynamics: conductance variations can be modelled by nucleation-dominated reversal of domains. Simulations show that arrays of ferroelectric nanosynapses can autonomously learn to recognize patterns in a predictable way: opening the path towards unsupervised learning. Akida is a completely digital event-based neural processing device with 1.2 million artificial neurons and 10 billion artificial synapses developed by BrainChip. Utilizing event-based possessing: it analyzes essential inputs at specific points. Results are stored in the on-chip memory units. Neurogrid is a board that can simulate spiking neural networks directly in hardware. (Stanford University) SpiNNaker (Spiking Neural Network Architecture) uses ARM processors as the building blocks of a massively parallel computing platform based on a six-layer thalamocortical model. (University of Manchester) The SpiNNaker system is based on numerical models running in real time on custom digital multicore chips using the ARM architecture. It provides custom digital chips: each with eighteen cores and a shared local 128 Mbyte RAM: with a total of over 1:000:000 cores. A single chip can simulate 16:000 neurons with eight million plastic synapses running in real time. TrueNorth is a processor that contains 5.4 billion transistors that consumes only 70 milliwatts; most processors in personal computers contain about 1.4\nbillion transistors and require 35 watts or more. IBM refers to the design principle behind TrueNorth as neuromorphic computing. Its primary purpose is pattern recognition. While critics say the chip isn't powerful enough: its supporters point out that this is only the first generation: and the capabilities of improved iterations will become clear. (IBM) == Benchmarks == Classification capabilities of spiking networks trained according to unsupervised learning methods have been tested on the common benchmark datasets: such as: Iris: Wisconsin Breast Cancer or Statlog Landsat dataset. Various approaches to information encoding and network design have been used. For example: a 2-layer feedforward network for data clustering and classification. Based on the idea proposed in Hopfield (1995) the authors implemented models of local receptive fields combining the properties of radial basis functions (RBF) and spiking neurons to convert input signals (classified data) having a floating-point representation into a spiking representation. == See also == == References =="": 'original_similarity': 0.805156424110009: 'slide': 203}",{'chunk_text': 'powerful enough: its supporters point out that this is only the first generation: and the capabilities of improved iterations will become clear. (IBM) == Benchmarks == Classification capabilities of spiking networks trained according to unsupervised learning methods have been tested on the common benchmark datasets: such as: Iris: Wisconsin Breast Cancer or Statlog Landsat dataset. Various approaches to information encoding and network design have been used. For example: a 2-layer feedforward network for data clustering and classification. Based on the idea proposed in Hopfield (1995) the authors implemented models of local receptive fields combining the properties of radial basis functions (RBF) and spiking neurons to convert input signals (classified data) having a floating-point representation into a spiking representation. == See also == == References ==\nThere are many types of artificial neural networks (ANN). Artificial neural networks are computational models inspired by biological neural networks: and are used to approximate functions that are generally unknown. Particularly: they': 'original_similarity': 0.8042061136607321: 'slide': 30}
0.4384307000145782,How is learning conducted in twin networks?,"Learning in twin networks can be achieved through two methods: triplet loss or contrastive loss. For learning by triplet loss, a baseline vector, positive vector, and negative vector are used where the negative vector pushes learning in the network and the positive vector acts as a regularizer. For learning by contrastive loss, there must be a weight decay to regularize the weights.","{'chunk_text': 'It is not obvious at first: but there are two slightly different problems. One is recognizing a person among a large number of other persons: that is the facial recognition problem. DeepFace is an example of such a system. In its most extreme form this is recognizing a single person at a train station or airport. The other is face verification: that is to verify whether the photo in a pass is the same as the person claiming he or she is the same person. The twin network might be the same: but the implementation can be quite different. == Learning == Learning in twin networks can be done with triplet loss or contrastive loss. For learning by triplet loss a baseline vector (anchor image) is compared against a positive vector (truthy image) and a negative vector (falsy image). The negative vector will force learning in the network: while the positive vector will act like a regularizer. For learning by contrastive loss there must be a weight decay to regularize the weights: or some similar operation like a normalization. A distance metric for a loss function may have the following properties Non-negativity: δ(x:y)≥0{\\displaystyle \\delta (x:y)\\geq 0} Identity of Non-discernibles: δ(x:y)=0⟺x=y{\\displaystyle \\delta (x:y)=0\\iff x=y} Commutativity: δ(x:y)=δ(y:x){\\displaystyle \\delta (x:y)=\\delta (y:x)} Triangle inequality: δ(x:z)≤δ(x:y)+δ(y:z){\\displaystyle \\delta (x:z)\\leq \\delta (x:y)+\\delta (y:z)}In particular: the triplet loss algorithm is often defined with squared Euclidean (which unlike Euclidean: does not have triangle inequality) distance at its core. === Predefined metrics: Euclidean distance metric === The common learning goal is to minimize a distance metric for similar objects and maximize for distinct ones. This gives a loss function like δ(x(i):x(j))={min ‖f\u2061(x(i))−f\u2061(x(j))‖:i=jmax ‖f\u2061(x(i))−f\u2061(x(j))‖:i≠j{\\displaystyle {\\begin{aligned}\\delta (x^{(i)}:x^{(j)})={\\begin{cases}\\min \\ \\|\\operatorname {f} \\left(x^{(i)}\\right)-\\operatorname {f} \\left(x^{(j)}\\right)\\|\\::i=j\\\\\\max \\ \\|\\operatorname {f} \\left(x^{(i)}\\right)-\\operatorname {f} \\left(x^{(j)}\\right)\\|\\::i\\neq j\\end{cases}}\\end{aligned}}} i:j{\\displaystyle i:j} are indexes into a set of vectors f\u2061(⋅){\\displaystyle \\operatorname {f} (\\cdot )} function implemented by the twin networkThe most common distance metric used is Euclidean distance: in case of which the loss function can be rewritten in matrix form as δ\u2061(x(i):x(j))≈(x(i)−x(j))T(x(i)−x(j)){\\displaystyle \\operatorname {\\delta } (\\mathbf {x} ^{(i)}:\\mathbf {x} ^{(j)})\\approx (\\mathbf {x} ^{(i)}-\\mathbf {x} ^{(j)})^{T}(\\mathbf {x} ^{(i)}-\\mathbf {x} ^{(j)})} === Learned metrics: nonlinear distance metric === A more general case is where the output vector from the twin network is passed through additional network layers implementing non-linear distance metrics. ifi=jthenδ\u2061[f\u2061(x(i)):f\u2061(x(j))]is smallotherwiseδ\u2061[f\u2061(x(i)):f\u2061(x(j))]is large{\\displaystyle {\\begin{aligned}{\\text{if}}\\:i=j\\:{\\text{then}}&\\:\\operatorname {\\delta } \\left[\\operatorname {f} \\left(x^{(i)}\\right):\\:\\operatorname {f} \\left(x^{(j)}\\right)\\right]\\:{\\text{is small}}\\\\{\\text{otherwise}}&\\:\\operatorname {\\delta } \\left[\\operatorname {f} \\left(x^{(i)}\\right):\\:\\operatorname {f} \\left(x^{(j)}\\right)\\right]\\:{\\text{is large}}\\end{aligned}}} i:j{\\displaystyle i:j} are indexes into a set of vectors f\u2061(⋅){\\displaystyle \\operatorname {f} (\\cdot )}function implemented by the twin network δ\u2061(⋅){\\displaystyle \\operatorname {\\delta } (\\cdot )}function implemented by the network joining outputs from the twin networkOn a matrix form the previous is often approximated as a Mahalanobis distance for a linear space as δ\u2061(x(i):x(j))≈(x(i)−x(j))TM(x(i)−x(j)){\\displaystyle \\operatorname {\\delta } (\\mathbf {x} ^{(i)}:\\mathbf {x} ^{(j)})\\approx (\\mathbf {x} ^{(i)}-\\mathbf {x} ^{(j)})^{T}\\mathbf {M} (\\mathbf {x} ^{(i)}-\\mathbf {x} ^{(j)})}This can be further subdivided in at least Unsupervised learning and Supervised learning. === Learned metrics: half-twin networks === This form also allows the twin network to be more of a half-twin: implementing a slightly different functions ifi=jthenδ\u2061[f\u2061(x(i)):g\u2061(x(j))]is smallotherwiseδ\u2061[f\u2061(x(i)):g\u2061(x(j))]is large{\\displaystyle {\\begin{aligned}{\\text{if}}\\:i=j\\:{\\text{then}}&\\:\\operatorname {\\delta } \\left[\\operatorname {f} \\left(x^{(i)}\\right):\\:\\operatorname {g} \\left(x^{(j)}\\right)\\right]\\:{\\text{is small}}\\\\{\\text{otherwise}}&\\:\\operatorname {\\delta } \\left[\\operatorname {f} \\left(x^{(i)}\\right):\\:\\operatorname {g} \\left(x^{(j)}\\right)\\right]\\:{\\text{is large}}\\end{aligned}}} i:j{\\displaystyle i:j} are indexes into a set of vectors f\u2061(⋅):g\u2061(⋅){\\displaystyle \\operatorname {f} (\\cdot ):\\operatorname {g} (\\cdot )}function implemented by the half-twin network δ\u2061(⋅){\\displaystyle \\operatorname {\\delta } (\\cdot )}function implemented by the network joining outputs from the twin network == Twin networks for object tracking == Twin networks have been used in object tracking because of its unique two tandem inputs and similarity measurement. In object tracking: one input of the twin network is user pre-selected exemplar image: the other input is a larger search image: which twin network\'s job is to locate exemplar inside of search image. By measuring the similarity between exemplar and each part of the search image: a map of similarity score can be given by the twin network. Furthermore: using a Fully Convolutional Network: the process of computing each sector\'s similarity score can be replaced with only one cross correlation layer.After being first introduced in 2016: Twin fully convolutional network has been used in many High-performance Real-time Object Tracking Neural Networks. Like CFnet: StructSiam: SiamFC-tri: DSiam: SA-Siam: SiamRPN: DaSiamRPN: Cascaded SiamRPN: SiamMask: SiamRPN++: Deeper and Wider SiamRPN. == See also == Artificial neural network Triplet loss == Further reading == Chicco: Davide (2020): ""Siamese neural networks: an overview"": Artificial Neural Networks: Methods in Molecular Biology: vol. 2190 (3rd ed.): New York City: New York: USA: Springer Protocols: Humana Press: pp. 73–94: doi:10.1007/978-1-0716-0826-5_3: ISBN 978-1-0716-0826-5: PMID 32804361: S2CID 221144012 == References ==\nSpiking neural networks (SNNs) are artificial neural networks (ANN) that more closely mimic natural neural networks. In addition to neuronal and synaptic state: SNNs incorporate the concept of time into their operating model. The idea is that neurons in the SNN do not transmit information at each propagation cycle (as it happens with typical multi-layer perceptron networks): but rather transmit information only when a membrane potential—an intrinsic quality of the neuron related to its membrane electrical charge—reaches a specific value: called the threshold. When the membrane potential reaches the threshold: the neuron fires: and generates a signal that travels to other neurons which: in turn: increase or decrease their potentials in response to this signal. A neuron model that fires at the moment of threshold crossing is also called a spiking neuron model.Although it was previously believed that the brain encoded information through spike rates: which can be considered as the analogue variable output of a traditional ANN: research in the field of neurobiology': 'original_similarity': 0.80669946547799: 'slide': 162}","{'chunk_text': 'of controllers in proportion to error gradients. In 1982: Paul Werbos applied backpropagation to MLPs in the way that has become standard. In 1986 Rumelhart: Hinton and Williams showed that backpropagation learned interesting internal representations of words as feature vectors when trained to predict the next word in a sequence.In the late 1970s to early 1980s: interest briefly emerged in theoretically investigating the Ising model created by Wilhelm Lenz (1920) and Ernst Ising (1925) in relation to Cayley tree topologies and large neural networks. The Ising model is essentially a non-learning artificial recurrent neural network (RNN) consisting of neuron-like threshold elements. In 1972: Shun\'ichi Amari described an adaptive version of this architecture: In 1981: the Ising model was solved exactly by Peter Barth for the general case of closed Cayley trees (with loops) with an arbitrary branching ratio and found to exhibit unusual phase transition behavior in its local-apex and long-range site-site correlations.John Hopfield popularised this architecture in 1982: and it is now known as a Hopfield network. The time delay neural network (TDNN) of Alex Waibel (1987) combined convolutions and weight sharing and backpropagation. In 1988: Wei Zhang et al. applied backpropagation to a CNN (a simplified Neocognitron with convolutional interconnections between the image feature layers and the last fully connected layer) for alphabet recognition. In 1989: Yann LeCun et al. trained a CNN to recognize handwritten ZIP codes on mail. In 1992: max-pooling for CNNs was introduced by Juan Weng et al. to help with least-shift invariance and tolerance to deformation to aid 3D object recognition. LeNet-5 (1998): a 7-level CNN by Yann LeCun et al.: that classifies digits: was applied by several banks to recognize hand-written numbers on checks digitized in 32x32 pixel images. From 1988 onward: the use of neural networks transformed the field of protein structure prediction: in particular when the first cascading networks were trained on profiles (matrices) produced by multiple sequence alignments.In 1991: Sepp Hochreiter\'s diploma thesis identified and analyzed the vanishing gradient problem and proposed recurrent residual connections to solve it. His thesis was called ""one of the most important documents in the history of machine learning"" by his supervisor Juergen Schmidhuber.In 1991: Juergen Schmidhuber published adversarial neural networks that contest with each other in the form of a zero-sum game: where one network\'s gain is the other network\'s loss. The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called ""artificial curiosity."" In 1992: Juergen Schmidhuber proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning. It uses predictive coding to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN: by distilling a higher level chunker network into a lower level automatizer network. In the same year he also published an alternative to RNNs which is a precursor of a linear Transformer. It introduces the concept internal spotlights of attention: a slow feedforward neural network learns by gradient descent to control the fast weights of another neural network through outer products of self-generated activation patterns. The development of metal–oxide–semiconductor (MOS) very-large-scale integration (VLSI): in the form of complementary MOS (CMOS) technology: enabled increasing MOS transistor counts in digital electronics. This provided more processing power for the development of practical artificial neural networks in the 1980s.Neural networks\' early successes included predicting the stock market and in 1995 a (mostly) self-driving car.1997: Sepp Hochreite and Juergen Schmidhuber introduced the deep learning method called long short-term memory (LSTM): published in Neural Computation. LSTM recurrent neural networks can learn ""very deep learning"" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. The ""vanilla LSTM"" with forget gate was introduced in 1999 by Felix Gers: Schmidhuber and Fred Cummins.Geoffrey Hinton et al. (2006) proposed learning a high-level representation using successive layers of binary or real-valued latent variables with a restricted Boltzmann machine to model each layer. In 2012: Ng and Dean created a network that learned to recognize higher-level concepts: such as cats: only from watching unlabeled images. Unsupervised pre-training and increased computing power from GPUs and distributed computing allowed the use of larger networks: particularly in image and visual recognition problems: which became known as ""deep learning"".Variants of the back-propagation algorithm: as well as unsupervised methods by Geoff Hinton and colleagues at the University of Toronto: can be used to train deep: highly nonlinear neural architectures: similar to the 1980 Neocognitron by Kunihiko Fukushima: and the ""standard architecture of vision"": inspired by the simple and complex cells identified by David H. Hubel\nand Torsten Wiesel in the primary visual cortex. Computational devices have been created in CMOS for both biophysical simulation and neuromorphic computing. More recent efforts show promise for creating nanodevices for very large scale principal components analyses and convolution. If successful: these efforts could usher in a new era of neural computing that is a step beyond digital computing: because it depends on learning rather than programming and because it is fundamentally analog rather than digital even though the first instantiations may in fact be with CMOS digital devices. Ciresan and colleagues (2010) showed that despite the vanishing gradient problem: GPUs make backpropagation feasible for many-layered feedforward neural networks. Between 2009 and 2012: ANNs began winning prizes in image recognition contests: approaching human level performance on various tasks: initially in pattern recognition and handwriting recognition. For example: the bi-directional and multi-dimensional long short-term memory (LSTM) of Graves et al. won three competitions in connected handwriting recognition in 2009 without any prior knowledge about the three languages to be learned.Ciresan and colleagues built the first pattern recognizers to achieve human-competitive/superhuman performance on benchmarks such as traffic sign recognition (IJCNN 2012). Radial basis function and wavelet networks were introduced in 2013. These can be shown to offer best approximation properties and have': 'original_similarity': 0.8001310966115827: 'slide': 208}","{'chunk_text': 'thousands of discrete time steps before. The ""vanilla LSTM"" with forget gate was introduced in 1999 by Felix Gers: Schmidhuber and Fred Cummins. LSTM has become the most cited neural network of the 20th century. In 2015: Rupesh Kumar Srivastava: Klaus Greff: and Schmidhuber used LSTM principles to create the Highway network: a feedforward neural network with hundreds of layers: much deeper than previous networks. 7 months later: Kaiming He: Xiangyu Zhang; Shaoqing Ren: and Jian Sun won the ImageNet 2015 competition with an open-gated or gateless Highway network variant called Residual neural network. This has become the most cited neural network of the 21st century.In 1994: André de Carvalho: together with Mike Fairhurst and David Bisset: published experimental results of a multi-layer boolean neural network: also known as a weightless neural network: composed of a 3-layers self-organising feature extraction neural network module (SOFT) followed by a multi-layer classification neural network module (GSN): which were independently trained. Each layer in the feature extraction module extracted features with growing complexity regarding the previous layer.In 1995: Brendan Frey demonstrated that it was possible to train (over two days) a network containing six fully connected layers and several hundred hidden units using the wake-sleep algorithm: co-developed with Peter Dayan and Hinton.Since 1997: Sven Behnke extended the feed-forward hierarchical convolutional approach in the Neural Abstraction Pyramid by lateral and backward connections in order to flexibly incorporate context into decisions and iteratively resolve local ambiguities. Simpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) were a popular choice in the 1990s and 2000s: because of artificial neural networks\' computational cost and a lack of understanding of how the brain wires its biological networks. Both shallow and deep learning (e.g.: recurrent nets) of ANNs for speech recognition have been explored for many years. These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively. Key difficulties have been analyzed: including gradient diminishing and weak temporal correlation structure in neural predictive models. Additional difficulties were the lack of training data and limited computing power. Most speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government\'s NSA and DARPA: SRI studied deep neural networks (DNNs) in speech and speaker recognition. The speaker recognition team led by Larry Heck reported significant success with deep neural networks in speech processing in the 1998 National Institute of Standards and Technology Speaker Recognition evaluation. The SRI deep neural network was then deployed in the Nuance Verifier: representing the first major industrial application of deep learning. The principle of elevating ""raw"" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the ""raw"" spectrogram or linear filter-bank features in the late 1990s: showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech: waveforms: later produced excellent larger-scale results.Speech recognition was taken over by LSTM. In 2003: LSTM started to become competitive with traditional speech recognizers on certain tasks. In 2006: Alex Graves: Santiago Fernández: Faustino Gomez: and Schmidhuber combined it with connectionist temporal classification (CTC) in stacks of LSTM RNNs. In 2015: Google\'s speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM: which they made available through Google Voice Search.The impact of deep learning in industry began in the early 2000s: when CNNs already processed an estimated 10% to 20% of all the checks written in the US: according to Yann LeCun. Industrial applications of deep learning to large-scale speech recognition started around 2010. In 2006: publications by Geoff Hinton: Ruslan Salakhutdinov: Osindero and Teh showed how a many-layered feedforward neural network could be effectively pre-trained one layer at a time: treating each layer in turn as an unsupervised restricted Boltzmann machine: then fine-tuning it using supervised backpropagation. The papers referred to learning for deep belief nets. The 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech: and the possibility that given more capable hardware and large-scale data sets that deep neural nets might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets. However: it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large: context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems. The nature of the recognition errors produced by the two types of systems was characteristically different: offering technical insights into how to integrate deep learning into the existing highly\nefficient: run-time speech decoding system deployed by all major speech recognition systems. Analysis around 2009–2010: contrasting the GMM (and other generative speech models) vs. DNN models: stimulated early industrial investment in deep learning for speech recognition. That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models. In 2010: researchers extended deep learning from TIMIT to large vocabulary speech recognition: by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees.Deep learning is part of state-of-the-art systems in various disciplines: particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification): as well as a range of large-vocabulary speech recognition tasks have steadily improved. Convolutional neural networks were superseded for ASR by CTC for LSTM. but are more successful in computer vision. Advances in hardware have driven renewed interest in deep learning. In 2009: Nvidia was involved in what was called the ""big bang"" of deep learning: ""as deep-learning neural networks were trained with Nvidia graphics processing units (GPUs)"". That year: Andrew Ng determined that GPUs could increase the speed of deep-learning systems': 'original_similarity': 0.7993929903054852: 'slide': 196}"
0.9381774000066798,What is a common goal during learning and what is a frequently used metric?,The common goal during learning is to minimize a distance metric for similar objects and to maximize it for distinct ones. The most common distance metric used is the Euclidean distance. ,"{'chunk_text': 'rate defines the size of the corrective steps that the model takes to adjust for errors in each observation. A high learning rate shortens the training time: but with lower ultimate accuracy: while a lower learning rate takes longer: but with the potential for greater accuracy. Optimizations such as Quickprop are primarily aimed at speeding up error minimization: while other improvements mainly try to increase reliability. In order to avoid oscillation inside the network such as alternating connection weights: and to improve the rate of convergence: refinements use an adaptive learning rate that increases or decreases as appropriate. The concept of momentum allows the balance between the gradient and the previous change to be weighted such that the weight adjustment depends to some degree on the previous change. A momentum close to 0 emphasizes the gradient: while a value close to 1 emphasizes the last change. ==== Cost function ==== While it is possible to define a cost function ad hoc: frequently the choice is determined by the function\'s desirable properties (such as convexity) or because it arises from the model (e.g. in a probabilistic model the model\'s posterior probability can be used as an inverse cost). ==== Backpropagation ==== Backpropagation is a method used to adjust the connection weights to compensate for each error found during learning. The error amount is effectively divided among the connections. Technically: backprop calculates the gradient (the derivative) of the cost function associated with a given state with respect to the weights. The weight updates can be done via stochastic gradient descent or other methods: such as extreme learning machines: ""no-prop"" networks: training without backtracking: ""weightless"" networks: and non-connectionist neural networks. === Learning paradigms === Machine learning is commonly separated into three main learning paradigms: supervised learning: unsupervised learning and reinforcement learning. Each corresponds to a particular learning task. ==== Supervised learning ==== Supervised learning uses a set of paired inputs and desired outputs. The learning task is to produce the desired output for each input. In this case: the cost function is related to eliminating incorrect deductions. A commonly used cost is the mean-squared error: which tries to minimize the average squared error between the network\'s output and the desired output. Tasks suited for supervised learning are pattern recognition (also known as classification) and regression (also known as function approximation). Supervised learning is also applicable to sequential data (e.g.: for handwriting: speech and gesture recognition). This can be thought of as learning with a ""teacher"": in the form of a function that provides continuous feedback on the quality of solutions obtained thus far. ==== Unsupervised learning ==== In unsupervised learning: input data is given along with the cost function: some function of the data x{\\displaystyle \\textstyle x} and the network\'s output. The cost function is dependent on the task (the model domain) and any a priori assumptions (the implicit properties of the model: its parameters and the observed variables). As a trivial example: consider the model f(x)=a{\\displaystyle \\textstyle f(x)=a} where a{\\displaystyle \\textstyle a} is a constant and the cost C=E[(x−f(x))2]{\\displaystyle \\textstyle C=E[(x-f(x))^{2}]}. Minimizing this cost produces a value of a{\\displaystyle \\textstyle a} that is equal to the mean of the data. The cost function can be much more complicated. Its form depends on the application: for example: in compression it could be related to the mutual information between x{\\displaystyle \\textstyle x} and f(x){\\displaystyle \\textstyle f(x)}: whereas in statistical modeling: it could be related to the posterior probability of the model given the data (note that in both of those examples: those quantities would be maximized rather than minimized). Tasks that fall within the paradigm of unsupervised learning are in general estimation problems; the applications include clustering: the estimation of statistical distributions: compression and filtering. ==== Reinforcement learning ==== In applications such as playing video games: an actor takes a string of actions: receiving a generally unpredictable response from the environment after each one. The goal is to win the game: i.e.: generate the most positive (lowest cost) responses. In reinforcement learning: the aim is to weight the network (devise a policy) to perform actions that minimize long-term (expected cumulative) cost. At each point in time the agent performs an action and the environment generates an observation and an instantaneous cost: according to some (usually unknown) rules. The rules and the long-term cost usually only can be estimated. At any juncture: the agent decides whether to explore new actions to uncover their costs or to exploit prior learning to proceed more quickly. Formally the environment is modeled as a Markov decision process (MDP) with states s1:...:sn∈S{\\displaystyle \\textstyle {s_{1}:...:s_{n}}\\in S} and actions a1:...:am∈A{\\displaystyle \\textstyle {a_{1}:...:a_{m}}\\in A}. Because the state transitions are not known: probability distributions are used instead: the instantaneous cost distribution P(ct|st){\\displaystyle \\textstyle P(c_{t}|s_{t})}: the observation distribution P(xt|st){\\displaystyle \\textstyle P(x_{t}|s_{t})} and the transition\ndistribution P(st+1|st:at){\\displaystyle \\textstyle P(s_{t+1}|s_{t}:a_{t})}: while a policy is defined as the conditional distribution over actions given the observations. Taken together: the two define a Markov chain (MC). The aim is to discover the lowest-cost MC. ANNs serve as the learning component in such applications. Dynamic programming coupled with ANNs (giving neurodynamic programming) has been applied to problems such as those involved in vehicle routing: video games: natural resource management and medicine because of ANNs ability to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of control problems. Tasks that fall within the paradigm of reinforcement learning are control problems: games and other sequential decision making tasks. ==== Self-learning ==== Self-learning in neural networks was introduced in 1982 along with a neural network capable of self-learning named crossbar adaptive array (CAA). It is a system with only one input: situation s: and only one output: action (or behavior) a. It has neither external advice input nor external reinforcement input from the environment. The CAA computes: in a crossbar fashion: both decisions about actions and emotions (feelings) about encountered situations. The system is driven by the interaction between cognition and emotion. Given the memory matrix: W': 'original_similarity': 0.7905605012279181: 'slide': 200}","{'chunk_text': 'soft weights. For example: the small (i.e. 117M parameter sized) GPT-2 model: has had twelve attention heads and a context window of only 1k token. In its medium version it has 345M parameters and contains 24 layers: each with 12 attention heads. For the training with gradient descent a batch size of 512 was utilized.The largest models: such as Google\'s Gemini 1.5: presented in February 2024: can have a context window sized up to 1 million (context window of 10 million was also ""successfully tested""). Other models with large context windows includes Anthropic\'s Claude 2.1: with a context window of up to 200k tokens. Note that this maximum refers to the number of input tokens and that the maximum number of output tokens differs from the input and is often smaller. For example: the GPT-4 Turbo model has a maximum output of 4096 tokens.Length of a conversation that the model can take into account when generating its next answer is limited by the size of a context window: as well. If the length of a conversation: for example with Chat-GPT: is longer than its context window: only the parts inside the context window are taken into account when generating the next answer: or the model needs to apply some algorithm to summarize the too distant parts of conversation. The shortcomings of making a context window larger include higher computational cost and possibly diluting the focus on local context: while making it smaller can cause a model to miss an important long-range dependency. Balancing them are a matter of experimentation and domain-specific considerations. A model may be pre-trained either to predict how the segment continues: or what is missing in the segment: given a segment from its training dataset. It can be either autoregressive (i.e. predicting how the segment continues: the way GPTs do it): for example given a segment ""I like to eat"": the model predicts ""ice cream"": or ""sushi"". ""masked"" (i.e. filling in the parts missing from the segment: the way ""BERT"" does it): for example: given a segment ""I like to [__] [__] cream"": the model predicts that ""eat"" and ""ice"" are missing.Models may be trained on auxiliary tasks which test their understanding of the data distribution: such as Next Sentence Prediction (NSP): in which pairs of sentences are presented and the model must predict whether they appear consecutively in the training corpus. During training: regularization loss is also used to stabilize training. However regularization loss is usually not used during testing and evaluation. == Training cost == Advances in software and hardware have reduced the cost substantially since 2020: such that in 2023 training of a 12-billion-parameter LLM computational cost is 72:300 A100-GPU-hours: while in 2020 the cost of training a 1.5-billion-parameter LLM (which was two orders of magnitude smaller than the state of the art in 2020) was between $80 thousand and $1.6 million. Since 2020: large sums were invested in increasingly large models. For example: training of the GPT-2 (i.e. a 1.5-billion-parameters model) in 2019 cost $50:000: while training of the PaLM (i.e. a 540-billion-parameters model) in 2022 cost $8 million: and Megatron-Turing NLG 530B (in 2021) cost around $11 million.For Transformer-based LLM: training cost is much higher than inference cost. It costs 6 FLOPs per parameter to train on one token: whereas it costs 1 to 2 FLOPs per parameter to infer on one token. == Tool use == There are certain tasks that: in principle: cannot be solved by any LLM: at least not without the use of external tools or additional software. An example of such a task is responding to the user\'s input \'354 * 139 = \': provided that the LLM has not already encountered a continuation of this calculation in its training corpus. In such cases: the LLM needs to resort to running program code that calculates the result: which can then be included in its response. Another example is \'What is the time now? It is \': where a separate program interpreter would need to execute a code to get system time on the computer: so LLM could include it in its reply. This basic strategy can be sophisticated with multiple attempts of generated programs: and other sampling strategies. Cost Savings and Reduced Vendor Dependency Generally: in order to get an LLM to use tools: one must finetune it for tool-use. If the number of tools is finite: then finetuning may be done just once. If the number of tools can grow arbitrarily: as with online API services: then the LLM can be fine-tuned to be able to read API documentation and call API correctly.A simpler\nform of tool use is Retrieval Augmented Generation: augment an LLM with document retrieval: sometimes using a vector database. Given a query: a document retriever is called to retrieve the most relevant (usually measured by first encoding the query and the documents into vectors: then finding the documents with vectors closest in Euclidean norm to the query vector). The LLM then generates an output based on both the query and the retrieved documents. == Agency == An LLM is a language model: which is not an agent as it has no goal: but it can be used as a component of an intelligent agent. Researchers have described several methods for such integrations.The ReAct (""Reason + Act"") method constructs an agent out of an LLM: using the LLM as a planner. The LLM is prompted to ""think out loud"". Specifically: the language model is prompted with a textual description of the environment: a goal: a list of possible actions: and a record of the actions and observations so far. It generates one or more thoughts before generating an action: which is then executed in the environment. The linguistic description of the environment given to the LLM planner can even be the LaTeX code of a paper describing the environment.In the DEPS (""Describe: Explain: Plan and Select"") method: an LLM is first connected to the visual world via image descriptions: then it is prompted to': 'original_similarity': 0.7792015853109622: 'slide': 231}","{'chunk_text': 'soft weights. For example: the small (i.e. 117M parameter sized) GPT-2 model: has had twelve attention heads and a context window of only 1k token. In its medium version it has 345M parameters and contains 24 layers: each with 12 attention heads. For the training with gradient descent a batch size of 512 was utilized.The largest models: such as Google\'s Gemini 1.5: presented in February 2024: can have a context window sized up to 1 million (context window of 10 million was also ""successfully tested""). Other models with large context windows includes Anthropic\'s Claude 2.1: with a context window of up to 200k tokens. Note that this maximum refers to the number of input tokens and that the maximum number of output tokens differs from the input and is often smaller. For example: the GPT-4 Turbo model has a maximum output of 4096 tokens.Length of a conversation that the model can take into account when generating its next answer is limited by the size of a context window: as well. If the length of a conversation: for example with Chat-GPT: is longer than its context window: only the parts inside the context window are taken into account when generating the next answer: or the model needs to apply some algorithm to summarize the too distant parts of conversation. The shortcomings of making a context window larger include higher computational cost and possibly diluting the focus on local context: while making it smaller can cause a model to miss an important long-range dependency. Balancing them are a matter of experimentation and domain-specific considerations. A model may be pre-trained either to predict how the segment continues: or what is missing in the segment: given a segment from its training dataset. It can be either autoregressive (i.e. predicting how the segment continues: the way GPTs do it): for example given a segment ""I like to eat"": the model predicts ""ice cream"": or ""sushi"". ""masked"" (i.e. filling in the parts missing from the segment: the way ""BERT"" does it): for example: given a segment ""I like to [__] [__] cream"": the model predicts that ""eat"" and ""ice"" are missing.Models may be trained on auxiliary tasks which test their understanding of the data distribution: such as Next Sentence Prediction (NSP): in which pairs of sentences are presented and the model must predict whether they appear consecutively in the training corpus. During training: regularization loss is also used to stabilize training. However regularization loss is usually not used during testing and evaluation. == Training cost == Advances in software and hardware have reduced the cost substantially since 2020: such that in 2023 training of a 12-billion-parameter LLM computational cost is 72:300 A100-GPU-hours: while in 2020 the cost of training a 1.5-billion-parameter LLM (which was two orders of magnitude smaller than the state of the art in 2020) was between $80 thousand and $1.6 million. Since 2020: large sums were invested in increasingly large models. For example: training of the GPT-2 (i.e. a 1.5-billion-parameters model) in 2019 cost $50:000: while training of the PaLM (i.e. a 540-billion-parameters model) in 2022 cost $8 million: and Megatron-Turing NLG 530B (in 2021) cost around $11 million.For Transformer-based LLM: training cost is much higher than inference cost. It costs 6 FLOPs per parameter to train on one token: whereas it costs 1 to 2 FLOPs per parameter to infer on one token. == Tool use == There are certain tasks that: in principle: cannot be solved by any LLM: at least not without the use of external tools or additional software. An example of such a task is responding to the user\'s input \'354 * 139 = \': provided that the LLM has not already encountered a continuation of this calculation in its training corpus. In such cases: the LLM needs to resort to running program code that calculates the result: which can then be included in its response. Another example is \'What is the time now? It is \': where a separate program interpreter would need to execute a code to get system time on the computer: so LLM could include it in its reply. This basic strategy can be sophisticated with multiple attempts of generated programs: and other sampling strategies. Cost Savings and Reduced Vendor Dependency Generally: in order to get an LLM to use tools: one must finetune it for tool-use. If the number of tools is finite: then finetuning may be done just once. If the number of tools can grow arbitrarily: as with online API services: then the LLM can be fine-tuned to be able to read API documentation and call API correctly.A simpler\nform of tool use is Retrieval Augmented Generation: augment an LLM with document retrieval: sometimes using a vector database. Given a query: a document retriever is called to retrieve the most relevant (usually measured by first encoding the query and the documents into vectors: then finding the documents with vectors closest in Euclidean norm to the query vector). The LLM then generates an output based on both the query and the retrieved documents. == Agency == An LLM is a language model: which is not an agent as it has no goal: but it can be used as a component of an intelligent agent. Researchers have described several methods for such integrations.The ReAct (""Reason + Act"") method constructs an agent out of an LLM: using the LLM as a planner. The LLM is prompted to ""think out loud"". Specifically: the language model is prompted with a textual description of the environment: a goal: a list of possible actions: and a record of the actions and observations so far. It generates one or more thoughts before generating an action: which is then executed in the environment. The linguistic description of the environment given to the LLM planner can even be the LaTeX code of a paper describing the environment.In the DEPS (""Describe: Explain: Plan and Select"") method: an LLM is first connected to the visual world via image descriptions: then it is prompted to': 'original_similarity': 0.7792015853109622: 'slide': 231}"
0.4388905999949202,How is a 'half-twin' network different from a twin network?,"While similar to a twin network, a 'half-twin' network implements slightly different functions. The delta function (distance calculation) between the two functions implemented by 'half-twin' network varies based on whether the indexes of the two vectors are the same or different.","{'chunk_text': 'two slightly different problems. One is recognizing a person among a large number of other persons: that is the facial recognition problem. DeepFace is an example of such a system. In its most extreme form this is recognizing a single person at a train station or airport. The other is face verification: that is to verify whether the photo in a pass is the same as the person claiming he or she is the same person. The twin network might be the same: but the implementation can be quite different. == Learning == Learning in twin networks can be done with triplet loss or contrastive loss. For learning by triplet loss a baseline vector (anchor image) is compared against a positive vector (truthy image) and a negative vector (falsy image). The negative vector will force learning in the network: while the positive vector will act like a regularizer. For learning by contrastive loss there must be a weight decay to regularize the weights: or some similar operation like a normalization. A distance metric for a loss function may have the following properties Non-negativity: δ(x:y)≥0{\\displaystyle \\delta (x:y)\\geq 0} Identity of Non-discernibles: δ(x:y)=0⟺x=y{\\displaystyle \\delta (x:y)=0\\iff x=y} Commutativity: δ(x:y)=δ(y:x){\\displaystyle \\delta (x:y)=\\delta (y:x)} Triangle inequality: δ(x:z)≤δ(x:y)+δ(y:z){\\displaystyle \\delta (x:z)\\leq \\delta (x:y)+\\delta (y:z)}In particular: the triplet loss algorithm is often defined with squared Euclidean (which unlike Euclidean: does not have triangle inequality) distance at its core. === Predefined metrics: Euclidean distance metric === The common learning goal is to minimize a distance metric for similar objects and maximize for distinct ones. This gives a loss function like δ(x(i):x(j))={min ‖f\u2061(x(i))−f\u2061(x(j))‖:i=jmax ‖f\u2061(x(i))−f\u2061(x(j))‖:i≠j{\\displaystyle {\\begin{aligned}\\delta (x^{(i)}:x^{(j)})={\\begin{cases}\\min \\ \\|\\operatorname {f} \\left(x^{(i)}\\right)-\\operatorname {f} \\left(x^{(j)}\\right)\\|\\::i=j\\\\\\max \\ \\|\\operatorname {f} \\left(x^{(i)}\\right)-\\operatorname {f} \\left(x^{(j)}\\right)\\|\\::i\\neq j\\end{cases}}\\end{aligned}}} i:j{\\displaystyle i:j} are indexes into a set of vectors f\u2061(⋅){\\displaystyle \\operatorname {f} (\\cdot )} function implemented by the twin networkThe most common distance metric used is Euclidean distance: in case of which the loss function can be rewritten in matrix form as δ\u2061(x(i):x(j))≈(x(i)−x(j))T(x(i)−x(j)){\\displaystyle \\operatorname {\\delta } (\\mathbf {x} ^{(i)}:\\mathbf {x} ^{(j)})\\approx (\\mathbf {x} ^{(i)}-\\mathbf {x} ^{(j)})^{T}(\\mathbf {x} ^{(i)}-\\mathbf {x} ^{(j)})} === Learned metrics: nonlinear distance metric === A more general case is where the output vector from the twin network is passed through additional network layers implementing non-linear distance metrics. ifi=jthenδ\u2061[f\u2061(x(i)):f\u2061(x(j))]is smallotherwiseδ\u2061[f\u2061(x(i)):f\u2061(x(j))]is large{\\displaystyle {\\begin{aligned}{\\text{if}}\\:i=j\\:{\\text{then}}&\\:\\operatorname {\\delta } \\left[\\operatorname {f} \\left(x^{(i)}\\right):\\:\\operatorname {f} \\left(x^{(j)}\\right)\\right]\\:{\\text{is small}}\\\\{\\text{otherwise}}&\\:\\operatorname {\\delta } \\left[\\operatorname {f} \\left(x^{(i)}\\right):\\:\\operatorname {f} \\left(x^{(j)}\\right)\\right]\\:{\\text{is large}}\\end{aligned}}} i:j{\\displaystyle i:j} are indexes into a set of vectors f\u2061(⋅){\\displaystyle \\operatorname {f} (\\cdot )}function implemented by the twin network δ\u2061(⋅){\\displaystyle \\operatorname {\\delta } (\\cdot )}function implemented by the network joining outputs from the twin networkOn a matrix form the previous is often approximated as a Mahalanobis distance for a linear space as δ\u2061(x(i):x(j))≈(x(i)−x(j))TM(x(i)−x(j)){\\displaystyle \\operatorname {\\delta } (\\mathbf {x} ^{(i)}:\\mathbf {x} ^{(j)})\\approx (\\mathbf {x} ^{(i)}-\\mathbf {x} ^{(j)})^{T}\\mathbf {M} (\\mathbf {x} ^{(i)}-\\mathbf {x} ^{(j)})}This can be further subdivided in at least Unsupervised learning and Supervised learning. === Learned metrics: half-twin networks === This form also allows the twin network to be more of a half-twin: implementing a slightly different functions ifi=jthenδ\u2061[f\u2061(x(i)):g\u2061(x(j))]is smallotherwiseδ\u2061[f\u2061(x(i)):g\u2061(x(j))]is large{\\displaystyle {\\begin{aligned}{\\text{if}}\\:i=j\\:{\\text{then}}&\\:\\operatorname {\\delta } \\left[\\operatorname {f} \\left(x^{(i)}\\right):\\:\\operatorname {g} \\left(x^{(j)}\\right)\\right]\\:{\\text{is small}}\\\\{\\text{otherwise}}&\\:\\operatorname {\\delta } \\left[\\operatorname {f} \\left(x^{(i)}\\right):\\:\\operatorname {g} \\left(x^{(j)}\\right)\\right]\\:{\\text{is large}}\\end{aligned}}} i:j{\\displaystyle i:j} are indexes into a set of vectors f\u2061(⋅):g\u2061(⋅){\\displaystyle \\operatorname {f} (\\cdot ):\\operatorname {g} (\\cdot )}function implemented by the half-twin network δ\u2061(⋅){\\displaystyle \\operatorname {\\delta } (\\cdot )}function implemented by the network joining outputs from the twin network == Twin networks for object tracking == Twin networks have been used in object tracking because of its unique two tandem inputs and similarity measurement. In object tracking: one input of the twin network is user pre-selected exemplar image: the other input is a larger search image: which twin network\'s job is to locate exemplar inside of search image. By measuring the similarity between exemplar and each part of the search image: a map of similarity score can be given by the twin network. Furthermore: using a Fully Convolutional Network: the process of computing each sector\'s similarity score can be replaced with only one cross correlation layer.After being first introduced in 2016: Twin fully convolutional network has been used in many High-performance Real-time Object Tracking Neural Networks. Like CFnet: StructSiam: SiamFC-tri: DSiam: SA-Siam: SiamRPN: DaSiamRPN: Cascaded SiamRPN: SiamMask: SiamRPN++: Deeper and Wider SiamRPN. == See also == Artificial neural network Triplet loss == Further reading == Chicco: Davide (2020): ""Siamese neural networks: an overview"": Artificial Neural Networks: Methods in Molecular Biology: vol. 2190 (3rd ed.): New York City: New York: USA: Springer Protocols: Humana Press: pp. 73–94: doi:10.1007/978-1-0716-0826-5_3: ISBN 978-1-0716-0826-5: PMID 32804361: S2CID 221144012 == References ==\nSpiking neural networks (SNNs) are artificial neural networks (ANN) that more closely mimic natural neural networks. In addition to neuronal and synaptic state: SNNs incorporate the concept of time into their operating model. The idea is that neurons in the SNN do not transmit information at each propagation cycle (as it happens with typical multi-layer perceptron networks): but rather transmit information only when a membrane potential—an intrinsic quality of the neuron related to its membrane electrical charge—reaches a specific value: called the threshold. When the membrane potential reaches the threshold: the neuron fires: and generates a signal that travels to other neurons which: in turn: increase or decrease their potentials in response to this signal. A neuron model that fires at the moment of threshold crossing is also called a spiking neuron model.Although it was previously believed that the brain encoded information through spike rates: which can be considered as the analogue variable output of a traditional ANN: research in the field of neurobiology has indicated that high speed processing cannot solely be': 'original_similarity': 0.79474469432082: 'slide': 171}","{'chunk_text': ""design to larger (deeper) architectures and data sets. The basic architecture is suitable for diverse tasks such as classification and regression. == Regulatory feedback == Regulatory feedback networks started as a model to explain brain phenomena found during recognition including network-wide bursting and difficulty with similarity found universally in sensory recognition. A mechanism to perform optimization during recognition is created using inhibitory feedback connections back to the same inputs that activate them. This reduces requirements during learning and allows learning and updating to be easier while still being able to perform complex recognition. A regulatory feedback network makes inferences using negative feedback. The feedback is used to find the optimal activation of units. It is most similar to a non-parametric method but is different from K-nearest neighbor in that it mathematically emulates feedforward networks. == Radial basis function (RBF) == Radial basis functions are functions that have a distance criterion with respect to a center. Radial basis functions have been applied as a replacement for the sigmoidal hidden layer transfer characteristic in multi-layer perceptrons. RBF networks have two layers: In the first: input is mapped onto each RBF in the 'hidden' layer. The RBF chosen is usually a Gaussian. In regression problems the output layer is a linear combination of hidden layer values representing mean predicted output. The interpretation of this output layer value is the same as a regression model in statistics. In classification problems the output layer is typically a sigmoid function of a linear combination of hidden layer values: representing a posterior probability. Performance in both cases is often improved by shrinkage techniques: known as ridge regression in classical statistics. This corresponds to a prior belief in small parameter values (and therefore smooth output functions) in a Bayesian framework. RBF networks have the advantage of avoiding local minima in the same way as multi-layer perceptrons. This is because the only parameters that are adjusted in the learning process are the linear mapping from hidden layer to output layer. Linearity ensures that the error surface is quadratic and therefore has a single easily found minimum. In regression problems this can be found in one matrix operation. In classification problems the fixed non-linearity introduced by the sigmoid output function is most efficiently dealt with using iteratively re-weighted least squares. RBF networks have the disadvantage of requiring good coverage of the input space by radial basis functions. RBF centres are determined with reference to the distribution of the input data: but without reference to the prediction task. As a result: representational resources may be wasted on areas of the input space that are irrelevant to the task. A common solution is to associate each data point with its own centre: although this can expand the linear system to be solved in the final layer and requires shrinkage techniques to avoid overfitting. Associating each input datum with an RBF leads naturally to kernel methods such as support vector machines (SVM) and Gaussian processes (the RBF is the kernel function). All three approaches use a non-linear kernel function to project the input data into a space where the learning problem can be solved using a linear model. Like Gaussian processes: and unlike SVMs: RBF networks are typically trained in a maximum likelihood framework by maximizing the probability (minimizing the error). SVMs avoid overfitting by maximizing instead a margin. SVMs outperform RBF networks in most classification applications. In regression applications they can be competitive when the dimensionality of the input space is relatively small. === How RBF networks work === RBF neural networks are conceptually similar to K-Nearest Neighbor (k-NN) models. The basic idea is that similar inputs produce similar outputs. Assume that each case in a training set has two predictor variables: x and y: and the target variable has two categories: positive and negative. Given a new case with predictor values x=6: y=5.1: how is the target variable computed? The nearest neighbor classification performed for this example depends on how many neighboring points are considered. If 1-NN is used and the closest point is negative: then the new point should be classified as negative. Alternatively: if 9-NN classification is used and the closest 9 points are considered: then the effect of the surrounding 8 positive points may outweigh the closest 9-th (negative) point. An RBF network positions neurons in the space described by the predictor variables (x:y in this example). This space has as many dimensions as predictor variables. The Euclidean distance is computed from the new point to the center of each neuron: and a radial basis function (RBF: also called a kernel function) is applied to the distance to compute the weight (influence) for each neuron. The radial basis function is so named because the radius distance is the argument to the function. Weight = RBF(distance) ==== Radial Basis Function ====\nThe value for the new point is found by summing the output values of the RBF functions multiplied by weights computed for each neuron. The radial basis function for a neuron has a center and a radius (also called a spread). The radius may be different for each neuron: and: in RBF networks generated by DTREG: the radius may be different in each dimension. With larger spread: neurons at a distance from a point have a greater influence. ==== Architecture ==== RBF networks have three layers: Input layer: One neuron appears in the input layer for each predictor variable. In the case of categorical variables: N-1 neurons are used where N is the number of categories. The input neurons standardizes the value ranges by subtracting the median and dividing by the interquartile range. The input neurons then feed the values to each of the neurons in the hidden layer. Hidden layer: This layer has a variable number of neurons (determined by the training process). Each neuron consists of a radial basis function centered on a point with as many dimensions as predictor variables. The spread (radius) of the RBF function may be different for each dimension. The centers and"": 'original_similarity': 0.7697416151979296: 'slide': 197}","{'chunk_text': 'output are written sentences in two natural languages. In that work: an LSTM RNN or CNN was used as an encoder to summarize a source sentence: and the summary was decoded using a conditional RNN language model to produce the translation. These systems share building blocks: gated RNNs and CNNs and trained attention mechanisms. == Other types == === Instantaneously trained === Instantaneously trained neural networks (ITNN) were inspired by the phenomenon of short-term learning that seems to occur instantaneously. In these networks the weights of the hidden and the output layers are mapped directly from the training vector data. Ordinarily: they work on binary data: but versions for continuous data that require small additional processing exist. === Spiking === Spiking neural networks (SNN) explicitly consider the timing of inputs. The network input and output are usually represented as a series of spikes (delta function or more complex shapes). SNN can process information in the time domain (signals that vary over time). They are often implemented as recurrent networks. SNN are also a form of pulse computer.Spiking neural networks with axonal conduction delays exhibit polychronization: and hence could have a very large memory capacity.SNN and the temporal correlations of neural assemblies in such networks—have been used to model figure/ground separation and region linking in the visual system. === Spatial === Spatial neural networks (SNNs) constitute a supercategory of tailored neural networks (NNs) for representing and predicting geographic phenomena. They generally improve both the statistical accuracy and reliability of the a-spatial/classic NNs whenever they handle geo-spatial datasets: and also of the other spatial (statistical) models (e.g. spatial regression models) whenever the geo-spatial datasets\' variables depict non-linear relations. Examples of SNNs are the OSFA spatial neural networks: SVANNs and GWNNs. === Neocognitron === The neocognitron is a hierarchical: multilayered network that was modeled after the visual cortex. It uses multiple types of units: (originally two: called simple and complex cells): as a cascading model for use in pattern recognition tasks. Local features are extracted by S-cells whose deformation is tolerated by C-cells. Local features in the input are integrated gradually and classified at higher layers. Among the various kinds of neocognitron are systems that can detect multiple patterns in the same input by using back propagation to achieve selective attention. It has been used for pattern recognition tasks and inspired convolutional neural networks. === Compound hierarchical-deep models === Compound hierarchical-deep models compose deep networks with non-parametric Bayesian models. Features can be learned using deep architectures such as DBNs: deep Boltzmann machines (DBM): deep auto encoders: convolutional variants: ssRBMs: deep coding networks: DBNs with sparse feature learning: RNNs: conditional DBNs: denoising autoencoders. This provides a better representation: allowing faster learning and more accurate classification with high-dimensional data. However: these architectures are poor at learning novel classes with few examples: because all network units are involved in representing the input (a distributed representation) and must be adjusted together (high degree of freedom). Limiting the degree of freedom reduces the number of parameters to learn: facilitating learning of new classes from few examples. Hierarchical Bayesian (HB) models allow learning from few examples: for example for computer vision: statistics and cognitive science. Compound HD architectures aim to integrate characteristics of both HB and deep networks. The compound HDP-DBM architecture is a hierarchical Dirichlet process (HDP) as a hierarchical model: incorporating DBM architecture. It is a full generative model: generalized from abstract concepts flowing through the model layers: which is able to synthesize new examples in novel classes that look ""reasonably"" natural. All the levels are learned jointly by maximizing a joint log-probability score.In a DBM with three hidden layers: the probability of a visible input \'\'ν\'\' is: p(ν:ψ)=1Z∑hexp\u2061(∑ijWij(1)νihj1+∑jℓWjℓ(2)hj1hℓ2+∑ℓmWℓm(3)hℓ2hm3):{\\displaystyle p({\\boldsymbol {\\nu }}:\\psi )={\\frac {1}{Z}}\\sum _{h}\\exp \\left(\\sum _{ij}W_{ij}^{(1)}\\nu _{i}h_{j}^{1}+\\sum _{j\\ell }W_{j\\ell }^{(2)}h_{j}^{1}h_{\\ell }^{2}+\\sum _{\\ell m}W_{\\ell m}^{(3)}h_{\\ell }^{2}h_{m}^{3}\\right):}where h={h(1):h(2):h(3)}{\\displaystyle {\\boldsymbol {h}}=\\{{\\boldsymbol {h}}^{(1)}:{\\boldsymbol {h}}^{(2)}:{\\boldsymbol {h}}^{(3)}\\}} is the set of hidden units: and ψ={W(1):W(2):W(3)}{\\displaystyle \\psi =\\{{\\boldsymbol {W}}^{(1)}:{\\boldsymbol {W}}^{(2)}:{\\boldsymbol {W}}^{(3)}\\}} are the model parameters: representing visible-hidden and hidden-hidden symmetric interaction terms. A learned DBM model is an undirected model that defines the joint distribution P(ν:h1:h2:h3){\\displaystyle P(\\nu :h^{1}:h^{2}:h^{3})}. One way to express what has been learned is the conditional model P(ν:h1:h2∣h3){\\displaystyle P(\\nu :h^{1}:h^{2}\\mid h^{3})} and a prior term P(h3){\\displaystyle P(h^{3})}. Here P(ν:h1:h2∣h3){\\displaystyle P(\\nu :h^{1}:h^{2}\\mid h^{3})} represents a conditional DBM model: which can be viewed as a two-layer DBM but with bias terms given by the states of h3{\\displaystyle h^{3}}: P(ν:h1:h2∣h3)=1Z(ψ:h3)exp\u2061(∑ijWij(1)νihj1+∑jℓWjℓ(2)hj1hℓ2+∑ℓmWℓm(3)hℓ2hm3).{\\displaystyle P(\\nu :h^{1}:h^{2}\\mid h^{3})={\\frac {1}{Z(\\psi :h^{3})}}\\exp \\left(\\sum _{ij}W_{ij}^{(1)}\\nu _{i}h_{j}^{1}+\\sum _{j\\ell }W_{j\\ell }^{(2)}h_{j}^{1}h_{\\ell }^{2}+\\sum _{\\ell m}W_{\\ell m}^{(3)}h_{\\ell }^{2}h_{m}^{3}\\right).} === Deep predictive coding networks === A deep predictive coding network (DPCN) is a predictive coding scheme that uses top-down information to empirically adjust the priors needed for a bottom-up inference procedure by means of a deep: locally connected: generative model. This works by extracting sparse features from time-varying observations using\na linear dynamical model. Then: a pooling strategy is used to learn invariant feature representations. These units compose to form a deep architecture and are trained by greedy layer-wise unsupervised learning. The layers constitute a kind of Markov chain such that the states at any layer depend only on the preceding and succeeding layers. DPCNs predict the representation of the layer: by using a top-down approach using the information in upper layer and temporal dependencies from previous states.DPCNs can be extended to form a convolutional network. === Multilayer kernel machine === Multilayer kernel machines (MKM) are a way of learning highly nonlinear functions by iterative application of weakly nonlinear kernels. They use kernel principal component analysis (KPCA): as a method for the unsupervised greedy layer-wise pre-training step of deep learning.Layer ℓ+1{\\displaystyle \\ell +1} learns the representation of the previous layer ℓ{\\displaystyle \\ell }: extracting the nl{\\displaystyle n_{l}} principal component (PC) of the projection layer l{\\displaystyle l} output in the feature domain induced by the kernel. To reduce the dimensionaliity of the updated representation in each layer: a supervised strategy selects the best informative features among features extracted by KPCA. The process is: rank the nℓ{\\displaystyle n_{\\ell }} features according to their mutual information with the class labels;': 'original_similarity': 0.7679024107529242: 'slide': 205}"
0.6409268999996129,How are twin networks used in object tracking?,"Twin networks have been used in object tracking due to their unique two tandem inputs and similarity measurement capabilities. One input of the twin network is a user pre-selected exemplar image, and the other is a larger search image. The twin network's job is to locate the exemplar image within the search image by measuring the similarity between the exemplar and each part of the search image, thereby producing a map of similarity scores.","{'chunk_text': 'images of people are precomputed and compared to an image from a turnstile or similar. It is not obvious at first: but there are two slightly different problems. One is recognizing a person among a large number of other persons: that is the facial recognition problem. DeepFace is an example of such a system. In its most extreme form this is recognizing a single person at a train station or airport. The other is face verification: that is to verify whether the photo in a pass is the same as the person claiming he or she is the same person. The twin network might be the same: but the implementation can be quite different. == Learning == Learning in twin networks can be done with triplet loss or contrastive loss. For learning by triplet loss a baseline vector (anchor image) is compared against a positive vector (truthy image) and a negative vector (falsy image). The negative vector will force learning in the network: while the positive vector will act like a regularizer. For learning by contrastive loss there must be a weight decay to regularize the weights: or some similar operation like a normalization. A distance metric for a loss function may have the following properties Non-negativity: δ(x:y)≥0{\\displaystyle \\delta (x:y)\\geq 0} Identity of Non-discernibles: δ(x:y)=0⟺x=y{\\displaystyle \\delta (x:y)=0\\iff x=y} Commutativity: δ(x:y)=δ(y:x){\\displaystyle \\delta (x:y)=\\delta (y:x)} Triangle inequality: δ(x:z)≤δ(x:y)+δ(y:z){\\displaystyle \\delta (x:z)\\leq \\delta (x:y)+\\delta (y:z)}In particular: the triplet loss algorithm is often defined with squared Euclidean (which unlike Euclidean: does not have triangle inequality) distance at its core. === Predefined metrics: Euclidean distance metric === The common learning goal is to minimize a distance metric for similar objects and maximize for distinct ones. This gives a loss function like δ(x(i):x(j))={min ‖f\u2061(x(i))−f\u2061(x(j))‖:i=jmax ‖f\u2061(x(i))−f\u2061(x(j))‖:i≠j{\\displaystyle {\\begin{aligned}\\delta (x^{(i)}:x^{(j)})={\\begin{cases}\\min \\ \\|\\operatorname {f} \\left(x^{(i)}\\right)-\\operatorname {f} \\left(x^{(j)}\\right)\\|\\::i=j\\\\\\max \\ \\|\\operatorname {f} \\left(x^{(i)}\\right)-\\operatorname {f} \\left(x^{(j)}\\right)\\|\\::i\\neq j\\end{cases}}\\end{aligned}}} i:j{\\displaystyle i:j} are indexes into a set of vectors f\u2061(⋅){\\displaystyle \\operatorname {f} (\\cdot )} function implemented by the twin networkThe most common distance metric used is Euclidean distance: in case of which the loss function can be rewritten in matrix form as δ\u2061(x(i):x(j))≈(x(i)−x(j))T(x(i)−x(j)){\\displaystyle \\operatorname {\\delta } (\\mathbf {x} ^{(i)}:\\mathbf {x} ^{(j)})\\approx (\\mathbf {x} ^{(i)}-\\mathbf {x} ^{(j)})^{T}(\\mathbf {x} ^{(i)}-\\mathbf {x} ^{(j)})} === Learned metrics: nonlinear distance metric === A more general case is where the output vector from the twin network is passed through additional network layers implementing non-linear distance metrics. ifi=jthenδ\u2061[f\u2061(x(i)):f\u2061(x(j))]is smallotherwiseδ\u2061[f\u2061(x(i)):f\u2061(x(j))]is large{\\displaystyle {\\begin{aligned}{\\text{if}}\\:i=j\\:{\\text{then}}&\\:\\operatorname {\\delta } \\left[\\operatorname {f} \\left(x^{(i)}\\right):\\:\\operatorname {f} \\left(x^{(j)}\\right)\\right]\\:{\\text{is small}}\\\\{\\text{otherwise}}&\\:\\operatorname {\\delta } \\left[\\operatorname {f} \\left(x^{(i)}\\right):\\:\\operatorname {f} \\left(x^{(j)}\\right)\\right]\\:{\\text{is large}}\\end{aligned}}} i:j{\\displaystyle i:j} are indexes into a set of vectors f\u2061(⋅){\\displaystyle \\operatorname {f} (\\cdot )}function implemented by the twin network δ\u2061(⋅){\\displaystyle \\operatorname {\\delta } (\\cdot )}function implemented by the network joining outputs from the twin networkOn a matrix form the previous is often approximated as a Mahalanobis distance for a linear space as δ\u2061(x(i):x(j))≈(x(i)−x(j))TM(x(i)−x(j)){\\displaystyle \\operatorname {\\delta } (\\mathbf {x} ^{(i)}:\\mathbf {x} ^{(j)})\\approx (\\mathbf {x} ^{(i)}-\\mathbf {x} ^{(j)})^{T}\\mathbf {M} (\\mathbf {x} ^{(i)}-\\mathbf {x} ^{(j)})}This can be further subdivided in at least Unsupervised learning and Supervised learning. === Learned metrics: half-twin networks === This form also allows the twin network to be more of a half-twin: implementing a slightly different functions ifi=jthenδ\u2061[f\u2061(x(i)):g\u2061(x(j))]is smallotherwiseδ\u2061[f\u2061(x(i)):g\u2061(x(j))]is large{\\displaystyle {\\begin{aligned}{\\text{if}}\\:i=j\\:{\\text{then}}&\\:\\operatorname {\\delta } \\left[\\operatorname {f} \\left(x^{(i)}\\right):\\:\\operatorname {g} \\left(x^{(j)}\\right)\\right]\\:{\\text{is small}}\\\\{\\text{otherwise}}&\\:\\operatorname {\\delta } \\left[\\operatorname {f} \\left(x^{(i)}\\right):\\:\\operatorname {g} \\left(x^{(j)}\\right)\\right]\\:{\\text{is large}}\\end{aligned}}} i:j{\\displaystyle i:j} are indexes into a set of vectors f\u2061(⋅):g\u2061(⋅){\\displaystyle \\operatorname {f} (\\cdot ):\\operatorname {g} (\\cdot )}function implemented by the half-twin network δ\u2061(⋅){\\displaystyle \\operatorname {\\delta } (\\cdot )}function implemented by the network joining outputs from the twin network == Twin networks for object tracking == Twin networks have been used in object tracking because of its unique two tandem inputs and similarity measurement. In object tracking: one input of the twin network is user pre-selected exemplar image: the other input is a larger search image: which twin network\'s job is to locate exemplar inside of search image. By measuring the similarity between exemplar and each part of the search image: a map of similarity score can be given by the twin network. Furthermore: using a Fully Convolutional Network: the process of computing each sector\'s similarity score can be replaced with only one cross correlation layer.After being first introduced in 2016: Twin fully convolutional network has been used in many High-performance Real-time Object Tracking Neural Networks. Like CFnet: StructSiam: SiamFC-tri: DSiam: SA-Siam: SiamRPN: DaSiamRPN: Cascaded SiamRPN: SiamMask: SiamRPN++: Deeper and Wider SiamRPN. == See also == Artificial neural network Triplet loss == Further reading == Chicco: Davide (2020): ""Siamese neural networks: an overview"": Artificial Neural Networks: Methods in Molecular Biology: vol. 2190 (3rd ed.): New York City: New York: USA: Springer Protocols: Humana Press: pp. 73–94: doi:10.1007/978-1-0716-0826-5_3: ISBN 978-1-0716-0826-5: PMID 32804361: S2CID 221144012 == References ==\nSpiking neural networks (SNNs) are artificial neural networks (ANN) that more closely mimic natural neural networks. In addition to neuronal and synaptic state: SNNs incorporate the concept of time into their operating model. The idea is that neurons in the SNN do not transmit information at each propagation cycle (as it happens with typical multi-layer perceptron networks): but rather transmit information only when a membrane potential—an intrinsic quality of the neuron related to its membrane electrical charge—reaches a specific value: called the threshold. When the membrane potential reaches the threshold: the neuron fires: and generates a signal that travels to other neurons which: in turn: increase or decrease their potentials in response to this signal. A neuron model that fires at the moment of threshold crossing is also called a spiking neuron model.Although it was previously believed that the brain encoded information through spike rates: which can be considered': 'original_similarity': 0.8140250252592023: 'slide': 147}","{'chunk_text': 'visual system imposes coordinate frames in order to represent shapes. == Applications == === Image recognition === CNNs are often used in image recognition systems. In 2012: an error rate of 0.23% on the MNIST database was reported. Another paper on using CNN for image classification reported that the learning process was ""surprisingly fast""; in the same paper: the best published results as of 2011 were achieved in the MNIST database and the NORB database. Subsequently: a similar CNN called AlexNet won the ImageNet Large Scale Visual Recognition Challenge 2012. When applied to facial recognition: CNNs achieved a large decrease in error rate. Another paper reported a 97.6% recognition rate on ""5:600 still images of more than 10 subjects"". CNNs were used to assess video quality in an objective way after manual training; the resulting system had a very low root mean square error.The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object classification and detection: with millions of images and hundreds of object classes. In the ILSVRC 2014: a large-scale visual recognition challenge: almost every highly ranked team used CNN as their basic framework. The winner GoogLeNet (the foundation of DeepDream) increased the mean average precision of object detection to 0.439329: and reduced classification error to 0.06656: the best result to date. Its network applied more than 30 layers. That performance of convolutional neural networks on the ImageNet tests was close to that of humans. The best algorithms still struggle with objects that are small or thin: such as a small ant on a stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters: an increasingly common phenomenon with modern digital cameras. By contrast: those kinds of images rarely trouble humans. Humans: however: tend to have trouble with other issues. For example: they are not good at classifying objects into fine-grained categories such as the particular breed of dog or species of bird: whereas convolutional neural networks handle this.In 2015: a many-layered CNN demonstrated the ability to spot faces from a wide range of angles: including upside down: even when partially occluded: with competitive performance. The network was trained on a database of 200:000 images that included faces at various angles and orientations and a further 20 million images without faces. They used batches of 128 images over 50:000 iterations. === Video analysis === Compared to image data domains: there is relatively little work on applying CNNs to video classification. Video is more complex than images since it has another (temporal) dimension. However: some extensions of CNNs into the video domain have been explored. One approach is to treat space and time as equivalent dimensions of the input and perform convolutions in both time and space. Another way is to fuse the features of two convolutional neural networks: one for the spatial and one for the temporal stream. Long short-term memory (LSTM) recurrent units are typically incorporated after the CNN to account for inter-frame or inter-clip dependencies. Unsupervised learning schemes for training spatio-temporal features have been introduced: based on Convolutional Gated Restricted Boltzmann Machines and Independent Subspace Analysis. It\'s Application can be seen in Text-to-Video model. === Natural language processing === CNNs have also been explored for natural language processing. CNN models are effective for various NLP problems and achieved excellent results in semantic parsing: search query retrieval: sentence modeling: classification: prediction and other traditional NLP tasks. Compared to traditional language processing methods such as recurrent neural networks: CNNs can represent different contextual realities of language that do not rely on a series-sequence assumption: while RNNs are better suitable when classical time series modeling is required. === Anomaly Detection === A CNN with 1-D convolutions was used on time series in the frequency domain (spectral residual) by an unsupervised model to detect anomalies in the time domain. === Drug discovery === CNNs have been used in drug discovery. Predicting the interaction between molecules and biological proteins can identify potential treatments. In 2015: Atomwise introduced AtomNet: the first deep learning neural network for structure-based drug design. The system trains directly on 3-dimensional representations of chemical interactions. Similar to how image recognition networks learn to compose smaller: spatially proximate features into larger: complex structures: AtomNet discovers chemical features: such as aromaticity: sp3 carbons: and hydrogen bonding. Subsequently: AtomNet was used to predict novel candidate biomolecules for multiple disease targets: most notably treatments for the Ebola virus and multiple sclerosis. === Checkers game === CNNs have been used in the game of checkers. From 1999 to 2001: Fogel and Chellapilla published papers showing how a convolutional neural network could learn to play checker using co-evolution. The learning\nprocess did not use prior human professional games: but rather focused on a minimal set of information contained in the checkerboard: the location and type of pieces: and the difference in number of pieces between the two sides. Ultimately: the program (Blondie24) was tested on 165 games against players and ranked in the highest 0.4%. It also earned a win against the program Chinook at its ""expert"" level of play. === Go === CNNs have been used in computer Go. In December 2014: Clark and Storkey published a paper showing that a CNN trained by supervised learning from a database of human professional games could outperform GNU Go and win some games against Monte Carlo tree search Fuego 1.1 in a fraction of the time it took Fuego to play. Later it was announced that a large 12-layer convolutional neural network had correctly predicted the professional move in 55% of positions: equalling the accuracy of a 6 dan human player. When the trained convolutional network was used directly to play games of Go: without any search: it beat the traditional search program GNU Go in 97% of games: and matched the performance of the Monte Carlo tree search program Fuego simulating ten thousand playouts (about a million positions) per move.A couple of CNNs for choosing moves to try (""policy': 'original_similarity': 0.7905225525685734: 'slide': 217}",{'chunk_text': 'pretty often. == References == == External links == PINN – repository to implement physics-informed neural network in Python XPINN – repository to implement extended physics-informed neural network (XPINN) in Python PIPN [2]– repository to implement physics-informed PointNet (PIPN) in Python\nQuantum neural networks are computational neural network models which are based': 'original_similarity': 0.7701482288946966: 'slide': 9}
0.7760533000109717,What are Spiking neural networks (SNNs)?,Spiking neural networks (SNNs) are artificial neural networks that closely mimic natural neural networks. They incorporate the concept of time into their operating model and transmit information only when a neuron's membrane potential reaches a specific threshold value. This model represents a significant departure from typical multi-layer perceptron networks that transmit information at each propagation cycle.,"{'chunk_text': 'the field of neurobiology has indicated that high speed processing cannot solely be performed through a rate based scheme. For example humans can perform an image recognition task at rate requiring no more than 10ms of processing time per neuron through the successive layers (going from the retina to the temporal lobe). This time window is too short for a rate based encoding. The precise spike timings in a small set of spiking neurons also has a higher information coding capacity compared with a rate based approach.The most prominent spiking neuron model is the leaky integrate-and-fire model. In the integrate-and-fire model: the momentary activation level (modeled as a differential equation) is normally considered to be the neuron\'s state: with incoming spikes pushing this value higher or lower: until the state eventually either decays or—if the firing threshold is reached—the neuron fires. After firing: the state variable is reset to a lower value. Various decoding methods exist for interpreting the outgoing spike train as a real-value number: relying on either the frequency of spikes (rate-code): the time-to-first-spike after stimulation: or the interval between spikes. == History == Many multi-layer artificial neural networks are fully connected: receiving input from every neuron in the previous layer and signalling every neuron in the subsequent layer. Although these networks have achieved breakthroughs in many fields: they are biologically inaccurate and do not mimic the operation mechanism of neurons in the brain of a living thing. The biologically inspired Hodgkin–Huxley model of a spiking neuron was proposed in 1952. This model describes how action potentials are initiated and propagated. Communication between neurons: which requires the exchange of chemical neurotransmitters in the synaptic gap: is described in various models: such as the integrate-and-fire model: FitzHugh–Nagumo model (1961–1962): and Hindmarsh–Rose model (1984). The leaky integrate-and-fire model (or a derivative) is commonly used as it is easier to compute than the Hodgkin–Huxley model. == Underpinnings == Information in the brain is represented as action potentials (neuron spikes): which may be grouped into spike trains or even coordinated waves of brain activity. A fundamental question of neuroscience is to determine whether neurons communicate by a rate or temporal code. Temporal coding suggests that a single spiking neuron can replace hundreds of hidden units on a sigmoidal neural net.An SNN computes in the continuous rather than the discrete domain. The idea is that neurons may not test for activation in every iteration of propagation (as is the case in a typical multilayer perceptron network): but only when their membrane potentials reach a certain value. When a neuron is activated: it produces a signal that is passed to connected neurons: raising or lowering their membrane potential. In a spiking neural network: a neuron\'s current state is defined as its membrane potential (possibly modeled as a differential equation). An input pulse causes the membrane potential to rise for a period of time and then gradually decline. Encoding schemes have been constructed to interpret these pulse sequences as a number: taking into account both pulse frequency and pulse interval. A neural network model based on pulse generation time can be established. Using the exact time of pulse occurrence: a neural network can employ more information and offer better computing properties. The SNN approach produces a continuous output instead of the binary output of traditional artificial neural networks (ANNs). Pulse trains are not easily interpretable: hence the need for encoding schemes as above. However: a pulse train representation may be more suited for processing spatiotemporal data (or continual real-world sensory data classification). SNNs consider space by connecting neurons only to nearby neurons so that they process input blocks separately (similar to CNN using filters). They consider time by encoding information as pulse trains so as not to lose information in a binary encoding. This avoids the additional complexity of a recurrent neural network (RNN). It turns out that impulse neurons are more powerful computational units than traditional artificial neurons.SNNs are theoretically more powerful than so called ""second-generation networks"" defined in as ""[ANNs] based on computational units that apply activation function with a continuous set of possible output values to a weighted sum (or polynomial) of the inputs; however: SNN training issues and hardware requirements limit their use. Although unsupervised biologically inspired learning methods are available such as Hebbian learning and STDP: no effective supervised training method is suitable for SNNs that can provide better performance than second-generation networks. Spike-based activation of SNNs is not differentiable thus making it hard to develop gradient descent based training methods to perform error backpropagation. SNNs have much larger computational costs for simulating realistic neural models than traditional ANNs.Pulse-coupled neural networks (PCNN) are often confused with SNNs. A PCNN can be seen as a kind of SNN. Currently there are a few challenges when using SNNs that researchers are actively working on. The first challenge concerns the nondifferentiability of the spiking nonlinearity. The expressions for both the forward- and backward-learning methods contain the derivative of the neural activation function which is non-differentiable because neuron\'s output is either 1 when it\nspikes: and 0 otherwise. This all-or-nothing behavior of the binary spiking nonlinearity stops gradients from “flowing” and makes LIF neurons unsuitable for gradient-based optimization. The second challenge concerns the implementation of the optimization algorithm itself. Standard BP can be expensive in terms of computation: memory: and communication and may be poorly suited to the constraints dictated by the hardware that implements it (e.g.: a computer: brain: or neuromorphic device). Regarding the first challenge there are several approached in order to overcome it. A few of them are: resorting to entirely biologically inspired local learning rules for the hidden units translating conventionally trained “rate-based” NNs to SNNs smoothing the network model to be continuously differentiable defining an SG (Surogate Gradient) as a continuous relaxation of the real gradientsIn the development of SNNs: incorporating additional neuron dynamics like Spike Frequency Adaptation (SFA) into neuron models marks a notable advance: enhancing both efficiency and computational power. These neurons stand in between biological complexity and': 'original_similarity': 0.9074617220860108: 'slide': 159}","{'chunk_text': ""in terms of computation: memory: and communication and may be poorly suited to the constraints dictated by the hardware that implements it (e.g.: a computer: brain: or neuromorphic device). Regarding the first challenge there are several approached in order to overcome it. A few of them are: resorting to entirely biologically inspired local learning rules for the hidden units translating conventionally trained “rate-based” NNs to SNNs smoothing the network model to be continuously differentiable defining an SG (Surogate Gradient) as a continuous relaxation of the real gradientsIn the development of SNNs: incorporating additional neuron dynamics like Spike Frequency Adaptation (SFA) into neuron models marks a notable advance: enhancing both efficiency and computational power. These neurons stand in between biological complexity and compuational complexity. Originating from biological insights: SFA offers significant computational benefits by reducing power usage through efficient coding: especially in cases of repetitive or intense stimuli. This adaptation improves signal clarity against background noise and introduces an elementary short-term memory at the neuron level: which in turn: refines the accuracy and efficiency of information processing. Recently: This phenomenon is achieved mostly achieved using Compartmental neuron models. The simpler versions are of neuron models with adaptive thresholds: indirect way of achieving SFA: equips SNNs with improved learning capabilities: even with constrained synaptic plasticity: and elevates computational efficiency. This feature lessens the demand on network layers by decreasing the need for spike processing: thus cutting down on computational load and memory access time—essential aspects of neural computation. Moreover: SNNs utilizing neurons capable of SFA achieve levels of accuracy that rival those of conventional artificial neural networks: including those based on long short-term memory models: while also requiring fewer neurons for comparable computational tasks. This efficiency not only streamlines the computational workflow but also conserves space and energy: offering a pragmatic step forward in the practical application of SNNs for complex computing tasks: all while maintaining a commitment to technical integrity. == Applications == SNNs can in principle apply to the same applications as traditional ANNs. In addition: SNNs can model the central nervous system of biological organisms: such as an insect seeking food without prior knowledge of the environment. Due to their relative realism: they can be used to study the operation of biological neural circuits. Starting with a hypothesis about the topology of a biological neuronal circuit and its function: recordings of this circuit can be compared to the output of the corresponding SNN: evaluating the plausibility of the hypothesis. However: there is a lack of effective training mechanisms for SNNs: which can be inhibitory for some applications: including computer vision tasks. As of 2019 SNNs lag behind ANNs in terms of accuracy: but the gap is decreasing: and has vanished on some tasks.When using SNNs for image based data we need to convert static images into binary spike trains coding. Types of encodings: Temporal coding generates one spike per neuron in which spike latency is inversely proportional to the pixel intensity. Rate coding converts pixel intensity into a spike train where the number of spikes is proportional to the pixel intensity. Direct coding uses a trainable layer to generate float value for each time-step. We have a learnable layer which converts each pixel at certain time step in float number and then threshold is used on the generated floating numbers to see if they will be 1 or 0. Phase coding encodes temporal information into spike patterns based on a global oscillator. Burst coding transmits the burst of spikes in a small-time duration: increasing the reliability of synaptic communication between neurons. == Software == A diverse range of application software can simulate SNNs. This software can be classified according to its uses: === SNN simulation === These simulate complex neural models with a high level of detail and accuracy. Large networks usually require lengthy processing. Candidates include:Brian – developed by Romain Brette and Dan Goodman at the École Normale Supérieure; GENESIS (the GEneral NEural SImulation System) – developed in James Bower's laboratory at Caltech; NEST – developed by the NEST Initiative; NEURON – mainly developed by Michael Hines: John W. Moore and Ted Carnevale in Yale University and Duke University; RAVSim (Runtime Tool) – mainly developed by Sanaullah in Bielefeld University of Applied Sciences and Arts; == Hardware == Future neuromorphic architectures will comprise billions of such nanosynapses: which require a clear understanding of the physical mechanisms responsible for plasticity. Experimental systems based on ferroelectric tunnel junctions have been used to show that STDP can be harnessed from heterogeneous polarization switching. Through combined scanning probe imaging: electrical transport and atomic-scale molecular dynamics: conductance variations can be modelled by nucleation-dominated reversal of domains. Simulations show that arrays of ferroelectric nanosynapses can autonomously learn to recognize patterns in a predictable way: opening the path towards unsupervised learning. Akida is a completely digital event-based neural processing device with 1.2 million artificial neurons and 10 billion artificial synapses developed by BrainChip. Utilizing event-based possessing: it analyzes essential inputs at specific points. Results are stored in the on-chip memory units. Neurogrid is a board that can simulate spiking neural networks directly in hardware. (Stanford University) SpiNNaker (Spiking Neural Network Architecture) uses ARM processors as the building blocks of a massively parallel computing platform based on a six-layer thalamocortical model. (University of Manchester) The SpiNNaker system is based on numerical models running in real time on custom digital multicore chips using the ARM architecture. It provides custom digital chips: each with eighteen cores and a shared local 128 Mbyte RAM: with a total of over 1:000:000 cores. A single chip can simulate 16:000 neurons with eight million plastic synapses running in real time. TrueNorth is a processor that contains 5.4 billion transistors that consumes only 70 milliwatts; most processors in personal computers contain about 1.4\nbillion transistors and require 35 watts or more. IBM refers to the design principle behind TrueNorth as neuromorphic computing. Its primary purpose is pattern recognition. While critics say the chip isn't powerful enough: its supporters point out that this is only"": 'original_similarity': 0.8781531871257079: 'slide': 39}","{'chunk_text': 'trained using skip connections. === Recursive === A recursive neural network is created by applying the same set of weights recursively over a differentiable graph-like structure by traversing the structure in topological order. Such networks are typically also trained by the reverse mode of automatic differentiation. They can process distributed representations of structure: such as logical terms. A special case of recursive neural networks is the RNN whose structure corresponds to a linear chain. Recursive neural networks have been applied to natural language processing. The Recursive Neural Tensor Network uses a tensor-based composition function for all nodes in the tree. === Neural history compressor === The neural history compressor is an unsupervised stack of RNNs. At the input level: it learns to predict its next input from the previous inputs. Only unpredictable inputs of some RNN in the hierarchy become inputs to the next higher level RNN: which therefore recomputes its internal state only rarely. Each higher level RNN thus studies a compressed representation of the information in the RNN below. This is done such that the input sequence can be precisely reconstructed from the representation at the highest level. The system effectively minimizes the description length or the negative logarithm of the probability of the data. Given a lot of learnable predictability in the incoming data sequence: the highest level RNN can use supervised learning to easily classify even deep sequences with long intervals between important events. It is possible to distill the RNN hierarchy into two RNNs: the ""conscious"" chunker (higher level) and the ""subconscious"" automatizer (lower level). Once the chunker has learned to predict and compress inputs that are unpredictable by the automatizer: then the automatizer can be forced in the next learning phase to predict or imitate through additional units the hidden units of the more slowly changing chunker. This makes it easy for the automatizer to learn appropriate: rarely changing memories across long intervals. In turn: this helps the automatizer to make many of its once unpredictable inputs predictable: such that the chunker can focus on the remaining unpredictable events.A generative model partially overcame the vanishing gradient problem of automatic differentiation or backpropagation in neural networks in 1992. In 1993: such a system solved a ""Very Deep Learning"" task that required more than 1000 subsequent layers in an RNN unfolded in time. === Second order RNNs === Second-order RNNs use higher order weights wijk{\\displaystyle w{}_{ijk}} instead of the standard wij{\\displaystyle w{}_{ij}} weights: and states can be a product. This allows a direct mapping to a finite-state machine both in training: stability: and representation. Long short-term memory is an example of this but has no such formal mappings or proof of stability. === Long short-term memory === Long short-term memory (LSTM) is a deep learning system that avoids the vanishing gradient problem. LSTM is normally augmented by recurrent gates called ""forget gates"". LSTM prevents backpropagated errors from vanishing or exploding. Instead: errors can flow backward through unlimited numbers of virtual layers unfolded in space. That is: LSTM can learn tasks that require memories of events that happened thousands or even millions of discrete time steps earlier. Problem-specific LSTM-like topologies can be evolved. LSTM works even given long delays between significant events and can handle signals that mix low and high-frequency components. Many applications use stacks of LSTM RNNs and train them by connectionist temporal classification (CTC) to find an RNN weight matrix that maximizes the probability of the label sequences in a training set: given the corresponding input sequences. CTC achieves both alignment and recognition. LSTM can learn to recognize context-sensitive languages unlike previous models based on hidden Markov models (HMM) and similar concepts. === Gated recurrent unit === Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks introduced in 2014. They are used in the full form and several simplified variants. Their performance on polyphonic music modeling and speech signal modeling was found to be similar to that of long short-term memory. They have fewer parameters than LSTM: as they lack an output gate. === Bi-directional === Bi-directional RNNs use a finite sequence to predict or label each element of the sequence based on the element\'s past and future contexts. This is done by concatenating the outputs of two RNNs: one processing the sequence from left to right: and the other one from right to left. The combined outputs are the predictions of the teacher-given target signals. This technique has been proven to be especially useful when combined with LSTM RNNs. === Continuous-time === A continuous-time recurrent neural network (CTRNN) uses a system of ordinary differential equations to model the effects on a neuron of the incoming inputs. For a neuron i{\\displaystyle i} in the network with activation yi{\\displaystyle y_{i}}: the rate of change of activation is given by: τiy˙i=−yi+∑j=1nwjiσ(yj−Θj)+Ii(t){\\displaystyle \\tau _{i}{\\dot {y}}_{i}=-y_{i}+\\sum _{j=1}^{n}w_{ji}\\sigma (y_{j}-\\Theta _{j})+I_{i}(t)}Where: τi{\\displaystyle \\tau _{i}} : Time constant of postsynaptic node yi{\\displaystyle y_{i}} : Activation of postsynaptic node y˙i{\\displaystyle {\\dot {y}}_{i}} : Rate of change of activation of postsynaptic node wji{\\displaystyle w{}_{ji}} : Weight of connection from pre to postsynaptic node σ(x){\\displaystyle \\sigma (x)} : Sigmoid of x e.g. σ(x)=1/(1+e−x){\\displaystyle \\sigma (x)=1/(1+e^{-x})}. yj{\\displaystyle y_{j}} : Activation of\npresynaptic node Θj{\\displaystyle \\Theta _{j}} : Bias of presynaptic node Ii(t){\\displaystyle I_{i}(t)} : Input (if any) to nodeCTRNNs have been applied to evolutionary robotics where they have been used to address vision: co-operation: and minimal cognitive behaviour.Note that: by the Shannon sampling theorem: discrete-time recurrent neural networks can be viewed as continuous-time recurrent neural networks where the differential equations have transformed into equivalent difference equations. This transformation can be thought of as occurring after the post-synaptic node activation functions yi(t){\\displaystyle y_{i}(t)} have been low-pass filtered but prior to sampling. === Hierarchical recurrent neural network === Hierarchical recurrent neural networks (HRNN) connect their neurons in various ways to decompose hierarchical behavior into useful subprograms. Such hierarchical structures of cognition are present in theories of memory presented by philosopher Henri Bergson: whose philosophical views have inspired hierarchical models.Hierarchical recurrent neural networks are useful in': 'original_similarity': 0.8421051939167279: 'slide': 140}"
0.42913380000391044,What is the leaky integrate-and-fire model in SNNs?,"The leaky integrate-and-fire model is the most prominent spiking neuron model. In this model, the neuron's state is considered to be its momentary activation level. Incoming spikes push this value higher or lower until the state either decays or, if the firing threshold is reached, the neuron fires. After firing, the state variable is reset to a lower value.","{'chunk_text': 'processing time per neuron through the successive layers (going from the retina to the temporal lobe). This time window is too short for a rate based encoding. The precise spike timings in a small set of spiking neurons also has a higher information coding capacity compared with a rate based approach.The most prominent spiking neuron model is the leaky integrate-and-fire model. In the integrate-and-fire model: the momentary activation level (modeled as a differential equation) is normally considered to be the neuron\'s state: with incoming spikes pushing this value higher or lower: until the state eventually either decays or—if the firing threshold is reached—the neuron fires. After firing: the state variable is reset to a lower value. Various decoding methods exist for interpreting the outgoing spike train as a real-value number: relying on either the frequency of spikes (rate-code): the time-to-first-spike after stimulation: or the interval between spikes. == History == Many multi-layer artificial neural networks are fully connected: receiving input from every neuron in the previous layer and signalling every neuron in the subsequent layer. Although these networks have achieved breakthroughs in many fields: they are biologically inaccurate and do not mimic the operation mechanism of neurons in the brain of a living thing. The biologically inspired Hodgkin–Huxley model of a spiking neuron was proposed in 1952. This model describes how action potentials are initiated and propagated. Communication between neurons: which requires the exchange of chemical neurotransmitters in the synaptic gap: is described in various models: such as the integrate-and-fire model: FitzHugh–Nagumo model (1961–1962): and Hindmarsh–Rose model (1984). The leaky integrate-and-fire model (or a derivative) is commonly used as it is easier to compute than the Hodgkin–Huxley model. == Underpinnings == Information in the brain is represented as action potentials (neuron spikes): which may be grouped into spike trains or even coordinated waves of brain activity. A fundamental question of neuroscience is to determine whether neurons communicate by a rate or temporal code. Temporal coding suggests that a single spiking neuron can replace hundreds of hidden units on a sigmoidal neural net.An SNN computes in the continuous rather than the discrete domain. The idea is that neurons may not test for activation in every iteration of propagation (as is the case in a typical multilayer perceptron network): but only when their membrane potentials reach a certain value. When a neuron is activated: it produces a signal that is passed to connected neurons: raising or lowering their membrane potential. In a spiking neural network: a neuron\'s current state is defined as its membrane potential (possibly modeled as a differential equation). An input pulse causes the membrane potential to rise for a period of time and then gradually decline. Encoding schemes have been constructed to interpret these pulse sequences as a number: taking into account both pulse frequency and pulse interval. A neural network model based on pulse generation time can be established. Using the exact time of pulse occurrence: a neural network can employ more information and offer better computing properties. The SNN approach produces a continuous output instead of the binary output of traditional artificial neural networks (ANNs). Pulse trains are not easily interpretable: hence the need for encoding schemes as above. However: a pulse train representation may be more suited for processing spatiotemporal data (or continual real-world sensory data classification). SNNs consider space by connecting neurons only to nearby neurons so that they process input blocks separately (similar to CNN using filters). They consider time by encoding information as pulse trains so as not to lose information in a binary encoding. This avoids the additional complexity of a recurrent neural network (RNN). It turns out that impulse neurons are more powerful computational units than traditional artificial neurons.SNNs are theoretically more powerful than so called ""second-generation networks"" defined in as ""[ANNs] based on computational units that apply activation function with a continuous set of possible output values to a weighted sum (or polynomial) of the inputs; however: SNN training issues and hardware requirements limit their use. Although unsupervised biologically inspired learning methods are available such as Hebbian learning and STDP: no effective supervised training method is suitable for SNNs that can provide better performance than second-generation networks. Spike-based activation of SNNs is not differentiable thus making it hard to develop gradient descent based training methods to perform error backpropagation. SNNs have much larger computational costs for simulating realistic neural models than traditional ANNs.Pulse-coupled neural networks (PCNN) are often confused with SNNs. A PCNN can be seen as a kind of SNN. Currently there are a few challenges when using SNNs that researchers are actively working on. The first challenge concerns the nondifferentiability of the spiking nonlinearity. The expressions for both the forward- and backward-learning methods contain the derivative of the neural activation function which is non-differentiable because neuron\'s output is either 1 when it\nspikes: and 0 otherwise. This all-or-nothing behavior of the binary spiking nonlinearity stops gradients from “flowing” and makes LIF neurons unsuitable for gradient-based optimization. The second challenge concerns the implementation of the optimization algorithm itself. Standard BP can be expensive in terms of computation: memory: and communication and may be poorly suited to the constraints dictated by the hardware that implements it (e.g.: a computer: brain: or neuromorphic device). Regarding the first challenge there are several approached in order to overcome it. A few of them are: resorting to entirely biologically inspired local learning rules for the hidden units translating conventionally trained “rate-based” NNs to SNNs smoothing the network model to be continuously differentiable defining an SG (Surogate Gradient) as a continuous relaxation of the real gradientsIn the development of SNNs: incorporating additional neuron dynamics like Spike Frequency Adaptation (SFA) into neuron models marks a notable advance: enhancing both efficiency and computational power. These neurons stand in between biological complexity and compuational complexity. Originating from biological insights: SFA offers significant computational benefits by reducing power usage through efficient coding: especially in cases of repetitive or intense stimuli. This adaptation improves signal clarity against background noise and introduces': 'original_similarity': 0.8532393029741429: 'slide': 195}","{'chunk_text': ""conventionally trained “rate-based” NNs to SNNs smoothing the network model to be continuously differentiable defining an SG (Surogate Gradient) as a continuous relaxation of the real gradientsIn the development of SNNs: incorporating additional neuron dynamics like Spike Frequency Adaptation (SFA) into neuron models marks a notable advance: enhancing both efficiency and computational power. These neurons stand in between biological complexity and compuational complexity. Originating from biological insights: SFA offers significant computational benefits by reducing power usage through efficient coding: especially in cases of repetitive or intense stimuli. This adaptation improves signal clarity against background noise and introduces an elementary short-term memory at the neuron level: which in turn: refines the accuracy and efficiency of information processing. Recently: This phenomenon is achieved mostly achieved using Compartmental neuron models. The simpler versions are of neuron models with adaptive thresholds: indirect way of achieving SFA: equips SNNs with improved learning capabilities: even with constrained synaptic plasticity: and elevates computational efficiency. This feature lessens the demand on network layers by decreasing the need for spike processing: thus cutting down on computational load and memory access time—essential aspects of neural computation. Moreover: SNNs utilizing neurons capable of SFA achieve levels of accuracy that rival those of conventional artificial neural networks: including those based on long short-term memory models: while also requiring fewer neurons for comparable computational tasks. This efficiency not only streamlines the computational workflow but also conserves space and energy: offering a pragmatic step forward in the practical application of SNNs for complex computing tasks: all while maintaining a commitment to technical integrity. == Applications == SNNs can in principle apply to the same applications as traditional ANNs. In addition: SNNs can model the central nervous system of biological organisms: such as an insect seeking food without prior knowledge of the environment. Due to their relative realism: they can be used to study the operation of biological neural circuits. Starting with a hypothesis about the topology of a biological neuronal circuit and its function: recordings of this circuit can be compared to the output of the corresponding SNN: evaluating the plausibility of the hypothesis. However: there is a lack of effective training mechanisms for SNNs: which can be inhibitory for some applications: including computer vision tasks. As of 2019 SNNs lag behind ANNs in terms of accuracy: but the gap is decreasing: and has vanished on some tasks.When using SNNs for image based data we need to convert static images into binary spike trains coding. Types of encodings: Temporal coding generates one spike per neuron in which spike latency is inversely proportional to the pixel intensity. Rate coding converts pixel intensity into a spike train where the number of spikes is proportional to the pixel intensity. Direct coding uses a trainable layer to generate float value for each time-step. We have a learnable layer which converts each pixel at certain time step in float number and then threshold is used on the generated floating numbers to see if they will be 1 or 0. Phase coding encodes temporal information into spike patterns based on a global oscillator. Burst coding transmits the burst of spikes in a small-time duration: increasing the reliability of synaptic communication between neurons. == Software == A diverse range of application software can simulate SNNs. This software can be classified according to its uses: === SNN simulation === These simulate complex neural models with a high level of detail and accuracy. Large networks usually require lengthy processing. Candidates include:Brian – developed by Romain Brette and Dan Goodman at the École Normale Supérieure; GENESIS (the GEneral NEural SImulation System) – developed in James Bower's laboratory at Caltech; NEST – developed by the NEST Initiative; NEURON – mainly developed by Michael Hines: John W. Moore and Ted Carnevale in Yale University and Duke University; RAVSim (Runtime Tool) – mainly developed by Sanaullah in Bielefeld University of Applied Sciences and Arts; == Hardware == Future neuromorphic architectures will comprise billions of such nanosynapses: which require a clear understanding of the physical mechanisms responsible for plasticity. Experimental systems based on ferroelectric tunnel junctions have been used to show that STDP can be harnessed from heterogeneous polarization switching. Through combined scanning probe imaging: electrical transport and atomic-scale molecular dynamics: conductance variations can be modelled by nucleation-dominated reversal of domains. Simulations show that arrays of ferroelectric nanosynapses can autonomously learn to recognize patterns in a predictable way: opening the path towards unsupervised learning. Akida is a completely digital event-based neural processing device with 1.2 million artificial neurons and 10 billion artificial synapses developed by BrainChip. Utilizing event-based possessing: it analyzes essential inputs at specific points. Results are stored in the on-chip memory units. Neurogrid is a board that can simulate spiking neural networks directly in hardware. (Stanford University) SpiNNaker (Spiking Neural Network Architecture) uses ARM processors as the building blocks of a massively parallel computing platform based on a six-layer thalamocortical model. (University of Manchester) The SpiNNaker system is based on numerical models running in real time on custom digital multicore chips using the ARM architecture. It provides custom digital chips: each with eighteen cores and a shared local 128 Mbyte RAM: with a total of over 1:000:000 cores. A single chip can simulate 16:000 neurons with eight million plastic synapses running in real time. TrueNorth is a processor that contains 5.4 billion transistors that consumes only 70 milliwatts; most processors in personal computers contain about 1.4\nbillion transistors and require 35 watts or more. IBM refers to the design principle behind TrueNorth as neuromorphic computing. Its primary purpose is pattern recognition. While critics say the chip isn't powerful enough: its supporters point out that this is only the first generation: and the capabilities of improved iterations will become clear. (IBM) == Benchmarks == Classification capabilities of spiking networks trained according to unsupervised learning methods have been tested on the common benchmark datasets: such as: Iris: Wisconsin Breast Cancer or Statlog Landsat dataset. Various approaches to information encoding and network design have been used. For example: a 2-layer"": 'original_similarity': 0.8312544860834103: 'slide': 99}","{'chunk_text': 'the structure in topological order. Such networks are typically also trained by the reverse mode of automatic differentiation. They can process distributed representations of structure: such as logical terms. A special case of recursive neural networks is the RNN whose structure corresponds to a linear chain. Recursive neural networks have been applied to natural language processing. The Recursive Neural Tensor Network uses a tensor-based composition function for all nodes in the tree. === Neural history compressor === The neural history compressor is an unsupervised stack of RNNs. At the input level: it learns to predict its next input from the previous inputs. Only unpredictable inputs of some RNN in the hierarchy become inputs to the next higher level RNN: which therefore recomputes its internal state only rarely. Each higher level RNN thus studies a compressed representation of the information in the RNN below. This is done such that the input sequence can be precisely reconstructed from the representation at the highest level. The system effectively minimizes the description length or the negative logarithm of the probability of the data. Given a lot of learnable predictability in the incoming data sequence: the highest level RNN can use supervised learning to easily classify even deep sequences with long intervals between important events. It is possible to distill the RNN hierarchy into two RNNs: the ""conscious"" chunker (higher level) and the ""subconscious"" automatizer (lower level). Once the chunker has learned to predict and compress inputs that are unpredictable by the automatizer: then the automatizer can be forced in the next learning phase to predict or imitate through additional units the hidden units of the more slowly changing chunker. This makes it easy for the automatizer to learn appropriate: rarely changing memories across long intervals. In turn: this helps the automatizer to make many of its once unpredictable inputs predictable: such that the chunker can focus on the remaining unpredictable events.A generative model partially overcame the vanishing gradient problem of automatic differentiation or backpropagation in neural networks in 1992. In 1993: such a system solved a ""Very Deep Learning"" task that required more than 1000 subsequent layers in an RNN unfolded in time. === Second order RNNs === Second-order RNNs use higher order weights wijk{\\displaystyle w{}_{ijk}} instead of the standard wij{\\displaystyle w{}_{ij}} weights: and states can be a product. This allows a direct mapping to a finite-state machine both in training: stability: and representation. Long short-term memory is an example of this but has no such formal mappings or proof of stability. === Long short-term memory === Long short-term memory (LSTM) is a deep learning system that avoids the vanishing gradient problem. LSTM is normally augmented by recurrent gates called ""forget gates"". LSTM prevents backpropagated errors from vanishing or exploding. Instead: errors can flow backward through unlimited numbers of virtual layers unfolded in space. That is: LSTM can learn tasks that require memories of events that happened thousands or even millions of discrete time steps earlier. Problem-specific LSTM-like topologies can be evolved. LSTM works even given long delays between significant events and can handle signals that mix low and high-frequency components. Many applications use stacks of LSTM RNNs and train them by connectionist temporal classification (CTC) to find an RNN weight matrix that maximizes the probability of the label sequences in a training set: given the corresponding input sequences. CTC achieves both alignment and recognition. LSTM can learn to recognize context-sensitive languages unlike previous models based on hidden Markov models (HMM) and similar concepts. === Gated recurrent unit === Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks introduced in 2014. They are used in the full form and several simplified variants. Their performance on polyphonic music modeling and speech signal modeling was found to be similar to that of long short-term memory. They have fewer parameters than LSTM: as they lack an output gate. === Bi-directional === Bi-directional RNNs use a finite sequence to predict or label each element of the sequence based on the element\'s past and future contexts. This is done by concatenating the outputs of two RNNs: one processing the sequence from left to right: and the other one from right to left. The combined outputs are the predictions of the teacher-given target signals. This technique has been proven to be especially useful when combined with LSTM RNNs. === Continuous-time === A continuous-time recurrent neural network (CTRNN) uses a system of ordinary differential equations to model the effects on a neuron of the incoming inputs. For a neuron i{\\displaystyle i} in the network with activation yi{\\displaystyle y_{i}}: the rate of change of activation is given by: τiy˙i=−yi+∑j=1nwjiσ(yj−Θj)+Ii(t){\\displaystyle \\tau _{i}{\\dot {y}}_{i}=-y_{i}+\\sum _{j=1}^{n}w_{ji}\\sigma (y_{j}-\\Theta _{j})+I_{i}(t)}Where: τi{\\displaystyle \\tau _{i}} : Time constant of postsynaptic node yi{\\displaystyle y_{i}} : Activation of postsynaptic node y˙i{\\displaystyle {\\dot {y}}_{i}} : Rate of change of activation of postsynaptic node wji{\\displaystyle w{}_{ji}} : Weight of connection from pre to postsynaptic node σ(x){\\displaystyle \\sigma (x)} : Sigmoid of x e.g. σ(x)=1/(1+e−x){\\displaystyle \\sigma (x)=1/(1+e^{-x})}. yj{\\displaystyle y_{j}} : Activation of\npresynaptic node Θj{\\displaystyle \\Theta _{j}} : Bias of presynaptic node Ii(t){\\displaystyle I_{i}(t)} : Input (if any) to nodeCTRNNs have been applied to evolutionary robotics where they have been used to address vision: co-operation: and minimal cognitive behaviour.Note that: by the Shannon sampling theorem: discrete-time recurrent neural networks can be viewed as continuous-time recurrent neural networks where the differential equations have transformed into equivalent difference equations. This transformation can be thought of as occurring after the post-synaptic node activation functions yi(t){\\displaystyle y_{i}(t)} have been low-pass filtered but prior to sampling. === Hierarchical recurrent neural network === Hierarchical recurrent neural networks (HRNN) connect their neurons in various ways to decompose hierarchical behavior into useful subprograms. Such hierarchical structures of cognition are present in theories of memory presented by philosopher Henri Bergson: whose philosophical views have inspired hierarchical models.Hierarchical recurrent neural networks are useful in forecasting: helping to predict disaggregated inflation components of the consumer price index (CPI). The HRNN model leverages information from higher levels in the CPI hierarchy to enhance lower-level': 'original_similarity': 0.8241195092279593: 'slide': 168}"
0.4432182000018656,What are some of the challenges in using SNNs?,"Some challenges in using SNNs include the non-differentiability of the spiking nonlinearity and the implementation of the optimization algorithm. The all-or-nothing behavior of the binary spiking nonlinearity makes neurons unsuitable for gradient-based optimization, and standard Backpropagation can be computationally expensive. ","{'chunk_text': ""relaxation of the real gradientsIn the development of SNNs: incorporating additional neuron dynamics like Spike Frequency Adaptation (SFA) into neuron models marks a notable advance: enhancing both efficiency and computational power. These neurons stand in between biological complexity and compuational complexity. Originating from biological insights: SFA offers significant computational benefits by reducing power usage through efficient coding: especially in cases of repetitive or intense stimuli. This adaptation improves signal clarity against background noise and introduces an elementary short-term memory at the neuron level: which in turn: refines the accuracy and efficiency of information processing. Recently: This phenomenon is achieved mostly achieved using Compartmental neuron models. The simpler versions are of neuron models with adaptive thresholds: indirect way of achieving SFA: equips SNNs with improved learning capabilities: even with constrained synaptic plasticity: and elevates computational efficiency. This feature lessens the demand on network layers by decreasing the need for spike processing: thus cutting down on computational load and memory access time—essential aspects of neural computation. Moreover: SNNs utilizing neurons capable of SFA achieve levels of accuracy that rival those of conventional artificial neural networks: including those based on long short-term memory models: while also requiring fewer neurons for comparable computational tasks. This efficiency not only streamlines the computational workflow but also conserves space and energy: offering a pragmatic step forward in the practical application of SNNs for complex computing tasks: all while maintaining a commitment to technical integrity. == Applications == SNNs can in principle apply to the same applications as traditional ANNs. In addition: SNNs can model the central nervous system of biological organisms: such as an insect seeking food without prior knowledge of the environment. Due to their relative realism: they can be used to study the operation of biological neural circuits. Starting with a hypothesis about the topology of a biological neuronal circuit and its function: recordings of this circuit can be compared to the output of the corresponding SNN: evaluating the plausibility of the hypothesis. However: there is a lack of effective training mechanisms for SNNs: which can be inhibitory for some applications: including computer vision tasks. As of 2019 SNNs lag behind ANNs in terms of accuracy: but the gap is decreasing: and has vanished on some tasks.When using SNNs for image based data we need to convert static images into binary spike trains coding. Types of encodings: Temporal coding generates one spike per neuron in which spike latency is inversely proportional to the pixel intensity. Rate coding converts pixel intensity into a spike train where the number of spikes is proportional to the pixel intensity. Direct coding uses a trainable layer to generate float value for each time-step. We have a learnable layer which converts each pixel at certain time step in float number and then threshold is used on the generated floating numbers to see if they will be 1 or 0. Phase coding encodes temporal information into spike patterns based on a global oscillator. Burst coding transmits the burst of spikes in a small-time duration: increasing the reliability of synaptic communication between neurons. == Software == A diverse range of application software can simulate SNNs. This software can be classified according to its uses: === SNN simulation === These simulate complex neural models with a high level of detail and accuracy. Large networks usually require lengthy processing. Candidates include:Brian – developed by Romain Brette and Dan Goodman at the École Normale Supérieure; GENESIS (the GEneral NEural SImulation System) – developed in James Bower's laboratory at Caltech; NEST – developed by the NEST Initiative; NEURON – mainly developed by Michael Hines: John W. Moore and Ted Carnevale in Yale University and Duke University; RAVSim (Runtime Tool) – mainly developed by Sanaullah in Bielefeld University of Applied Sciences and Arts; == Hardware == Future neuromorphic architectures will comprise billions of such nanosynapses: which require a clear understanding of the physical mechanisms responsible for plasticity. Experimental systems based on ferroelectric tunnel junctions have been used to show that STDP can be harnessed from heterogeneous polarization switching. Through combined scanning probe imaging: electrical transport and atomic-scale molecular dynamics: conductance variations can be modelled by nucleation-dominated reversal of domains. Simulations show that arrays of ferroelectric nanosynapses can autonomously learn to recognize patterns in a predictable way: opening the path towards unsupervised learning. Akida is a completely digital event-based neural processing device with 1.2 million artificial neurons and 10 billion artificial synapses developed by BrainChip. Utilizing event-based possessing: it analyzes essential inputs at specific points. Results are stored in the on-chip memory units. Neurogrid is a board that can simulate spiking neural networks directly in hardware. (Stanford University) SpiNNaker (Spiking Neural Network Architecture) uses ARM processors as the building blocks of a massively parallel computing platform based on a six-layer thalamocortical model. (University of Manchester) The SpiNNaker system is based on numerical models running in real time on custom digital multicore chips using the ARM architecture. It provides custom digital chips: each with eighteen cores and a shared local 128 Mbyte RAM: with a total of over 1:000:000 cores. A single chip can simulate 16:000 neurons with eight million plastic synapses running in real time. TrueNorth is a processor that contains 5.4 billion transistors that consumes only 70 milliwatts; most processors in personal computers contain about 1.4\nbillion transistors and require 35 watts or more. IBM refers to the design principle behind TrueNorth as neuromorphic computing. Its primary purpose is pattern recognition. While critics say the chip isn't powerful enough: its supporters point out that this is only the first generation: and the capabilities of improved iterations will become clear. (IBM) == Benchmarks == Classification capabilities of spiking networks trained according to unsupervised learning methods have been tested on the common benchmark datasets: such as: Iris: Wisconsin Breast Cancer or Statlog Landsat dataset. Various approaches to information encoding and network design have been used. For example: a 2-layer feedforward network for data clustering and classification. Based on the idea proposed in Hopfield (1995) the authors implemented models of local receptive"": 'original_similarity': 0.8408981306040068: 'slide': 121}","{'chunk_text': 'spiking neurons also has a higher information coding capacity compared with a rate based approach.The most prominent spiking neuron model is the leaky integrate-and-fire model. In the integrate-and-fire model: the momentary activation level (modeled as a differential equation) is normally considered to be the neuron\'s state: with incoming spikes pushing this value higher or lower: until the state eventually either decays or—if the firing threshold is reached—the neuron fires. After firing: the state variable is reset to a lower value. Various decoding methods exist for interpreting the outgoing spike train as a real-value number: relying on either the frequency of spikes (rate-code): the time-to-first-spike after stimulation: or the interval between spikes. == History == Many multi-layer artificial neural networks are fully connected: receiving input from every neuron in the previous layer and signalling every neuron in the subsequent layer. Although these networks have achieved breakthroughs in many fields: they are biologically inaccurate and do not mimic the operation mechanism of neurons in the brain of a living thing. The biologically inspired Hodgkin–Huxley model of a spiking neuron was proposed in 1952. This model describes how action potentials are initiated and propagated. Communication between neurons: which requires the exchange of chemical neurotransmitters in the synaptic gap: is described in various models: such as the integrate-and-fire model: FitzHugh–Nagumo model (1961–1962): and Hindmarsh–Rose model (1984). The leaky integrate-and-fire model (or a derivative) is commonly used as it is easier to compute than the Hodgkin–Huxley model. == Underpinnings == Information in the brain is represented as action potentials (neuron spikes): which may be grouped into spike trains or even coordinated waves of brain activity. A fundamental question of neuroscience is to determine whether neurons communicate by a rate or temporal code. Temporal coding suggests that a single spiking neuron can replace hundreds of hidden units on a sigmoidal neural net.An SNN computes in the continuous rather than the discrete domain. The idea is that neurons may not test for activation in every iteration of propagation (as is the case in a typical multilayer perceptron network): but only when their membrane potentials reach a certain value. When a neuron is activated: it produces a signal that is passed to connected neurons: raising or lowering their membrane potential. In a spiking neural network: a neuron\'s current state is defined as its membrane potential (possibly modeled as a differential equation). An input pulse causes the membrane potential to rise for a period of time and then gradually decline. Encoding schemes have been constructed to interpret these pulse sequences as a number: taking into account both pulse frequency and pulse interval. A neural network model based on pulse generation time can be established. Using the exact time of pulse occurrence: a neural network can employ more information and offer better computing properties. The SNN approach produces a continuous output instead of the binary output of traditional artificial neural networks (ANNs). Pulse trains are not easily interpretable: hence the need for encoding schemes as above. However: a pulse train representation may be more suited for processing spatiotemporal data (or continual real-world sensory data classification). SNNs consider space by connecting neurons only to nearby neurons so that they process input blocks separately (similar to CNN using filters). They consider time by encoding information as pulse trains so as not to lose information in a binary encoding. This avoids the additional complexity of a recurrent neural network (RNN). It turns out that impulse neurons are more powerful computational units than traditional artificial neurons.SNNs are theoretically more powerful than so called ""second-generation networks"" defined in as ""[ANNs] based on computational units that apply activation function with a continuous set of possible output values to a weighted sum (or polynomial) of the inputs; however: SNN training issues and hardware requirements limit their use. Although unsupervised biologically inspired learning methods are available such as Hebbian learning and STDP: no effective supervised training method is suitable for SNNs that can provide better performance than second-generation networks. Spike-based activation of SNNs is not differentiable thus making it hard to develop gradient descent based training methods to perform error backpropagation. SNNs have much larger computational costs for simulating realistic neural models than traditional ANNs.Pulse-coupled neural networks (PCNN) are often confused with SNNs. A PCNN can be seen as a kind of SNN. Currently there are a few challenges when using SNNs that researchers are actively working on. The first challenge concerns the nondifferentiability of the spiking nonlinearity. The expressions for both the forward- and backward-learning methods contain the derivative of the neural activation function which is non-differentiable because neuron\'s output is either 1 when it\nspikes: and 0 otherwise. This all-or-nothing behavior of the binary spiking nonlinearity stops gradients from “flowing” and makes LIF neurons unsuitable for gradient-based optimization. The second challenge concerns the implementation of the optimization algorithm itself. Standard BP can be expensive in terms of computation: memory: and communication and may be poorly suited to the constraints dictated by the hardware that implements it (e.g.: a computer: brain: or neuromorphic device). Regarding the first challenge there are several approached in order to overcome it. A few of them are: resorting to entirely biologically inspired local learning rules for the hidden units translating conventionally trained “rate-based” NNs to SNNs smoothing the network model to be continuously differentiable defining an SG (Surogate Gradient) as a continuous relaxation of the real gradientsIn the development of SNNs: incorporating additional neuron dynamics like Spike Frequency Adaptation (SFA) into neuron models marks a notable advance: enhancing both efficiency and computational power. These neurons stand in between biological complexity and compuational complexity. Originating from biological insights: SFA offers significant computational benefits by reducing power usage through efficient coding: especially in cases of repetitive or intense stimuli. This adaptation improves signal clarity against background noise and introduces an elementary short-term memory at the neuron level: which in turn: refines the accuracy and efficiency of information processing. Recently: This phenomenon is achieved mostly achieved using Compartmental neuron models. The simpler versions are of neuron': 'original_similarity': 0.8276000234139623: 'slide': 231}","{'chunk_text': 'speed of deep-learning systems by about 100 times. In particular: GPUs are well-suited for the matrix/vector computations involved in machine learning. GPUs speed up training algorithms by orders of magnitude: reducing running times from weeks to days. Further: specialized hardware and algorithm optimizations can be used for efficient processing of deep learning models. === Deep learning revolution === In the late 2000s: deep learning started to outperform other methods in machine learning competitions. In 2009: a long short-term memory trained by connectionist temporal classification (Alex Graves: Santiago Fernández: Faustino Gomez: and Jürgen Schmidhuber: 2006) was the first RNN to win pattern recognition contests: winning three competitions in connected handwriting recognition. Google later used CTC-trained LSTM for speech recognition on the smartphone.Significant impacts in image or object recognition were felt from 2011 to 2012. Although CNNs trained by backpropagation had been around for decades: and GPU implementations of NNs for years: including CNNs: faster implementations of CNNs on GPUs were needed to progress on computer vision. In 2011: the DanNet by Dan Ciresan: Ueli Meier: Jonathan Masci: Luca Maria Gambardella: and Jürgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest: outperforming traditional methods by a factor of 3. Also in 2011: DanNet won the ICDAR Chinese handwriting contest: and in May 2012: it won the ISBI image segmentation contest. Until 2011: CNNs did not play a major role at computer vision conferences: but in June 2012: a paper by Ciresan et al. at the leading conference CVPR showed how max-pooling CNNs on GPU can dramatically improve many vision benchmark records. In September 2012: DanNet also won the ICPR contest on analysis of large medical images for cancer detection: and in the following year also the MICCAI Grand Challenge on the same topic. In October 2012: the similar AlexNet by Alex Krizhevsky: Ilya Sutskever: and Geoffrey Hinton won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. The VGG-16 network by Karen Simonyan and Andrew Zisserman further reduced the error rate and won the ImageNet 2014 competition: following a similar trend in large-scale speech recognition. Image classification was then extended to the more challenging task of generating descriptions (captions) for images: often as a combination of CNNs and LSTMs.In 2012: a team led by George E. Dahl won the ""Merck Molecular Activity Challenge"" using multi-task deep neural networks to predict the biomolecular target of one drug. In 2014: Sepp Hochreiter\'s group used deep learning to detect off-target and toxic effects of environmental chemicals in nutrients: household products and drugs and won the ""Tox21 Data Challenge"" of NIH: FDA and NCATS.In 2016: Roger Parloff mentioned a ""deep learning revolution"" that has transformed the AI industry.In March 2019: Yoshua Bengio: Geoffrey Hinton and Yann LeCun were awarded the Turing Award for conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing. == Neural networks == Artificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve their ability) to do tasks by considering examples: generally without task-specific programming. For example: in image recognition: they might learn to identify images that contain cats by analyzing example images that have been manually labeled as ""cat"" or ""no cat"" and using the analytic results to identify cats in other images. They have found most use in applications difficult to express with a traditional computer algorithm using rule-based programming. An ANN is based on a collection of connected units called artificial neurons: (analogous to biological neurons in a biological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it. Neurons may have state: generally represented by real numbers: typically between 0 and 1. Neurons and synapses may also have a weight that varies as learning proceeds: which can increase or decrease the strength of the signal that it sends downstream. Typically: neurons are organized in layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first (input): to the last (output) layer: possibly after traversing the layers multiple times. The original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time: attention focused on matching specific mental abilities: leading to deviations from biology such as backpropagation: or passing information in the reverse direction and adjusting the network to reflect that information. Neural networks have been used on a variety of tasks: including computer vision: speech recognition: machine translation: social network filtering: playing board and video games and medical diagnosis. As of 2017: neural networks typically have a few thousand to a few million units and millions of connections. Despite\nthis number being several order of magnitude less than the number of neurons on a human brain: these networks can perform many tasks at a level beyond that of humans (e.g.: recognizing faces: or playing ""Go""). === Deep neural networks === A deep neural network (DNN) is an artificial neural network with multiple layers between the input and output layers. There are different types of neural networks but they always consist of the same components: neurons: synapses: weights: biases: and functions. These components as a whole function in a way that mimics functions of the human brain: and can be trained like any other ML algorithm.For example: a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed. The user can review the results and select which probabilities the network should display (above a certain threshold: etc.) and return the proposed label. Each mathematical manipulation as such is considered a layer: and complex DNN have many layers: hence the name ""deep"" networks. DNNs can model complex non-linear relationships. DNN architectures generate compositional models where the object is': 'original_similarity': 0.8194635720251388: 'slide': 193}"
0.5180318999919109,What are some applications of SNNs?,"SNNs can apply to the same applications as traditional artificial neural networks (ANNs), and in addition, they can model the central nervous system of biological organisms. However, a lack of effective training mechanisms for SNNs can be inhibitory for some applications, like computer vision tasks. ","{'chunk_text': ""artificial intelligence program developed by Google's DeepMind for solving the protein folding problem in biology. AlphaFold achieved first place in several CASP competitions. === Social networks === Social networks are a major application domain for GNNs due to their natural representation as social graphs. GNNs are used to develop recommender systems based on both social relations and item relations. === Combinatorial optimization === GNNs are used as fundamental building blocks for several combinatorial optimization algorithms. Examples include computing shortest paths or Eulerian circuits for a given graph: deriving chip placements superior or competitive to handcrafted human solutions: and improving expert-designed branching rules in branch and bound. === Cyber security === When viewed as a graph: a network of computers can be analyzed with GNNs for anomaly detection. Anomalies within provenance graphs often correlate to malicious activity within the network. GNNs have been used to identify these anomalies on individual nodes and within paths to detect malicious processes: or on the edge level to detect lateral movement. == References == == External links == https://distill.pub/2021/gnn-intro/\nArtificial neural networks (ANNs) are models created using machine learning to perform a number of tasks. Their creation was inspired by neural circuitry. While some of the computational implementations ANNs relate to earlier discoveries in mathematics: the first implementation of ANNs was by psychologist Frank Rosenblatt: who developed the"": 'original_similarity': 0.8392624563938249: 'slide': 47}","{'chunk_text': '(PER): have been summarized since 1991. The debut of DNNs for speaker recognition in the late 1990s and speech recognition around 2009-2011 and of LSTM around 2003–2007: accelerated progress in eight major areas: Scale-up/out and accelerated DNN training and decoding Sequence discriminative training Feature processing by deep models with solid understanding of the underlying mechanisms Adaptation of DNNs and related deep models Multi-task and transfer learning by DNNs and related deep models CNNs and how to design them to best exploit domain knowledge of speech RNN and its rich LSTM variants Other types of deep models including tensor-based models and integrated deep generative/discriminative models.All major commercial speech recognition systems (e.g.: Microsoft Cortana: Xbox: Skype Translator: Amazon Alexa: Google Now: Apple Siri: Baidu and iFlyTek voice search: and a range of Nuance speech products: etc.) are based on deep learning. === Image recognition === A common evaluation set for image classification is the MNIST database data set. MNIST is composed of handwritten digits and includes 60:000 training examples and 10:000 test examples. As with TIMIT: its small size lets users test multiple configurations. A comprehensive list of results on this set is available.Deep learning-based image recognition has become ""superhuman"": producing more accurate results than human contestants. This first occurred in 2011 in recognition of traffic signs: and in 2014: with recognition of human faces.Deep learning-trained vehicles now interpret 360° camera views. Another example is Facial Dysmorphology Novel Analysis (FDNA) used to analyze cases of human malformation connected to a large database of genetic syndromes. === Visual art processing === Closely related to the progress that has been made in image recognition is the increasing application of deep learning techniques to various visual art tasks. DNNs have proven themselves capable: for example: of identifying the style period of a given painting Neural Style Transfer – capturing the style of a given artwork and applying it in a visually pleasing manner to an arbitrary photograph or video generating striking imagery based on random visual input fields. === Natural language processing === Neural networks have been used for implementing language models since the early 2000s. LSTM helped to improve machine translation and language modeling.Other key techniques in this field are negative sampling and word embedding. Word embedding: such as word2vec: can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a vector space. Using word embedding as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar. A compositional vector grammar can be thought of as probabilistic context free grammar (PCFG) implemented by an RNN. Recursive auto-encoders built atop word embeddings can assess sentence similarity and detect paraphrasing. Deep neural architectures provide the best results for constituency parsing: sentiment analysis: information retrieval: spoken language understanding: machine translation: contextual entity linking: writing style recognition: named-entity recognition (token classification): text classification: and others.Recent developments generalize word embedding to sentence embedding. Google Translate (GT) uses a large end-to-end long short-term memory (LSTM) network. Google Neural Machine Translation (GNMT) uses an example-based machine translation method in which the system ""learns from millions of examples"". It translates ""whole sentences at a time: rather than pieces"". Google Translate supports over one hundred languages. The network encodes the ""semantics of the sentence rather than simply memorizing phrase-to-phrase translations"". GT uses English as an intermediate between most language pairs. === Drug discovery and toxicology === A large percentage of candidate drugs fail to win regulatory approval. These failures are caused by insufficient efficacy (on-target effect): undesired interactions (off-target effects): or unanticipated toxic effects. Research has explored use of deep learning to predict the biomolecular targets: off-targets: and toxic effects of environmental chemicals in nutrients: household products and drugs.AtomNet is a deep learning system for structure-based rational drug design. AtomNet was used to predict novel candidate biomolecules for disease targets such as the Ebola virus and multiple sclerosis.In 2017 graph neural networks were used for the first time to predict various properties of molecules in a large toxicology data set. In 2019: generative neural networks were used to produce molecules that were validated experimentally all the way into mice. === Customer relationship management === Deep reinforcement learning has been used to approximate the value of possible direct marketing actions: defined in terms of RFM variables. The estimated value function was shown to have a natural interpretation as customer lifetime value. === Recommendation systems === Recommendation systems have used deep learning to extract meaningful features for a latent factor model for content-based music and journal recommendations. Multi-view deep learning has been applied for learning user preferences from multiple domains. The model uses a hybrid collaborative and content-based approach and enhances recommendations in multiple tasks. === Bioinformatics === An autoencoder ANN was used in bioinformatics: to predict gene ontology annotations and gene-function relationships.In medical informatics: deep learning was used to predict sleep quality based on data\nfrom wearables and predictions of health complications from electronic health record data. === Deep Neural Network Estimations === Deep neural networks can be used to estimate the entropy of a stochastic process and called Neural Joint Entropy Estimator (NJEE). Such an estimation provides insights on the effects of input random variables on an independent random variable. Practically: the DNN is trained as a classifier that maps an input vector or matrix X to an output probability distribution over the possible classes of random variable Y: given input X. For example: in image classification tasks: the NJEE maps a vector of pixels\' color values to probabilities over possible image classes. In practice: the probability distribution of Y is obtained by a Softmax layer with number of nodes that is equal to the alphabet size of Y. NJEE uses continuously differentiable activation functions: such that the conditions for the universal approximation theorem holds. It is shown that this method provides a strongly consistent estimator': 'original_similarity': 0.8372593657031393: 'slide': 160}","{'chunk_text': ""real gradientsIn the development of SNNs: incorporating additional neuron dynamics like Spike Frequency Adaptation (SFA) into neuron models marks a notable advance: enhancing both efficiency and computational power. These neurons stand in between biological complexity and compuational complexity. Originating from biological insights: SFA offers significant computational benefits by reducing power usage through efficient coding: especially in cases of repetitive or intense stimuli. This adaptation improves signal clarity against background noise and introduces an elementary short-term memory at the neuron level: which in turn: refines the accuracy and efficiency of information processing. Recently: This phenomenon is achieved mostly achieved using Compartmental neuron models. The simpler versions are of neuron models with adaptive thresholds: indirect way of achieving SFA: equips SNNs with improved learning capabilities: even with constrained synaptic plasticity: and elevates computational efficiency. This feature lessens the demand on network layers by decreasing the need for spike processing: thus cutting down on computational load and memory access time—essential aspects of neural computation. Moreover: SNNs utilizing neurons capable of SFA achieve levels of accuracy that rival those of conventional artificial neural networks: including those based on long short-term memory models: while also requiring fewer neurons for comparable computational tasks. This efficiency not only streamlines the computational workflow but also conserves space and energy: offering a pragmatic step forward in the practical application of SNNs for complex computing tasks: all while maintaining a commitment to technical integrity. == Applications == SNNs can in principle apply to the same applications as traditional ANNs. In addition: SNNs can model the central nervous system of biological organisms: such as an insect seeking food without prior knowledge of the environment. Due to their relative realism: they can be used to study the operation of biological neural circuits. Starting with a hypothesis about the topology of a biological neuronal circuit and its function: recordings of this circuit can be compared to the output of the corresponding SNN: evaluating the plausibility of the hypothesis. However: there is a lack of effective training mechanisms for SNNs: which can be inhibitory for some applications: including computer vision tasks. As of 2019 SNNs lag behind ANNs in terms of accuracy: but the gap is decreasing: and has vanished on some tasks.When using SNNs for image based data we need to convert static images into binary spike trains coding. Types of encodings: Temporal coding generates one spike per neuron in which spike latency is inversely proportional to the pixel intensity. Rate coding converts pixel intensity into a spike train where the number of spikes is proportional to the pixel intensity. Direct coding uses a trainable layer to generate float value for each time-step. We have a learnable layer which converts each pixel at certain time step in float number and then threshold is used on the generated floating numbers to see if they will be 1 or 0. Phase coding encodes temporal information into spike patterns based on a global oscillator. Burst coding transmits the burst of spikes in a small-time duration: increasing the reliability of synaptic communication between neurons. == Software == A diverse range of application software can simulate SNNs. This software can be classified according to its uses: === SNN simulation === These simulate complex neural models with a high level of detail and accuracy. Large networks usually require lengthy processing. Candidates include:Brian – developed by Romain Brette and Dan Goodman at the École Normale Supérieure; GENESIS (the GEneral NEural SImulation System) – developed in James Bower's laboratory at Caltech; NEST – developed by the NEST Initiative; NEURON – mainly developed by Michael Hines: John W. Moore and Ted Carnevale in Yale University and Duke University; RAVSim (Runtime Tool) – mainly developed by Sanaullah in Bielefeld University of Applied Sciences and Arts; == Hardware == Future neuromorphic architectures will comprise billions of such nanosynapses: which require a clear understanding of the physical mechanisms responsible for plasticity. Experimental systems based on ferroelectric tunnel junctions have been used to show that STDP can be harnessed from heterogeneous polarization switching. Through combined scanning probe imaging: electrical transport and atomic-scale molecular dynamics: conductance variations can be modelled by nucleation-dominated reversal of domains. Simulations show that arrays of ferroelectric nanosynapses can autonomously learn to recognize patterns in a predictable way: opening the path towards unsupervised learning. Akida is a completely digital event-based neural processing device with 1.2 million artificial neurons and 10 billion artificial synapses developed by BrainChip. Utilizing event-based possessing: it analyzes essential inputs at specific points. Results are stored in the on-chip memory units. Neurogrid is a board that can simulate spiking neural networks directly in hardware. (Stanford University) SpiNNaker (Spiking Neural Network Architecture) uses ARM processors as the building blocks of a massively parallel computing platform based on a six-layer thalamocortical model. (University of Manchester) The SpiNNaker system is based on numerical models running in real time on custom digital multicore chips using the ARM architecture. It provides custom digital chips: each with eighteen cores and a shared local 128 Mbyte RAM: with a total of over 1:000:000 cores. A single chip can simulate 16:000 neurons with eight million plastic synapses running in real time. TrueNorth is a processor that contains 5.4 billion transistors that consumes only 70 milliwatts; most processors in personal computers contain about 1.4\nbillion transistors and require 35 watts or more. IBM refers to the design principle behind TrueNorth as neuromorphic computing. Its primary purpose is pattern recognition. While critics say the chip isn't powerful enough: its supporters point out that this is only the first generation: and the capabilities of improved iterations will become clear. (IBM) == Benchmarks == Classification capabilities of spiking networks trained according to unsupervised learning methods have been tested on the common benchmark datasets: such as: Iris: Wisconsin Breast Cancer or Statlog Landsat dataset. Various approaches to information encoding and network design have been used. For example: a 2-layer feedforward network for data clustering and classification. Based on the idea proposed in Hopfield (1995) the authors implemented models of local receptive fields combining the"": 'original_similarity': 0.8358353543844147: 'slide': 124}"
0.3918229999835603,What developments have improved the efficiency and computational power of SNNs?,"Incorporating additional neuron dynamics like Spike Frequency Adaptation (SFA) into neuron models is one such development. SFA offers computational benefits by reducing power usage and increasing coding efficiency, especially in repetitive or intense stimuli. This adaptation also enhances signal clarity and introduces short-term memory at the neuron level, refining information processing accuracy and efficiency.","{'chunk_text': 'the ""big bang"" of deep learning: ""as deep-learning neural networks were trained with Nvidia graphics processing units (GPUs)"". That year: Andrew Ng determined that GPUs could increase the speed of deep-learning systems by about 100 times. In particular: GPUs are well-suited for the matrix/vector computations involved in machine learning. GPUs speed up training algorithms by orders of magnitude: reducing running times from weeks to days. Further: specialized hardware and algorithm optimizations can be used for efficient processing of deep learning models. === Deep learning revolution === In the late 2000s: deep learning started to outperform other methods in machine learning competitions. In 2009: a long short-term memory trained by connectionist temporal classification (Alex Graves: Santiago Fernández: Faustino Gomez: and Jürgen Schmidhuber: 2006) was the first RNN to win pattern recognition contests: winning three competitions in connected handwriting recognition. Google later used CTC-trained LSTM for speech recognition on the smartphone.Significant impacts in image or object recognition were felt from 2011 to 2012. Although CNNs trained by backpropagation had been around for decades: and GPU implementations of NNs for years: including CNNs: faster implementations of CNNs on GPUs were needed to progress on computer vision. In 2011: the DanNet by Dan Ciresan: Ueli Meier: Jonathan Masci: Luca Maria Gambardella: and Jürgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest: outperforming traditional methods by a factor of 3. Also in 2011: DanNet won the ICDAR Chinese handwriting contest: and in May 2012: it won the ISBI image segmentation contest. Until 2011: CNNs did not play a major role at computer vision conferences: but in June 2012: a paper by Ciresan et al. at the leading conference CVPR showed how max-pooling CNNs on GPU can dramatically improve many vision benchmark records. In September 2012: DanNet also won the ICPR contest on analysis of large medical images for cancer detection: and in the following year also the MICCAI Grand Challenge on the same topic. In October 2012: the similar AlexNet by Alex Krizhevsky: Ilya Sutskever: and Geoffrey Hinton won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. The VGG-16 network by Karen Simonyan and Andrew Zisserman further reduced the error rate and won the ImageNet 2014 competition: following a similar trend in large-scale speech recognition. Image classification was then extended to the more challenging task of generating descriptions (captions) for images: often as a combination of CNNs and LSTMs.In 2012: a team led by George E. Dahl won the ""Merck Molecular Activity Challenge"" using multi-task deep neural networks to predict the biomolecular target of one drug. In 2014: Sepp Hochreiter\'s group used deep learning to detect off-target and toxic effects of environmental chemicals in nutrients: household products and drugs and won the ""Tox21 Data Challenge"" of NIH: FDA and NCATS.In 2016: Roger Parloff mentioned a ""deep learning revolution"" that has transformed the AI industry.In March 2019: Yoshua Bengio: Geoffrey Hinton and Yann LeCun were awarded the Turing Award for conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing. == Neural networks == Artificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve their ability) to do tasks by considering examples: generally without task-specific programming. For example: in image recognition: they might learn to identify images that contain cats by analyzing example images that have been manually labeled as ""cat"" or ""no cat"" and using the analytic results to identify cats in other images. They have found most use in applications difficult to express with a traditional computer algorithm using rule-based programming. An ANN is based on a collection of connected units called artificial neurons: (analogous to biological neurons in a biological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it. Neurons may have state: generally represented by real numbers: typically between 0 and 1. Neurons and synapses may also have a weight that varies as learning proceeds: which can increase or decrease the strength of the signal that it sends downstream. Typically: neurons are organized in layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first (input): to the last (output) layer: possibly after traversing the layers multiple times. The original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time: attention focused on matching specific mental abilities: leading to deviations from biology such as backpropagation: or passing information in the reverse direction and adjusting the network to reflect that information. Neural networks have been used on a variety of tasks: including computer vision: speech recognition: machine translation: social network filtering: playing board and video games and medical diagnosis. As of 2017: neural networks typically have a few thousand to a few million units and millions of connections. Despite\nthis number being several order of magnitude less than the number of neurons on a human brain: these networks can perform many tasks at a level beyond that of humans (e.g.: recognizing faces: or playing ""Go""). === Deep neural networks === A deep neural network (DNN) is an artificial neural network with multiple layers between the input and output layers. There are different types of neural networks but they always consist of the same components: neurons: synapses: weights: biases: and functions. These components as a whole function in a way that mimics functions of the human brain: and can be trained like any other ML algorithm.For example: a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed. The user can review the results and select which probabilities the network should display (above a certain threshold: etc.) and return the proposed label. Each mathematical manipulation as such is considered': 'original_similarity': 0.8452432617441664: 'slide': 165}","{'chunk_text': 'Ciresan and colleagues (2010) showed that despite the vanishing gradient problem: GPUs make backpropagation feasible for many-layered feedforward neural networks. Between 2009 and 2012: ANNs began winning prizes in image recognition contests: approaching human level performance on various tasks: initially in pattern recognition and handwriting recognition. For example: the bi-directional and multi-dimensional long short-term memory (LSTM) of Graves et al. won three competitions in connected handwriting recognition in 2009 without any prior knowledge about the three languages to be learned.Ciresan and colleagues built the first pattern recognizers to achieve human-competitive/superhuman performance on benchmarks such as traffic sign recognition (IJCNN 2012). Radial basis function and wavelet networks were introduced in 2013. These can be shown to offer best approximation properties and have been applied in nonlinear system identification and classification applications.In 2014: the adversarial network principle was used in a generative adversarial network (GAN) by Ian Goodfellow et al. Here the adversarial network (discriminator) outputs a value between 1 and 0 depending on the likelihood of the first network\'s (generator) output is in a given set. This can be used to create realistic deepfakes. Excellent image quality is achieved by Nvidia\'s StyleGAN (2018) based on the Progressive GAN by Tero Karras: Timo Aila: Samuli Laine: and Jaakko Lehtinen. Here the GAN generator is grown from small to large scale in a pyramidal fashion. In 2015: Rupesh Kumar Srivastava: Klaus Greff: and Schmidhuber used the LSTM principle to create the Highway network: a feedforward neural network with hundreds of layers: much deeper than previous networks. 7 months later: Kaiming He: Xiangyu Zhang; Shaoqing Ren: and Jian Sun won the ImageNet 2015 competition with an open-gated or gateless Highway network variant called Residual neural network.In 2017: Ashish Vaswani et al. introduced the modern Transformer architecture in their paper ""Attention Is All You Need."" It combines this with a softmax operator and a projection matrix. Transformers have increasingly become the model of choice for natural language processing. Many modern large language models such as ChatGPT: GPT-4: and BERT use it. Transformers are also increasingly being used in computer vision.Ramenzanpour et al. showed in 2020 that analytical and computational techniques derived from statistical physics of disordered systems can be extended to large-scale problems: including machine learning: e.g.: to analyze the weight space of deep neural networks. == Models == ANNs began as an attempt to exploit the architecture of the human brain to perform tasks that conventional algorithms had little success with. They soon reoriented towards improving empirical results: abandoning attempts to remain true to their biological precursors. ANNs have the ability to learn and model non-linearities and complex relationships. This is achieved by neurons being connected in various patterns: allowing the output of some neurons to become the input of others. The network forms a directed: weighted graph.An artificial neural network consists of simulated neurons. Each neuron is connected to other nodes via links like a biological axon-synapse-dendrite connection. All the nodes connected by links take in some data and use it to perform specific operations and tasks on the data. Each link has a weight: determining the strength of one node\'s influence on another: allowing weights to choose the signal between neurons. === Artificial neurons === ANNs are composed of artificial neurons which are conceptually derived from biological neurons. Each artificial neuron has inputs and produces a single output which can be sent to multiple other neurons. The inputs can be the feature values of a sample of external data: such as images or documents: or they can be the outputs of other neurons. The outputs of the final output neurons of the neural net accomplish the task: such as recognizing an object in an image. To find the output of the neuron we take the weighted sum of all the inputs: weighted by the weights of the connections from the inputs to the neuron. We add a bias term to this sum. This weighted sum is sometimes called the activation. This weighted sum is then passed through a (usually nonlinear) activation function to produce the output. The initial inputs are external data: such as images and documents. The ultimate outputs accomplish the task: such as recognizing an object in an image. === Organization === The neurons are typically organized into multiple layers: especially in deep learning. Neurons of one layer connect only to neurons of the immediately preceding and immediately following layers. The layer that receives external data is the input layer. The layer that produces the ultimate result is the output layer. In between them are zero or more hidden layers. Single layer and unlayered networks are also used. Between two layers: multiple connection patterns are possible. They can be \'fully connected\': with every neuron in one layer connecting to every neuron in the next layer. They can be pooling: where a group of neurons in one layer connects to a single neuron in the next layer: thereby reducing the number of neurons in that layer. Neurons with only such connections form a directed acyclic graph and are known as feedforward networks. Alternatively: networks that allow connections between neurons in the same or previous layers are known as recurrent networks. === Hyperparameter === A hyperparameter is a constant parameter whose value is set before the learning process begins. The values of parameters are derived via learning. Examples of hyperparameters include learning rate: the number of hidden layers and batch size. The values of some hyperparameters can be dependent on those of other\nhyperparameters. For example: the size of some layers can depend on the overall number of layers. === Learning === Learning is the adaptation of the network to better handle a task by considering sample observations. Learning involves adjusting the weights (and optional thresholds) of the network to improve the accuracy of the result. This is done by minimizing the observed errors. Learning is complete when examining additional observations does not usefully reduce the error rate. Even after learning: the error rate typically does not reach 0. If after learning: the': 'original_similarity': 0.8337039343306025: 'slide': 88}","{'chunk_text': ""between biological complexity and compuational complexity. Originating from biological insights: SFA offers significant computational benefits by reducing power usage through efficient coding: especially in cases of repetitive or intense stimuli. This adaptation improves signal clarity against background noise and introduces an elementary short-term memory at the neuron level: which in turn: refines the accuracy and efficiency of information processing. Recently: This phenomenon is achieved mostly achieved using Compartmental neuron models. The simpler versions are of neuron models with adaptive thresholds: indirect way of achieving SFA: equips SNNs with improved learning capabilities: even with constrained synaptic plasticity: and elevates computational efficiency. This feature lessens the demand on network layers by decreasing the need for spike processing: thus cutting down on computational load and memory access time—essential aspects of neural computation. Moreover: SNNs utilizing neurons capable of SFA achieve levels of accuracy that rival those of conventional artificial neural networks: including those based on long short-term memory models: while also requiring fewer neurons for comparable computational tasks. This efficiency not only streamlines the computational workflow but also conserves space and energy: offering a pragmatic step forward in the practical application of SNNs for complex computing tasks: all while maintaining a commitment to technical integrity. == Applications == SNNs can in principle apply to the same applications as traditional ANNs. In addition: SNNs can model the central nervous system of biological organisms: such as an insect seeking food without prior knowledge of the environment. Due to their relative realism: they can be used to study the operation of biological neural circuits. Starting with a hypothesis about the topology of a biological neuronal circuit and its function: recordings of this circuit can be compared to the output of the corresponding SNN: evaluating the plausibility of the hypothesis. However: there is a lack of effective training mechanisms for SNNs: which can be inhibitory for some applications: including computer vision tasks. As of 2019 SNNs lag behind ANNs in terms of accuracy: but the gap is decreasing: and has vanished on some tasks.When using SNNs for image based data we need to convert static images into binary spike trains coding. Types of encodings: Temporal coding generates one spike per neuron in which spike latency is inversely proportional to the pixel intensity. Rate coding converts pixel intensity into a spike train where the number of spikes is proportional to the pixel intensity. Direct coding uses a trainable layer to generate float value for each time-step. We have a learnable layer which converts each pixel at certain time step in float number and then threshold is used on the generated floating numbers to see if they will be 1 or 0. Phase coding encodes temporal information into spike patterns based on a global oscillator. Burst coding transmits the burst of spikes in a small-time duration: increasing the reliability of synaptic communication between neurons. == Software == A diverse range of application software can simulate SNNs. This software can be classified according to its uses: === SNN simulation === These simulate complex neural models with a high level of detail and accuracy. Large networks usually require lengthy processing. Candidates include:Brian – developed by Romain Brette and Dan Goodman at the École Normale Supérieure; GENESIS (the GEneral NEural SImulation System) – developed in James Bower's laboratory at Caltech; NEST – developed by the NEST Initiative; NEURON – mainly developed by Michael Hines: John W. Moore and Ted Carnevale in Yale University and Duke University; RAVSim (Runtime Tool) – mainly developed by Sanaullah in Bielefeld University of Applied Sciences and Arts; == Hardware == Future neuromorphic architectures will comprise billions of such nanosynapses: which require a clear understanding of the physical mechanisms responsible for plasticity. Experimental systems based on ferroelectric tunnel junctions have been used to show that STDP can be harnessed from heterogeneous polarization switching. Through combined scanning probe imaging: electrical transport and atomic-scale molecular dynamics: conductance variations can be modelled by nucleation-dominated reversal of domains. Simulations show that arrays of ferroelectric nanosynapses can autonomously learn to recognize patterns in a predictable way: opening the path towards unsupervised learning. Akida is a completely digital event-based neural processing device with 1.2 million artificial neurons and 10 billion artificial synapses developed by BrainChip. Utilizing event-based possessing: it analyzes essential inputs at specific points. Results are stored in the on-chip memory units. Neurogrid is a board that can simulate spiking neural networks directly in hardware. (Stanford University) SpiNNaker (Spiking Neural Network Architecture) uses ARM processors as the building blocks of a massively parallel computing platform based on a six-layer thalamocortical model. (University of Manchester) The SpiNNaker system is based on numerical models running in real time on custom digital multicore chips using the ARM architecture. It provides custom digital chips: each with eighteen cores and a shared local 128 Mbyte RAM: with a total of over 1:000:000 cores. A single chip can simulate 16:000 neurons with eight million plastic synapses running in real time. TrueNorth is a processor that contains 5.4 billion transistors that consumes only 70 milliwatts; most processors in personal computers contain about 1.4\nbillion transistors and require 35 watts or more. IBM refers to the design principle behind TrueNorth as neuromorphic computing. Its primary purpose is pattern recognition. While critics say the chip isn't powerful enough: its supporters point out that this is only the first generation: and the capabilities of improved iterations will become clear. (IBM) == Benchmarks == Classification capabilities of spiking networks trained according to unsupervised learning methods have been tested on the common benchmark datasets: such as: Iris: Wisconsin Breast Cancer or Statlog Landsat dataset. Various approaches to information encoding and network design have been used. For example: a 2-layer feedforward network for data clustering and classification. Based on the idea proposed in Hopfield (1995) the authors implemented models of local receptive fields combining the properties of radial basis functions (RBF) and spiking neurons to convert input signals (classified data) having a floating-point representation into a spiking representation. == See also == == References =="": 'original_similarity': 0.8334569344164751: 'slide': 156}"
0.5615130999940448,What is the Unified Modeling Language (UML)? ,"The Unified Modeling Language (UML) is a general-purpose visual modeling language that provides a standard way to visualize the design of a system. It offers a standard notation for many types of diagrams which can be roughly categorized into behavior diagrams, interaction diagrams, and structure diagrams.","{'chunk_text': 'the object-oriented programming methods developed in the late 1980s and early 1990s. The timeline (see image) shows the highlights of the history of object-oriented modeling methods and notation. It is originally based on the notations of the Booch method: the object-modeling technique (OMT) and object-oriented software engineering (OOSE): which it has integrated into a single language.Rational Software Corporation hired James Rumbaugh from General Electric in 1994 and after that the company became the source for two of the most popular object-oriented modeling approaches of the day: Rumbaugh\'s object-modeling technique (OMT) and Grady Booch\'s method. They were soon assisted in their efforts by Ivar Jacobson: the creator of the object-oriented software engineering (OOSE) method: who joined them at Rational in 1995. === UML 1.x === Under the technical leadership of those three (Rumbaugh: Jacobson and Booch): a consortium called the UML Partners was organized in 1996 to complete the Unified Modeling Language (UML) specification: and propose it to the Object Management Group (OMG) for standardization. The partnership also contained additional interested parties (for example HP: DEC: IBM and Microsoft). The UML Partners\' UML 1.0 draft was proposed to the OMG in January 1997 by the consortium. During the same month the UML Partners formed a group: designed to define the exact meaning of language constructs: chaired by Cris Kobryn and administered by Ed Eykholt: to finalize the specification and integrate it with other standardization efforts. The result of this work: UML 1.1: was submitted to the OMG in August 1997 and adopted by the OMG in November 1997.After the first release a task force was formed to improve the language: which released several minor revisions: 1.3: 1.4: and 1.5.The standards it produced (as well as the original standard) have been noted as being ambiguous and inconsistent. ==== Cardinality notation ==== As with database Chen: Bachman: and ISO ER diagrams: class models are specified to use ""look-across"" cardinalities: even though several authors (Merise: Elmasri & Navathe amongst others) prefer same-side or ""look-here"" for roles and both minimum and maximum cardinalities. Recent researchers (Feinerer: Dullea et al.) have shown that the ""look-across"" technique used by UML and ER diagrams is less effective and less coherent when applied to n-ary relationships of order strictly greater than 2. Feinerer says: ""Problems arise if we operate under the look-across semantics as used for UML associations. Hartmann investigates this situation and shows how and why different transformations fail."": and: ""As we will see on the next few pages: the look-across interpretation introduces several difficulties which prevent the extension of simple mechanisms from binary to n-ary associations."" === UML 2 === UML 2.0 major revision replaced version 1.5 in 2005: which was developed with an enlarged consortium to improve the language further to reflect new experience on usage of its features.Although UML 2.1 was never released as a formal specification: versions 2.1.1 and 2.1.2 appeared in 2007: followed by UML 2.2 in February 2009. UML 2.3 was formally released in May 2010. UML 2.4.1 was formally released in August 2011. UML 2.5 was released in October 2012 as an ""In progress"" version and was officially released in June 2015. Formal version 2.5.1 was adopted in December 2017.There are four parts to the UML 2.x specification: The Superstructure that defines the notation and semantics for diagrams and their model elements The Infrastructure that defines the core metamodel on which the Superstructure is based The Object Constraint Language (OCL) for defining rules for model elements The UML Diagram Interchange that defines how UML 2 diagram layouts are exchangedUntil UML 2.4.1: the latest versions of these standards were: UML Superstructure version 2.4.1 UML Infrastructure version 2.4.1 OCL version 2.3.1 UML Diagram Interchange version 1.0.Since version 2.5: the UML Specification has been simplified (without Superstructure and Infrastructure): and the latest versions of these standards are now: UML Specification 2.5.1 OCL version 2.4It continues to be updated and improved by the revision task force: who resolve any issues with the language. == Design == UML offers a way to visualize a system\'s architectural blueprints in a diagram: including elements such as: any activities (jobs); individual components of the system; and how they can interact with other software components; how the system will run; how entities interact with others (components and interfaces); external user interface.Although originally intended for object-oriented design documentation: UML has been extended to a larger set of design documentation (as listed above): and been found useful in many contexts. === Software development methods === UML is not a development method by itself; however: it was designed to be compatible with the leading object-oriented software development methods of its time: for example OMT: Booch method: Objectory and especially RUP that it was originally intended to be used with when work began at Rational Software. === Modeling === It is important to distinguish between the UML\nmodel and the set of diagrams of a system. A diagram is a partial graphic representation of a system\'s model. The set of diagrams need not completely cover the model and deleting a diagram does not change the model. The model may also contain documentation that drives the model elements and diagrams (such as written use cases). UML diagrams represent two different views of a system model: Static (or structural) view: emphasizes the static structure of the system using objects: attributes: operations and relationships. It includes class diagrams and composite structure diagrams. Dynamic (or behavioral) view: emphasizes the dynamic behavior of the system by showing collaborations among objects and changes to the internal states of objects. This view includes sequence diagrams: activity diagrams and state machine diagrams.UML models can be exchanged among UML tools by using the XML Metadata Interchange (XMI) format. In UML: one of the key tools for behavior modeling is the use-case model: caused by OOSE. Use cases are a way of specifying required usages of a system. Typically: they are used to capture the requirements of a system: that is: what a system is supposed to do. == Diagrams == UML 2 has many': 'original_similarity': 0.9117334139041919: 'slide': 196}","{'chunk_text': 'It includes class diagrams and composite structure diagrams. Dynamic (or behavioral) view: emphasizes the dynamic behavior of the system by showing collaborations among objects and changes to the internal states of objects. This view includes sequence diagrams: activity diagrams and state machine diagrams.UML models can be exchanged among UML tools by using the XML Metadata Interchange (XMI) format. In UML: one of the key tools for behavior modeling is the use-case model: caused by OOSE. Use cases are a way of specifying required usages of a system. Typically: they are used to capture the requirements of a system: that is: what a system is supposed to do. == Diagrams == UML 2 has many types of diagrams: which are divided into two categories. Some types represent structural information: and the rest represent general types of behavior: including a few that represent different aspects of interactions. These diagrams can be categorized hierarchically as shown in the following class diagram: These diagrams may all contain comments or notes explaining usage: constraint: or intent. === Structure diagrams === Structure diagrams represent the static aspects of the system. It emphasizes the things that must be present in the system being modeled. Since structure diagrams represent the structure: they are used extensively in documenting the software architecture of software systems. For example: the component diagram describes how a software system is split up into components and shows the dependencies among these components. === Behavior diagrams === Behavior diagrams represent the dynamic aspect of the system. It emphasizes what must happen in the system being modeled. Since behavior diagrams illustrate the behavior of a system: they are used extensively to describe the functionality of software systems. As an example: the activity diagram describes the business and operational step-by-step activities of the components in a system. ==== Interaction diagrams ==== Interaction diagrams: a subset of behavior diagrams: emphasize the flow of control and data among the things in the system being modeled. For example: the sequence diagram shows how objects communicate with each other regarding a sequence of messages. == Metamodeling == The Object Management Group (OMG) has developed a metamodeling architecture to define the UML: called the Meta-Object Facility. MOF is designed as a four-layered architecture: as shown in the image at right. It provides a meta-meta model at the top: called the M3 layer. This M3-model is the language used by Meta-Object Facility to build metamodels: called M2-models. The most prominent example of a Layer 2 Meta-Object Facility model is the UML metamodel: which describes the UML itself. These M2-models describe elements of the M1-layer: and thus M1-models. These would be: for example: models written in UML. The last layer is the M0-layer or data layer. It is used to describe runtime instances of the system.The meta-model can be extended using a mechanism called stereotyping. This has been criticized as being insufficient/untenable by Brian Henderson-Sellers and Cesar Gonzalez-Perez in ""Uses and Abuses of the Stereotype Mechanism in UML 1.x and 2.0"". == Adoption == Back in 2013 UML has been marketed by OMG for many contexts: but aimed primarily at software development with limited success.It has been treated: at times: as a design silver bullet: which leads to problems. UML misuse includes overuse (designing every part of the system with it: which is unnecessary) and assuming that novices can design with it.It is considered a large language: with many constructs. Some people (including Jacobson) feel that UML\'s size hinders learning (and therefore using) it.MS Visual Studio dropped support for UML in 2016 due to lack of usage.According to Google Trends UML has been on steady decline since 2004. == See also == Applications of UML Business Process Model and Notation (BPMN) C4 model Department of Defense Architecture Framework DOT (graph description language) List of Unified Modeling Language tools MODAF Model-based testing Model-driven engineering Object-oriented role analysis and modeling Process Specification Language Systems Modeling Language (SysML) == References == == Further reading == Ambler: Scott William (2004). The Object Primer: Agile Model Driven Development with UML 2. Cambridge University Press. ISBN 0-521-54018-6. Archived from the original on 31 January 2010. Retrieved 29 April 2006. Chonoles: Michael Jesse; James A. Schardt (2003). UML 2 for Dummies. Wiley Publishing. ISBN 0-7645-2614-6. Fowler: Martin (2004). UML Distilled: A Brief Guide to the Standard Object Modeling Language (3rd ed.). Addison-Wesley. ISBN 0-321-19368-7. Jacobson: Ivar; Grady Booch; James Rumbaugh (1998). The Unified Software Development Process. Addison Wesley Longman. ISBN 0-201-57169-2. Martin: Robert Cecil (2003). UML for Java Programmers. Prentice Hall. ISBN 0-13-142848-9. Noran: Ovidiu S. ""Business Modelling: UML vs. IDEF"" (PDF). Retrieved 14 November 2022. Horst Kargl. ""Interactive UML Metamodel with additional Examples"". Penker: Magnus; Hans-Erik Eriksson (2000). Business Modeling with UML. John Wiley & Sons. ISBN 0-471-29551-5. Douglass: Bruce Powel. ""Bruce Douglass: Real-Time Agile Systems and Software Development"" (web). Retrieved 1 January 2019. Douglass: Bruce (2014). Real-Time UML Workshop 2nd Edition. Newnes. ISBN 978-0-471-29551-8. Douglass: Bruce (2004). Real-Time UML 3rd Edition. Newnes. ISBN 978-0321160768. Douglass: Bruce (2002). Real-Time Design Patterns. Addison-Wesley Professional. ISBN 978-0201699562. Douglass: Bruce (2009). Real-Time Agility. Addison-Wesley Professional. ISBN 978-0321545497. Douglass: Bruce (2010). Design Patterns for Embedded Systems in C. Newnes. ISBN 978-1856177078. == External links == Official website Current Version Specification\nmodel and the set of diagrams of a system. A diagram is a partial graphic representation of a system\'s model. The set of diagrams need not completely cover the model and deleting a diagram does not change the model. The model may also contain documentation that drives the model elements and diagrams (such as written use cases). UML diagrams represent two different views of a system model: Static (or structural) view: emphasizes the static structure of the system using objects: attributes: operations and relationships. It': 'original_similarity': 0.864502690010909: 'slide': 83}","{'chunk_text': 'management and systems engineering: Behavior Trees are a formal: graphical modeling language used primarily in systems and software engineering. Commonly used to unambiguously represent the hundreds or even thousands of natural language requirements that are typically used to express the stakeholder needs for a large-scale software-integrated system. Business Process Modeling Notation (BPMN: and the XML form BPML) is an example of a Process Modeling language. C-K theory consists of a modeling language for design processes. DRAKON is a general-purpose algorithmic modeling language for specifying software-intensive systems: a schematic representation of an algorithm or a stepwise process: and a family of programming languages. EXPRESS and EXPRESS-G (ISO 10303-11) is an international standard general-purpose data modeling language. Extended Enterprise Modeling Language (EEML) is commonly used for business process modeling across a number of layers. Flowchart is a schematic representation of an algorithm or a stepwise process. Fundamental Modeling Concepts (FMC) modeling language for software-intensive systems. IDEF is a family of modeling languages: which include IDEF0 for functional modeling: IDEF1X for information modeling: IDEF3 for business process modeling: IDEF4 for Object-Oriented Design and IDEF5 for modeling ontologies. Jackson Structured Programming (JSP) is a method for structured programming based on correspondences between data stream structure and program structure. LePUS3 is an object-oriented visual Design Description Language and a formal specification language that is suitable primarily for modeling large object-oriented (Java: C++: C#) programs and design patterns. Lifecycle Modeling Language is an open-standard language for systems engineering that supports the full system lifecycle: conceptual: utilization: support and retirement stages. Object-Role Modeling (ORM) in the field of software engineering is a method for conceptual modeling: and can be used as a tool for information and rules analysis. Petri nets use variations on exactly one diagramming technique and topology: namely the bipartite graph. The simplicity of its basic user interface easily enabled extensive tool support over the years: particularly in the areas of model checking: graphically oriented simulation: and software verification. Southbeach Notation is a visual modeling language used to describe situations in terms of agents that are considered useful or harmful from the modeler\'s perspective. The notation shows how the agents interact with each other and whether this interaction improves or worsens the situation. Specification and Description Language (SDL) is a specification language targeted at the unambiguous specification and description of the behavior of reactive and distributed systems. SysML is a Domain-Specific Modeling language for systems engineering that is defined as a UML profile (customization). Unified Modeling Language (UML) is a general-purpose modeling language that is an industry standard for specifying software-intensive systems. UML 2.0: the current version: supports thirteen different diagram techniques: and has widespread tool support. Service-oriented modeling framework (SOMF) is a holistic language for designing enterprise and application level architecture models in the space of enterprise architecture: virtualization: service-oriented architecture (SOA): cloud computing: and more. Architecture description language (ADL) is a language used to describe and represent the systems architecture of a system. Architecture Analysis & Design Language (AADL) is a modeling language that supports early and repeated analyses of a system\'s architecture with respect to performance-critical properties through an extendable notation: a tool framework: and precisely defined semantics.Examples of graphical modeling languages in other fields of science. EAST-ADL is a Domain-Specific Modeling language dedicated to automotive system design. Energy Systems Language (ESL): a language that aims to model ecological energetics & global economics. IEC 61499 defines Domain-Specific Modeling language dedicated to distribute industrial process measurement and control systems. === Textual types === Information models can also be expressed in formalized natural languages: such as Gellish. Gellish has natural language variants such as Gellish Formal English and Gellish Formal Dutch (Gellish Formeel Nederlands): etc. Gellish Formal English is an information representation language or semantic modeling language that is defined in the Gellish English Dictionary-Taxonomy: which has the form of a Taxonomy-Ontology (similarly for Dutch). Gellish Formal English is not only suitable to express knowledge: requirements and dictionaries: taxonomies and ontologies: but also information about individual things. All that information is expressed in one language and therefore it can all be integrated: independent of the question whether it is stored in central or distributed or in federated databases. Information models in Gellish Formal English consists of collections of Gellish Formal English expressions: that use natural language terms and formalized phrases. For example: a geographic information model might consist of a number of Gellish Formal English expressions: such as: - the Eiffel tower <is located in> Paris - Paris <is classified as a> city whereas information requirements and knowledge can be expressed for example as follows: - tower <shall be located in a> geographical area - city <is a kind of> geographical area Such Gellish Formal English expressions use names of concepts (such as ""city"") and phrases\nthat represent relation types (such as ⟨is located in⟩ and ⟨is classified as a⟩) that should be selected from the Gellish English Dictionary-Taxonomy (or of your own domain dictionary). The Gellish English Dictionary-Taxonomy enables the creation of semantically rich information models: because the dictionary contains more than 600 standard relation types and contains definitions of more than 40000 concepts. An information model in Gellish can express facts or make statements: queries and answers. === More specific types === In the field of computer science recently more specific types of modeling languages have emerged. ==== Algebraic ==== Algebraic Modeling Languages (AML) are high-level programming languages for describing and solving high complexity problems for large scale mathematical computation (i.e. large scale optimization type problems). One particular advantage of AMLs like AIMMS: AMPL: GAMS: Gekko: Mosel: OPL and OptimJ is the similarity of its syntax to the mathematical notation of optimization problems. This allows for a very concise and readable definition of problems in the domain of optimization: which is supported by certain language elements like sets: indices: algebraic expressions: powerful sparse index and data handling variables: constraints with arbitrary names. The algebraic formulation of a model does not contain any hints how to process it. ==== Behavioral ==== Behavioral languages are': 'original_similarity': 0.8528263138225397: 'slide': 207}"
0.4458091000269633,What was the motivation behind the creation of UML? ,The creation of UML was primarily motivated by the desire to standardize the disparate notational systems and approaches to software design. ,"{'chunk_text': 'modeling methods and notation. It is originally based on the notations of the Booch method: the object-modeling technique (OMT) and object-oriented software engineering (OOSE): which it has integrated into a single language.Rational Software Corporation hired James Rumbaugh from General Electric in 1994 and after that the company became the source for two of the most popular object-oriented modeling approaches of the day: Rumbaugh\'s object-modeling technique (OMT) and Grady Booch\'s method. They were soon assisted in their efforts by Ivar Jacobson: the creator of the object-oriented software engineering (OOSE) method: who joined them at Rational in 1995. === UML 1.x === Under the technical leadership of those three (Rumbaugh: Jacobson and Booch): a consortium called the UML Partners was organized in 1996 to complete the Unified Modeling Language (UML) specification: and propose it to the Object Management Group (OMG) for standardization. The partnership also contained additional interested parties (for example HP: DEC: IBM and Microsoft). The UML Partners\' UML 1.0 draft was proposed to the OMG in January 1997 by the consortium. During the same month the UML Partners formed a group: designed to define the exact meaning of language constructs: chaired by Cris Kobryn and administered by Ed Eykholt: to finalize the specification and integrate it with other standardization efforts. The result of this work: UML 1.1: was submitted to the OMG in August 1997 and adopted by the OMG in November 1997.After the first release a task force was formed to improve the language: which released several minor revisions: 1.3: 1.4: and 1.5.The standards it produced (as well as the original standard) have been noted as being ambiguous and inconsistent. ==== Cardinality notation ==== As with database Chen: Bachman: and ISO ER diagrams: class models are specified to use ""look-across"" cardinalities: even though several authors (Merise: Elmasri & Navathe amongst others) prefer same-side or ""look-here"" for roles and both minimum and maximum cardinalities. Recent researchers (Feinerer: Dullea et al.) have shown that the ""look-across"" technique used by UML and ER diagrams is less effective and less coherent when applied to n-ary relationships of order strictly greater than 2. Feinerer says: ""Problems arise if we operate under the look-across semantics as used for UML associations. Hartmann investigates this situation and shows how and why different transformations fail."": and: ""As we will see on the next few pages: the look-across interpretation introduces several difficulties which prevent the extension of simple mechanisms from binary to n-ary associations."" === UML 2 === UML 2.0 major revision replaced version 1.5 in 2005: which was developed with an enlarged consortium to improve the language further to reflect new experience on usage of its features.Although UML 2.1 was never released as a formal specification: versions 2.1.1 and 2.1.2 appeared in 2007: followed by UML 2.2 in February 2009. UML 2.3 was formally released in May 2010. UML 2.4.1 was formally released in August 2011. UML 2.5 was released in October 2012 as an ""In progress"" version and was officially released in June 2015. Formal version 2.5.1 was adopted in December 2017.There are four parts to the UML 2.x specification: The Superstructure that defines the notation and semantics for diagrams and their model elements The Infrastructure that defines the core metamodel on which the Superstructure is based The Object Constraint Language (OCL) for defining rules for model elements The UML Diagram Interchange that defines how UML 2 diagram layouts are exchangedUntil UML 2.4.1: the latest versions of these standards were: UML Superstructure version 2.4.1 UML Infrastructure version 2.4.1 OCL version 2.3.1 UML Diagram Interchange version 1.0.Since version 2.5: the UML Specification has been simplified (without Superstructure and Infrastructure): and the latest versions of these standards are now: UML Specification 2.5.1 OCL version 2.4It continues to be updated and improved by the revision task force: who resolve any issues with the language. == Design == UML offers a way to visualize a system\'s architectural blueprints in a diagram: including elements such as: any activities (jobs); individual components of the system; and how they can interact with other software components; how the system will run; how entities interact with others (components and interfaces); external user interface.Although originally intended for object-oriented design documentation: UML has been extended to a larger set of design documentation (as listed above): and been found useful in many contexts. === Software development methods === UML is not a development method by itself; however: it was designed to be compatible with the leading object-oriented software development methods of its time: for example OMT: Booch method: Objectory and especially RUP that it was originally intended to be used with when work began at Rational Software. === Modeling === It is important to distinguish between the UML\nmodel and the set of diagrams of a system. A diagram is a partial graphic representation of a system\'s model. The set of diagrams need not completely cover the model and deleting a diagram does not change the model. The model may also contain documentation that drives the model elements and diagrams (such as written use cases). UML diagrams represent two different views of a system model: Static (or structural) view: emphasizes the static structure of the system using objects: attributes: operations and relationships. It includes class diagrams and composite structure diagrams. Dynamic (or behavioral) view: emphasizes the dynamic behavior of the system by showing collaborations among objects and changes to the internal states of objects. This view includes sequence diagrams: activity diagrams and state machine diagrams.UML models can be exchanged among UML tools by using the XML Metadata Interchange (XMI) format. In UML: one of the key tools for behavior modeling is the use-case model: caused by OOSE. Use cases are a way of specifying required usages of a system. Typically: they are used to capture the requirements of a system: that is: what a system is supposed to do. == Diagrams == UML 2 has many types of diagrams: which are divided into two categories. Some types represent structural information: and the rest represent general types of behavior: including a': 'original_similarity': 0.8656893754034432: 'slide': 220}","{'chunk_text': 'can be exchanged among UML tools by using the XML Metadata Interchange (XMI) format. In UML: one of the key tools for behavior modeling is the use-case model: caused by OOSE. Use cases are a way of specifying required usages of a system. Typically: they are used to capture the requirements of a system: that is: what a system is supposed to do. == Diagrams == UML 2 has many types of diagrams: which are divided into two categories. Some types represent structural information: and the rest represent general types of behavior: including a few that represent different aspects of interactions. These diagrams can be categorized hierarchically as shown in the following class diagram: These diagrams may all contain comments or notes explaining usage: constraint: or intent. === Structure diagrams === Structure diagrams represent the static aspects of the system. It emphasizes the things that must be present in the system being modeled. Since structure diagrams represent the structure: they are used extensively in documenting the software architecture of software systems. For example: the component diagram describes how a software system is split up into components and shows the dependencies among these components. === Behavior diagrams === Behavior diagrams represent the dynamic aspect of the system. It emphasizes what must happen in the system being modeled. Since behavior diagrams illustrate the behavior of a system: they are used extensively to describe the functionality of software systems. As an example: the activity diagram describes the business and operational step-by-step activities of the components in a system. ==== Interaction diagrams ==== Interaction diagrams: a subset of behavior diagrams: emphasize the flow of control and data among the things in the system being modeled. For example: the sequence diagram shows how objects communicate with each other regarding a sequence of messages. == Metamodeling == The Object Management Group (OMG) has developed a metamodeling architecture to define the UML: called the Meta-Object Facility. MOF is designed as a four-layered architecture: as shown in the image at right. It provides a meta-meta model at the top: called the M3 layer. This M3-model is the language used by Meta-Object Facility to build metamodels: called M2-models. The most prominent example of a Layer 2 Meta-Object Facility model is the UML metamodel: which describes the UML itself. These M2-models describe elements of the M1-layer: and thus M1-models. These would be: for example: models written in UML. The last layer is the M0-layer or data layer. It is used to describe runtime instances of the system.The meta-model can be extended using a mechanism called stereotyping. This has been criticized as being insufficient/untenable by Brian Henderson-Sellers and Cesar Gonzalez-Perez in ""Uses and Abuses of the Stereotype Mechanism in UML 1.x and 2.0"". == Adoption == Back in 2013 UML has been marketed by OMG for many contexts: but aimed primarily at software development with limited success.It has been treated: at times: as a design silver bullet: which leads to problems. UML misuse includes overuse (designing every part of the system with it: which is unnecessary) and assuming that novices can design with it.It is considered a large language: with many constructs. Some people (including Jacobson) feel that UML\'s size hinders learning (and therefore using) it.MS Visual Studio dropped support for UML in 2016 due to lack of usage.According to Google Trends UML has been on steady decline since 2004. == See also == Applications of UML Business Process Model and Notation (BPMN) C4 model Department of Defense Architecture Framework DOT (graph description language) List of Unified Modeling Language tools MODAF Model-based testing Model-driven engineering Object-oriented role analysis and modeling Process Specification Language Systems Modeling Language (SysML) == References == == Further reading == Ambler: Scott William (2004). The Object Primer: Agile Model Driven Development with UML 2. Cambridge University Press. ISBN 0-521-54018-6. Archived from the original on 31 January 2010. Retrieved 29 April 2006. Chonoles: Michael Jesse; James A. Schardt (2003). UML 2 for Dummies. Wiley Publishing. ISBN 0-7645-2614-6. Fowler: Martin (2004). UML Distilled: A Brief Guide to the Standard Object Modeling Language (3rd ed.). Addison-Wesley. ISBN 0-321-19368-7. Jacobson: Ivar; Grady Booch; James Rumbaugh (1998). The Unified Software Development Process. Addison Wesley Longman. ISBN 0-201-57169-2. Martin: Robert Cecil (2003). UML for Java Programmers. Prentice Hall. ISBN 0-13-142848-9. Noran: Ovidiu S. ""Business Modelling: UML vs. IDEF"" (PDF). Retrieved 14 November 2022. Horst Kargl. ""Interactive UML Metamodel with additional Examples"". Penker: Magnus; Hans-Erik Eriksson (2000). Business Modeling with UML. John Wiley & Sons. ISBN 0-471-29551-5. Douglass: Bruce Powel. ""Bruce Douglass: Real-Time Agile Systems and Software Development"" (web). Retrieved 1 January 2019. Douglass: Bruce (2014). Real-Time UML Workshop 2nd Edition. Newnes. ISBN 978-0-471-29551-8. Douglass: Bruce (2004). Real-Time UML 3rd Edition. Newnes. ISBN 978-0321160768. Douglass: Bruce (2002). Real-Time Design Patterns. Addison-Wesley Professional. ISBN 978-0201699562. Douglass: Bruce (2009). Real-Time Agility. Addison-Wesley Professional. ISBN 978-0321545497. Douglass: Bruce (2010). Design Patterns for Embedded Systems in C. Newnes. ISBN 978-1856177078. == External links == Official website Current Version Specification\nmodel and the set of diagrams of a system. A diagram is a partial graphic representation of a system\'s model. The set of diagrams need not completely cover the model and deleting a diagram does not change the model. The model may also contain documentation that drives the model elements and diagrams (such as written use cases). UML diagrams represent two different views of a system model: Static (or structural) view: emphasizes the static structure of the system using objects: attributes: operations and relationships. It includes class diagrams and composite structure diagrams. Dynamic (or behavioral) view: emphasizes the dynamic behavior of the system by showing collaborations among objects and changes to the internal states of objects. This view includes sequence diagrams: activity diagrams and state machine diagrams.UML models can': 'original_similarity': 0.8349554609678221: 'slide': 127}","{'chunk_text': ""of a model does not contain any hints how to process it. ==== Behavioral ==== Behavioral languages are designed to describe the observable behavior of complex systems consisting of components that execute concurrently. These languages focus on the description of key concepts such as: concurrency: nondeterminism: synchronization: and communication. The semantic foundations of Behavioral languages are process calculus or process algebra. ==== Discipline-specific ==== A discipline-specific modeling (DspM) language is focused on deliverables affiliated with a specific software development life cycle stage. Therefore: such language offers a distinct vocabulary: syntax: and notation for each stage: such as discovery: analysis: design: architecture: contraction: etc. For example: for the analysis phase of a project: the modeler employs specific analysis notation to deliver an analysis proposition diagram. During the design phase: however: logical design notation is used to depict relationship between software entities. In addition: the discipline-specific modeling language best practices does not preclude practitioners from combining the various notations in a single diagram. ==== Domain-specific ==== Domain-specific modeling (DSM) is a software engineering methodology for designing and developing systems: most often IT systems such as computer software. It involves systematic use of a graphical domain-specific language (DSL) to represent the various facets of a system. DSM languages tend to support higher-level abstractions than General-purpose modeling languages: so they require less effort and fewer low-level details to specify a given system. ==== Framework-specific ==== A framework-specific modeling language (FSML) is a kind of domain-specific modeling language which is designed for an object-oriented application framework. FSMLs define framework-provided abstractions as FSML concepts and decompose the abstractions into features. The features represent implementation steps or choices. A FSML concept can be configured by selecting features and providing values for features. Such a concept configuration represents how the concept should be implemented in the code. In other words: concept configuration describes how the framework should be completed in order to create the implementation of the concept. ==== Information and knowledge modeling ==== Linked data and ontology engineering require 'host languages' to represent entities and the relations between them: constraints between the properties of entities and relations: and metadata attributes. JSON-LD and RDF are two major (and semantically almost equivalent) languages in this context: primarily because they support statement reification and contextualisation which are essential properties to support the higher-order logic needed to reason about models. Model transformation is a common example of such reasoning. ==== Object-oriented ==== Object modeling languages are modeling languages based on a standardized set of symbols and ways of arranging them to model (part of) an object oriented software design or system design. Some organizations use them extensively in combination with a software development methodology to progress from initial specification to an implementation plan and to communicate that plan to an entire team of developers and stakeholders. Because a modeling language is visual and at a higher-level of abstraction than code: using models encourages the generation of a shared vision that may prevent problems of differing interpretation later in development. Often software modeling tools are used to construct these models: which may then be capable of automatic translation to code. ==== Virtual reality ==== Virtual Reality Modeling Language (VRML): before 1995 known as the Virtual Reality Markup Language is a standard file format for representing 3-dimensional (3D) interactive vector graphics: designed particularly with the World Wide Web in mind. ==== Others ==== Architecture Description Language Face Modeling Language Generative Modelling Language Java Modeling Language Promela Rebeca Modeling Language Service Modeling Language Web Services Modeling Language X3D == Applications == Various kinds of modeling languages are applied in different disciplines: including computer science: information management: business process modeling: software engineering: and systems engineering. Modeling languages can be used to specify: system requirements: structures and behaviors.Modeling languages are intended to be used to precisely specify systems so that stakeholders (e.g.: customers: operators: analysts: designers) can better understand the system being modeled. The more mature modeling languages are precise: consistent and executable. Informal diagramming techniques applied with drawing tools are expected to produce useful pictorial representations of system requirements: structures and behaviors: which can be useful for communication: design: and problem solving but cannot be used programmatically.: 539 Executable modeling languages applied with proper tool support: however: are expected to automate system verification and validation: simulation and code generation from the same representations. == Quality == A review of modelling languages is essential to be able to assign which languages are appropriate for different modelling settings. In the term settings we include stakeholders: domain and the knowledge connected. Assessing the language quality is a means that aims to achieve better models. === Framework for evaluation === Here language quality is stated in accordance with the SEQUAL framework for quality of models developed by Krogstie: Sindre and Lindland (2003): since this is a framework that connects the language quality to a framework for general model\nquality. Five areas are used in this framework to describe language quality and these are supposed to express both the conceptual as well as the visual notation of the language. We will not go into a thoroughly explanation of the underlying quality framework of models but concentrate on the areas used to explain the language quality framework. ==== Domain appropriateness ==== The framework states the ability to represent the domain as domain appropriateness. The statement appropriateness can be a bit vague: but in this particular context it means able to express. You should ideally only be able to express things that are in the domain but be powerful enough to include everything that is in the domain. This requirement might seem a bit strict: but the aim is to get a visually expressed model which includes everything relevant to the domain and excludes everything not appropriate for the domain. To achieve this: the language has to have a good distinction of which notations and syntaxes that are advantageous to present. ==== Participant appropriateness ==== To evaluate the participant appropriateness we try to identify how well the language expresses the knowledge held by"": 'original_similarity': 0.8054998482994803: 'slide': 190}"
0.5162869999767281,When and by whom was UML developed? ,"UML was developed at Rational Software in 1994–1995, with further development led by them through 1996. ","{'chunk_text': 'of the Booch method: the object-modeling technique (OMT) and object-oriented software engineering (OOSE): which it has integrated into a single language.Rational Software Corporation hired James Rumbaugh from General Electric in 1994 and after that the company became the source for two of the most popular object-oriented modeling approaches of the day: Rumbaugh\'s object-modeling technique (OMT) and Grady Booch\'s method. They were soon assisted in their efforts by Ivar Jacobson: the creator of the object-oriented software engineering (OOSE) method: who joined them at Rational in 1995. === UML 1.x === Under the technical leadership of those three (Rumbaugh: Jacobson and Booch): a consortium called the UML Partners was organized in 1996 to complete the Unified Modeling Language (UML) specification: and propose it to the Object Management Group (OMG) for standardization. The partnership also contained additional interested parties (for example HP: DEC: IBM and Microsoft). The UML Partners\' UML 1.0 draft was proposed to the OMG in January 1997 by the consortium. During the same month the UML Partners formed a group: designed to define the exact meaning of language constructs: chaired by Cris Kobryn and administered by Ed Eykholt: to finalize the specification and integrate it with other standardization efforts. The result of this work: UML 1.1: was submitted to the OMG in August 1997 and adopted by the OMG in November 1997.After the first release a task force was formed to improve the language: which released several minor revisions: 1.3: 1.4: and 1.5.The standards it produced (as well as the original standard) have been noted as being ambiguous and inconsistent. ==== Cardinality notation ==== As with database Chen: Bachman: and ISO ER diagrams: class models are specified to use ""look-across"" cardinalities: even though several authors (Merise: Elmasri & Navathe amongst others) prefer same-side or ""look-here"" for roles and both minimum and maximum cardinalities. Recent researchers (Feinerer: Dullea et al.) have shown that the ""look-across"" technique used by UML and ER diagrams is less effective and less coherent when applied to n-ary relationships of order strictly greater than 2. Feinerer says: ""Problems arise if we operate under the look-across semantics as used for UML associations. Hartmann investigates this situation and shows how and why different transformations fail."": and: ""As we will see on the next few pages: the look-across interpretation introduces several difficulties which prevent the extension of simple mechanisms from binary to n-ary associations."" === UML 2 === UML 2.0 major revision replaced version 1.5 in 2005: which was developed with an enlarged consortium to improve the language further to reflect new experience on usage of its features.Although UML 2.1 was never released as a formal specification: versions 2.1.1 and 2.1.2 appeared in 2007: followed by UML 2.2 in February 2009. UML 2.3 was formally released in May 2010. UML 2.4.1 was formally released in August 2011. UML 2.5 was released in October 2012 as an ""In progress"" version and was officially released in June 2015. Formal version 2.5.1 was adopted in December 2017.There are four parts to the UML 2.x specification: The Superstructure that defines the notation and semantics for diagrams and their model elements The Infrastructure that defines the core metamodel on which the Superstructure is based The Object Constraint Language (OCL) for defining rules for model elements The UML Diagram Interchange that defines how UML 2 diagram layouts are exchangedUntil UML 2.4.1: the latest versions of these standards were: UML Superstructure version 2.4.1 UML Infrastructure version 2.4.1 OCL version 2.3.1 UML Diagram Interchange version 1.0.Since version 2.5: the UML Specification has been simplified (without Superstructure and Infrastructure): and the latest versions of these standards are now: UML Specification 2.5.1 OCL version 2.4It continues to be updated and improved by the revision task force: who resolve any issues with the language. == Design == UML offers a way to visualize a system\'s architectural blueprints in a diagram: including elements such as: any activities (jobs); individual components of the system; and how they can interact with other software components; how the system will run; how entities interact with others (components and interfaces); external user interface.Although originally intended for object-oriented design documentation: UML has been extended to a larger set of design documentation (as listed above): and been found useful in many contexts. === Software development methods === UML is not a development method by itself; however: it was designed to be compatible with the leading object-oriented software development methods of its time: for example OMT: Booch method: Objectory and especially RUP that it was originally intended to be used with when work began at Rational Software. === Modeling === It is important to distinguish between the UML\nmodel and the set of diagrams of a system. A diagram is a partial graphic representation of a system\'s model. The set of diagrams need not completely cover the model and deleting a diagram does not change the model. The model may also contain documentation that drives the model elements and diagrams (such as written use cases). UML diagrams represent two different views of a system model: Static (or structural) view: emphasizes the static structure of the system using objects: attributes: operations and relationships. It includes class diagrams and composite structure diagrams. Dynamic (or behavioral) view: emphasizes the dynamic behavior of the system by showing collaborations among objects and changes to the internal states of objects. This view includes sequence diagrams: activity diagrams and state machine diagrams.UML models can be exchanged among UML tools by using the XML Metadata Interchange (XMI) format. In UML: one of the key tools for behavior modeling is the use-case model: caused by OOSE. Use cases are a way of specifying required usages of a system. Typically: they are used to capture the requirements of a system: that is: what a system is supposed to do. == Diagrams == UML 2 has many types of diagrams: which are divided into two categories. Some types represent structural information: and the rest represent general types of behavior: including a few that represent different aspects of interactions. These diagrams can be': 'original_similarity': 0.8511250154832539: 'slide': 231}","{'chunk_text': 'In UML: one of the key tools for behavior modeling is the use-case model: caused by OOSE. Use cases are a way of specifying required usages of a system. Typically: they are used to capture the requirements of a system: that is: what a system is supposed to do. == Diagrams == UML 2 has many types of diagrams: which are divided into two categories. Some types represent structural information: and the rest represent general types of behavior: including a few that represent different aspects of interactions. These diagrams can be categorized hierarchically as shown in the following class diagram: These diagrams may all contain comments or notes explaining usage: constraint: or intent. === Structure diagrams === Structure diagrams represent the static aspects of the system. It emphasizes the things that must be present in the system being modeled. Since structure diagrams represent the structure: they are used extensively in documenting the software architecture of software systems. For example: the component diagram describes how a software system is split up into components and shows the dependencies among these components. === Behavior diagrams === Behavior diagrams represent the dynamic aspect of the system. It emphasizes what must happen in the system being modeled. Since behavior diagrams illustrate the behavior of a system: they are used extensively to describe the functionality of software systems. As an example: the activity diagram describes the business and operational step-by-step activities of the components in a system. ==== Interaction diagrams ==== Interaction diagrams: a subset of behavior diagrams: emphasize the flow of control and data among the things in the system being modeled. For example: the sequence diagram shows how objects communicate with each other regarding a sequence of messages. == Metamodeling == The Object Management Group (OMG) has developed a metamodeling architecture to define the UML: called the Meta-Object Facility. MOF is designed as a four-layered architecture: as shown in the image at right. It provides a meta-meta model at the top: called the M3 layer. This M3-model is the language used by Meta-Object Facility to build metamodels: called M2-models. The most prominent example of a Layer 2 Meta-Object Facility model is the UML metamodel: which describes the UML itself. These M2-models describe elements of the M1-layer: and thus M1-models. These would be: for example: models written in UML. The last layer is the M0-layer or data layer. It is used to describe runtime instances of the system.The meta-model can be extended using a mechanism called stereotyping. This has been criticized as being insufficient/untenable by Brian Henderson-Sellers and Cesar Gonzalez-Perez in ""Uses and Abuses of the Stereotype Mechanism in UML 1.x and 2.0"". == Adoption == Back in 2013 UML has been marketed by OMG for many contexts: but aimed primarily at software development with limited success.It has been treated: at times: as a design silver bullet: which leads to problems. UML misuse includes overuse (designing every part of the system with it: which is unnecessary) and assuming that novices can design with it.It is considered a large language: with many constructs. Some people (including Jacobson) feel that UML\'s size hinders learning (and therefore using) it.MS Visual Studio dropped support for UML in 2016 due to lack of usage.According to Google Trends UML has been on steady decline since 2004. == See also == Applications of UML Business Process Model and Notation (BPMN) C4 model Department of Defense Architecture Framework DOT (graph description language) List of Unified Modeling Language tools MODAF Model-based testing Model-driven engineering Object-oriented role analysis and modeling Process Specification Language Systems Modeling Language (SysML) == References == == Further reading == Ambler: Scott William (2004). The Object Primer: Agile Model Driven Development with UML 2. Cambridge University Press. ISBN 0-521-54018-6. Archived from the original on 31 January 2010. Retrieved 29 April 2006. Chonoles: Michael Jesse; James A. Schardt (2003). UML 2 for Dummies. Wiley Publishing. ISBN 0-7645-2614-6. Fowler: Martin (2004). UML Distilled: A Brief Guide to the Standard Object Modeling Language (3rd ed.). Addison-Wesley. ISBN 0-321-19368-7. Jacobson: Ivar; Grady Booch; James Rumbaugh (1998). The Unified Software Development Process. Addison Wesley Longman. ISBN 0-201-57169-2. Martin: Robert Cecil (2003). UML for Java Programmers. Prentice Hall. ISBN 0-13-142848-9. Noran: Ovidiu S. ""Business Modelling: UML vs. IDEF"" (PDF). Retrieved 14 November 2022. Horst Kargl. ""Interactive UML Metamodel with additional Examples"". Penker: Magnus; Hans-Erik Eriksson (2000). Business Modeling with UML. John Wiley & Sons. ISBN 0-471-29551-5. Douglass: Bruce Powel. ""Bruce Douglass: Real-Time Agile Systems and Software Development"" (web). Retrieved 1 January 2019. Douglass: Bruce (2014). Real-Time UML Workshop 2nd Edition. Newnes. ISBN 978-0-471-29551-8. Douglass: Bruce (2004). Real-Time UML 3rd Edition. Newnes. ISBN 978-0321160768. Douglass: Bruce (2002). Real-Time Design Patterns. Addison-Wesley Professional. ISBN 978-0201699562. Douglass: Bruce (2009). Real-Time Agility. Addison-Wesley Professional. ISBN 978-0321545497. Douglass: Bruce (2010). Design Patterns for Embedded Systems in C. Newnes. ISBN 978-1856177078. == External links == Official website Current Version Specification\nmodel and the set of diagrams of a system. A diagram is a partial graphic representation of a system\'s model. The set of diagrams need not completely cover the model and deleting a diagram does not change the model. The model may also contain documentation that drives the model elements and diagrams (such as written use cases). UML diagrams represent two different views of a system model: Static (or structural) view: emphasizes the static structure of the system using objects: attributes: operations and relationships. It includes class diagrams and composite structure diagrams. Dynamic (or behavioral) view: emphasizes the dynamic behavior of the system by showing collaborations among objects and changes to the internal states of objects. This view includes sequence diagrams: activity diagrams and state machine diagrams.UML models can be exchanged among UML tools by using the XML Metadata Interchange (XMI) format. In': 'original_similarity': 0.8143844032186577: 'slide': 141}","{'chunk_text': ""that execute concurrently. These languages focus on the description of key concepts such as: concurrency: nondeterminism: synchronization: and communication. The semantic foundations of Behavioral languages are process calculus or process algebra. ==== Discipline-specific ==== A discipline-specific modeling (DspM) language is focused on deliverables affiliated with a specific software development life cycle stage. Therefore: such language offers a distinct vocabulary: syntax: and notation for each stage: such as discovery: analysis: design: architecture: contraction: etc. For example: for the analysis phase of a project: the modeler employs specific analysis notation to deliver an analysis proposition diagram. During the design phase: however: logical design notation is used to depict relationship between software entities. In addition: the discipline-specific modeling language best practices does not preclude practitioners from combining the various notations in a single diagram. ==== Domain-specific ==== Domain-specific modeling (DSM) is a software engineering methodology for designing and developing systems: most often IT systems such as computer software. It involves systematic use of a graphical domain-specific language (DSL) to represent the various facets of a system. DSM languages tend to support higher-level abstractions than General-purpose modeling languages: so they require less effort and fewer low-level details to specify a given system. ==== Framework-specific ==== A framework-specific modeling language (FSML) is a kind of domain-specific modeling language which is designed for an object-oriented application framework. FSMLs define framework-provided abstractions as FSML concepts and decompose the abstractions into features. The features represent implementation steps or choices. A FSML concept can be configured by selecting features and providing values for features. Such a concept configuration represents how the concept should be implemented in the code. In other words: concept configuration describes how the framework should be completed in order to create the implementation of the concept. ==== Information and knowledge modeling ==== Linked data and ontology engineering require 'host languages' to represent entities and the relations between them: constraints between the properties of entities and relations: and metadata attributes. JSON-LD and RDF are two major (and semantically almost equivalent) languages in this context: primarily because they support statement reification and contextualisation which are essential properties to support the higher-order logic needed to reason about models. Model transformation is a common example of such reasoning. ==== Object-oriented ==== Object modeling languages are modeling languages based on a standardized set of symbols and ways of arranging them to model (part of) an object oriented software design or system design. Some organizations use them extensively in combination with a software development methodology to progress from initial specification to an implementation plan and to communicate that plan to an entire team of developers and stakeholders. Because a modeling language is visual and at a higher-level of abstraction than code: using models encourages the generation of a shared vision that may prevent problems of differing interpretation later in development. Often software modeling tools are used to construct these models: which may then be capable of automatic translation to code. ==== Virtual reality ==== Virtual Reality Modeling Language (VRML): before 1995 known as the Virtual Reality Markup Language is a standard file format for representing 3-dimensional (3D) interactive vector graphics: designed particularly with the World Wide Web in mind. ==== Others ==== Architecture Description Language Face Modeling Language Generative Modelling Language Java Modeling Language Promela Rebeca Modeling Language Service Modeling Language Web Services Modeling Language X3D == Applications == Various kinds of modeling languages are applied in different disciplines: including computer science: information management: business process modeling: software engineering: and systems engineering. Modeling languages can be used to specify: system requirements: structures and behaviors.Modeling languages are intended to be used to precisely specify systems so that stakeholders (e.g.: customers: operators: analysts: designers) can better understand the system being modeled. The more mature modeling languages are precise: consistent and executable. Informal diagramming techniques applied with drawing tools are expected to produce useful pictorial representations of system requirements: structures and behaviors: which can be useful for communication: design: and problem solving but cannot be used programmatically.: 539 Executable modeling languages applied with proper tool support: however: are expected to automate system verification and validation: simulation and code generation from the same representations. == Quality == A review of modelling languages is essential to be able to assign which languages are appropriate for different modelling settings. In the term settings we include stakeholders: domain and the knowledge connected. Assessing the language quality is a means that aims to achieve better models. === Framework for evaluation === Here language quality is stated in accordance with the SEQUAL framework for quality of models developed by Krogstie: Sindre and Lindland (2003): since this is a framework that connects the language quality to a framework for general model\nquality. Five areas are used in this framework to describe language quality and these are supposed to express both the conceptual as well as the visual notation of the language. We will not go into a thoroughly explanation of the underlying quality framework of models but concentrate on the areas used to explain the language quality framework. ==== Domain appropriateness ==== The framework states the ability to represent the domain as domain appropriateness. The statement appropriateness can be a bit vague: but in this particular context it means able to express. You should ideally only be able to express things that are in the domain but be powerful enough to include everything that is in the domain. This requirement might seem a bit strict: but the aim is to get a visually expressed model which includes everything relevant to the domain and excludes everything not appropriate for the domain. To achieve this: the language has to have a good distinction of which notations and syntaxes that are advantageous to present. ==== Participant appropriateness ==== To evaluate the participant appropriateness we try to identify how well the language expresses the knowledge held by the stakeholders. This involves challenges since a stakeholder's knowledge is subjective. The knowledge of the stakeholder is both tacit and explicit. Both types of knowledge are of dynamic character. In"": 'original_similarity': 0.781041996048007: 'slide': 220}"
0.443722500000149,Who manages UML and when was it adopted as a standard? ,"UML was adopted as a standard by the Object Management Group (OMG) in 1997 and has been managed by this organization ever since. In 2005, UML was also published by the International Organization for Standardization (ISO) and the International Electrotechnical Commission (IEC) as the ISO/IEC 19501 standard.","{'chunk_text': 'object-modeling technique (OMT) and object-oriented software engineering (OOSE): which it has integrated into a single language.Rational Software Corporation hired James Rumbaugh from General Electric in 1994 and after that the company became the source for two of the most popular object-oriented modeling approaches of the day: Rumbaugh\'s object-modeling technique (OMT) and Grady Booch\'s method. They were soon assisted in their efforts by Ivar Jacobson: the creator of the object-oriented software engineering (OOSE) method: who joined them at Rational in 1995. === UML 1.x === Under the technical leadership of those three (Rumbaugh: Jacobson and Booch): a consortium called the UML Partners was organized in 1996 to complete the Unified Modeling Language (UML) specification: and propose it to the Object Management Group (OMG) for standardization. The partnership also contained additional interested parties (for example HP: DEC: IBM and Microsoft). The UML Partners\' UML 1.0 draft was proposed to the OMG in January 1997 by the consortium. During the same month the UML Partners formed a group: designed to define the exact meaning of language constructs: chaired by Cris Kobryn and administered by Ed Eykholt: to finalize the specification and integrate it with other standardization efforts. The result of this work: UML 1.1: was submitted to the OMG in August 1997 and adopted by the OMG in November 1997.After the first release a task force was formed to improve the language: which released several minor revisions: 1.3: 1.4: and 1.5.The standards it produced (as well as the original standard) have been noted as being ambiguous and inconsistent. ==== Cardinality notation ==== As with database Chen: Bachman: and ISO ER diagrams: class models are specified to use ""look-across"" cardinalities: even though several authors (Merise: Elmasri & Navathe amongst others) prefer same-side or ""look-here"" for roles and both minimum and maximum cardinalities. Recent researchers (Feinerer: Dullea et al.) have shown that the ""look-across"" technique used by UML and ER diagrams is less effective and less coherent when applied to n-ary relationships of order strictly greater than 2. Feinerer says: ""Problems arise if we operate under the look-across semantics as used for UML associations. Hartmann investigates this situation and shows how and why different transformations fail."": and: ""As we will see on the next few pages: the look-across interpretation introduces several difficulties which prevent the extension of simple mechanisms from binary to n-ary associations."" === UML 2 === UML 2.0 major revision replaced version 1.5 in 2005: which was developed with an enlarged consortium to improve the language further to reflect new experience on usage of its features.Although UML 2.1 was never released as a formal specification: versions 2.1.1 and 2.1.2 appeared in 2007: followed by UML 2.2 in February 2009. UML 2.3 was formally released in May 2010. UML 2.4.1 was formally released in August 2011. UML 2.5 was released in October 2012 as an ""In progress"" version and was officially released in June 2015. Formal version 2.5.1 was adopted in December 2017.There are four parts to the UML 2.x specification: The Superstructure that defines the notation and semantics for diagrams and their model elements The Infrastructure that defines the core metamodel on which the Superstructure is based The Object Constraint Language (OCL) for defining rules for model elements The UML Diagram Interchange that defines how UML 2 diagram layouts are exchangedUntil UML 2.4.1: the latest versions of these standards were: UML Superstructure version 2.4.1 UML Infrastructure version 2.4.1 OCL version 2.3.1 UML Diagram Interchange version 1.0.Since version 2.5: the UML Specification has been simplified (without Superstructure and Infrastructure): and the latest versions of these standards are now: UML Specification 2.5.1 OCL version 2.4It continues to be updated and improved by the revision task force: who resolve any issues with the language. == Design == UML offers a way to visualize a system\'s architectural blueprints in a diagram: including elements such as: any activities (jobs); individual components of the system; and how they can interact with other software components; how the system will run; how entities interact with others (components and interfaces); external user interface.Although originally intended for object-oriented design documentation: UML has been extended to a larger set of design documentation (as listed above): and been found useful in many contexts. === Software development methods === UML is not a development method by itself; however: it was designed to be compatible with the leading object-oriented software development methods of its time: for example OMT: Booch method: Objectory and especially RUP that it was originally intended to be used with when work began at Rational Software. === Modeling === It is important to distinguish between the UML\nmodel and the set of diagrams of a system. A diagram is a partial graphic representation of a system\'s model. The set of diagrams need not completely cover the model and deleting a diagram does not change the model. The model may also contain documentation that drives the model elements and diagrams (such as written use cases). UML diagrams represent two different views of a system model: Static (or structural) view: emphasizes the static structure of the system using objects: attributes: operations and relationships. It includes class diagrams and composite structure diagrams. Dynamic (or behavioral) view: emphasizes the dynamic behavior of the system by showing collaborations among objects and changes to the internal states of objects. This view includes sequence diagrams: activity diagrams and state machine diagrams.UML models can be exchanged among UML tools by using the XML Metadata Interchange (XMI) format. In UML: one of the key tools for behavior modeling is the use-case model: caused by OOSE. Use cases are a way of specifying required usages of a system. Typically: they are used to capture the requirements of a system: that is: what a system is supposed to do. == Diagrams == UML 2 has many types of diagrams: which are divided into two categories. Some types represent structural information: and the rest represent general types of behavior: including a few that represent different aspects of interactions. These diagrams can be categorized hierarchically as shown in': 'original_similarity': 0.8458898307349555: 'slide': 236}","{'chunk_text': 'key tools for behavior modeling is the use-case model: caused by OOSE. Use cases are a way of specifying required usages of a system. Typically: they are used to capture the requirements of a system: that is: what a system is supposed to do. == Diagrams == UML 2 has many types of diagrams: which are divided into two categories. Some types represent structural information: and the rest represent general types of behavior: including a few that represent different aspects of interactions. These diagrams can be categorized hierarchically as shown in the following class diagram: These diagrams may all contain comments or notes explaining usage: constraint: or intent. === Structure diagrams === Structure diagrams represent the static aspects of the system. It emphasizes the things that must be present in the system being modeled. Since structure diagrams represent the structure: they are used extensively in documenting the software architecture of software systems. For example: the component diagram describes how a software system is split up into components and shows the dependencies among these components. === Behavior diagrams === Behavior diagrams represent the dynamic aspect of the system. It emphasizes what must happen in the system being modeled. Since behavior diagrams illustrate the behavior of a system: they are used extensively to describe the functionality of software systems. As an example: the activity diagram describes the business and operational step-by-step activities of the components in a system. ==== Interaction diagrams ==== Interaction diagrams: a subset of behavior diagrams: emphasize the flow of control and data among the things in the system being modeled. For example: the sequence diagram shows how objects communicate with each other regarding a sequence of messages. == Metamodeling == The Object Management Group (OMG) has developed a metamodeling architecture to define the UML: called the Meta-Object Facility. MOF is designed as a four-layered architecture: as shown in the image at right. It provides a meta-meta model at the top: called the M3 layer. This M3-model is the language used by Meta-Object Facility to build metamodels: called M2-models. The most prominent example of a Layer 2 Meta-Object Facility model is the UML metamodel: which describes the UML itself. These M2-models describe elements of the M1-layer: and thus M1-models. These would be: for example: models written in UML. The last layer is the M0-layer or data layer. It is used to describe runtime instances of the system.The meta-model can be extended using a mechanism called stereotyping. This has been criticized as being insufficient/untenable by Brian Henderson-Sellers and Cesar Gonzalez-Perez in ""Uses and Abuses of the Stereotype Mechanism in UML 1.x and 2.0"". == Adoption == Back in 2013 UML has been marketed by OMG for many contexts: but aimed primarily at software development with limited success.It has been treated: at times: as a design silver bullet: which leads to problems. UML misuse includes overuse (designing every part of the system with it: which is unnecessary) and assuming that novices can design with it.It is considered a large language: with many constructs. Some people (including Jacobson) feel that UML\'s size hinders learning (and therefore using) it.MS Visual Studio dropped support for UML in 2016 due to lack of usage.According to Google Trends UML has been on steady decline since 2004. == See also == Applications of UML Business Process Model and Notation (BPMN) C4 model Department of Defense Architecture Framework DOT (graph description language) List of Unified Modeling Language tools MODAF Model-based testing Model-driven engineering Object-oriented role analysis and modeling Process Specification Language Systems Modeling Language (SysML) == References == == Further reading == Ambler: Scott William (2004). The Object Primer: Agile Model Driven Development with UML 2. Cambridge University Press. ISBN 0-521-54018-6. Archived from the original on 31 January 2010. Retrieved 29 April 2006. Chonoles: Michael Jesse; James A. Schardt (2003). UML 2 for Dummies. Wiley Publishing. ISBN 0-7645-2614-6. Fowler: Martin (2004). UML Distilled: A Brief Guide to the Standard Object Modeling Language (3rd ed.). Addison-Wesley. ISBN 0-321-19368-7. Jacobson: Ivar; Grady Booch; James Rumbaugh (1998). The Unified Software Development Process. Addison Wesley Longman. ISBN 0-201-57169-2. Martin: Robert Cecil (2003). UML for Java Programmers. Prentice Hall. ISBN 0-13-142848-9. Noran: Ovidiu S. ""Business Modelling: UML vs. IDEF"" (PDF). Retrieved 14 November 2022. Horst Kargl. ""Interactive UML Metamodel with additional Examples"". Penker: Magnus; Hans-Erik Eriksson (2000). Business Modeling with UML. John Wiley & Sons. ISBN 0-471-29551-5. Douglass: Bruce Powel. ""Bruce Douglass: Real-Time Agile Systems and Software Development"" (web). Retrieved 1 January 2019. Douglass: Bruce (2014). Real-Time UML Workshop 2nd Edition. Newnes. ISBN 978-0-471-29551-8. Douglass: Bruce (2004). Real-Time UML 3rd Edition. Newnes. ISBN 978-0321160768. Douglass: Bruce (2002). Real-Time Design Patterns. Addison-Wesley Professional. ISBN 978-0201699562. Douglass: Bruce (2009). Real-Time Agility. Addison-Wesley Professional. ISBN 978-0321545497. Douglass: Bruce (2010). Design Patterns for Embedded Systems in C. Newnes. ISBN 978-1856177078. == External links == Official website Current Version Specification\nmodel and the set of diagrams of a system. A diagram is a partial graphic representation of a system\'s model. The set of diagrams need not completely cover the model and deleting a diagram does not change the model. The model may also contain documentation that drives the model elements and diagrams (such as written use cases). UML diagrams represent two different views of a system model: Static (or structural) view: emphasizes the static structure of the system using objects: attributes: operations and relationships. It includes class diagrams and composite structure diagrams. Dynamic (or behavioral) view: emphasizes the dynamic behavior of the system by showing collaborations among objects and changes to the internal states of objects. This view includes sequence diagrams: activity diagrams and state machine diagrams.UML models can be exchanged among UML tools by using the XML Metadata Interchange (XMI) format. In UML: one of the key': 'original_similarity': 0.8055486862564931: 'slide': 146}","{'chunk_text': 'to represent the domain as domain appropriateness. The statement appropriateness can be a bit vague: but in this particular context it means able to express. You should ideally only be able to express things that are in the domain but be powerful enough to include everything that is in the domain. This requirement might seem a bit strict: but the aim is to get a visually expressed model which includes everything relevant to the domain and excludes everything not appropriate for the domain. To achieve this: the language has to have a good distinction of which notations and syntaxes that are advantageous to present. ==== Participant appropriateness ==== To evaluate the participant appropriateness we try to identify how well the language expresses the knowledge held by the stakeholders. This involves challenges since a stakeholder\'s knowledge is subjective. The knowledge of the stakeholder is both tacit and explicit. Both types of knowledge are of dynamic character. In this framework only the explicit type of knowledge is taken into account. The language should to a large extent express all the explicit knowledge of the stakeholders relevant to the domain. ==== Modeller appropriateness ==== Last paragraph stated that knowledge of the stakeholders should be presented in a good way. In addition it is imperative that the language should be able to express all possible explicit knowledge of the stakeholders. No knowledge should be left unexpressed due to lacks in the language. ==== Comprehensibility appropriateness ==== Comprehensibility appropriateness makes sure that the social actors understand the model due to a consistent use of the language. To achieve this the framework includes a set of criteria. The general importance that these express is that the language should be flexible: easy to organize and easy to distinguish different parts of the language internally as well as from other languages. In addition to this: the goal should be as simple as possible and that each symbol in the language has a unique representation. This is in connection to also to the structure of the development requirements. . ==== Tool appropriateness ==== To ensure that the domain actually modelled is usable for analyzing and further processing: the language has to ensure that it is possible to reason in an automatic way. To achieve this it has to include formal syntax and semantics. Another advantage by formalizing is the ability to discover errors in an early stage. It is not always that the language best fitted for the technical actors is the same as for the social actors. ==== Organizational appropriateness ==== The language used is appropriate for the organizational context: e.g. that the language is standardized within the organization: or that it is supported by tools that are chosen as standard in the organization. == See also == == References == == Further reading == John Krogstie (2003) ""Evaluating UML using a generic quality framework"" . SINTEF Telecom and Informatics and IDI: NTNU: Norway Krogstie and Sølvsberg (2003). Information Systems Engineering: Conceptual Modeling in a Quality Perspective. Institute of computer and information sciences.\\ Anna Gunhild Nysetvold and John Krogstie (2005). ""Assessing business processing modeling languages using a generic quality framework"". Institute of computer and information sciences. == External links == Fundamental Modeling Concepts Software Modeling Languages Portal BIP -- Incremental Component-based Construction of Real-time Systems Gellish Formal English\nA neural network: also called a neuronal network: is an interconnected population of neurons (typically containing multiple neural circuits). Biological neural networks are studied to understand the organization and functioning of nervous systems. Closely related are artificial neural networks: machine learning models inspired by biological neural networks. They consist of artificial neurons: which are mathematical functions that are designed to be analogous to the mechanisms used by': 'original_similarity': 0.7744744068047781: 'slide': 65}"
0.4632668999838643,What are the main groups that UML diagrams can be divided into?,"UML diagrams can be largely divided into three main groups: behavior diagrams, interaction diagrams, and structure diagrams.","{'chunk_text': 'the key tools for behavior modeling is the use-case model: caused by OOSE. Use cases are a way of specifying required usages of a system. Typically: they are used to capture the requirements of a system: that is: what a system is supposed to do. == Diagrams == UML 2 has many types of diagrams: which are divided into two categories. Some types represent structural information: and the rest represent general types of behavior: including a few that represent different aspects of interactions. These diagrams can be categorized hierarchically as shown in the following class diagram: These diagrams may all contain comments or notes explaining usage: constraint: or intent. === Structure diagrams === Structure diagrams represent the static aspects of the system. It emphasizes the things that must be present in the system being modeled. Since structure diagrams represent the structure: they are used extensively in documenting the software architecture of software systems. For example: the component diagram describes how a software system is split up into components and shows the dependencies among these components. === Behavior diagrams === Behavior diagrams represent the dynamic aspect of the system. It emphasizes what must happen in the system being modeled. Since behavior diagrams illustrate the behavior of a system: they are used extensively to describe the functionality of software systems. As an example: the activity diagram describes the business and operational step-by-step activities of the components in a system. ==== Interaction diagrams ==== Interaction diagrams: a subset of behavior diagrams: emphasize the flow of control and data among the things in the system being modeled. For example: the sequence diagram shows how objects communicate with each other regarding a sequence of messages. == Metamodeling == The Object Management Group (OMG) has developed a metamodeling architecture to define the UML: called the Meta-Object Facility. MOF is designed as a four-layered architecture: as shown in the image at right. It provides a meta-meta model at the top: called the M3 layer. This M3-model is the language used by Meta-Object Facility to build metamodels: called M2-models. The most prominent example of a Layer 2 Meta-Object Facility model is the UML metamodel: which describes the UML itself. These M2-models describe elements of the M1-layer: and thus M1-models. These would be: for example: models written in UML. The last layer is the M0-layer or data layer. It is used to describe runtime instances of the system.The meta-model can be extended using a mechanism called stereotyping. This has been criticized as being insufficient/untenable by Brian Henderson-Sellers and Cesar Gonzalez-Perez in ""Uses and Abuses of the Stereotype Mechanism in UML 1.x and 2.0"". == Adoption == Back in 2013 UML has been marketed by OMG for many contexts: but aimed primarily at software development with limited success.It has been treated: at times: as a design silver bullet: which leads to problems. UML misuse includes overuse (designing every part of the system with it: which is unnecessary) and assuming that novices can design with it.It is considered a large language: with many constructs. Some people (including Jacobson) feel that UML\'s size hinders learning (and therefore using) it.MS Visual Studio dropped support for UML in 2016 due to lack of usage.According to Google Trends UML has been on steady decline since 2004. == See also == Applications of UML Business Process Model and Notation (BPMN) C4 model Department of Defense Architecture Framework DOT (graph description language) List of Unified Modeling Language tools MODAF Model-based testing Model-driven engineering Object-oriented role analysis and modeling Process Specification Language Systems Modeling Language (SysML) == References == == Further reading == Ambler: Scott William (2004). The Object Primer: Agile Model Driven Development with UML 2. Cambridge University Press. ISBN 0-521-54018-6. Archived from the original on 31 January 2010. Retrieved 29 April 2006. Chonoles: Michael Jesse; James A. Schardt (2003). UML 2 for Dummies. Wiley Publishing. ISBN 0-7645-2614-6. Fowler: Martin (2004). UML Distilled: A Brief Guide to the Standard Object Modeling Language (3rd ed.). Addison-Wesley. ISBN 0-321-19368-7. Jacobson: Ivar; Grady Booch; James Rumbaugh (1998). The Unified Software Development Process. Addison Wesley Longman. ISBN 0-201-57169-2. Martin: Robert Cecil (2003). UML for Java Programmers. Prentice Hall. ISBN 0-13-142848-9. Noran: Ovidiu S. ""Business Modelling: UML vs. IDEF"" (PDF). Retrieved 14 November 2022. Horst Kargl. ""Interactive UML Metamodel with additional Examples"". Penker: Magnus; Hans-Erik Eriksson (2000). Business Modeling with UML. John Wiley & Sons. ISBN 0-471-29551-5. Douglass: Bruce Powel. ""Bruce Douglass: Real-Time Agile Systems and Software Development"" (web). Retrieved 1 January 2019. Douglass: Bruce (2014). Real-Time UML Workshop 2nd Edition. Newnes. ISBN 978-0-471-29551-8. Douglass: Bruce (2004). Real-Time UML 3rd Edition. Newnes. ISBN 978-0321160768. Douglass: Bruce (2002). Real-Time Design Patterns. Addison-Wesley Professional. ISBN 978-0201699562. Douglass: Bruce (2009). Real-Time Agility. Addison-Wesley Professional. ISBN 978-0321545497. Douglass: Bruce (2010). Design Patterns for Embedded Systems in C. Newnes. ISBN 978-1856177078. == External links == Official website Current Version Specification\nmodel and the set of diagrams of a system. A diagram is a partial graphic representation of a system\'s model. The set of diagrams need not completely cover the model and deleting a diagram does not change the model. The model may also contain documentation that drives the model elements and diagrams (such as written use cases). UML diagrams represent two different views of a system model: Static (or structural) view: emphasizes the static structure of the system using objects: attributes: operations and relationships. It includes class diagrams and composite structure diagrams. Dynamic (or behavioral) view: emphasizes the dynamic behavior of the system by showing collaborations among objects and changes to the internal states of objects. This view includes sequence diagrams: activity diagrams and state machine diagrams.UML models can be exchanged among UML tools by using the XML Metadata Interchange (XMI) format. In UML: one of the': 'original_similarity': 0.8490160397896068: 'slide': 145}","{'chunk_text': 'day: Rumbaugh\'s object-modeling technique (OMT) and Grady Booch\'s method. They were soon assisted in their efforts by Ivar Jacobson: the creator of the object-oriented software engineering (OOSE) method: who joined them at Rational in 1995. === UML 1.x === Under the technical leadership of those three (Rumbaugh: Jacobson and Booch): a consortium called the UML Partners was organized in 1996 to complete the Unified Modeling Language (UML) specification: and propose it to the Object Management Group (OMG) for standardization. The partnership also contained additional interested parties (for example HP: DEC: IBM and Microsoft). The UML Partners\' UML 1.0 draft was proposed to the OMG in January 1997 by the consortium. During the same month the UML Partners formed a group: designed to define the exact meaning of language constructs: chaired by Cris Kobryn and administered by Ed Eykholt: to finalize the specification and integrate it with other standardization efforts. The result of this work: UML 1.1: was submitted to the OMG in August 1997 and adopted by the OMG in November 1997.After the first release a task force was formed to improve the language: which released several minor revisions: 1.3: 1.4: and 1.5.The standards it produced (as well as the original standard) have been noted as being ambiguous and inconsistent. ==== Cardinality notation ==== As with database Chen: Bachman: and ISO ER diagrams: class models are specified to use ""look-across"" cardinalities: even though several authors (Merise: Elmasri & Navathe amongst others) prefer same-side or ""look-here"" for roles and both minimum and maximum cardinalities. Recent researchers (Feinerer: Dullea et al.) have shown that the ""look-across"" technique used by UML and ER diagrams is less effective and less coherent when applied to n-ary relationships of order strictly greater than 2. Feinerer says: ""Problems arise if we operate under the look-across semantics as used for UML associations. Hartmann investigates this situation and shows how and why different transformations fail."": and: ""As we will see on the next few pages: the look-across interpretation introduces several difficulties which prevent the extension of simple mechanisms from binary to n-ary associations."" === UML 2 === UML 2.0 major revision replaced version 1.5 in 2005: which was developed with an enlarged consortium to improve the language further to reflect new experience on usage of its features.Although UML 2.1 was never released as a formal specification: versions 2.1.1 and 2.1.2 appeared in 2007: followed by UML 2.2 in February 2009. UML 2.3 was formally released in May 2010. UML 2.4.1 was formally released in August 2011. UML 2.5 was released in October 2012 as an ""In progress"" version and was officially released in June 2015. Formal version 2.5.1 was adopted in December 2017.There are four parts to the UML 2.x specification: The Superstructure that defines the notation and semantics for diagrams and their model elements The Infrastructure that defines the core metamodel on which the Superstructure is based The Object Constraint Language (OCL) for defining rules for model elements The UML Diagram Interchange that defines how UML 2 diagram layouts are exchangedUntil UML 2.4.1: the latest versions of these standards were: UML Superstructure version 2.4.1 UML Infrastructure version 2.4.1 OCL version 2.3.1 UML Diagram Interchange version 1.0.Since version 2.5: the UML Specification has been simplified (without Superstructure and Infrastructure): and the latest versions of these standards are now: UML Specification 2.5.1 OCL version 2.4It continues to be updated and improved by the revision task force: who resolve any issues with the language. == Design == UML offers a way to visualize a system\'s architectural blueprints in a diagram: including elements such as: any activities (jobs); individual components of the system; and how they can interact with other software components; how the system will run; how entities interact with others (components and interfaces); external user interface.Although originally intended for object-oriented design documentation: UML has been extended to a larger set of design documentation (as listed above): and been found useful in many contexts. === Software development methods === UML is not a development method by itself; however: it was designed to be compatible with the leading object-oriented software development methods of its time: for example OMT: Booch method: Objectory and especially RUP that it was originally intended to be used with when work began at Rational Software. === Modeling === It is important to distinguish between the UML\nmodel and the set of diagrams of a system. A diagram is a partial graphic representation of a system\'s model. The set of diagrams need not completely cover the model and deleting a diagram does not change the model. The model may also contain documentation that drives the model elements and diagrams (such as written use cases). UML diagrams represent two different views of a system model: Static (or structural) view: emphasizes the static structure of the system using objects: attributes: operations and relationships. It includes class diagrams and composite structure diagrams. Dynamic (or behavioral) view: emphasizes the dynamic behavior of the system by showing collaborations among objects and changes to the internal states of objects. This view includes sequence diagrams: activity diagrams and state machine diagrams.UML models can be exchanged among UML tools by using the XML Metadata Interchange (XMI) format. In UML: one of the key tools for behavior modeling is the use-case model: caused by OOSE. Use cases are a way of specifying required usages of a system. Typically: they are used to capture the requirements of a system: that is: what a system is supposed to do. == Diagrams == UML 2 has many types of diagrams: which are divided into two categories. Some types represent structural information: and the rest represent general types of behavior: including a few that represent different aspects of interactions. These diagrams can be categorized hierarchically as shown in the following class diagram: These diagrams may all contain comments or notes explaining usage: constraint: or intent. === Structure diagrams === Structure diagrams represent the static aspects of the system. It emphasizes the things that must be present in the system being modeled. Since structure': 'original_similarity': 0.846782967714564: 'slide': 281}","{'chunk_text': ""==== Behavioral ==== Behavioral languages are designed to describe the observable behavior of complex systems consisting of components that execute concurrently. These languages focus on the description of key concepts such as: concurrency: nondeterminism: synchronization: and communication. The semantic foundations of Behavioral languages are process calculus or process algebra. ==== Discipline-specific ==== A discipline-specific modeling (DspM) language is focused on deliverables affiliated with a specific software development life cycle stage. Therefore: such language offers a distinct vocabulary: syntax: and notation for each stage: such as discovery: analysis: design: architecture: contraction: etc. For example: for the analysis phase of a project: the modeler employs specific analysis notation to deliver an analysis proposition diagram. During the design phase: however: logical design notation is used to depict relationship between software entities. In addition: the discipline-specific modeling language best practices does not preclude practitioners from combining the various notations in a single diagram. ==== Domain-specific ==== Domain-specific modeling (DSM) is a software engineering methodology for designing and developing systems: most often IT systems such as computer software. It involves systematic use of a graphical domain-specific language (DSL) to represent the various facets of a system. DSM languages tend to support higher-level abstractions than General-purpose modeling languages: so they require less effort and fewer low-level details to specify a given system. ==== Framework-specific ==== A framework-specific modeling language (FSML) is a kind of domain-specific modeling language which is designed for an object-oriented application framework. FSMLs define framework-provided abstractions as FSML concepts and decompose the abstractions into features. The features represent implementation steps or choices. A FSML concept can be configured by selecting features and providing values for features. Such a concept configuration represents how the concept should be implemented in the code. In other words: concept configuration describes how the framework should be completed in order to create the implementation of the concept. ==== Information and knowledge modeling ==== Linked data and ontology engineering require 'host languages' to represent entities and the relations between them: constraints between the properties of entities and relations: and metadata attributes. JSON-LD and RDF are two major (and semantically almost equivalent) languages in this context: primarily because they support statement reification and contextualisation which are essential properties to support the higher-order logic needed to reason about models. Model transformation is a common example of such reasoning. ==== Object-oriented ==== Object modeling languages are modeling languages based on a standardized set of symbols and ways of arranging them to model (part of) an object oriented software design or system design. Some organizations use them extensively in combination with a software development methodology to progress from initial specification to an implementation plan and to communicate that plan to an entire team of developers and stakeholders. Because a modeling language is visual and at a higher-level of abstraction than code: using models encourages the generation of a shared vision that may prevent problems of differing interpretation later in development. Often software modeling tools are used to construct these models: which may then be capable of automatic translation to code. ==== Virtual reality ==== Virtual Reality Modeling Language (VRML): before 1995 known as the Virtual Reality Markup Language is a standard file format for representing 3-dimensional (3D) interactive vector graphics: designed particularly with the World Wide Web in mind. ==== Others ==== Architecture Description Language Face Modeling Language Generative Modelling Language Java Modeling Language Promela Rebeca Modeling Language Service Modeling Language Web Services Modeling Language X3D == Applications == Various kinds of modeling languages are applied in different disciplines: including computer science: information management: business process modeling: software engineering: and systems engineering. Modeling languages can be used to specify: system requirements: structures and behaviors.Modeling languages are intended to be used to precisely specify systems so that stakeholders (e.g.: customers: operators: analysts: designers) can better understand the system being modeled. The more mature modeling languages are precise: consistent and executable. Informal diagramming techniques applied with drawing tools are expected to produce useful pictorial representations of system requirements: structures and behaviors: which can be useful for communication: design: and problem solving but cannot be used programmatically.: 539 Executable modeling languages applied with proper tool support: however: are expected to automate system verification and validation: simulation and code generation from the same representations. == Quality == A review of modelling languages is essential to be able to assign which languages are appropriate for different modelling settings. In the term settings we include stakeholders: domain and the knowledge connected. Assessing the language quality is a means that aims to achieve better models. === Framework for evaluation === Here language quality is stated in accordance with the SEQUAL framework for quality of models developed by Krogstie: Sindre and Lindland (2003): since this is a framework that connects the language quality to a framework for general model\nquality. Five areas are used in this framework to describe language quality and these are supposed to express both the conceptual as well as the visual notation of the language. We will not go into a thoroughly explanation of the underlying quality framework of models but concentrate on the areas used to explain the language quality framework. ==== Domain appropriateness ==== The framework states the ability to represent the domain as domain appropriateness. The statement appropriateness can be a bit vague: but in this particular context it means able to express. You should ideally only be able to express things that are in the domain but be powerful enough to include everything that is in the domain. This requirement might seem a bit strict: but the aim is to get a visually expressed model which includes everything relevant to the domain and excludes everything not appropriate for the domain. To achieve this: the language has to have a good distinction of which notations and syntaxes that are advantageous to present. ==== Participant appropriateness ==== To evaluate the participant appropriateness we try to identify how well the language expresses the knowledge held by the stakeholders. This involves challenges since a stakeholder's knowledge is subjective. The"": 'original_similarity': 0.7791778964326107: 'slide': 202}"
