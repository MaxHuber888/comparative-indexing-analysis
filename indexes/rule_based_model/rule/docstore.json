{"docstore/metadata": {"5b373bb3-a9ab-4c60-a7e3-7173dc51ff40": {"doc_hash": "342c6e0c57269c37b59cf8f191dbdc0690bd44452440e0b809e50e03abf70be5"}, "2bb55be0-1379-4a71-99be-3263be74fa83": {"doc_hash": "af3fe124075537122e8d602eea81932850f37c58dafa3dd995e58e4b72790ef1"}, "9e6898af-c176-436b-8a3e-151be88edbcf": {"doc_hash": "4aec30877d77ca8da7d1716b8a9674d9e8bf1fe226031949525c7d0178428579"}, "d5eaea50-ecd2-4c57-85d7-276f893e80e9": {"doc_hash": "6973e2fe4d2dfba16517668232875125896ba07026f38e42c55fabac533ca654"}, "ddfbb310-f268-43fc-b55c-2576fb322b7c": {"doc_hash": "e7d513908c396612dce2e17a696215067d905c10d21294966b9b16bb6b408821"}, "65fe6622-9e19-4c31-912b-a1676a217719": {"doc_hash": "31edaf0411c36a59c41ae21354f4047f70757c774e9eafd489e202e1df78e65e"}, "ffb0bcfe-5aba-4c18-9723-b82a7d2219a0": {"doc_hash": "ea9e4ea0c041aa25b372c7cdd793cded3f71663ae1b7fdaff19aa17921d30ab8"}, "bde9526a-8edb-41c5-b464-118aec879b48": {"doc_hash": "46c5bee80999c453527ccbfaf53ba555980ac5732442dfd658da24d93d39185a"}, "bceab45b-1acb-44b4-8433-ff78ba533ae9": {"doc_hash": "9ea03d7071b7f6b676c241f81cc02216f81ba99c552caf4b89c2fc9a4bc76e45"}, "142e908e-5e0d-4a4f-b83b-48b2efd56aab": {"doc_hash": "a9d0ef342c9898053db15af61cfd484eff8231f28a15548ee4af2c8af71b6ca9"}, "1a1d4c52-08d8-4f72-a6e8-e70208ed030e": {"doc_hash": "7ea73bd1f6dd068b36c7282f33047dcca146fdefe3836b129dcb8cb0da39fe3c"}, "2a159bcb-23ad-4890-bd3e-8c468e4d113d": {"doc_hash": "d53802146e3714daade4a189ecb3aa282ab4ff70e6b786b4e6680218e2fac290"}, "45a7af36-d76d-4c97-bd71-02a965a1a62c": {"doc_hash": "2bc1a5dc16ed20f96e3d969d83d6f7a8a6dbcbc130015f451af56093fd12adf3"}, "08db6b24-b93e-40bb-8bc4-355238604f5a": {"doc_hash": "32763edc3605646ddd5748404e08034d92f094732e3c86b45202d51f7205e731"}, "b6d044df-fa62-42ce-9a08-dd7e535d47c4": {"doc_hash": "2d8f9779951f0c19a17511035efd4f237df80fe1cb343506834e21fc58e84ac9"}, "ebe409b7-516e-4dac-aa8f-60c282a209da": {"doc_hash": "c8db0d2eb49f36aef1f5ada4fd9779850799eee4005db290bc86d84d10c83b3c"}, "7a67ba02-86ba-4e06-8582-0b6f20c739d5": {"doc_hash": "13b4017e744cf09d04ed2d04485e0b6f30e7f2d5328eae17f98eeceafeb7392d"}, "95d73cd8-87d1-4e32-b31f-76e61e9440df": {"doc_hash": "915db8c9186faad254a05533f81a57f63a88e970749f0bc3f8e3ba866592ab07"}, "3ac1d604-830e-41b8-a9a8-de008f222fb1": {"doc_hash": "480165125e0c224c2cba9eefb72a335de46e97a0056c67c49a064af6dd915fa2"}, "b3dd0d56-5656-4ec4-898f-0583c55fee61": {"doc_hash": "d85412bd6c5c08f1070a897b0d7b5df980bd11a5f1f275b7831f4434f80a9ce4"}, "8723d808-8df2-4c09-85af-69e73ce0845d": {"doc_hash": "6961ef56d143fe582cab4b3b962043c943f1411eb0eb44b998d75ee8e3bc5a10"}, "ce81106e-199e-40e5-9c27-b066438275c8": {"doc_hash": "48248a05e0c85a7f679f36f1a10f2df6a636967b2592335e4a774027fb7f8429"}, "b5ae85da-3fb2-408e-b8a5-2dfb8d152245": {"doc_hash": "5272016b971be97a3ddd2f65be8e349029d512006a61be48203d8aa214b84cc2"}, "9ccd033a-d520-40e3-9826-a461456a44c3": {"doc_hash": "ed9303c22ea9b950b95f3de52b9a752e0b4cea7e7aeed756c7338f1b3af8dd9b"}, "cb6471bf-2893-4828-b5b5-0d44aa11201d": {"doc_hash": "eec251e99ecb878c757de3abb5b6e769ec64078fe7d70e72c33607a29a5d67da"}, "72e5032e-1334-4ccc-8191-84c5c54f0619": {"doc_hash": "d2b0c26c183cf943f2761114c363ab3a620056196826ab86518bb2ecaecf31f5"}, "cf3ccf7b-1635-46d8-939e-567bb626882a": {"doc_hash": "3273124fc155f541000afbb8d7cf00c92bb60b9720c7af7b04080c64a2755252"}, "445f09fa-ef83-4d20-bc51-e3c582e9f4ab": {"doc_hash": "184ef86967fab077506da0b3d31444dc594fa477a8104708f0409f1039a4e404"}, "bb0d5604-c0f9-4a94-b06f-5075d5cf5210": {"doc_hash": "d277accaa570b90c3c6dfbfddc510a116fa13b6f836e4a224859f2f8403fea65"}, "37f790c9-29da-4d45-bab5-71dc6795e41e": {"doc_hash": "1bbb4ba053b19b112181ae724233ae3673e97b226b2896a3461e039b1f1bd9ba"}, "f6603d07-509e-4575-89c4-bccf2206fc47": {"doc_hash": "9def6e725c3417b86603ba6cb3cbf7578ee6ebebfd543c80c33a64d4c146fbc6", "ref_doc_id": "5b373bb3-a9ab-4c60-a7e3-7173dc51ff40"}, "63dab1e2-42a3-4eb3-bb6c-57f3319ccb11": {"doc_hash": "ad97031057d660ddb17ca27838273e68c290858c18b032c7e4ea9244fe945721", "ref_doc_id": "5b373bb3-a9ab-4c60-a7e3-7173dc51ff40"}, "ab4ab5a1-8ef3-4081-8845-a0329d1f315e": {"doc_hash": "d56e81c2f5f70d41ea5fb643a9acca5367b89ec194fa0d80788f5fea15151479", "ref_doc_id": "5b373bb3-a9ab-4c60-a7e3-7173dc51ff40"}, "c89e3b13-2c6d-42c4-8843-132a050766a5": {"doc_hash": "946e37652d8f2af2c88ec533b6b3de226153aa24a049ec88991f1ea28873085a", "ref_doc_id": "5b373bb3-a9ab-4c60-a7e3-7173dc51ff40"}, "69e369bd-aace-44c5-8f38-46c7513b1343": {"doc_hash": "c0a860bdbe055ded6159fcd5a4ab395776c41d5e825ba8a8ff2144a083c4cfc3", "ref_doc_id": "2bb55be0-1379-4a71-99be-3263be74fa83"}, "018fb6c0-3406-4546-bac9-d817682f16f3": {"doc_hash": "4d41fe49a327a0771b0035be3553cc2223264f02ce19722e40c56402defd408f", "ref_doc_id": "2bb55be0-1379-4a71-99be-3263be74fa83"}, "cd292843-4843-4185-925f-64d9380d9246": {"doc_hash": "f908a650376982bff6c2b89bb222bab018e03d1b2ba1036148bd3870e63e9247", "ref_doc_id": "2bb55be0-1379-4a71-99be-3263be74fa83"}, "bb4bc2b4-7bdb-4111-b4d6-78c30911d723": {"doc_hash": "4aec30877d77ca8da7d1716b8a9674d9e8bf1fe226031949525c7d0178428579", "ref_doc_id": "9e6898af-c176-436b-8a3e-151be88edbcf"}, "a21b3823-2625-44e8-8214-131b22a77683": {"doc_hash": "6973e2fe4d2dfba16517668232875125896ba07026f38e42c55fabac533ca654", "ref_doc_id": "d5eaea50-ecd2-4c57-85d7-276f893e80e9"}, "9e6d6aaf-f4e4-43b6-93de-c206c324a10d": {"doc_hash": "e7d513908c396612dce2e17a696215067d905c10d21294966b9b16bb6b408821", "ref_doc_id": "ddfbb310-f268-43fc-b55c-2576fb322b7c"}, "bbfb1614-1d95-474b-8672-b6909c2f58f6": {"doc_hash": "a08f6dc195c3fd117f7575555f148b0ce89f3eb22736ef6e92ac49164ed60846", "ref_doc_id": "65fe6622-9e19-4c31-912b-a1676a217719"}, "22f2b7f2-67e1-4a70-8d7a-f4a3932826d9": {"doc_hash": "f1791e1a52ebef1d26194181c2d00b24e4f0cf41891fa36830df820aa19043b6", "ref_doc_id": "65fe6622-9e19-4c31-912b-a1676a217719"}, "a573ad77-b1e0-48e9-94fc-d40d5abdd3e7": {"doc_hash": "eaea15c52760d2e53d9b0da0d6416f600cf438f709fd115e80a0d5341d70ff1b", "ref_doc_id": "65fe6622-9e19-4c31-912b-a1676a217719"}, "4b340d61-1a2a-48da-aab9-f429ed28aa5a": {"doc_hash": "a79c39e6e42f9937848109c078fa1ca57ac4aff34b0ad0a1443f6d9b7fcb7049", "ref_doc_id": "65fe6622-9e19-4c31-912b-a1676a217719"}, "0f387ccc-464e-46c4-9703-dadf47ad89b6": {"doc_hash": "9552455aea6655e861140404a4d52f84c7bb6248d95c863cea39fbc4c0b4e9c8", "ref_doc_id": "65fe6622-9e19-4c31-912b-a1676a217719"}, "724b2112-4bba-4b29-9023-e6ef6755e5ce": {"doc_hash": "1438726e91d1dbd0f2e33f247d360e14c1cd9c0a9e49e0f30ef6e2cf98d80481", "ref_doc_id": "65fe6622-9e19-4c31-912b-a1676a217719"}, "9b961931-c0b6-4ff5-99db-198244be0ee5": {"doc_hash": "fa14cc168dc454c159c3995a6438b2b013657b123585d5824722a9362f47b33c", "ref_doc_id": "65fe6622-9e19-4c31-912b-a1676a217719"}, "ab5cbcc7-f164-476c-80ed-9e0f2fcc8c12": {"doc_hash": "7c52e375f6e1762b5d97d4ec2e66a1300b31931dbce9627b8406ce1898edb6c2", "ref_doc_id": "65fe6622-9e19-4c31-912b-a1676a217719"}, "ce9ea363-3406-43bf-94ee-ae5f55ff43b9": {"doc_hash": "f3a2533453bff0285ee100dbfd87c00044a3b90e9302fa4ee5c1b340d999a853", "ref_doc_id": "65fe6622-9e19-4c31-912b-a1676a217719"}, "6b6f2954-0cb0-4cd8-a915-92b11769eb07": {"doc_hash": "76e7411204c11f3af2dbe8b87ead77f2c8b23e8a28318b9bea6fb83e74a83c8b", "ref_doc_id": "65fe6622-9e19-4c31-912b-a1676a217719"}, "f4112708-7739-4316-9865-8f212a7cc78b": {"doc_hash": "8f4372efb12d874512b3fa2288725f1563d79a88af26da2977117bce0c0074ce", "ref_doc_id": "65fe6622-9e19-4c31-912b-a1676a217719"}, "b6835997-6770-420c-9493-970c71189cd4": {"doc_hash": "7799f7f309ebad471333b8ba6eff409e46ba5edd10d5c62ecbf69067245a82c6", "ref_doc_id": "65fe6622-9e19-4c31-912b-a1676a217719"}, "75104176-b291-48ea-a2b8-8140b770c225": {"doc_hash": "281ca34284b85c5de0b7d20ef220ea5880ebeb33a25118516af00ff7a605dd61", "ref_doc_id": "65fe6622-9e19-4c31-912b-a1676a217719"}, "8967e8c2-4793-4e11-821c-7bb603de0868": {"doc_hash": "f9865d0d6a5e596b7fd3af8fe943a2a57dd3718cb5c60e05c9185674cd41061f", "ref_doc_id": "65fe6622-9e19-4c31-912b-a1676a217719"}, "c6a646ee-0c0e-40ca-b662-ec9fc6602de4": {"doc_hash": "fa36ec2a88601e337c8c49923aa4387181f4128c2f73d9965acf9e251f7905a8", "ref_doc_id": "65fe6622-9e19-4c31-912b-a1676a217719"}, "ef8b8a5d-4ad7-423f-9a30-b254427348b5": {"doc_hash": "99a82acb69552f1e2c029222ce24cb19fe4fe77a4b20c68c498027c24bcd8341", "ref_doc_id": "65fe6622-9e19-4c31-912b-a1676a217719"}, "a7c7a032-a671-47d6-8cf3-442865fedbfe": {"doc_hash": "526865068a093896d7b728b881d94c6f92e16bcd1e1969f1f6d3aacc83e55dbd", "ref_doc_id": "ffb0bcfe-5aba-4c18-9723-b82a7d2219a0"}, "01017277-fbf6-4e56-8c0d-cc072f2d4159": {"doc_hash": "829178a6b92b62bc73d0f03f9232c47b573fbb3336c35fb290ff4cb197776bbb", "ref_doc_id": "ffb0bcfe-5aba-4c18-9723-b82a7d2219a0"}, "b65258e9-3101-4364-b643-8c44a4cee41b": {"doc_hash": "3e6bb8eb35fb6f08f1ec9b8f9e628685489dcad0b3329b76bf5c0b1060ef7df2", "ref_doc_id": "ffb0bcfe-5aba-4c18-9723-b82a7d2219a0"}, "d7db71c4-6a6f-411b-aabd-b91fb6e7af8a": {"doc_hash": "551c45bd6aad557a358a62720871c30bac8324e66f3714c961d0584a3714d70d", "ref_doc_id": "ffb0bcfe-5aba-4c18-9723-b82a7d2219a0"}, "23e48601-3899-4045-933d-605792de6c3a": {"doc_hash": "eadc256b249db81faddddca193c4f21bf25f10c58584799d800fa88a808ef821", "ref_doc_id": "ffb0bcfe-5aba-4c18-9723-b82a7d2219a0"}, "a38fabc6-41d4-4f3a-a7d5-e8523f309e87": {"doc_hash": "ef7ca3493836e59b731ef2706cc26a2b6425875c2f1fcd05d93c78fd7d998116", "ref_doc_id": "ffb0bcfe-5aba-4c18-9723-b82a7d2219a0"}, "17ebeed7-747f-4cb5-acd6-b58b73906628": {"doc_hash": "5b112f699873652ffee478f8d0330c0dfe6e2e36ae0b70b7bc405e59322f51ce", "ref_doc_id": "ffb0bcfe-5aba-4c18-9723-b82a7d2219a0"}, "fe3234a9-3da2-41f7-8957-78c1899ca95f": {"doc_hash": "3d23ff70593764666389d2e0b77dd177ffad6dc4e255448fe541b241381198d7", "ref_doc_id": "ffb0bcfe-5aba-4c18-9723-b82a7d2219a0"}, "6339df8d-aef9-4b85-8948-b89a05bf2611": {"doc_hash": "46fa79c9449da5275ea33ddef19ab9caeca1f15eda22d9b7727bc6957873b044", "ref_doc_id": "ffb0bcfe-5aba-4c18-9723-b82a7d2219a0"}, "fc4ab0ea-dfb5-40c0-b7aa-02100c580b72": {"doc_hash": "40eff679e65d3dfdbc4c789abe6da3f977f1c075a7efdabb4c93159daf12c173", "ref_doc_id": "ffb0bcfe-5aba-4c18-9723-b82a7d2219a0"}, "5fa7d90f-7654-41d9-980c-4725812841e7": {"doc_hash": "c03931dba2e5920149121546788021e619f4bdf75795e089c323e54568008aed", "ref_doc_id": "ffb0bcfe-5aba-4c18-9723-b82a7d2219a0"}, "aea60cca-465e-42c2-9fce-ffeaec09a8a8": {"doc_hash": "f216fda405b8bc866f1e25e01478c2e8da4be55c740615fbd1fca68ac6d66e95", "ref_doc_id": "ffb0bcfe-5aba-4c18-9723-b82a7d2219a0"}, "b5d6518e-adad-4314-8ab0-908733ab7041": {"doc_hash": "1f52e188332291fff5ea2ce43037b1cef4dd3697d7df3653d10484ea9f15ff1c", "ref_doc_id": "ffb0bcfe-5aba-4c18-9723-b82a7d2219a0"}, "d640173d-29fb-4b73-bd12-b8a306a1fc88": {"doc_hash": "464184e05584c2fe58de45c9ea9c40dfebeab78c9b9baec93cfeb812df21c602", "ref_doc_id": "ffb0bcfe-5aba-4c18-9723-b82a7d2219a0"}, "e9df0acf-8aa1-4692-9705-936f8f312670": {"doc_hash": "fa9e4abf33d4432fd5bc4d124fe10d7d3ca9ba0eee7982c2e05200195b15e6c3", "ref_doc_id": "ffb0bcfe-5aba-4c18-9723-b82a7d2219a0"}, "e3461faa-e9b6-4d6a-8e7b-b9b6b70ac760": {"doc_hash": "0489ef98981d11b1006ad3aadc0a683098f967e9e0fc40d4907fd7dd10703852", "ref_doc_id": "bde9526a-8edb-41c5-b464-118aec879b48"}, "1b28a924-ee18-45d9-88d6-02c825d938b6": {"doc_hash": "95656ef41d1b9c3210aa1fe65db18dc76156141cf393d13a26574ef3b28dd727", "ref_doc_id": "bde9526a-8edb-41c5-b464-118aec879b48"}, "da1a514a-3dfd-485e-9c48-1c2a3b3ba794": {"doc_hash": "a5a62f454e8590ce32be18a7e2e8eb8252df9abbbe1ef9a06bc01dc3c9e37164", "ref_doc_id": "bceab45b-1acb-44b4-8433-ff78ba533ae9"}, "e0e51bb9-edad-41f5-adf9-bf87c2553d41": {"doc_hash": "e76e72cb9a2b9e8502d9aca9e18ae64c6dd5013a5094a74c30eddcecc9c7e25a", "ref_doc_id": "bceab45b-1acb-44b4-8433-ff78ba533ae9"}, "f7efb63a-737a-4718-bdcf-239637fd47ca": {"doc_hash": "b9b1b140c2b954ce747b9938a809075ccf72538eb9639353f104b713bb3a37f0", "ref_doc_id": "bceab45b-1acb-44b4-8433-ff78ba533ae9"}, "2317dda9-0b15-4b00-b3e1-2408c656da63": {"doc_hash": "8d4ee8a94562194aa16ba99c98e5dbbe9c7e561af4175d81b141ccec3504837b", "ref_doc_id": "142e908e-5e0d-4a4f-b83b-48b2efd56aab"}, "01835aca-d678-457b-beea-cb21d4b03af1": {"doc_hash": "f3bc27263e27e5e0005e03a7af40d2f0ce3c0353ea29b6034a3cde8d7ede7a1d", "ref_doc_id": "142e908e-5e0d-4a4f-b83b-48b2efd56aab"}, "91a97cc5-6178-48ba-b244-f3cbd0af1b40": {"doc_hash": "5dc6eeebbf3de267d1b7caec19354836e598c3e4eff4a78fcc741486deb69b3c", "ref_doc_id": "142e908e-5e0d-4a4f-b83b-48b2efd56aab"}, "8c57d7c4-866f-4231-90a7-588b76aef41c": {"doc_hash": "2e89b90fd83063ba9fa956e0746e01c862d3022d7e11a55475bdcc5b68a5fbcc", "ref_doc_id": "1a1d4c52-08d8-4f72-a6e8-e70208ed030e"}, "0dd2c726-cf32-42d1-b131-dd77071d95f2": {"doc_hash": "a3fd847fa66c40ce58411e83b64459c64ceb47106e579900ca63e6bda0bf6339", "ref_doc_id": "1a1d4c52-08d8-4f72-a6e8-e70208ed030e"}, "6fe21eeb-0445-446f-84c2-5f96339864fe": {"doc_hash": "61fabee77d8f693c2ef674b52ff289b03b8d0df36668e0ee33442a8939c6d052", "ref_doc_id": "1a1d4c52-08d8-4f72-a6e8-e70208ed030e"}, "6d5aa1f4-3297-4551-a9bc-bf8027bc90aa": {"doc_hash": "53c4b8c2d3e8e4bfe5db72021e4579b402d1ec5715a05af9e1790b6775e6026c", "ref_doc_id": "1a1d4c52-08d8-4f72-a6e8-e70208ed030e"}, "c66c9279-119e-4954-8e9e-a7393aa10742": {"doc_hash": "38b303d28990ee49fc964e88b7fe0dd6deebb39e961a5a32227ac3dd5955b3ab", "ref_doc_id": "1a1d4c52-08d8-4f72-a6e8-e70208ed030e"}, "b8de6e8a-2d14-47b7-88c1-ef2b3b33fcb7": {"doc_hash": "4de7f28ea826c9f2ea3bd18b933efe547b14c8881f2c359b3b179c3eb596078f", "ref_doc_id": "1a1d4c52-08d8-4f72-a6e8-e70208ed030e"}, "db7d8046-1f87-46b3-8af9-a52581308b68": {"doc_hash": "15f4debe45fd2228cc48b0239feae968efe9d96f9e89599f85238acab6e73ceb", "ref_doc_id": "1a1d4c52-08d8-4f72-a6e8-e70208ed030e"}, "c5d9510f-9b4e-4aac-a106-be3d6cfc6158": {"doc_hash": "b5411a2985b853a0a53a5ff703b005ac54a3313f7204e4c9216929d9a04ef2bc", "ref_doc_id": "1a1d4c52-08d8-4f72-a6e8-e70208ed030e"}, "dcc1f2fc-613c-4f2a-988a-835f97b475c2": {"doc_hash": "c9d48d265dad99cb9456e09768325608d5a050b026a4c4bb82e41cd9dca9cd1c", "ref_doc_id": "1a1d4c52-08d8-4f72-a6e8-e70208ed030e"}, "38e3d038-36b1-48ed-b79b-21271ac6517e": {"doc_hash": "685213bb66d4c5e999b70b5fc58329c4788de3271155fa2b26e187f51af3d353", "ref_doc_id": "1a1d4c52-08d8-4f72-a6e8-e70208ed030e"}, "da474919-403b-488e-8482-f2b19bba8b33": {"doc_hash": "1f4487a565b4e2c6cab044eb254d41cc582d4413040adbc38f7613d9ccdb6aca", "ref_doc_id": "1a1d4c52-08d8-4f72-a6e8-e70208ed030e"}, "7655f3a0-21e5-466e-a93a-fb5ed4791b0c": {"doc_hash": "ca9e8c254f6a964e6f27e56a79a0c510045f790a86807148eb72024da56dabac", "ref_doc_id": "1a1d4c52-08d8-4f72-a6e8-e70208ed030e"}, "704b76fb-1c25-4c2b-9a52-9b54acddd768": {"doc_hash": "aad935c2b6d5a32fdbb2ff10b072bafc54a0b89d083d29ac28f6641e10a5c9d7", "ref_doc_id": "1a1d4c52-08d8-4f72-a6e8-e70208ed030e"}, "03bbdcab-f1ff-43e5-9ae6-70d056230b19": {"doc_hash": "e4443b1e1ccf6f02e1ef7ca80c7651f95fb3beec8301ca3057f71f6ce9efc48e", "ref_doc_id": "1a1d4c52-08d8-4f72-a6e8-e70208ed030e"}, "cf76431f-351a-4322-b7bb-c47aeee0a607": {"doc_hash": "e43b8fb36ce622a84b441ce2322c4cef902810267792919511326c99587f3888", "ref_doc_id": "1a1d4c52-08d8-4f72-a6e8-e70208ed030e"}, "33e7eb09-0bd0-4c61-b41c-5b97383623fd": {"doc_hash": "b7c80868ef6fcec06aadb878e15026dadfb570f2ad117b3eb1d6ede77b4f0c6f", "ref_doc_id": "2a159bcb-23ad-4890-bd3e-8c468e4d113d"}, "7fa895fa-fe1d-426b-b8b6-2816ad3e0fbd": {"doc_hash": "419cc65ed57ff6bddaef88dbde10a911ac06fa23af876d1f427f2fce97bc27d0", "ref_doc_id": "2a159bcb-23ad-4890-bd3e-8c468e4d113d"}, "9341a911-be4f-49e3-abfc-6c0ea4b86498": {"doc_hash": "e8d6ed1556870e156c24e2cebc73f99ecb5c0e710cb17dee53e82bd924e3d10d", "ref_doc_id": "2a159bcb-23ad-4890-bd3e-8c468e4d113d"}, "5a310d23-81e9-4909-a0be-c9ef7a85d856": {"doc_hash": "595a240a5528fe8b78dd17e0c0deda5fa67b78c292b0b01e9501ee860fc868fc", "ref_doc_id": "2a159bcb-23ad-4890-bd3e-8c468e4d113d"}, "7f67bb38-bd16-4aa1-98c1-8ad6b1a71e93": {"doc_hash": "8894466bbe866054e17c251a914d93cb8904c62b6ef16b5c797e181d0b688225", "ref_doc_id": "45a7af36-d76d-4c97-bd71-02a965a1a62c"}, "36b241ff-a8d1-4b1c-b5e3-9ed98e29f2e3": {"doc_hash": "aabb1aef7aa4235ed6844e18f661af271f961de9c1a791f1fd7585bc5813c7c4", "ref_doc_id": "45a7af36-d76d-4c97-bd71-02a965a1a62c"}, "2f4c12dd-969f-426e-aab7-bc08d64d9783": {"doc_hash": "1e400ef7dd05098e35803ce565e1a09204188c610564c7e7ecf765ea591ab35a", "ref_doc_id": "45a7af36-d76d-4c97-bd71-02a965a1a62c"}, "e6c6979e-ff99-4310-8fb4-1cf2250ee34e": {"doc_hash": "a467d85a8cc845afd680b9df22c54230553246a9ca3b98c419752222a79bc8d1", "ref_doc_id": "45a7af36-d76d-4c97-bd71-02a965a1a62c"}, "c0b7bd4d-34d3-492d-8a74-5d0df135efc8": {"doc_hash": "83bbee19304571a76a23e69cd2d7e5d923edcc302fbd27642ae3761847df6835", "ref_doc_id": "45a7af36-d76d-4c97-bd71-02a965a1a62c"}, "a8232541-5d5d-42e0-bc08-316615718517": {"doc_hash": "3e148e9096bd077137a84edd1fea49e37692f87b56b934065f35ed2e28d7d0cb", "ref_doc_id": "45a7af36-d76d-4c97-bd71-02a965a1a62c"}, "241c4b6f-3158-40f5-bca3-709d04e41d88": {"doc_hash": "17ead80a45975d4f6aa4ee37e3053a61c6c8320f8680ace462c0980562880ae0", "ref_doc_id": "08db6b24-b93e-40bb-8bc4-355238604f5a"}, "a33ef89f-f9f5-4f21-9691-ebed7e5d9084": {"doc_hash": "99f8ce66d0259ad81cec26c9d2be93aed1da64365adcf3c152c43c476f6d4534", "ref_doc_id": "08db6b24-b93e-40bb-8bc4-355238604f5a"}, "e40b6fbf-d71f-4dff-a596-ca62afb71b14": {"doc_hash": "43fc18ad207d9097f4f18661fad6db24fcd11dea8df80f50ed031b72fe3d36c2", "ref_doc_id": "08db6b24-b93e-40bb-8bc4-355238604f5a"}, "6bb52853-a030-41ce-96a1-9dcf8439288a": {"doc_hash": "482be9a7899af2825f79d0726df65ac134500bb645f0bd42fa36113dab5986f5", "ref_doc_id": "08db6b24-b93e-40bb-8bc4-355238604f5a"}, "d12ee3f5-f57e-4046-ba66-5e25f2f4d9fd": {"doc_hash": "b770a7cebcf2020a167e7231cbf35795a785ebe4ba39f829c7f75ad3e32a7c45", "ref_doc_id": "08db6b24-b93e-40bb-8bc4-355238604f5a"}, "104b72a0-ad9a-4f2d-9e0e-3566926aacac": {"doc_hash": "14c67adfb0315780f6ca4e66781857b952a8318f34b9732608726e142f0fc3ae", "ref_doc_id": "08db6b24-b93e-40bb-8bc4-355238604f5a"}, "a1303869-5250-4660-8a0a-49e048b24471": {"doc_hash": "3b27fbd9839450dd7c25ceaddd75a59d1d815edd3f50d2004e18aa37b3d48cb7", "ref_doc_id": "b6d044df-fa62-42ce-9a08-dd7e535d47c4"}, "a9f464e4-cc53-43d3-a403-cdee2b520204": {"doc_hash": "4c9c4ff41fe518bb6bfff33ac2d3b97be97d5ef523968b5ef20ae6e2a0c59cd4", "ref_doc_id": "b6d044df-fa62-42ce-9a08-dd7e535d47c4"}, "851ec5d2-4952-4ecf-afee-ef5cdd8b545f": {"doc_hash": "005703b35f54889a4a0e65a82a72f7c9441165518cd4642fad7e0d655a2c0fe4", "ref_doc_id": "b6d044df-fa62-42ce-9a08-dd7e535d47c4"}, "bcfbcfda-25ca-4286-96af-9a6162b32ce6": {"doc_hash": "38c1b2aa2cb10f5bbf3573b3a9fbe2331a58745f99a916397bf92c69cddb3989", "ref_doc_id": "b6d044df-fa62-42ce-9a08-dd7e535d47c4"}, "bd2c7395-c9fe-4151-a0d3-4a4cec6c808d": {"doc_hash": "b95d6b6eb870905a6abf309ab019cc9712d276f438de6691ee79487738572c85", "ref_doc_id": "b6d044df-fa62-42ce-9a08-dd7e535d47c4"}, "5a14c2a1-c786-460a-9629-e197a849150a": {"doc_hash": "eb3ec33a8ec2c384dd705e72e470be350d4e6187f3f758bd8139ac03bdff1a06", "ref_doc_id": "b6d044df-fa62-42ce-9a08-dd7e535d47c4"}, "c5e8606d-dd66-42c7-808a-3a794a97ca8d": {"doc_hash": "4b5874f731c3faddfd0861d7129714349793e8f560d38644d4a1a9b10b66d8be", "ref_doc_id": "b6d044df-fa62-42ce-9a08-dd7e535d47c4"}, "d636e607-8626-4a08-9b76-534b6aeb32a6": {"doc_hash": "82e8d0e80047166135a2c4c892c2cd82fcab5bda6ad362e4ab6af38290dddb7b", "ref_doc_id": "b6d044df-fa62-42ce-9a08-dd7e535d47c4"}, "4d20d041-0bde-4d51-9c99-9037511755eb": {"doc_hash": "5f17bfed3916880ded70579bbed25c5dd9c7256d3cdad466fcd58c3ef154805a", "ref_doc_id": "b6d044df-fa62-42ce-9a08-dd7e535d47c4"}, "6cec6a3d-cb13-42f3-b385-a7d463bc9e39": {"doc_hash": "0a81be320b153597ca861ae7204200d532ca3ad69cf03c82c1950753c78104a8", "ref_doc_id": "b6d044df-fa62-42ce-9a08-dd7e535d47c4"}, "9df1a7c6-4710-4278-8eeb-10e170eb926b": {"doc_hash": "5c78eccc1f957cbb250c18b6e40ee3b686bb0d9bdbec960f6686da35d1f53a07", "ref_doc_id": "b6d044df-fa62-42ce-9a08-dd7e535d47c4"}, "66b3d6cf-184f-4521-8fee-73874b0cf634": {"doc_hash": "31e4edfc07f2b5af181bd132d0e0e341af3bbd5d4e4d08b3442bfa168e2f26f1", "ref_doc_id": "ebe409b7-516e-4dac-aa8f-60c282a209da"}, "81833af5-dcc8-4f6d-9988-898c5fe855c3": {"doc_hash": "abacc1f5e508e22100d46b667db3d24be6c3903596755897055a47334e6563ce", "ref_doc_id": "ebe409b7-516e-4dac-aa8f-60c282a209da"}, "3ddd7211-4b50-4aef-9499-faf9dce24e85": {"doc_hash": "fde007dd55df0667a1c3dffc325b1717023f96db99b97229dc6080353d6f4056", "ref_doc_id": "ebe409b7-516e-4dac-aa8f-60c282a209da"}, "5584ae86-cdfe-4802-8a58-00239a12b1f7": {"doc_hash": "2ac87859432a504827a7bb26332eca3a28a0274563cc3cef2cb97320c01c29b4", "ref_doc_id": "ebe409b7-516e-4dac-aa8f-60c282a209da"}, "38772f86-2ba7-40df-89bc-16dce75ac534": {"doc_hash": "2b68b90236de253ef83f89cf376a20fa79589770736df0d53f10f2eb59b44467", "ref_doc_id": "ebe409b7-516e-4dac-aa8f-60c282a209da"}, "394e7e43-4f21-4085-a0a9-edaa587483ba": {"doc_hash": "2afe8a237df6b9c7bda5e6c2c998d71fa62dee96efa4123fc76ab2951575ccbf", "ref_doc_id": "ebe409b7-516e-4dac-aa8f-60c282a209da"}, "1723ce2c-f8ca-4a28-9b01-3eec082bc36b": {"doc_hash": "478970ce744bd7f58a98d85967418a2d33b2ea2ab9c735512a20abe3c76ffd0d", "ref_doc_id": "ebe409b7-516e-4dac-aa8f-60c282a209da"}, "3e94c9b0-6114-4cbb-b8c2-32e2633554eb": {"doc_hash": "d3cbf8e423397d23df19a1d0dd4e9433080b7a0084b65aecbd1baf766839a837", "ref_doc_id": "ebe409b7-516e-4dac-aa8f-60c282a209da"}, "28691699-6a7d-4170-b0da-896bfa259b87": {"doc_hash": "c2dd397b704b59f7803b49b1078da0df4a591314dc9183723afea4e3dffd17d2", "ref_doc_id": "ebe409b7-516e-4dac-aa8f-60c282a209da"}, "ee0a8472-4293-4470-b8a8-3c18b284f187": {"doc_hash": "903d3e47bb70345880d2f1cc2b5d00040e2f4201d6f27107a411ea36a86b01af", "ref_doc_id": "ebe409b7-516e-4dac-aa8f-60c282a209da"}, "f2ad616c-04ed-49ce-870e-ee6bb412d1df": {"doc_hash": "2356cdcfc0796b6bcda09676c0b560f812361ef2e1743fcb1f7de5e1bceb0ad9", "ref_doc_id": "ebe409b7-516e-4dac-aa8f-60c282a209da"}, "db3db074-0ef5-46c2-8333-0a520128fbf0": {"doc_hash": "964bcb37f786969541568899db5f98cb53575c8b8518a38e8520c0164228c29f", "ref_doc_id": "7a67ba02-86ba-4e06-8582-0b6f20c739d5"}, "a3b114e1-a5b5-481a-9091-81ce30b3a493": {"doc_hash": "a676ffddb3e2cfc1df687de7e22b40d85e286f43288f7ac902854e0ac331e5dc", "ref_doc_id": "7a67ba02-86ba-4e06-8582-0b6f20c739d5"}, "ba4cf269-9d2b-4372-b49c-f6a1bf18f3c5": {"doc_hash": "d68415717382cb3c3d1ad7d14eb2c1384943a8818c09e71fa43f627bed82796c", "ref_doc_id": "7a67ba02-86ba-4e06-8582-0b6f20c739d5"}, "fcb67fcf-cc23-49e5-bd56-fb23f3638f96": {"doc_hash": "dfec649e3f2988e15ea0a59a3d25f7cf56adcda5e6c839df657ca1e03c7c4964", "ref_doc_id": "7a67ba02-86ba-4e06-8582-0b6f20c739d5"}, "e8a043dd-2885-4e30-80a7-9e7f6fe67635": {"doc_hash": "e4d5bed89015fc41586748f131c70bd58fc63eed26c74be423d986aea8f66115", "ref_doc_id": "7a67ba02-86ba-4e06-8582-0b6f20c739d5"}, "b2c46eb6-a135-4273-9157-d669aefaa747": {"doc_hash": "4008c0c18694c5f1cfe63107bcd7c1428bf50b260fa17692aea3b1e8267c72fe", "ref_doc_id": "95d73cd8-87d1-4e32-b31f-76e61e9440df"}, "ac7184a3-b11d-40d9-a40c-4c9c2069d5b3": {"doc_hash": "6e7f121d19239f641e4a72d5b8533af87bf3f98d5b04c291dc030101e51be3a4", "ref_doc_id": "95d73cd8-87d1-4e32-b31f-76e61e9440df"}, "38dadf53-fb63-4ecf-b70e-fa6cd6abe080": {"doc_hash": "86bf271195323a99ea045793c3df7a2700f14a8530325a51228f87c4175baca0", "ref_doc_id": "3ac1d604-830e-41b8-a9a8-de008f222fb1"}, "204bf980-74c0-4079-8bbd-66a596330306": {"doc_hash": "8182d0723b8d77a002dda8182b5ec7f34e1af1c96f0c67c1b2f5867556c0cdec", "ref_doc_id": "3ac1d604-830e-41b8-a9a8-de008f222fb1"}, "56bca366-e230-4313-824e-6b5cb9f81af9": {"doc_hash": "8cfa488180aee83c9f28eb82b6e7bfa708fc0851dfd82247cd6650ed6b3083a3", "ref_doc_id": "3ac1d604-830e-41b8-a9a8-de008f222fb1"}, "16a72916-0807-4278-bfa9-3e59b9366f18": {"doc_hash": "c0feff8bfb2119389a7120d94063f0f57f3027cf6df7914d180a7621a24805f1", "ref_doc_id": "3ac1d604-830e-41b8-a9a8-de008f222fb1"}, "a891be65-ecb9-48af-8146-47a6138a2463": {"doc_hash": "42d37196d8ae70fcd903c23c4510b12cbeec5bbf198c8ad030a09a50e2275942", "ref_doc_id": "3ac1d604-830e-41b8-a9a8-de008f222fb1"}, "2dafca1b-2e26-44ff-b71c-3c0e026a6474": {"doc_hash": "911df3440ff6d9de555c3c7d515b044aec0a388045e4e05df74b3e28366c6de7", "ref_doc_id": "3ac1d604-830e-41b8-a9a8-de008f222fb1"}, "c9c0b731-45a8-492f-a739-af16b9ec9cd2": {"doc_hash": "d305e89af01cda204fe718e2b35f299bb6b03dff0831e9fd3a9c5448feaeca5c", "ref_doc_id": "3ac1d604-830e-41b8-a9a8-de008f222fb1"}, "0fb55cac-6853-4bbf-8997-306d32a1948b": {"doc_hash": "21d63e9115fc9929f1955d2151025efe685433e9ff59a506089d206dacdd1370", "ref_doc_id": "3ac1d604-830e-41b8-a9a8-de008f222fb1"}, "15674a21-cfe4-45f6-a35b-48b16f779273": {"doc_hash": "7f49a0da558245c191aca2ea7240b9049c8adfcf70852e86e0cbe8164a016bf6", "ref_doc_id": "3ac1d604-830e-41b8-a9a8-de008f222fb1"}, "0527f943-aba2-4020-ad19-a015f8e1c209": {"doc_hash": "93a5a755bec0d7c4a21dbf44dea096bb69b9f2f97b6148c2f6c8dee6e8cf6606", "ref_doc_id": "3ac1d604-830e-41b8-a9a8-de008f222fb1"}, "901af7fb-6f17-4137-b50c-c6f3d5af6b66": {"doc_hash": "fa44e59c99645b9a316b72a001c505afc18c8182c4d12f4badbbb9998b066978", "ref_doc_id": "3ac1d604-830e-41b8-a9a8-de008f222fb1"}, "2b0142e5-043e-4e63-8f04-8de04859dc14": {"doc_hash": "900083f03f6e7089d3f0c62bfa7630818657d060ad24ae883bf73a45d68ae29b", "ref_doc_id": "3ac1d604-830e-41b8-a9a8-de008f222fb1"}, "f8b7c665-e589-48ad-9671-d189464ab198": {"doc_hash": "acdceb65c3eb6c6fe2db405fc1e088db797acdca5fda4f64c6b76d512511fd2d", "ref_doc_id": "3ac1d604-830e-41b8-a9a8-de008f222fb1"}, "78d6f382-b599-4b32-b2b5-6e048e806702": {"doc_hash": "a93afa1a59f947692e6828b7def10558df2694e004aa0dc2e829c0997cf931ac", "ref_doc_id": "3ac1d604-830e-41b8-a9a8-de008f222fb1"}, "b6d7ca1a-2865-4317-a75c-a33a6b7be282": {"doc_hash": "d85412bd6c5c08f1070a897b0d7b5df980bd11a5f1f275b7831f4434f80a9ce4", "ref_doc_id": "b3dd0d56-5656-4ec4-898f-0583c55fee61"}, "1ee57893-890b-4a65-aa27-1a9a53a7e5d3": {"doc_hash": "6fc5ab31b7df347dcd3e69bf56c386bede0c4d9f5bd838f4584f1ffa71edf2f8", "ref_doc_id": "8723d808-8df2-4c09-85af-69e73ce0845d"}, "db403fbb-faf5-44eb-9d16-d1cb01910355": {"doc_hash": "402407fbce98fb8c3e906b4ba9c244e8ad2e54346942aebd8f6a853ce5421607", "ref_doc_id": "8723d808-8df2-4c09-85af-69e73ce0845d"}, "d22e0f0c-429b-4e59-9af6-8240becc88ef": {"doc_hash": "ad1d2a653727e6d3f0e6d861fb5d3b99450fdba566fc2df4025f00d54a212418", "ref_doc_id": "ce81106e-199e-40e5-9c27-b066438275c8"}, "ea59c639-1d74-4547-91a2-9b4560711e16": {"doc_hash": "8588ca3eb7a9519e594d76aeeef4268f7a830e3f52cd995ce3ff6d4cb1005041", "ref_doc_id": "ce81106e-199e-40e5-9c27-b066438275c8"}, "3a47ed73-33e0-445f-97dc-e79b93ca9ec5": {"doc_hash": "d68b4a2057dd9f294df50d42db0f1dfe5538dbbb7a41e87ebc1b1c24e7ae95d8", "ref_doc_id": "ce81106e-199e-40e5-9c27-b066438275c8"}, "11ea7199-5210-433c-b512-b2f97d6632c1": {"doc_hash": "bdd8f825354a9d706ec60a4109af45d5aa0c7ebaf6c7fe0c1dddcba4b1d527a6", "ref_doc_id": "ce81106e-199e-40e5-9c27-b066438275c8"}, "a867abeb-9fb5-4e4d-8a2e-46f692632347": {"doc_hash": "9ef15bc5d48422a2b68c71fa33b18a8b061e97d1780221af7c9d7cda70d541fb", "ref_doc_id": "b5ae85da-3fb2-408e-b8a5-2dfb8d152245"}, "66988b57-ecea-479b-a63c-453d78d96ebc": {"doc_hash": "a27ab7c20874e0e0d12c2fc48bbac5a883308ed7ac3eca7bcbb97e41118b6dac", "ref_doc_id": "b5ae85da-3fb2-408e-b8a5-2dfb8d152245"}, "8586402a-2911-4dd4-8056-0ca56828b8eb": {"doc_hash": "ca64de853a1dd715cd2b8b5b107fec67daa846552e301c8482cf708c5da8d7d8", "ref_doc_id": "b5ae85da-3fb2-408e-b8a5-2dfb8d152245"}, "3d5d556a-2b5a-4d92-81de-de9613dcaf14": {"doc_hash": "ebf6b130c8773bcc69ca916b802763915af38a7d6f4ff80b1c8a3cb55ba266d9", "ref_doc_id": "9ccd033a-d520-40e3-9826-a461456a44c3"}, "c614bf6e-137a-467b-808c-ddca9eb19fee": {"doc_hash": "e7aca5cde7bb5d112b7faeb0f25f333a4ce8b86d29c26c0de7e03a9dae171d5e", "ref_doc_id": "9ccd033a-d520-40e3-9826-a461456a44c3"}, "004d0974-94af-4f09-b309-f480e2ac7873": {"doc_hash": "fba50796b7394f1c8ff7ab02d21b894f07ba21ccc96731c825f90121b5dac6e8", "ref_doc_id": "9ccd033a-d520-40e3-9826-a461456a44c3"}, "9b57f5cc-b285-4415-8954-2021cf0a79e7": {"doc_hash": "202ba5dcc2e429b0042655c175aca268006393c6f0ca55ec9eadf5a576503a50", "ref_doc_id": "9ccd033a-d520-40e3-9826-a461456a44c3"}, "4e62bd62-dd3b-4a27-a900-bf8014da78ec": {"doc_hash": "4d0ab24ed4c8f725c91a2e3168b981125034c85d32693e4f43ec42786af4aeb6", "ref_doc_id": "cb6471bf-2893-4828-b5b5-0d44aa11201d"}, "8282590e-446e-49e4-8706-8631ef149490": {"doc_hash": "685f541f2e5b02b670ca854db89c7cfa3f2e016704971008d8fecb6cfcc0b365", "ref_doc_id": "cb6471bf-2893-4828-b5b5-0d44aa11201d"}, "f0c6dbf3-3671-4fdc-a183-c4827b7226f4": {"doc_hash": "cdb55be836926c7145e7c9a6157291687c9a535c315762a64ca4fbc8aa2e28e8", "ref_doc_id": "cb6471bf-2893-4828-b5b5-0d44aa11201d"}, "4cbcecb1-2b04-4f2d-88ea-9f23f18f33c2": {"doc_hash": "9f131e8c44cff4a9e02cdf077d39e67a73e2d363984b36174faba4fc49fec02b", "ref_doc_id": "cb6471bf-2893-4828-b5b5-0d44aa11201d"}, "b557405d-f6e5-4aee-b6dd-02fc5f42c078": {"doc_hash": "1115957c311fd792311c33bdc48943510966485a22c403f13ea1a8e52379e4e9", "ref_doc_id": "cb6471bf-2893-4828-b5b5-0d44aa11201d"}, "512d8208-d923-4637-bfaf-ab61c1643f53": {"doc_hash": "3db8dca494cb8aa53d7fd3af817abb9e4cb78c0a45a7cb43c49edeef51360ce2", "ref_doc_id": "cb6471bf-2893-4828-b5b5-0d44aa11201d"}, "04ed0a87-ab76-4389-ae6f-656b9aa11037": {"doc_hash": "14a708e9a8ea895131f47c0bc3c74defefc0e2fb45b1c0e12e18b5a5f15504fe", "ref_doc_id": "cb6471bf-2893-4828-b5b5-0d44aa11201d"}, "662b93c3-d8df-42e9-b02c-bb6d2ec34f3b": {"doc_hash": "42c8da06dabbea3e6cb34aaf039ec362d1080a571591dd95426efa51bf26bce9", "ref_doc_id": "cb6471bf-2893-4828-b5b5-0d44aa11201d"}, "9f5b9277-3671-4c3e-a7e8-569a1a6c9080": {"doc_hash": "1321739e1d4f6ecb2751f07fcdfada9a2d2603e682507ec4011bc8ab3286bfc2", "ref_doc_id": "72e5032e-1334-4ccc-8191-84c5c54f0619"}, "0be24465-312b-4be7-8115-00e27a818048": {"doc_hash": "3eccb506a2dde058750d94ca44a1246ae361e36b6f3359efc25dd65518be6cc4", "ref_doc_id": "72e5032e-1334-4ccc-8191-84c5c54f0619"}, "c44331cb-8338-4a74-b20a-543ff0e64dff": {"doc_hash": "6243f9baf7937b1b6aaef7aff0402156f1f18f8fad85ef2daf6e28076f276a9f", "ref_doc_id": "72e5032e-1334-4ccc-8191-84c5c54f0619"}, "92a43d90-7087-4bab-9b7a-2a5997a6ac5c": {"doc_hash": "a3c33d8cbfaf25608de188c173104245d4eb9c0b43065b80abf5ca539a04bf2e", "ref_doc_id": "72e5032e-1334-4ccc-8191-84c5c54f0619"}, "c4f71283-9bbd-43f2-a96f-175fb7cafc7e": {"doc_hash": "a0b01bb876eddf5010146acb4465fb4477af07e2d3499c4041f81c06df3ae869", "ref_doc_id": "72e5032e-1334-4ccc-8191-84c5c54f0619"}, "57bc95d5-41c4-4203-bc03-970260c56dd1": {"doc_hash": "bc9f62d9a4f87e182b8387523e6ba3fb8e6bef51e5d50f5d6dc8dc1fa8a6cb24", "ref_doc_id": "cf3ccf7b-1635-46d8-939e-567bb626882a"}, "4a76c575-6013-4ca4-9283-7a04c1f75083": {"doc_hash": "d5374b29c9581b36062537b1d6914631aa2aaf175153d9fa6013ce917f3e064e", "ref_doc_id": "cf3ccf7b-1635-46d8-939e-567bb626882a"}, "71d825cc-39b1-41a0-aec4-6c0816f596c6": {"doc_hash": "7bd1cf37f0aef45c5a8c2a460f9746a480c60783e2536cc2024cb234dbbee582", "ref_doc_id": "cf3ccf7b-1635-46d8-939e-567bb626882a"}, "8ee44e45-414e-4229-87fb-d1c997d79310": {"doc_hash": "21ad60127cb1d1f70dcad294832405f0a416c8cefdbdac7ec3112e783c5af72c", "ref_doc_id": "cf3ccf7b-1635-46d8-939e-567bb626882a"}, "27f9a601-e371-4aa2-a75a-e97560d660ad": {"doc_hash": "21ad0659c377e3ce4f29b7d38ce67caee1892cd4b8ec661a95b6380cbbe5c470", "ref_doc_id": "445f09fa-ef83-4d20-bc51-e3c582e9f4ab"}, "1d0d06dc-33fe-4b18-b329-372fbb4e4e8e": {"doc_hash": "45c8cbbebaae10d686314c60620d64fe1686da742c8616720d0ac9218d51b38a", "ref_doc_id": "445f09fa-ef83-4d20-bc51-e3c582e9f4ab"}, "645252aa-2919-42d3-bb00-8c100099c178": {"doc_hash": "bd79a9dd1c6ce700f9a4e909a02eb08d8364e79e3233005486a5a4a8fd4fc61d", "ref_doc_id": "445f09fa-ef83-4d20-bc51-e3c582e9f4ab"}, "749b128b-e945-46d9-b5ec-90948f4af010": {"doc_hash": "05d151a0e10c6725d030bb8de7c8c0236aaa9002d870c0861ec1d44fc74a8e25", "ref_doc_id": "445f09fa-ef83-4d20-bc51-e3c582e9f4ab"}, "3784e59e-859b-4284-8301-a78cd765e3ed": {"doc_hash": "1e194acb28afa7ebb23858fa7d47732b3b7c5e2c47b0a717e33d653037b9d420", "ref_doc_id": "bb0d5604-c0f9-4a94-b06f-5075d5cf5210"}, "82d03e73-f80c-4e33-87f2-70583aac83dc": {"doc_hash": "a29a6358a59abbbf1ff0154e5c682210f1a69da524bb147cb5e5bc03fc90eab0", "ref_doc_id": "bb0d5604-c0f9-4a94-b06f-5075d5cf5210"}, "ff6d664b-1345-42ac-b869-def8f9de77d5": {"doc_hash": "fb45087e5f40509fd10bf098e9b65487e4f08920f71f1d7f25702d60a1f26630", "ref_doc_id": "bb0d5604-c0f9-4a94-b06f-5075d5cf5210"}, "2544d06a-6fcc-4bd0-a8fc-622e6cc88eea": {"doc_hash": "24e578c85ab601116d783d1ad1328fb5936820d7e49bae6958df2f7ce52999ff", "ref_doc_id": "bb0d5604-c0f9-4a94-b06f-5075d5cf5210"}, "6ca97326-c3f6-4865-bdfb-66199e0a01d8": {"doc_hash": "4e1a3c31aace5153d2d1f3a7d691a8dba91040501f4152e3c955ac7d283f2a6c", "ref_doc_id": "bb0d5604-c0f9-4a94-b06f-5075d5cf5210"}, "466a7fe3-71fe-4eaf-ada7-d94251b68b7e": {"doc_hash": "cfc4768884b555222fb66e51c3242b453138fe5b89934576d1c14b510ca7a7a8", "ref_doc_id": "bb0d5604-c0f9-4a94-b06f-5075d5cf5210"}, "1803e097-fef3-4177-bf1a-ff8603f52552": {"doc_hash": "056a765fd3437e82d3425d6a4e128f28922cfb316a7d0ed0d92fb76cdda71b35", "ref_doc_id": "bb0d5604-c0f9-4a94-b06f-5075d5cf5210"}, "10aadcfd-ade9-4d1a-b2c3-231617c52a1e": {"doc_hash": "06910648e5beb9666718503784ab219e0914dfa464fa1b7a9edf286855d8daae", "ref_doc_id": "bb0d5604-c0f9-4a94-b06f-5075d5cf5210"}, "615d4c2d-63c3-463c-a518-959f4bbe309d": {"doc_hash": "b6e4f5827c823ff59e446bd00646d7988146ea78e014be63f8af61ecca5965bc", "ref_doc_id": "bb0d5604-c0f9-4a94-b06f-5075d5cf5210"}, "f7d76aa1-75b0-49c5-a884-53a0ad72efef": {"doc_hash": "0fc6e29c7b9b1fc6e12914c2d43474abd02e7d0eaa5be2d3bdf39c2f55c2aeb2", "ref_doc_id": "bb0d5604-c0f9-4a94-b06f-5075d5cf5210"}, "2c2bed30-dcd6-4e54-a08b-cb12aa1a1829": {"doc_hash": "ebb043920732222409481efde7aecb863deab849255f41e59feae49b21a40c55", "ref_doc_id": "bb0d5604-c0f9-4a94-b06f-5075d5cf5210"}, "248e1579-94b9-4c0a-94df-803f58a29a0b": {"doc_hash": "ecc706f3a9c16a55f52bcd5f58583bebc057cf631e947502fb7836ea8699b7e6", "ref_doc_id": "bb0d5604-c0f9-4a94-b06f-5075d5cf5210"}, "075a5ef4-0673-48e0-b49e-966164696450": {"doc_hash": "ce61a1f089e8ca1b3f554b13854eccc081438ad9aad2005a7a5071e7622488e3", "ref_doc_id": "37f790c9-29da-4d45-bab5-71dc6795e41e"}, "743b900b-b021-45db-8b95-a57987c864b8": {"doc_hash": "2014cdd903b5e294409a96e5549bc35942e0d188e3f1dab51d65c285ea85ed61", "ref_doc_id": "37f790c9-29da-4d45-bab5-71dc6795e41e"}, "4b15aee2-e3ac-4b4f-9e0f-36ecabdce1bb": {"doc_hash": "004b6429444449653ae4c08286694d84cdb93c787babdd44b0657bc922c5676b", "ref_doc_id": "37f790c9-29da-4d45-bab5-71dc6795e41e"}, "d8fecf9a-aa70-4943-941f-cf7567fae1bd": {"doc_hash": "d474f18935cf561ae7c827457b14a594b0d6886e2e0060873eb1f54e677b2eeb", "ref_doc_id": "37f790c9-29da-4d45-bab5-71dc6795e41e"}}, "docstore/data": {"f6603d07-509e-4575-89c4-bccf2206fc47": {"__data__": {"id_": "f6603d07-509e-4575-89c4-bccf2206fc47", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\AI accelerator.txt", "file_name": "AI accelerator.txt", "file_type": "text/plain", "file_size": 15186, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b373bb3-a9ab-4c60-a7e3-7173dc51ff40", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\AI accelerator.txt", "file_name": "AI accelerator.txt", "file_type": "text/plain", "file_size": 15186, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "342c6e0c57269c37b59cf8f191dbdc0690bd44452440e0b809e50e03abf70be5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "63dab1e2-42a3-4eb3-bb6c-57f3319ccb11", "node_type": "1", "metadata": {}, "hash": "ce75618bd9072d723aefa907f2dada7652f1d4d7fc1ba81df502ea61de144398", "class_name": "RelatedNodeInfo"}}, "text": "An AI accelerator, deep learning processor, or neural processing unit (NPU) is a class of specialized hardware accelerator or computer system designed to accelerate artificial intelligence and machine learning applications, including artificial neural networks and machine vision. Typical applications include algorithms for robotics, Internet of Things, and other data-intensive or sensor-driven tasks. They are often manycore designs and generally focus on low-precision arithmetic, novel dataflow architectures or in-memory computing capability. As of 2024, a typical AI integrated circuit chip contains tens of billions of MOSFET transistors.AI accelerators are used in mobile devices, such as neural processing units (NPUs) in Apple iPhones or Huawei cellphones, and personal computers such as Apple silicon Macs, to cloud computing servers such as tensor processing units (TPU) in the Google Cloud Platform. A number of vendor-specific terms exist for devices in this category, and it is an emerging technology without a dominant design.\r\nGraphics processing units designed by companies such as Nvidia and AMD often include AI-specific hardware, and are commonly used as AI accelerators, both for training and inference.\r\n\r\n\r\n== History ==\r\nComputer systems have frequently complemented the CPU with special-purpose accelerators for specialized tasks, known as coprocessors. Notable application-specific hardware units include video cards for graphics, sound cards, graphics processing units and digital signal processors. As deep learning and artificial intelligence workloads rose in prominence in the 2010s, specialized hardware units were developed or adapted from existing products to accelerate these tasks.\r\n\r\n\r\n=== Early attempts ===\r\nFirst attempts like Intel's ETANN 80170NX incorporated analog circuits to compute neural functions.Later all-digital chips like the Nestor/Intel Ni1000 followed. As early as 1993, digital signal processors were used as neural network accelerators to accelerate optical character recognition software.By 1988, Wei Zhang et al. had discussed fast optical implementations of convolutional neural networks for alphabet recognition.In the 1990s, there were also attempts to create parallel high-throughput systems for workstations aimed at various applications, including neural network simulations.This presentation covers a past attempt at neural net accelerators, notes the similarity to the modern SLI GPGPU processor setup, and argues that general purpose vector accelerators are the way forward (in relation to RISC-V hwacha project. Argues that NN's are just dense and sparse matrices, one of several recurring algorithms)FPGA-based accelerators were also first explored in the 1990s for both inference and training.In 2014, Chen et al. proposed DianNao (Chinese for \"electric brain\"), to accelerate deep neural networks especially. DianNao provides the 452 Gop/s peak performance (of key operations in deep neural networks) only in a small footprint of 3.02 mm2 and 485 mW. Later, the successors (DaDianNao, ShiDianNao, PuDianNao) are proposed by the same group, forming the DianNao FamilySmartphones began incorporating AI accelerators starting with the Qualcomm Snapdragon 820 in 2015.\r\n\r\n\r\n=== Heterogeneous computing ===\r\n\r\nHeterogeneous computing incorporates many specialized processors in a single system, or a single chip, each optimized for a specific type of task. Architectures such as the Cell microprocessor have features significantly overlapping with AI accelerators including: support for packed low precision arithmetic, dataflow architecture, and prioritizing throughput over latency. The Cell microprocessor has been applied to a number of tasks including AI.In the 2000s, CPUs also gained increasingly wide SIMD units, driven by video and gaming workloads; as well as support for packed low-precision data types. Due to the increasing performance of CPUs, they are also used for running AI workloads. CPUs are superior for DNNs with small or medium-scale parallelism, for sparse DNNs and in low-batch-size scenarios.\r\n\r\n\r\n=== Use of GPU ===\r\nGraphics processing units or GPUs are specialized hardware for the manipulation of images and calculation of local image properties. The mathematical basis of neural networks and image manipulation are similar, embarrassingly parallel tasks involving matrices, leading GPUs to become increasingly used for machine learning tasks.In 2012, Alex Krizhevsky adopted two GPUs to train a deep learning network, i.e., AlexNet, which won the champion of the ISLVRC-2012 competition. During the 2010's, GPU manufacturers such as Nvidia added deep learning related features in both hardware (e.g., INT8 operators) and software (e.g., cuDNN Library).\r\nGPUs continue to be used in large-scale AI applications.", "start_char_idx": 0, "end_char_idx": 4811, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "63dab1e2-42a3-4eb3-bb6c-57f3319ccb11": {"__data__": {"id_": "63dab1e2-42a3-4eb3-bb6c-57f3319ccb11", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\AI accelerator.txt", "file_name": "AI accelerator.txt", "file_type": "text/plain", "file_size": 15186, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b373bb3-a9ab-4c60-a7e3-7173dc51ff40", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\AI accelerator.txt", "file_name": "AI accelerator.txt", "file_type": "text/plain", "file_size": 15186, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "342c6e0c57269c37b59cf8f191dbdc0690bd44452440e0b809e50e03abf70be5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f6603d07-509e-4575-89c4-bccf2206fc47", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\AI accelerator.txt", "file_name": "AI accelerator.txt", "file_type": "text/plain", "file_size": 15186, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "9def6e725c3417b86603ba6cb3cbf7578ee6ebebfd543c80c33a64d4c146fbc6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab4ab5a1-8ef3-4081-8845-a0329d1f315e", "node_type": "1", "metadata": {}, "hash": "6298b525912a741e8b50d1b969a8725be74517a766657c23d509adda1a36e30f", "class_name": "RelatedNodeInfo"}}, "text": "Due to the increasing performance of CPUs, they are also used for running AI workloads. CPUs are superior for DNNs with small or medium-scale parallelism, for sparse DNNs and in low-batch-size scenarios.\r\n\r\n\r\n=== Use of GPU ===\r\nGraphics processing units or GPUs are specialized hardware for the manipulation of images and calculation of local image properties. The mathematical basis of neural networks and image manipulation are similar, embarrassingly parallel tasks involving matrices, leading GPUs to become increasingly used for machine learning tasks.In 2012, Alex Krizhevsky adopted two GPUs to train a deep learning network, i.e., AlexNet, which won the champion of the ISLVRC-2012 competition. During the 2010's, GPU manufacturers such as Nvidia added deep learning related features in both hardware (e.g., INT8 operators) and software (e.g., cuDNN Library).\r\nGPUs continue to be used in large-scale AI applications. For example, Summit, a supercomputer from IBM for Oak Ridge National Laboratory, contains 27,648 Nvidia Tesla V100 cards, which can be used to accelerate deep learning algorithms.\r\nOver the 2010's GPUs continued to evolve in a direction to facilitate deep learning, both for training and inference in devices such as self-driving cars. GPU developers such as Nvidia NVLink are developing additional connective capability for the kind of dataflow workloads AI benefits from. As GPUs have been increasingly applied to AI acceleration, GPU manufacturers have incorporated neural network-specific hardware to further accelerate these tasks. Tensor cores are intended to speed up the training of neural networks.\r\n\r\n\r\n=== Use of FPGAs ===\r\nDeep learning frameworks are still evolving, making it hard to design custom hardware. Reconfigurable devices such as field-programmable gate arrays (FPGA) make it easier to evolve hardware, frameworks, and software alongside each other.Microsoft has used FPGA chips to accelerate inference for real-time deep learning services.\r\n\r\n\r\n=== Emergence of dedicated AI accelerator ASICs ===\r\nWhile GPUs and FPGAs perform far better than CPUs for AI-related tasks, a factor of up to 10 in efficiency may be gained with a more specific design, via an application-specific integrated circuit (ASIC). These accelerators employ strategies such as optimized memory use and the use of lower precision arithmetic to accelerate calculation and increase throughput of computation. Some low-precision floating-point formats used for AI acceleration are half-precision and the bfloat16 floating-point format. Companies such as Google, Qualcomm, Amazon, Apple, Facebook, AMD and Samsung are all designing their own AI ASICs. Cerebras Systems has built a dedicated AI accelerator based on the largest processor in the industry, the second-generation Wafer Scale Engine (WSE-2), to support deep learning workloads.\r\n\r\n\r\n== Ongoing research ==\r\n\r\n\r\n=== In-memory computing architectures ===\r\nIn June 2017, IBM researchers announced an architecture in contrast to the Von Neumann architecture based on in-memory computing and phase-change memory arrays applied to temporal correlation detection, intending to generalize the approach to heterogeneous computing and massively parallel systems. In October 2018, IBM researchers announced an architecture based on in-memory processing and modeled on the human brain's synaptic network to accelerate deep neural networks. The system is based on phase-change memory arrays.\r\n\r\n\r\n=== In-memory computing with analog resistive memories ===\r\nIn 2019, researchers from Politecnico di Milano found a way to solve systems of linear equations in a few tens of nanoseconds via a single operation. Their algorithm is based on in-memory computing with analog resistive memories which performs with high efficiencies of time and energy, via conducting matrix\u2013vector multiplication in one step using Ohm's law and Kirchhoff's law. The researchers showed that a feedback circuit with cross-point resistive memories can solve algebraic problems such as systems of linear equations, matrix eigenvectors, and differential equations in just one step. Such an approach improves computational times drastically in comparison with digital algorithms.\r\n\r\n\r\n=== Atomically thin semiconductors ===\r\nIn 2020, Marega et al. published experiments with a large-area active channel material for developing logic-in-memory devices and circuits based on floating-gate field-effect transistors (FGFETs). Such atomically thin semiconductors are considered promising for energy-efficient machine learning applications, where the same basic device structure is used for both logic operations and data storage. The authors used two-dimensional materials such as semiconducting molybdenum disulphide to precisely tune FGFETs as building blocks in which logic operations can be performed with the memory elements. \r\n\r\n\r\n=== Integrated photonic tensor core ===\r\nIn 1988, Wei Zhang et al.", "start_char_idx": 3885, "end_char_idx": 8818, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab4ab5a1-8ef3-4081-8845-a0329d1f315e": {"__data__": {"id_": "ab4ab5a1-8ef3-4081-8845-a0329d1f315e", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\AI accelerator.txt", "file_name": "AI accelerator.txt", "file_type": "text/plain", "file_size": 15186, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b373bb3-a9ab-4c60-a7e3-7173dc51ff40", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\AI accelerator.txt", "file_name": "AI accelerator.txt", "file_type": "text/plain", "file_size": 15186, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "342c6e0c57269c37b59cf8f191dbdc0690bd44452440e0b809e50e03abf70be5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "63dab1e2-42a3-4eb3-bb6c-57f3319ccb11", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\AI accelerator.txt", "file_name": "AI accelerator.txt", "file_type": "text/plain", "file_size": 15186, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "ad97031057d660ddb17ca27838273e68c290858c18b032c7e4ea9244fe945721", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c89e3b13-2c6d-42c4-8843-132a050766a5", "node_type": "1", "metadata": {}, "hash": "f3efb9fd25cdfddef20ca3fa369fa5727e92bc598fb6f2582bdf164377696ce5", "class_name": "RelatedNodeInfo"}}, "text": "Such an approach improves computational times drastically in comparison with digital algorithms.\r\n\r\n\r\n=== Atomically thin semiconductors ===\r\nIn 2020, Marega et al. published experiments with a large-area active channel material for developing logic-in-memory devices and circuits based on floating-gate field-effect transistors (FGFETs). Such atomically thin semiconductors are considered promising for energy-efficient machine learning applications, where the same basic device structure is used for both logic operations and data storage. The authors used two-dimensional materials such as semiconducting molybdenum disulphide to precisely tune FGFETs as building blocks in which logic operations can be performed with the memory elements. \r\n\r\n\r\n=== Integrated photonic tensor core ===\r\nIn 1988, Wei Zhang et al. discussed fast optical implementations of convolutional neural networks for alphabet recognition.\r\nIn 2021, J. Feldmann et al. proposed an integrated photonic hardware accelerator for parallel convolutional processing. The authors identify two key advantages of integrated photonics over its electronic counterparts: (1) massively parallel data transfer through wavelength division multiplexing in conjunction with frequency combs, and (2) extremely high data modulation speeds. Their system can execute trillions of multiply-accumulate operations per second, indicating the potential of integrated photonics in data-heavy AI applications. Optical processors that can also perform backpropagation for artificial neural networks have been experimentally developed.\r\n\r\n\r\n== Nomenclature ==\r\nAs of 2016, the field is still in flux and vendors are pushing their own marketing term for what amounts to an \"AI accelerator\", in the hope that their designs and APIs will become the dominant design. There is no consensus on the boundary between these devices, nor the exact form they will take; however several examples clearly aim to fill this new space, with a fair amount of overlap in capabilities.\r\nIn the past when consumer graphics accelerators emerged, the industry eventually adopted Nvidia's self-assigned term, \"the GPU\",\r\nas the collective noun for \"graphics accelerators\", which had taken many forms before settling on an overall pipeline implementing a model presented by Direct3D.\r\nAll models of Intel Meteor Lake processors have a Versatile Processor Unit (VPU) built-in for accelerating inference for computer vision and deep learning.\r\n\r\n\r\n== Deep Learning Processors (DLP) ==\r\nInspired from the pioneer work of DianNao Family, many DLPs are proposed in both academia and industry with design optimized to leverage the features of deep neural networks for high efficiency. Only at ISCA 2016, three sessions, 15% (!) of the accepted papers, are all architecture designs about deep learning. Such efforts include Eyeriss (MIT), EIE (Stanford), Minerva (Harvard), Stripes (University of Toronto) in academia, TPU (Google), and MLU (Cambricon) in industry. We listed several representative works in Table 1.\r\n\r\n\r\n=== Digital DLPs ===\r\nThe major components of DLPs architecture usually include a computation component, the on-chip memory hierarchy, and the control logic that manages the data communication and computing flows.\r\nRegarding the computation component, as most operations in deep learning can be aggregated into vector operations, the most common ways for building computation components in digital DLPs are the MAC-based (multiplier-accumulation) organization, either with vector MACs or scalar MACs. Rather than SIMD or SIMT in general processing devices, deep learning domain-specific parallelism is better explored on these MAC-based organizations. Regarding the memory hierarchy, as deep learning algorithms require high bandwidth to provide the computation component with sufficient data, DLPs usually employ a relatively larger size (tens of kilobytes or several megabytes) on-chip buffer but with dedicated on-chip data reuse strategy and data exchange strategy to alleviate the burden for memory bandwidth. For example, DianNao, 16 16-in vector MAC, requires 16 \u00d7 16 \u00d7 2 = 512 16-bit data, i.e., almost 1024GB/s bandwidth requirements between computation components and buffers. With on-chip reuse, such bandwidth requirements are reduced drastically. Instead of the widely used cache in general processing devices, DLPs always use scratchpad memory as it could provide higher data reuse opportunities by leveraging the relatively regular data access pattern in deep learning algorithms. Regarding the control logic, as the deep learning algorithms keep evolving at a dramatic speed, DLPs start to leverage dedicated ISA (instruction set architecture) to support the deep learning domain flexibly. At first, DianNao used a VLIW-style instruction set where each instruction could finish a layer in a DNN.", "start_char_idx": 8003, "end_char_idx": 12849, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c89e3b13-2c6d-42c4-8843-132a050766a5": {"__data__": {"id_": "c89e3b13-2c6d-42c4-8843-132a050766a5", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\AI accelerator.txt", "file_name": "AI accelerator.txt", "file_type": "text/plain", "file_size": 15186, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5b373bb3-a9ab-4c60-a7e3-7173dc51ff40", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\AI accelerator.txt", "file_name": "AI accelerator.txt", "file_type": "text/plain", "file_size": 15186, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "342c6e0c57269c37b59cf8f191dbdc0690bd44452440e0b809e50e03abf70be5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ab4ab5a1-8ef3-4081-8845-a0329d1f315e", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\AI accelerator.txt", "file_name": "AI accelerator.txt", "file_type": "text/plain", "file_size": 15186, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "d56e81c2f5f70d41ea5fb643a9acca5367b89ec194fa0d80788f5fea15151479", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "69e369bd-aace-44c5-8f38-46c7513b1343", "node_type": "1", "metadata": {}, "hash": "9c6cfb42de8ad32a6ba29534eaf6c3dd0ae7e0e374482c2d1bb4e306c6b87371", "class_name": "RelatedNodeInfo"}}, "text": "For example, DianNao, 16 16-in vector MAC, requires 16 \u00d7 16 \u00d7 2 = 512 16-bit data, i.e., almost 1024GB/s bandwidth requirements between computation components and buffers. With on-chip reuse, such bandwidth requirements are reduced drastically. Instead of the widely used cache in general processing devices, DLPs always use scratchpad memory as it could provide higher data reuse opportunities by leveraging the relatively regular data access pattern in deep learning algorithms. Regarding the control logic, as the deep learning algorithms keep evolving at a dramatic speed, DLPs start to leverage dedicated ISA (instruction set architecture) to support the deep learning domain flexibly. At first, DianNao used a VLIW-style instruction set where each instruction could finish a layer in a DNN. Cambricon introduces the first deep learning domain-specific ISA, which could support more than ten different deep learning algorithms. TPU also reveals five key instructions from the CISC-style ISA.\r\n\r\n\r\n=== Hybrid DLPs ===\r\nHybrid DLPs emerge for DNN inference and training acceleration because of their high efficiency. Processing-in-memory (PIM) architectures are one most important type of hybrid DLP. The key design concept of PIM is to bridge the gap between computing and memory, with the following manners: 1) Moving computation components into memory cells, controllers, or memory chips to alleviate the memory wall issue. Such architectures significantly shorten data paths and leverage much higher internal bandwidth, hence resulting in attractive performance improvement. 2) Build high efficient DNN engines by adopting computational devices. In 2013, HP Lab demonstrated the astonishing capability of adopting ReRAM crossbar structure for computing. Inspiring by this work, tremendous work are proposed to explore the new architecture and system design based on ReRAM, phase change memory, etc.\r\n\r\n\r\n== Benchmarks ==\r\nBenchmarks such as MLPerf and others may be used to evaluate the performance of AI accelerators. Table 2 lists several typical benchmarks for AI accelerators.\r\n\r\n\r\n== Potential applications ==\r\nAgricultural robots, for example, herbicide-free weed control.\r\nAutonomous vehicles: Nvidia has targeted their Drive PX-series boards at this application.\r\nComputer-aided diagnosis\r\nIndustrial robots, increasing the range of tasks that can be automated, by adding adaptability to variable situations.\r\nMachine translation\r\nMilitary robots\r\nNatural language processing\r\nSearch engines, increasing the energy efficiency of data centers and the ability to use increasingly advanced queries.\r\nUnmanned aerial vehicles, e.g. navigation systems, e.g. the Movidius Myriad 2 has been demonstrated successfully guiding autonomous drones.\r\nVoice user interface, e.g. in mobile phones, a target for Qualcomm Zeroth.\r\n\r\n\r\n== See also ==\r\nCognitive computer\r\nNeuromorphic engineering\r\nOptical neural network\r\nPhysical neural network\r\nCerebras Systems\r\n\r\n\r\n== References ==\r\n\r\n\r\n== External links ==\r\nNvidia Puts The Accelerator To The Metal With Pascal.htm, The Next Platform\r\nEyeriss Project, MIT\r\nhttps://alphaics.ai/", "start_char_idx": 12053, "end_char_idx": 15182, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "69e369bd-aace-44c5-8f38-46c7513b1343": {"__data__": {"id_": "69e369bd-aace-44c5-8f38-46c7513b1343", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\BERT (language model).txt", "file_name": "BERT (language model).txt", "file_type": "text/plain", "file_size": 9285, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2bb55be0-1379-4a71-99be-3263be74fa83", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\BERT (language model).txt", "file_name": "BERT (language model).txt", "file_type": "text/plain", "file_size": 9285, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "af3fe124075537122e8d602eea81932850f37c58dafa3dd995e58e4b72790ef1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c89e3b13-2c6d-42c4-8843-132a050766a5", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\AI accelerator.txt", "file_name": "AI accelerator.txt", "file_type": "text/plain", "file_size": 15186, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "946e37652d8f2af2c88ec533b6b3de226153aa24a049ec88991f1ea28873085a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "018fb6c0-3406-4546-bac9-d817682f16f3", "node_type": "1", "metadata": {}, "hash": "1c3ad93243277861cbb4797a185491b55dac99a7fa674ddefbc8c86cc211a251", "class_name": "RelatedNodeInfo"}}, "text": "Bidirectional Encoder Representations from Transformers (BERT) is a language model based on the transformer architecture, notable for its dramatic improvement over previous state of the art models. It was introduced in October 2018 by researchers at Google. A 2020 literature survey concluded that \"in a little over a year, BERT has become a ubiquitous baseline in Natural Language Processing (NLP) experiments counting over 150 research publications analyzing and improving the model.\"BERT was originally implemented in the English language at two model sizes: (1) BERTBASE: 12 encoders with 12 bidirectional self-attention heads totaling 110 million parameters, and (2) BERTLARGE: 24 encoders with 16 bidirectional self-attention heads totaling 340 million parameters. Both models were pre-trained on the Toronto BookCorpus (800M words) and English Wikipedia  (2,500M words).\r\n\r\n\r\n== Design ==\r\nBERT is an \"encoder-only\" transformer architecture. \r\nOn a high level, BERT consists of three modules:\r\n\r\nembedding. This module converts an array of one-hot encoded tokens into an array of vectors representing the tokens.\r\na stack of encoders. These encoders are the Transformer encoders. They perform transformations over the array of representation vectors.\r\nun-embedding. This module converts the final representation vectors into one-hot encoded tokens again.The un-embedding module is necessary for pretraining, but it is often unnecessary for downstream tasks. Instead, one would take the representation vectors output at the end of the stack of encoders, and use those as a vector representation of the text input, and train a smaller model on top of that.\r\nBERT uses WordPiece to convert each English word into an integer code. Its vocabulary has size 30,000. Any token not appearing in its vocabulary is replaced by [UNK] for \"unknown\".\r\n\r\n\r\n=== Pretraining ===\r\nBERT was pre-trained simultaneously on two tasks:language modeling: 15% of tokens were selected for prediction, and the training objective was to predict the selected token given its context. The selected token is \r\n\r\nreplaced with a [MASK] token with probability 80%,\r\nreplaced with a random word token with probability 10%,\r\nnot replaced with probability 10%.For example, the sentence \"my dog is cute\" may have the 4-th token selected for prediction. The model would have input text\r\n\r\n\"my dog is [MASK]\" with probability 80%,\r\n\"my dog is happy\" with probability 10%,\r\n\"my dog is cute\" with probability 10%.After processing the input text, the model's 4-th output vector is passed to a separate neural network, which outputs a probability distribution over its 30,000-large vocabulary.\r\nnext sentence prediction: Given two spans of text, the model predicts if these two spans appeared sequentially in the training corpus, outputting either [IsNext] or [NotNext]. The first span starts with a special token [CLS] (for \"classify\"). The two spans are separated by a special token [SEP] (for \"separate\"). After processing the two spans, the 1-st output vector (the vector coding for [CLS]) is passed to a separate neural network for the binary classification into [IsNext] and [NotNext].\r\n\r\nFor example, given \"[CLS] my dog is cute [SEP] he likes playing\" the model should output token [IsNext].\r\nGiven \"[CLS] my dog is cute [SEP] how do magnets work\" the model should output token [NotNext].As a result of this training process, BERT learns latent representations of words and sentences in context. After pre-training, BERT can be fine-tuned with fewer resources on smaller datasets to optimize its performance on specific tasks such as NLP tasks (language inference, text classification) and sequence-to-sequence based language generation tasks (question-answering, conversational response generation). The pre-training stage is significantly more computationally expensive than fine-tuning.\r\n\r\n\r\n=== Architecture details ===\r\nThis section describes BERTBASE. The other one, BERTLARGE, is similar, just larger. \r\nThe lowest layer is the embedding layer, which contains three components: word_embeddings, position_embeddings, token_type_embeddings.\r\n\r\nword_embeddings takes in a one-hot vector of the input token. The one-hot vector input has dimension 30,000, because BERT has a vocabulary size that large.\r\nposition_embeddings performs absolute position embedding.", "start_char_idx": 0, "end_char_idx": 4335, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "018fb6c0-3406-4546-bac9-d817682f16f3": {"__data__": {"id_": "018fb6c0-3406-4546-bac9-d817682f16f3", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\BERT (language model).txt", "file_name": "BERT (language model).txt", "file_type": "text/plain", "file_size": 9285, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2bb55be0-1379-4a71-99be-3263be74fa83", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\BERT (language model).txt", "file_name": "BERT (language model).txt", "file_type": "text/plain", "file_size": 9285, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "af3fe124075537122e8d602eea81932850f37c58dafa3dd995e58e4b72790ef1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "69e369bd-aace-44c5-8f38-46c7513b1343", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\BERT (language model).txt", "file_name": "BERT (language model).txt", "file_type": "text/plain", "file_size": 9285, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "c0a860bdbe055ded6159fcd5a4ab395776c41d5e825ba8a8ff2144a083c4cfc3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd292843-4843-4185-925f-64d9380d9246", "node_type": "1", "metadata": {}, "hash": "72c8350d96992da9623f760b7297581a8d80af1edfa01a87721918fcdeb1f700", "class_name": "RelatedNodeInfo"}}, "text": "After pre-training, BERT can be fine-tuned with fewer resources on smaller datasets to optimize its performance on specific tasks such as NLP tasks (language inference, text classification) and sequence-to-sequence based language generation tasks (question-answering, conversational response generation). The pre-training stage is significantly more computationally expensive than fine-tuning.\r\n\r\n\r\n=== Architecture details ===\r\nThis section describes BERTBASE. The other one, BERTLARGE, is similar, just larger. \r\nThe lowest layer is the embedding layer, which contains three components: word_embeddings, position_embeddings, token_type_embeddings.\r\n\r\nword_embeddings takes in a one-hot vector of the input token. The one-hot vector input has dimension 30,000, because BERT has a vocabulary size that large.\r\nposition_embeddings performs absolute position embedding. It is like word_embeddings, but on a vocabulary consisting of just the time-stamps 0 to 511, since BERT has a context window of 512.\r\ntoken_type_embeddings is like word_embeddings, but on a vocabulary consisting of just 0 and 1. The only type-1 tokens are those that appear after the [SEP]. All other tokens are type-0.The three outputs are added, then pushed through a LayerNorm (layer normalization), obtaining an array of representation vectors, each having 768 dimensions.\r\nAfter this, the representation vectors move through 12 Transformer encoders, then they are un-embedded by an affine-Add & LayerNorm-linear.\r\n\r\n\r\n== Performance ==\r\nWhen BERT was published, it achieved state-of-the-art performance on a number of natural language understanding tasks:\r\nGLUE (General Language Understanding Evaluation) task set (consisting of 9 tasks)\r\nSQuAD (Stanford Question Answering Dataset) v1.1 and v2.0\r\nSWAG (Situations With Adversarial Generations)\r\n\r\n\r\n== Analysis ==\r\nThe reasons for BERT's state-of-the-art performance on these natural language understanding tasks are not yet well understood. Current research has focused on investigating the relationship behind BERT's output as a result of carefully chosen input sequences, analysis of internal vector representations through probing classifiers, and the relationships represented by attention weights.\r\nThe high performance of the BERT model could also be attributed to the fact that it is bidirectionally trained. This means that BERT, based on the Transformer model architecture, applies its self-attention mechanism to learn information from a text from the left and right side during training, and consequently gains a deep understanding of the context. For example, the word fine can have two different meanings depending on the context (I feel fine today, She has fine blond hair).  BERT considers the words surrounding the target word fine from the left and right side.\r\nHowever it comes at a cost: due to encoder-only architecture lacking a decoder, BERT can't be prompted and can't generate text, while bidirectional models in general do not work effectively without the right side, thus being difficult to prompt, with even short text generation requiring sophisticated computationally expensive techniques.In contrast to deep learning neural networks which require very large amounts of data, BERT has already been pre-trained which means that it has learnt the representations of the words and sentences as well as the underlying semantic relations that they are connected with. BERT can then be fine-tuned on smaller datasets for specific tasks such as sentiment classification. The pre-trained models are chosen according to the content of the given dataset one uses but also the goal of the task. For example, if the task is a sentiment classification task on financial data, a pre-trained model for the analysis of sentiment of financial text should be chosen. The weights of the original pre-trained models were released on GitHub.\r\n\r\n\r\n== History ==\r\nBERT was originally published by Google researchers Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. The design has its origins from pre-training contextual representations, including semi-supervised sequence learning, generative pre-training, ELMo, and ULMFit. Unlike previous models, BERT is a deeply bidirectional, unsupervised language representation, pre-trained using only a plain text corpus. Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary, whereas BERT takes into account the context for each occurrence of a given word.", "start_char_idx": 3468, "end_char_idx": 7988, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cd292843-4843-4185-925f-64d9380d9246": {"__data__": {"id_": "cd292843-4843-4185-925f-64d9380d9246", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\BERT (language model).txt", "file_name": "BERT (language model).txt", "file_type": "text/plain", "file_size": 9285, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2bb55be0-1379-4a71-99be-3263be74fa83", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\BERT (language model).txt", "file_name": "BERT (language model).txt", "file_type": "text/plain", "file_size": 9285, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "af3fe124075537122e8d602eea81932850f37c58dafa3dd995e58e4b72790ef1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "018fb6c0-3406-4546-bac9-d817682f16f3", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\BERT (language model).txt", "file_name": "BERT (language model).txt", "file_type": "text/plain", "file_size": 9285, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "4d41fe49a327a0771b0035be3553cc2223264f02ce19722e40c56402defd408f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bb4bc2b4-7bdb-4111-b4d6-78c30911d723", "node_type": "1", "metadata": {}, "hash": "934b0859231953582e817b82867ff7378504b1268a4e7a504b61fd7fe5cab296", "class_name": "RelatedNodeInfo"}}, "text": "For example, if the task is a sentiment classification task on financial data, a pre-trained model for the analysis of sentiment of financial text should be chosen. The weights of the original pre-trained models were released on GitHub.\r\n\r\n\r\n== History ==\r\nBERT was originally published by Google researchers Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. The design has its origins from pre-training contextual representations, including semi-supervised sequence learning, generative pre-training, ELMo, and ULMFit. Unlike previous models, BERT is a deeply bidirectional, unsupervised language representation, pre-trained using only a plain text corpus. Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary, whereas BERT takes into account the context for each occurrence of a given word. For instance, whereas the vector for \"running\" will have the same word2vec vector representation for both of its occurrences in the sentences \"He is running a company\" and \"He is running a marathon\", BERT will provide a contextualized embedding that will be different according to the sentence.On October 25, 2019, Google announced that they had started applying BERT models for English language search queries within the US. On December 9, 2019, it was reported that BERT had been adopted by Google Search for over 70 languages. In October 2020, almost every single English-based query was processed by a BERT model.A later paper proposes RoBERTa, which preserves BERT's architecture, but improves its training, changing key hyperparameters, removing the next-sentence prediction task, and using much larger mini-batch sizes.\r\n\r\n\r\n== Recognition ==\r\nThe research paper describing BERT won the Best Long Paper Award at the 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL).\r\n\r\n\r\n== References ==\r\n\r\n\r\n== Further reading ==\r\nRogers, Anna; Kovaleva, Olga; Rumshisky, Anna (2020). \"A Primer in BERTology: What we know about how BERT works\". arXiv:2002.12327 [cs.CL].\r\n\r\n\r\n== External links ==\r\nOfficial GitHub repository\r\nBERT on Devopedia", "start_char_idx": 7107, "end_char_idx": 9285, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bb4bc2b4-7bdb-4111-b4d6-78c30911d723": {"__data__": {"id_": "bb4bc2b4-7bdb-4111-b4d6-78c30911d723", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\BLOOM (language model).txt", "file_name": "BLOOM (language model).txt", "file_type": "text/plain", "file_size": 1267, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9e6898af-c176-436b-8a3e-151be88edbcf", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\BLOOM (language model).txt", "file_name": "BLOOM (language model).txt", "file_type": "text/plain", "file_size": 1267, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "4aec30877d77ca8da7d1716b8a9674d9e8bf1fe226031949525c7d0178428579", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd292843-4843-4185-925f-64d9380d9246", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\BERT (language model).txt", "file_name": "BERT (language model).txt", "file_type": "text/plain", "file_size": 9285, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "f908a650376982bff6c2b89bb222bab018e03d1b2ba1036148bd3870e63e9247", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a21b3823-2625-44e8-8214-131b22a77683", "node_type": "1", "metadata": {}, "hash": "67d06e58d2c0902ff7b7bb7c14a9276be11c76c5bc7113a7efc92f950120b9a1", "class_name": "RelatedNodeInfo"}}, "text": "BigScience Large Open-science Open-access Multilingual Language Model (BLOOM) is a 176-billion-parameter transformer-based autoregressive large language model (LLM). The model, as well as the code base and the data used to train it, are distributed under free licences. BLOOM was trained on approximately 366 billion (1.6TB) tokens from March to July 2022.BLOOM is the main outcome of the BigScience collaborative initiative, a one-year-long research workshop that took place between May 2021 and May 2022. BigScience was led by HuggingFace and involved several hundreds of researchers and engineers from France and abroad representing both the academia and the private sector. BigScience was supported by a large-scale public compute grant on the French public supercomputer Jean Zay, managed by GENCI and IDRIS (CNRS), on which it was trained.\r\nBLOOM's training corpus, named ROOTS, combines data extracted from the then-latest version of the web-based OSCAR corpus (38% of ROOTS) and newly collected data extracted from a manually selected and documented list of language data sources. It encompasses 46 natural languages (in amounts ranging from 30% of the whole dataset for English to 0.00002% for Chi Tumbuka) and 13 programming languages.\r\n\r\n\r\n== References ==", "start_char_idx": 0, "end_char_idx": 1267, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a21b3823-2625-44e8-8214-131b22a77683": {"__data__": {"id_": "a21b3823-2625-44e8-8214-131b22a77683", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Chinchilla (language model).txt", "file_name": "Chinchilla (language model).txt", "file_type": "text/plain", "file_size": 2231, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d5eaea50-ecd2-4c57-85d7-276f893e80e9", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Chinchilla (language model).txt", "file_name": "Chinchilla (language model).txt", "file_type": "text/plain", "file_size": 2231, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "6973e2fe4d2dfba16517668232875125896ba07026f38e42c55fabac533ca654", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bb4bc2b4-7bdb-4111-b4d6-78c30911d723", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\BLOOM (language model).txt", "file_name": "BLOOM (language model).txt", "file_type": "text/plain", "file_size": 1267, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "4aec30877d77ca8da7d1716b8a9674d9e8bf1fe226031949525c7d0178428579", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9e6d6aaf-f4e4-43b6-93de-c206c324a10d", "node_type": "1", "metadata": {}, "hash": "d00f885dbb839d7341b1f131ae4b6e9a1efa4e2f1aa92b6944a3fa492f17a108", "class_name": "RelatedNodeInfo"}}, "text": "Chinchilla is a family of large language models developed by the research team at DeepMind, presented in March 2022. It is named \"chinchilla\" because it is a further development over a previous model family named Gopher. Both model families were trained in order to investigate the scaling laws of large language models.It claimed to outperform GPT-3. It considerably simplifies downstream utilization because it requires much less computer power for inference and fine-tuning. Based on the training of previously employed language models, it has been determined that if one doubles the model size, one must also have twice the number of training tokens. This hypothesis has been used to train Chinchilla by DeepMind. Similar to Gopher in terms of cost, Chinchilla has 70B parameters and four times as much data.Chinchilla has an average accuracy of 67.5% on the MMLU benchmark (Measuring Massive Multitask Language Understanding), which is 7% higher than Gopher's performance. Chinchilla was still in the testing phase as of January 12, 2023.Chinchilla contributes to developing an effective training paradigm for large autoregressive language models with limited compute resources. The Chinchilla team recommends that the number of training tokens is twice for every model size doubling, meaning that using larger, higher-quality training datasets can lead to better results on downstream tasks.\r\n\r\n\r\n== Architecture ==\r\nBoth the Gopher family and Chinchilla family are families of transformer models. \r\nIn particular, they are essentially the same as GPT-2, with different sizes and minor modifications. Gopher family uses RMSNorm instead of LayerNorm; relative positional encoding rather than absolute positional encoding. The Chinchilla family is the same as the Gopher family, but trained with AdamW instead of Adam optimizer.\r\nThe Gopher family contains six models of increasing size, from 44 million parameters to 280 billion parameters. They refer to the largest one as \"Gopher\" by default. Similar naming conventions apply for the Chinchilla family.\r\nTable 1 of  shows the entire Gopher family:\r\n\r\nTable 4 of  compares the 70-billion-parameter Chinchilla with Gopher 280B.\r\n\r\n\r\n== See also ==\r\nLaMDA\r\n\r\n\r\n== References ==", "start_char_idx": 0, "end_char_idx": 2231, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e6d6aaf-f4e4-43b6-93de-c206c324a10d": {"__data__": {"id_": "9e6d6aaf-f4e4-43b6-93de-c206c324a10d", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Claude (language model).txt", "file_name": "Claude (language model).txt", "file_type": "text/plain", "file_size": 4305, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ddfbb310-f268-43fc-b55c-2576fb322b7c", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Claude (language model).txt", "file_name": "Claude (language model).txt", "file_type": "text/plain", "file_size": 4305, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "e7d513908c396612dce2e17a696215067d905c10d21294966b9b16bb6b408821", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a21b3823-2625-44e8-8214-131b22a77683", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Chinchilla (language model).txt", "file_name": "Chinchilla (language model).txt", "file_type": "text/plain", "file_size": 2231, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "6973e2fe4d2dfba16517668232875125896ba07026f38e42c55fabac533ca654", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bbfb1614-1d95-474b-8672-b6909c2f58f6", "node_type": "1", "metadata": {}, "hash": "c41d6e657aac39a03099cd933d6f7a56797b32fc0edbd67f13dadc713e540a23", "class_name": "RelatedNodeInfo"}}, "text": "Claude is a family of large language models developed by Anthropic. The first model was released in March 2023. Claude 3, released in March 2024, can also analyze images.\r\n\r\n\r\n== Training ==\r\nClaude models are generative pre-trained transformers. They have been pre-trained to predict the next word in large amounts of text. Claude models have then been fine-tuned with Constitutional AI and Reinforcement Learning From Human Feedback, with the aim of making them less likely to give harmful responses, while still being helpful to the user.\r\n\r\n\r\n=== Constitutional AI ===\r\nConstitutional AI is an approach developed by Anthropic for training AI systems, particularly language models like Claude, to be harmless and helpful without relying on extensive human feedback. The method, detailed in the paper \"Constitutional AI: Harmlessness from AI Feedback\" involves two phases: supervised learning and reinforcement learning.\r\nIn the supervised learning phase, the model generates responses to prompts, self-critiques these responses based on a set of guiding principles (a \"constitution\"), and then revises the responses. The reinforcement learning phase involves training the model with AI-generated feedback, where the AI evaluates responses according to the constitutional principles.\r\nThis approach enables the training of AI assistants that are both helpful and harmless, and that can explain their objections to harmful requests, enhancing transparency and reducing reliance on human supervision.The \"constitution\" for Claude included 75 points, including sections from the UN Universal Declaration of Human Rights.\r\n\r\n\r\n== Models ==\r\n\r\n\r\n=== Claude ===\r\nClaude was the initial version of Anthropic's language model released in March 2023, Claude demonstrated proficiency in various tasks but had certain limitations in coding, math, and reasoning capabilities. Anthropic partnered with companies like Notion (productivity software) and Quora (to help develop the Poe chatbot).\r\n\r\n\r\n=== Claude Instant ===\r\nClaude was released as two versions, Claude and Claude Instant, with Claude Instant being a faster, less expensive and lighter version. Claude Instant has a input context length of 100,000 tokens (which corresponds to around 75,000 words).\r\n\r\n\r\n=== Claude 2 ===\r\nClaude 2 was the next major iteration of Claude, which was released in July 11 2023 and available to the general public, whereas the Claude 1 was only available to selected users approved by Anthropic.Claude 2 expanded its context window from 9,000 tokens to 100,000 tokens. Features included ability to upload PDFs and other documents that enables Claude to read, summarise and assist with tasks.\r\n\r\n\r\n==== Claude 2.1 ====\r\nClaude 2.1 doubled the number of tokens that the chatbot could handle, increasing it to a window of 200,000 tokens, which equals around 500 pages of written material.Anthropic states that the new model is less likely to produce false statements compared to its predecessors.\r\n\r\n\r\n=== Claude 3 ===\r\nClaude 3 was released on March 14, 2024 with claims in the press release to have set new industry benchmarks across a wide range of cognitive tasks. The Claude 3 family includes three state-of-the-art models in ascending order of capability: Haiku, Sonnet, and Opus. The default version of Claude 3 Opus has a context window of 200,000 tokens, but this is being expanded to 1 million for specific use cases.Claude 3 has seemed to perform meta-cognitive reasoning, including the ability to realize it is being artificially tested during needle in a haystack evaluations.\r\n\r\n\r\n== Access ==\r\nLimited-use access is free of charge, but requires both an e-mail address and a cellphone number.\r\n\r\n\r\n== Criticism ==\r\nClaude 2 has faced criticism for its stringent ethical alignment that may reduce usability and performance. Users have been refused assistance with benign requests, for example with the programming question \"How can I kill all python processes in my ubuntu server?\" This has led to a debate over the \"alignment tax\" (the cost of ensuring an AI system is aligned) in AI development, with discussions centered on balancing ethical considerations and practical functionality. Critics argue for user autonomy and effectiveness, while proponents stress the importance of ethical AI.\r\n\r\n\r\n== References ==", "start_char_idx": 0, "end_char_idx": 4305, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bbfb1614-1d95-474b-8672-b6909c2f58f6": {"__data__": {"id_": "bbfb1614-1d95-474b-8672-b6909c2f58f6", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "65fe6622-9e19-4c31-912b-a1676a217719", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "31edaf0411c36a59c41ae21354f4047f70757c774e9eafd489e202e1df78e65e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9e6d6aaf-f4e4-43b6-93de-c206c324a10d", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Claude (language model).txt", "file_name": "Claude (language model).txt", "file_type": "text/plain", "file_size": 4305, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "e7d513908c396612dce2e17a696215067d905c10d21294966b9b16bb6b408821", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "22f2b7f2-67e1-4a70-8d7a-f4a3932826d9", "node_type": "1", "metadata": {}, "hash": "33c055bdfeabbc516b564df2a49c438299c3b02b775e18290fd4dfdc611b8043", "class_name": "RelatedNodeInfo"}}, "text": "Convolutional neural network (CNN) is a regularized type of feed-forward neural network that learns feature engineering by itself via filters (or kernel) optimization. Vanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by using regularized weights over fewer connections. For example, for each neuron in the fully-connected layer 10,000 weights would be required for processing an image sized 100 \u00d7 100 pixels. However, applying cascaded convolution (or cross-correlation) kernels,  only 25 neurons are required to process 5x5-sized tiles. Higher-layer features are extracted  from wider context windows, compared to lower-layer features.\r\nThey have applications in: \r\n\r\nimage and video recognition,\r\nrecommender systems,\r\n\r\nimage classification,\r\n\r\nimage segmentation,\r\n\r\nmedical image analysis,\r\n\r\nnatural language processing,\r\n\r\nbrain\u2013computer interfaces, and\r\n\r\nfinancial time series.CNNs are also known as Shift Invariant or Space Invariant Artificial Neural Networks (SIANN), based on the shared-weight architecture of the convolution kernels or filters that slide along input features and provide translation-equivariant responses known as feature maps. Counter-intuitively, most convolutional neural networks are not invariant to translation, due to the downsampling operation they apply to the input.Feed-forward neural networks are usually fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer. The \"full connectivity\" of these networks make them prone to overfitting data. Typical ways of regularization, or preventing overfitting, include: penalizing parameters during training (such as weight decay) or trimming connectivity (skipped connections, dropout, etc.) Robust datasets also increases the probability that CNNs will learn the generalized principles that characterize a given dataset rather than the biases of a poorly-populated set.Convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.\r\nCNNs use relatively little pre-processing compared to other image classification algorithms. This means that the network learns to optimize the filters (or kernels) through automated learning, whereas in traditional algorithms these filters are hand-engineered. This independence from prior knowledge and human intervention in feature extraction is a major advantage.\r\n\r\n\r\n== Architecture ==\r\n\r\nA convolutional neural network consists of an input layer, hidden layers and an output layer. In a convolutional neural network, the hidden layers include one or more layers that perform convolutions. Typically this includes a layer that performs a dot product of the convolution kernel with the layer's input matrix. This product is usually the Frobenius inner product, and its activation function is commonly ReLU. As the convolution kernel slides along the input matrix for the layer, the convolution operation generates a feature map, which in turn contributes to the input of the next layer. This is followed by other layers such as pooling layers, fully connected layers, and normalization layers.\r\nHere it should be noted how close a convolutional neural network is to a matched filter.\r\n\r\n\r\n=== Convolutional layers ===\r\nIn a CNN, the input is a tensor with shape: \r\n(number of inputs) \u00d7 (input height) \u00d7 (input width) \u00d7 (input channels)\r\nAfter passing through a convolutional layer, the image becomes abstracted to a feature map, also called an activation map, with shape: \r\n(number of inputs) \u00d7 (feature map height) \u00d7 (feature map width) \u00d7 (feature map channels).\r\nConvolutional layers convolve the input and pass its result to the next layer. This is similar to the response of a neuron in the visual cortex to a specific stimulus. Each convolutional neuron processes data only for its receptive field. \r\n\r\nAlthough fully connected feedforward neural networks can be used to learn features and classify data, this architecture is generally impractical for larger inputs (e.g., high-resolution images), which would require massive numbers of neurons because each pixel is a relevant input feature. A fully connected layer for an image of size 100 \u00d7 100 has 10,000 weights for each neuron in the second layer. Convolution reduces the number of free parameters, allowing the network to be deeper. For example, using a 5 \u00d7 5 tiling region, each with the same shared weights, requires only 25 neurons.", "start_char_idx": 0, "end_char_idx": 4769, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "22f2b7f2-67e1-4a70-8d7a-f4a3932826d9": {"__data__": {"id_": "22f2b7f2-67e1-4a70-8d7a-f4a3932826d9", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "65fe6622-9e19-4c31-912b-a1676a217719", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "31edaf0411c36a59c41ae21354f4047f70757c774e9eafd489e202e1df78e65e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bbfb1614-1d95-474b-8672-b6909c2f58f6", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "a08f6dc195c3fd117f7575555f148b0ce89f3eb22736ef6e92ac49164ed60846", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a573ad77-b1e0-48e9-94fc-d40d5abdd3e7", "node_type": "1", "metadata": {}, "hash": "c2eb52f461ba72271f8f5abf3224c96b34d0f8a65a3d6fcccc34ea09ff59b098", "class_name": "RelatedNodeInfo"}}, "text": "Convolutional layers convolve the input and pass its result to the next layer. This is similar to the response of a neuron in the visual cortex to a specific stimulus. Each convolutional neuron processes data only for its receptive field. \r\n\r\nAlthough fully connected feedforward neural networks can be used to learn features and classify data, this architecture is generally impractical for larger inputs (e.g., high-resolution images), which would require massive numbers of neurons because each pixel is a relevant input feature. A fully connected layer for an image of size 100 \u00d7 100 has 10,000 weights for each neuron in the second layer. Convolution reduces the number of free parameters, allowing the network to be deeper. For example, using a 5 \u00d7 5 tiling region, each with the same shared weights, requires only 25 neurons. Using regularized weights over fewer parameters avoids the vanishing gradients and exploding gradients problems seen during backpropagation in earlier neural networks.To speed processing, standard convolutional layers can be replaced by depthwise separable convolutional layers, which are based on a depthwise convolution followed by a pointwise convolution. The depthwise convolution is a spatial convolution applied independently over each channel of the input tensor, while the pointwise convolution is a standard convolution restricted to the use of 1\u00d71{\\displaystyle 1\\times 1}  kernels.\r\n\r\n\r\n=== Pooling layers ===\r\nConvolutional networks may include local and/or global pooling layers along with traditional convolutional layers. Pooling layers reduce the dimensions of data by combining the outputs of neuron clusters at one layer into a single neuron in the next layer. Local pooling combines small clusters, tiling sizes such as 2 \u00d7 2 are commonly used. Global pooling acts on all the neurons of the feature map. There are two common types of pooling in popular use: max and average. Max pooling uses the maximum value of each local cluster of neurons in the feature map, while average pooling takes the average value.\r\n\r\n\r\n=== Fully connected layers ===\r\nFully connected layers connect every neuron in one layer to every neuron in another layer. It is the same as a traditional multilayer perceptron neural network (MLP). The flattened matrix goes through a fully connected layer to classify the images.\r\n\r\n\r\n=== Receptive field ===\r\nIn neural networks, each neuron receives input from some number of locations in the previous layer. In a convolutional layer, each neuron receives input from only a restricted area of the previous layer called the neuron's receptive field. Typically the area is a square (e.g. 5 by 5 neurons). Whereas, in a fully connected layer, the receptive field is the entire previous layer. Thus, in each convolutional layer, each neuron takes input from a larger area in the input than previous layers. This is due to applying the convolution over and over, which takes the value of a pixel into account, as well as its surrounding pixels. When using dilated layers, the number of pixels in the receptive field remains constant, but the field is more sparsely populated as its dimensions grow when combining the effect of several layers.\r\nTo manipulate the receptive field size as desired, there are some alternatives to the standard convolutional layer. For example, atrous or dilated convolution expands the receptive field size without increasing the number of parameters by interleaving visible and blind regions. Moreover, a single dilated convolutional layer can comprise filters with multiple dilation ratios, thus having a variable receptive field size.\r\n\r\n\r\n=== Weights ===\r\nEach neuron in a neural network computes an output value by applying a specific function to the input values received from the receptive field in the previous layer. The function that is applied to the input values is determined by a vector of weights and a bias (typically real numbers). Learning consists of iteratively adjusting these biases and weights.\r\nThe vectors of weights and biases are called filters and represent particular features of the input (e.g., a particular shape). A distinguishing feature of CNNs is that many neurons can share the same filter. This reduces the memory footprint because a single bias and a single vector of weights are used across all receptive fields that share that filter, as opposed to each receptive field having its own bias and vector weighting.\r\n\r\n\r\n== History ==\r\nCNN are often compared to the way the brain achieves vision processing in living organisms.\r\n\r\n\r\n=== Receptive fields in the visual cortex ===\r\nWork by Hubel and Wiesel in the 1950s and 1960s showed that cat visual cortices contain neurons that individually respond to small regions of the visual field. Provided the eyes are not moving, the region of visual space within which visual stimuli affect the firing of a single neuron is known as its receptive field.", "start_char_idx": 3937, "end_char_idx": 8865, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a573ad77-b1e0-48e9-94fc-d40d5abdd3e7": {"__data__": {"id_": "a573ad77-b1e0-48e9-94fc-d40d5abdd3e7", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "65fe6622-9e19-4c31-912b-a1676a217719", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "31edaf0411c36a59c41ae21354f4047f70757c774e9eafd489e202e1df78e65e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "22f2b7f2-67e1-4a70-8d7a-f4a3932826d9", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "f1791e1a52ebef1d26194181c2d00b24e4f0cf41891fa36830df820aa19043b6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b340d61-1a2a-48da-aab9-f429ed28aa5a", "node_type": "1", "metadata": {}, "hash": "2ac32209c56b8428526912d95cee67d1f1b310f82c481fe4cc158d25c0838d3a", "class_name": "RelatedNodeInfo"}}, "text": "Learning consists of iteratively adjusting these biases and weights.\r\nThe vectors of weights and biases are called filters and represent particular features of the input (e.g., a particular shape). A distinguishing feature of CNNs is that many neurons can share the same filter. This reduces the memory footprint because a single bias and a single vector of weights are used across all receptive fields that share that filter, as opposed to each receptive field having its own bias and vector weighting.\r\n\r\n\r\n== History ==\r\nCNN are often compared to the way the brain achieves vision processing in living organisms.\r\n\r\n\r\n=== Receptive fields in the visual cortex ===\r\nWork by Hubel and Wiesel in the 1950s and 1960s showed that cat visual cortices contain neurons that individually respond to small regions of the visual field. Provided the eyes are not moving, the region of visual space within which visual stimuli affect the firing of a single neuron is known as its receptive field. Neighboring cells have similar and overlapping receptive fields. Receptive field size and location varies systematically across the cortex to form a complete map of visual space. The cortex in each hemisphere represents the contralateral visual field.Their 1968 paper identified two basic visual cell types in the brain:\r\nsimple cells, whose output is maximized by straight edges having particular orientations within their receptive field\r\ncomplex cells, which have larger receptive fields, whose output is insensitive to the exact position of the edges in the field.Hubel and Wiesel also proposed a cascading model of these two types of cells for use in pattern recognition tasks.\r\n\r\n\r\n=== Neocognitron, origin of the CNN architecture ===\r\nThe \"neocognitron\" was introduced by Kunihiko Fukushima in 1980.\r\nIt was inspired by the above-mentioned work of Hubel and Wiesel. The neocognitron introduced the two basic types of layers in CNNs: \r\n\r\nA convolutional layer which contains units whose receptive fields cover a patch of the previous layer. The weight vector (the set of adaptive parameters) of such a unit is often called a filter. Units can share filters.\r\nDownsampling layers which contain units whose receptive fields cover patches of previous convolutional layers. Such a unit typically computes the average of the activations of the units in its patch. This downsampling helps to correctly classify objects in visual scenes even when the objects are shifted.In 1969, Kunihiko Fukushima also introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for CNNs and  deep neural networks in general.In a variant of the neocognitron called the cresceptron, instead of using Fukushima's spatial averaging, J. Weng et al. in 1993 introduced a method called max-pooling where a downsampling unit computes the maximum of the activations of the units in its patch. Max-pooling is often used in modern CNNs.Several supervised and unsupervised learning algorithms have been proposed over the decades to train the weights of a neocognitron. Today, however, the CNN architecture is usually trained through backpropagation.\r\nThe neocognitron is the first CNN which requires units located at multiple network positions to have shared weights.\r\nConvolutional neural networks were presented at the Neural Information Processing Workshop in 1987, automatically analyzing time-varying signals by replacing learned multiplication with convolution in time, and demonstrated for speech recognition.\r\n\r\n\r\n=== Time delay neural networks ===\r\nThe time delay neural network (TDNN) was introduced in 1987 by Alex Waibel et al. for phoneme recognition and was one of the first convolutional networks, as it achieved shift-invariance. A TDNN is a 1-D convolutional neural net where the convolution is performed along the time axis of the data. It is the first CNN utilizing weight sharing in combination with a training by gradient descent, using backpropagation. Thus, while also using a pyramidal structure as in the neocognitron, it performed a global optimization of the weights instead of a local one.. \r\nTDNNs are convolutional networks that share weights along the temporal dimension. They allow speech signals to be processed time-invariantly. In 1990 Hampshire and Waibel introduced a variant that performs a two-dimensional convolution. Since these TDNNs operated on spectrograms, the resulting phoneme recognition system was invariant to both time and frequency shifts. This inspired translation invariance in image processing with CNNs. The tiling of neuron outputs can cover timed stages.TDNNs now achieve the best performance in far-distance speech recognition.\r\n\r\n\r\n==== Max pooling ====\r\nIn 1990 Yamaguchi et al.", "start_char_idx": 7879, "end_char_idx": 12645, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b340d61-1a2a-48da-aab9-f429ed28aa5a": {"__data__": {"id_": "4b340d61-1a2a-48da-aab9-f429ed28aa5a", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "65fe6622-9e19-4c31-912b-a1676a217719", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "31edaf0411c36a59c41ae21354f4047f70757c774e9eafd489e202e1df78e65e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a573ad77-b1e0-48e9-94fc-d40d5abdd3e7", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "eaea15c52760d2e53d9b0da0d6416f600cf438f709fd115e80a0d5341d70ff1b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0f387ccc-464e-46c4-9703-dadf47ad89b6", "node_type": "1", "metadata": {}, "hash": "e46044da040e73956daf0f2a520fa7988b416e6fee4075162e66cd837b249e64", "class_name": "RelatedNodeInfo"}}, "text": "It is the first CNN utilizing weight sharing in combination with a training by gradient descent, using backpropagation. Thus, while also using a pyramidal structure as in the neocognitron, it performed a global optimization of the weights instead of a local one.. \r\nTDNNs are convolutional networks that share weights along the temporal dimension. They allow speech signals to be processed time-invariantly. In 1990 Hampshire and Waibel introduced a variant that performs a two-dimensional convolution. Since these TDNNs operated on spectrograms, the resulting phoneme recognition system was invariant to both time and frequency shifts. This inspired translation invariance in image processing with CNNs. The tiling of neuron outputs can cover timed stages.TDNNs now achieve the best performance in far-distance speech recognition.\r\n\r\n\r\n==== Max pooling ====\r\nIn 1990 Yamaguchi et al. introduced the concept of max pooling, a fixed filtering operation that calculates and propagates the maximum value of a given region. They did so by combining TDNNs with max pooling to realize a speaker-independent isolated word recognition system. In their system they used several TDNNs per word, one for each syllable. The results of each TDNN over the input signal were combined using max pooling and the outputs of the pooling layers were then passed on to networks performing the actual word classification.\r\n\r\n\r\n=== Image recognition with CNNs trained by gradient descent ===\r\nDenker et al. (1989) designed a 2-D CNN system to recognize hand-written ZIP Code numbers. However, the lack of an efficient training method to determine the kernel coefficients of the involved convolutions meant that all the coefficients had to be laboriously hand-designed.Following the advances in the training of 1-D CNNs by Waibel et al. (1987), Yann LeCun et al. (1989) used back-propagation to learn the convolution kernel coefficients directly from images of hand-written numbers. Learning was thus fully automatic, performed better than manual coefficient design, and was suited to a broader range of image recognition problems and image types. \r\nWei Zhang et al. (1988) used back-propagation to train the convolution kernels of a CNN for alphabets recognition. The model was called Shift-Invariant Artificial Neural Network (SIANN) before the name CNN was coined later in the early 1990s. Wei Zhang et al. also applied the same CNN without the last fully connected layer for medical image object segmentation (1991) and breast cancer detection in mammograms (1994).This approach became a foundation of modern computer vision.\r\n\r\n\r\n==== LeNet-5 ====\r\n\r\nLeNet-5, a pioneering 7-level convolutional network by LeCun et al. in 1995, that classifies digits, was applied by several banks to recognize hand-written numbers on checks (British English: cheques) digitized in 32x32 pixel images. The ability to process higher-resolution images requires larger and more layers of convolutional neural networks, so this technique is constrained by the availability of computing resources.\r\n\r\n\r\n=== Shift-invariant neural network ===\r\nA shift-invariant neural network was proposed by Wei Zhang et al. for image character recognition in 1988. It is a modified Neocognitron by keeping only the convolutional interconnections between the image feature layers and the last fully connected layer.  The model was trained with back-propagation. The training algorithm were further improved in 1991 to improve its generalization ability. The model architecture was modified by removing the last fully connected layer and applied for medical image segmentation (1991) and automatic detection of breast cancer in mammograms (1994).A different convolution-based design was proposed in 1988 for application to decomposition of one-dimensional electromyography convolved signals via de-convolution. This design was modified in 1989 to other de-convolution-based designs.\r\n\r\n\r\n=== Neural abstraction pyramid ===\r\nThe feed-forward architecture of convolutional neural networks was extended in the neural abstraction pyramid by lateral and feedback connections. The resulting recurrent convolutional network allows for the flexible incorporation of contextual information to iteratively resolve local ambiguities. In contrast to previous models, image-like outputs at the highest resolution were generated, e.g., for semantic segmentation, image reconstruction, and object localization tasks.\r\n\r\n\r\n=== GPU implementations ===\r\nAlthough CNNs were invented in the 1980s, their breakthrough in the 2000s required fast implementations on graphics processing units (GPUs).\r\nIn 2004, it was shown by K. S. Oh and K. Jung that standard neural networks can be greatly accelerated on GPUs.", "start_char_idx": 11761, "end_char_idx": 16492, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0f387ccc-464e-46c4-9703-dadf47ad89b6": {"__data__": {"id_": "0f387ccc-464e-46c4-9703-dadf47ad89b6", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "65fe6622-9e19-4c31-912b-a1676a217719", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "31edaf0411c36a59c41ae21354f4047f70757c774e9eafd489e202e1df78e65e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b340d61-1a2a-48da-aab9-f429ed28aa5a", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "a79c39e6e42f9937848109c078fa1ca57ac4aff34b0ad0a1443f6d9b7fcb7049", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "724b2112-4bba-4b29-9023-e6ef6755e5ce", "node_type": "1", "metadata": {}, "hash": "5505ebaa0fa55f87c36490b05340702c786503de3a44dc5b249037c17c114f78", "class_name": "RelatedNodeInfo"}}, "text": "This design was modified in 1989 to other de-convolution-based designs.\r\n\r\n\r\n=== Neural abstraction pyramid ===\r\nThe feed-forward architecture of convolutional neural networks was extended in the neural abstraction pyramid by lateral and feedback connections. The resulting recurrent convolutional network allows for the flexible incorporation of contextual information to iteratively resolve local ambiguities. In contrast to previous models, image-like outputs at the highest resolution were generated, e.g., for semantic segmentation, image reconstruction, and object localization tasks.\r\n\r\n\r\n=== GPU implementations ===\r\nAlthough CNNs were invented in the 1980s, their breakthrough in the 2000s required fast implementations on graphics processing units (GPUs).\r\nIn 2004, it was shown by K. S. Oh and K. Jung that standard neural networks can be greatly accelerated on GPUs. Their implementation was 20 times faster than an equivalent implementation on CPU. In 2005, another paper also emphasised the value of GPGPU for machine learning.The first GPU-implementation of a CNN was described in 2006 by K. Chellapilla et al. Their implementation was 4 times faster than an equivalent implementation on CPU. Subsequent work also used GPUs, initially for other types of neural networks (different from CNNs), especially unsupervised neural networks.In 2010, Dan Ciresan et al. at IDSIA showed that even deep standard neural networks with many layers can be quickly trained on GPU by supervised learning through the old method known as backpropagation. Their network outperformed previous machine learning methods on the MNIST handwritten digits benchmark. In 2011, they extended this GPU approach to CNNs, achieving an acceleration factor of 60, with impressive results. In 2011, they used such CNNs on GPU to win an image recognition contest where they achieved superhuman performance for the first time. Between May 15, 2011, and September 30, 2012, their CNNs won no less than four image competitions. In 2012, they also significantly improved on the best performance in the literature for multiple image databases, including the MNIST database, the NORB database, the HWDB1.0 dataset (Chinese characters) and the CIFAR10 dataset (dataset of 60000 32x32 labeled RGB images).Subsequently, a similar GPU-based CNN by Alex Krizhevsky et al. won the ImageNet Large Scale Visual Recognition Challenge 2012. A very deep CNN with over 100 layers by Microsoft won the ImageNet 2015 contest.\r\n\r\n\r\n=== Intel Xeon Phi implementations ===\r\nCompared to the training of CNNs using GPUs, not much attention was given to the Intel Xeon Phi coprocessor.\r\nA notable development is a parallelization method for training convolutional neural networks on the Intel Xeon Phi, named Controlled Hogwild with Arbitrary Order of Synchronization (CHAOS).\r\nCHAOS exploits both the thread- and SIMD-level parallelism that is available on the Intel Xeon Phi.\r\n\r\n\r\n== Distinguishing features ==\r\nIn the past, traditional multilayer perceptron (MLP) models were used for image recognition. However, the full connectivity between nodes caused the curse of dimensionality, and was computationally intractable with higher-resolution images. A 1000\u00d71000-pixel image with RGB color channels has 3 million weights per fully-connected neuron, which is too high to feasibly process efficiently at scale.\r\n\r\nFor example, in CIFAR-10, images are only of size 32\u00d732\u00d73 (32 wide, 32 high, 3 color channels), so a single fully connected neuron in the first hidden layer of a regular neural network would have 32*32*3 = 3,072 weights. A 200\u00d7200 image, however, would lead to neurons that have 200*200*3 = 120,000 weights.\r\nAlso, such network architecture does not take into account the spatial structure of data, treating input pixels which are far apart in the same way as pixels that are close together. This ignores locality of reference in data with a grid-topology (such as images), both computationally and semantically. Thus, full connectivity of neurons is wasteful for purposes such as image recognition that are dominated by spatially local input patterns.\r\nConvolutional neural networks are variants of multilayer perceptrons, designed to emulate the behavior of a visual cortex. These models mitigate the challenges posed by the MLP architecture by exploiting the strong spatially local correlation present in natural images. As opposed to MLPs, CNNs have the following distinguishing features:\r\n\r\n3D volumes of neurons.", "start_char_idx": 15614, "end_char_idx": 20101, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "724b2112-4bba-4b29-9023-e6ef6755e5ce": {"__data__": {"id_": "724b2112-4bba-4b29-9023-e6ef6755e5ce", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "65fe6622-9e19-4c31-912b-a1676a217719", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "31edaf0411c36a59c41ae21354f4047f70757c774e9eafd489e202e1df78e65e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0f387ccc-464e-46c4-9703-dadf47ad89b6", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "9552455aea6655e861140404a4d52f84c7bb6248d95c863cea39fbc4c0b4e9c8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9b961931-c0b6-4ff5-99db-198244be0ee5", "node_type": "1", "metadata": {}, "hash": "7697e77cc97565dd6abf38cc4287c86eba59556c9eb0ce16bb461bee37ac0715", "class_name": "RelatedNodeInfo"}}, "text": "A 200\u00d7200 image, however, would lead to neurons that have 200*200*3 = 120,000 weights.\r\nAlso, such network architecture does not take into account the spatial structure of data, treating input pixels which are far apart in the same way as pixels that are close together. This ignores locality of reference in data with a grid-topology (such as images), both computationally and semantically. Thus, full connectivity of neurons is wasteful for purposes such as image recognition that are dominated by spatially local input patterns.\r\nConvolutional neural networks are variants of multilayer perceptrons, designed to emulate the behavior of a visual cortex. These models mitigate the challenges posed by the MLP architecture by exploiting the strong spatially local correlation present in natural images. As opposed to MLPs, CNNs have the following distinguishing features:\r\n\r\n3D volumes of neurons. The layers of a CNN have neurons arranged in 3 dimensions: width, height and depth. Where each neuron inside a convolutional layer is connected to only a small region of the layer before it, called a receptive field. Distinct types of layers, both locally and completely connected, are stacked to form a CNN architecture.\r\nLocal connectivity: following the concept of receptive fields, CNNs exploit spatial locality by enforcing a local connectivity pattern between neurons of adjacent layers. The architecture thus ensures that the learned \"filters\" produce the strongest response to a spatially local input pattern. Stacking many such layers leads to nonlinear filters that become increasingly global (i.e. responsive to a larger region of pixel space) so that the network first creates representations of small parts of the input, then from them assembles representations of larger areas.\r\nShared weights: In CNNs, each filter is replicated across the entire visual field. These replicated units share the same parameterization (weight vector and bias) and form a feature map. This means that all the neurons in a given convolutional layer respond to the same feature within their specific response field. Replicating units in this way allows for the resulting activation map to be equivariant under shifts of the locations of input features in the visual field, i.e. they grant translational equivariance - given that the layer has a stride of one.\r\nPooling: In a CNN's pooling layers, feature maps are divided into rectangular sub-regions, and the features in each rectangle are independently down-sampled to a single value, commonly by taking their average or maximum value. In addition to reducing the sizes of feature maps, the pooling operation grants a degree of local translational invariance to the features contained therein, allowing the CNN to be more robust to variations in their positions.Together, these properties allow CNNs to achieve better generalization on vision problems. Weight sharing dramatically reduces the number of free parameters learned, thus lowering the memory requirements for running the network and allowing the training of larger, more powerful networks.\r\n\r\n\r\n== Building blocks ==\r\n\r\nA CNN architecture is formed by a stack of distinct layers that transform the input volume into an output volume (e.g. holding the class scores) through a differentiable function. A few distinct types of layers are commonly used. These are further discussed below.\r\n\r\n\r\n=== Convolutional layer ===\r\nThe convolutional layer is the core building block of a CNN. The layer's parameters consist of a set of learnable filters (or kernels), which have a small receptive field, but extend through the full depth of the input volume. During the forward pass, each filter is convolved across the width and height of the input volume, computing the dot product between the filter entries and the input, producing a 2-dimensional activation map of that filter. As a result, the network learns filters that activate when it detects some specific type of feature at some spatial position in the input.Stacking the activation maps for all filters along the depth dimension forms the full output volume of the convolution layer. Every entry in the output volume can thus also be interpreted as an output of a neuron that looks at a small region in the input. Each entry in an activation map use the same set of parameters that define the filter.\r\nSelf-supervised learning has been adapted for use in convolutional layers by using sparse patches with a high-mask ratio and a global response normalization layer.\r\n\r\n\r\n==== Local connectivity ====\r\nWhen dealing with high-dimensional inputs such as images, it is impractical to connect neurons to all neurons in the previous volume because such a network architecture does not take the spatial structure of the data into account. Convolutional networks exploit spatially local correlation by enforcing a sparse local connectivity pattern between neurons of adjacent layers: each neuron is connected to only a small region of the input volume.", "start_char_idx": 19204, "end_char_idx": 24202, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9b961931-c0b6-4ff5-99db-198244be0ee5": {"__data__": {"id_": "9b961931-c0b6-4ff5-99db-198244be0ee5", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "65fe6622-9e19-4c31-912b-a1676a217719", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "31edaf0411c36a59c41ae21354f4047f70757c774e9eafd489e202e1df78e65e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "724b2112-4bba-4b29-9023-e6ef6755e5ce", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "1438726e91d1dbd0f2e33f247d360e14c1cd9c0a9e49e0f30ef6e2cf98d80481", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ab5cbcc7-f164-476c-80ed-9e0f2fcc8c12", "node_type": "1", "metadata": {}, "hash": "37620a8ec5ca011aec01a35cfd107311f4c6b14cbc9c74e7af83a5691a3f9bd2", "class_name": "RelatedNodeInfo"}}, "text": "Every entry in the output volume can thus also be interpreted as an output of a neuron that looks at a small region in the input. Each entry in an activation map use the same set of parameters that define the filter.\r\nSelf-supervised learning has been adapted for use in convolutional layers by using sparse patches with a high-mask ratio and a global response normalization layer.\r\n\r\n\r\n==== Local connectivity ====\r\nWhen dealing with high-dimensional inputs such as images, it is impractical to connect neurons to all neurons in the previous volume because such a network architecture does not take the spatial structure of the data into account. Convolutional networks exploit spatially local correlation by enforcing a sparse local connectivity pattern between neurons of adjacent layers: each neuron is connected to only a small region of the input volume.\r\nThe extent of this connectivity is a hyperparameter called the receptive field of the neuron. The connections are local in space (along width and height), but always extend along the entire depth of the input volume. Such an architecture ensures that the learned (British English: learnt) filters produce the strongest response to a spatially local input pattern.\r\n\r\n\r\n==== Spatial arrangement ====\r\nThree hyperparameters control the size of the output volume of the convolutional layer: the depth, stride, and padding size:\r\n\r\nThe depth of the output volume controls the number of neurons in a layer that connect to the same region of the input volume. These neurons learn to activate for different features in the input. For example, if the first convolutional layer takes the raw image as input, then different neurons along the depth dimension may activate in the presence of various oriented edges, or blobs of color.\r\nStride controls how depth columns around the width and height are allocated. If the stride is 1, then we move the filters one pixel at a time. This leads to heavily overlapping receptive fields between the columns, and to large output volumes. For any integer S>0,{\\textstyle S>0,} a stride S means that the filter is translated S units at a time per output. In practice, S\u22653{\\textstyle S\\geq 3} is rare. A greater stride means smaller overlap of receptive fields and smaller spatial dimensions of the output volume.\r\nSometimes, it is convenient to pad the input with zeros (or other values, such as the average of the region) on the border of the input volume. The size of this padding is a third hyperparameter. Padding provides control of the output volume's spatial size. In particular, sometimes it is desirable to exactly preserve the spatial size of the input volume, this is commonly referred to as \"same\" padding.The spatial size of the output volume is a function of the input volume size W{\\displaystyle W}, the kernel field size K{\\displaystyle K} of the convolutional layer neurons, the stride S{\\displaystyle S}, and the amount of zero padding P{\\displaystyle P} on the border. The number of neurons that \"fit\" in a given volume is then:\r\n\r\nIf this number is not an integer, then the strides are incorrect and the neurons cannot be tiled to fit across the input volume in a symmetric way. In general, setting zero padding to be P=(K\u22121)/2{\\textstyle P=(K-1)/2} when the stride is S=1{\\displaystyle S=1} ensures that the input volume and output volume will have the same size spatially. However, it is not always completely necessary to use all of the neurons of the previous layer. For example, a neural network designer may decide to use just a portion of padding.\r\n\r\n\r\n==== Parameter sharing ====\r\nA parameter sharing scheme is used in convolutional layers to control the number of free parameters. It relies on the assumption that if a patch feature is useful to compute at some spatial position, then it should also be useful to compute at other positions. Denoting a single 2-dimensional slice of depth as a depth slice, the neurons in each depth slice are constrained to use the same weights and bias.\r\nSince all neurons in a single depth slice share the same parameters, the forward pass in each depth slice of the convolutional layer can be computed as a convolution of the neuron's weights with the input volume. Therefore, it is common to refer to the sets of weights as a filter (or a kernel), which is convolved with the input. The result of this convolution is an activation map, and the set of activation maps for each different filter are stacked together along the depth dimension to produce the output volume. Parameter sharing contributes to the translation invariance of the CNN architecture.Sometimes, the parameter sharing assumption may not make sense.", "start_char_idx": 23342, "end_char_idx": 28015, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab5cbcc7-f164-476c-80ed-9e0f2fcc8c12": {"__data__": {"id_": "ab5cbcc7-f164-476c-80ed-9e0f2fcc8c12", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "65fe6622-9e19-4c31-912b-a1676a217719", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "31edaf0411c36a59c41ae21354f4047f70757c774e9eafd489e202e1df78e65e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9b961931-c0b6-4ff5-99db-198244be0ee5", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "fa14cc168dc454c159c3995a6438b2b013657b123585d5824722a9362f47b33c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ce9ea363-3406-43bf-94ee-ae5f55ff43b9", "node_type": "1", "metadata": {}, "hash": "946163151b8e9373d5c7d30fa230849daf72b7614fd22114da9a5fb50775851b", "class_name": "RelatedNodeInfo"}}, "text": "It relies on the assumption that if a patch feature is useful to compute at some spatial position, then it should also be useful to compute at other positions. Denoting a single 2-dimensional slice of depth as a depth slice, the neurons in each depth slice are constrained to use the same weights and bias.\r\nSince all neurons in a single depth slice share the same parameters, the forward pass in each depth slice of the convolutional layer can be computed as a convolution of the neuron's weights with the input volume. Therefore, it is common to refer to the sets of weights as a filter (or a kernel), which is convolved with the input. The result of this convolution is an activation map, and the set of activation maps for each different filter are stacked together along the depth dimension to produce the output volume. Parameter sharing contributes to the translation invariance of the CNN architecture.Sometimes, the parameter sharing assumption may not make sense. This is especially the case when the input images to a CNN have some specific centered structure; for which we expect completely different features to be learned on different spatial locations. One practical example is when the inputs are faces that have been centered in the image: we might expect different eye-specific or hair-specific features to be learned in different parts of the image. In that case it is common to relax the parameter sharing scheme, and instead simply call the layer a \"locally connected layer\".\r\n\r\n\r\n=== Pooling layer ===\r\nAnother important concept of CNNs is pooling, which is a form of non-linear down-sampling. There are several non-linear functions to implement pooling, where max pooling is the most common. It partitions the input image into a set of rectangles and, for each such sub-region, outputs the maximum.\r\nIntuitively, the exact location of a feature is less important than its rough location relative to other features. This is the idea behind the use of pooling in convolutional neural networks. The pooling layer serves to progressively reduce the spatial size of the representation, to reduce the number of parameters, memory footprint and amount of computation in the network, and hence to also control overfitting. This is known as down-sampling. It is common to periodically insert a pooling layer between successive convolutional layers (each one typically followed by an activation function, such as a ReLU layer) in a CNN architecture.:\u200a460\u2013461\u200a While pooling layers contribute to local translation invariance, they do not provide global translation invariance in a CNN, unless a form of global pooling is used. The pooling layer commonly operates independently on every depth, or slice, of the input and resizes it spatially. A very common form of max pooling is a layer with filters of size 2\u00d72, applied with a stride of 2, which subsamples every depth slice in the input by 2 along both width and height, discarding 75% of the activations:\r\nIn this case, every max operation is over 4 numbers. The depth dimension remains unchanged (this is true for other forms of pooling as well).\r\nIn addition to max pooling, pooling units can use other functions, such as average pooling or \u21132-norm pooling. Average pooling was often used historically but has recently fallen out of favor compared to max pooling, which generally performs better in practice.Due to the effects of fast spatial reduction of the size of the representation, there is a recent trend towards using smaller filters or discarding pooling layers altogether.\r\n\"Region of Interest\" pooling (also known as RoI pooling) is a variant of max pooling, in which output size is fixed and input rectangle is a parameter.Pooling is a downsampling method and an important component of convolutional neural networks for object detection based on the Fast R-CNN architecture.\r\n\r\n\r\n=== Channel Max Pooling ===\r\nA CMP operation layer conducts the MP operation along the channel side among the corresponding positions of the consecutive feature maps for the purpose of redundant information elimination. The CMP makes the significant features gather together within fewer channels, which is important for fine-grained image classification that needs more discriminating features. Meanwhile, another advantage of the CMP operation is to make the channel number of feature maps smaller before it connects to the first fully connected (FC) layer. Similar to the MP operation, we denote the input feature maps and output feature maps of a CMP layer as F \u2208 R(C\u00d7M\u00d7N) and C \u2208 R(c\u00d7M\u00d7N), respectively, where C and c are the channel numbers of the input and output feature maps, M and N are the widths and the height of the feature maps, respectively. Note that the CMP operation only changes the channel number of the feature maps.", "start_char_idx": 27042, "end_char_idx": 31839, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ce9ea363-3406-43bf-94ee-ae5f55ff43b9": {"__data__": {"id_": "ce9ea363-3406-43bf-94ee-ae5f55ff43b9", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "65fe6622-9e19-4c31-912b-a1676a217719", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "31edaf0411c36a59c41ae21354f4047f70757c774e9eafd489e202e1df78e65e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ab5cbcc7-f164-476c-80ed-9e0f2fcc8c12", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "7c52e375f6e1762b5d97d4ec2e66a1300b31931dbce9627b8406ce1898edb6c2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6b6f2954-0cb0-4cd8-a915-92b11769eb07", "node_type": "1", "metadata": {}, "hash": "37b26aa88bb37b467b23e3642f0a1ae95d503e9bb2baf6b4cde52ae9cd02bbca", "class_name": "RelatedNodeInfo"}}, "text": "=== Channel Max Pooling ===\r\nA CMP operation layer conducts the MP operation along the channel side among the corresponding positions of the consecutive feature maps for the purpose of redundant information elimination. The CMP makes the significant features gather together within fewer channels, which is important for fine-grained image classification that needs more discriminating features. Meanwhile, another advantage of the CMP operation is to make the channel number of feature maps smaller before it connects to the first fully connected (FC) layer. Similar to the MP operation, we denote the input feature maps and output feature maps of a CMP layer as F \u2208 R(C\u00d7M\u00d7N) and C \u2208 R(c\u00d7M\u00d7N), respectively, where C and c are the channel numbers of the input and output feature maps, M and N are the widths and the height of the feature maps, respectively. Note that the CMP operation only changes the channel number of the feature maps. The width and the height of the feature maps are not changed, which is different from the MP operation.\r\n\r\n\r\n=== ReLU layer ===\r\nReLU is the abbreviation of rectified linear unit introduced by Kunihiko Fukushima in 1969. ReLU applies the non-saturating activation function f(x)=max(0,x){\\textstyle f(x)=\\max(0,x)}. It effectively removes negative values from an activation map by setting them to zero. It introduces nonlinearity to the decision function and in the overall network without affecting the receptive fields of the convolution layers.\r\nIn 2011, Xavier Glorot, Antoine Bordes and Yoshua Bengio found that ReLU enables better training of deeper networks, compared to widely used activation functions prior to 2011.\r\nOther functions can also be used to increase nonlinearity, for example the saturating hyperbolic tangent f(x)=tanh\u2061(x){\\displaystyle f(x)=\\tanh(x)}, f(x)=|tanh\u2061(x)|{\\displaystyle f(x)=|\\tanh(x)|}, and the sigmoid function \u03c3(x)=(1+e\u2212x)\u22121{\\textstyle \\sigma (x)=(1+e^{-x})^{-1}}. ReLU is often preferred to other functions because it trains the neural network several times faster without a significant penalty to generalization accuracy.\r\n\r\n\r\n=== Fully connected layer ===\r\nAfter several convolutional and max pooling layers, the final classification is done via fully connected layers. Neurons in a fully connected layer have connections to all activations in the previous layer, as seen in regular (non-convolutional) artificial neural networks. Their activations can thus be computed as an affine transformation, with matrix multiplication followed by a bias offset (vector addition of a learned or fixed bias term).\r\n\r\n\r\n=== Loss layer ===\r\n\r\nThe \"loss layer\", or \"loss function\", specifies how training penalizes the deviation between the predicted output of the network, and the true data labels (during supervised learning). Various loss functions can be used, depending on the specific task.\r\nThe Softmax loss function is used for predicting a single class of K mutually exclusive classes. Sigmoid cross-entropy loss is used for predicting K independent probability values in [0,1]{\\displaystyle [0,1]}. Euclidean loss is used for regressing to real-valued labels (\u2212\u221e,\u221e){\\displaystyle (-\\infty ,\\infty )}.\r\n\r\n\r\n== Hyperparameters ==\r\nHyperparameters are various settings that are used to control the learning process. CNNs use more hyperparameters than a standard multilayer perceptron (MLP).\r\n\r\n\r\n=== Kernel size ===\r\nThe kernel is the number of pixels processed together. It is typically expressed as the kernel's dimensions, e.g., 2x2, or 3x3.\r\n\r\n\r\n=== Padding ===\r\nPadding is the addition of (typically) 0-valued pixels on the borders of an image. This is done so that the border pixels are not undervalued (lost) from the output because they would ordinarily participate in only a single receptive field instance. The padding applied is typically one less than the corresponding kernel dimension. For example, a convolutional layer using 3x3 kernels would receive a 2-pixel pad, that is 1 pixel on each side of the image.\r\n\r\n\r\n=== Stride ===\r\nThe stride is the number of pixels that the analysis window moves on each iteration. A stride of 2 means that each kernel is offset by 2 pixels from its predecessor.\r\n\r\n\r\n=== Number of filters ===\r\nSince feature map size decreases with depth, layers near the input layer tend to have fewer filters while higher layers can have more.", "start_char_idx": 30901, "end_char_idx": 35252, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6b6f2954-0cb0-4cd8-a915-92b11769eb07": {"__data__": {"id_": "6b6f2954-0cb0-4cd8-a915-92b11769eb07", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "65fe6622-9e19-4c31-912b-a1676a217719", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "31edaf0411c36a59c41ae21354f4047f70757c774e9eafd489e202e1df78e65e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ce9ea363-3406-43bf-94ee-ae5f55ff43b9", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "f3a2533453bff0285ee100dbfd87c00044a3b90e9302fa4ee5c1b340d999a853", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f4112708-7739-4316-9865-8f212a7cc78b", "node_type": "1", "metadata": {}, "hash": "183b46e563eb36750f2e64003727e25422d9d0dd456e6aac82274d83baebcdd5", "class_name": "RelatedNodeInfo"}}, "text": "=== Padding ===\r\nPadding is the addition of (typically) 0-valued pixels on the borders of an image. This is done so that the border pixels are not undervalued (lost) from the output because they would ordinarily participate in only a single receptive field instance. The padding applied is typically one less than the corresponding kernel dimension. For example, a convolutional layer using 3x3 kernels would receive a 2-pixel pad, that is 1 pixel on each side of the image.\r\n\r\n\r\n=== Stride ===\r\nThe stride is the number of pixels that the analysis window moves on each iteration. A stride of 2 means that each kernel is offset by 2 pixels from its predecessor.\r\n\r\n\r\n=== Number of filters ===\r\nSince feature map size decreases with depth, layers near the input layer tend to have fewer filters while higher layers can have more. To equalize computation at each layer, the product of feature values va with pixel position is kept roughly constant across layers. Preserving more information about the input would require keeping the total number of activations (number of feature maps times number of pixel positions) non-decreasing from one layer to the next.\r\nThe number of feature maps directly controls the capacity and depends on the number of available examples and task complexity.\r\n\r\n\r\n=== Filter size ===\r\nCommon filter sizes found in the literature vary greatly, and are usually chosen based on the data set.\r\nThe challenge is to find the right level of granularity so as to create abstractions at the proper scale, given a particular data set, and without overfitting.\r\n\r\n\r\n=== Pooling type and size ===\r\nMax pooling is typically used, often with a 2x2 dimension. This implies that the input is drastically downsampled, reducing processing cost.\r\nGreater pooling reduces the dimension of the signal, and may result in unacceptable information loss. Often, non-overlapping pooling windows perform best.\r\n\r\n\r\n=== Dilation ===\r\nDilation involves ignoring pixels within a kernel. This reduces processing/memory potentially without significant signal loss. A dilation of 2 on a 3x3 kernel expands the kernel to 5x5, while still processing 9 (evenly spaced) pixels. Accordingly, dilation of 4 expands the kernel to 7x7.\r\n\r\n\r\n== Translation equivariance and aliasing ==\r\nIt is commonly assumed that CNNs are invariant to shifts of the input. Convolution or pooling layers within a CNN that do not have a stride greater than one are indeed equivariant to translations of the input. However, layers with a stride greater than one ignore the Nyquist-Shannon sampling theorem and might lead to aliasing of the input signal While, in principle, CNNs are capable of implementing anti-aliasing filters, it has been observed that this does not happen in practice  and yield models that are not equivariant to translations.\r\nFurthermore, if a CNN makes use of fully connected layers, translation equivariance does not imply translation invariance, as the fully connected layers are not invariant to shifts of the input. One solution for complete translation invariance is avoiding any down-sampling throughout the network and applying global average pooling at the last layer. Additionally, several other partial solutions have been proposed, such as anti-aliasing before downsampling operations, spatial transformer networks, data augmentation, subsampling combined with pooling, and capsule neural networks.\r\n\r\n\r\n== Evaluation ==\r\nThe accuracy of the final model is based on a sub-part of the dataset set apart at the start, often called a test-set. Other times methods such as k-fold cross-validation are applied. Other strategies include using conformal prediction.\r\n\r\n\r\n== Regularization methods ==\r\n\r\nRegularization is a process of introducing additional information to solve an ill-posed problem or to prevent overfitting. CNNs use various types of regularization.\r\n\r\n\r\n=== Empirical ===\r\n\r\n\r\n==== Dropout ====\r\nBecause a fully connected layer occupies most of the parameters, it is prone to overfitting. One method to reduce overfitting is dropout, introduced in 2014. At each training stage, individual nodes are either \"dropped out\" of the net (ignored) with probability 1\u2212p{\\displaystyle 1-p} or kept with probability p{\\displaystyle p}, so that a reduced network is left; incoming and outgoing edges to a dropped-out node are also removed. Only the reduced network is trained on the data in that stage. The removed nodes are then reinserted into the network with their original weights.\r\nIn the training stages, p{\\displaystyle p} is usually 0.5; for input nodes, it is typically much higher because information is directly lost when input nodes are ignored.", "start_char_idx": 34424, "end_char_idx": 39087, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f4112708-7739-4316-9865-8f212a7cc78b": {"__data__": {"id_": "f4112708-7739-4316-9865-8f212a7cc78b", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "65fe6622-9e19-4c31-912b-a1676a217719", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "31edaf0411c36a59c41ae21354f4047f70757c774e9eafd489e202e1df78e65e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6b6f2954-0cb0-4cd8-a915-92b11769eb07", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "76e7411204c11f3af2dbe8b87ead77f2c8b23e8a28318b9bea6fb83e74a83c8b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b6835997-6770-420c-9493-970c71189cd4", "node_type": "1", "metadata": {}, "hash": "ef804306a49ad4be715ecc6367ff28bccb4cab4d2c2fa4e712746b9f702e165f", "class_name": "RelatedNodeInfo"}}, "text": "CNNs use various types of regularization.\r\n\r\n\r\n=== Empirical ===\r\n\r\n\r\n==== Dropout ====\r\nBecause a fully connected layer occupies most of the parameters, it is prone to overfitting. One method to reduce overfitting is dropout, introduced in 2014. At each training stage, individual nodes are either \"dropped out\" of the net (ignored) with probability 1\u2212p{\\displaystyle 1-p} or kept with probability p{\\displaystyle p}, so that a reduced network is left; incoming and outgoing edges to a dropped-out node are also removed. Only the reduced network is trained on the data in that stage. The removed nodes are then reinserted into the network with their original weights.\r\nIn the training stages, p{\\displaystyle p} is usually 0.5; for input nodes, it is typically much higher because information is directly lost when input nodes are ignored.\r\nAt testing time after training has finished, we would ideally like to find a sample average of all possible 2n{\\displaystyle 2^{n}} dropped-out networks; unfortunately this is unfeasible for large values of n{\\displaystyle n}. However, we can find an approximation by using the full network with each node's output weighted by a factor of p{\\displaystyle p}, so the expected value of the output of any node is the same as in the training stages. This is the biggest contribution of the dropout method: although it effectively generates 2n{\\displaystyle 2^{n}} neural nets, and as such allows for model combination, at test time only a single network needs to be tested.\r\nBy avoiding training all nodes on all training data, dropout decreases overfitting. The method also significantly improves training speed. This makes the model combination practical, even for deep neural networks. The technique seems to reduce node interactions, leading them to learn more robust features that better generalize to new data.\r\n\r\n\r\n==== DropConnect ====\r\nDropConnect is the generalization of dropout in which each connection, rather than each output unit, can be dropped with probability 1\u2212p{\\displaystyle 1-p}. Each unit thus receives input from a random subset of units in the previous layer.DropConnect is similar to dropout as it introduces dynamic sparsity within the model, but differs in that the sparsity is on the weights, rather than the output vectors of a layer. In other words, the fully connected layer with DropConnect becomes a sparsely connected layer in which the connections are chosen at random during the training stage.\r\n\r\n\r\n==== Stochastic pooling ====\r\nA major drawback to Dropout is that it does not have the same benefits for convolutional layers, where the neurons are not fully connected.\r\nEven before Dropout, in 2013 a technique called stochastic pooling, the conventional deterministic pooling operations were replaced with a stochastic procedure, where the activation within each pooling region is picked randomly according to a multinomial distribution, given by the activities within the pooling region. This approach is free of hyperparameters and can be combined with other regularization approaches, such as dropout and data augmentation.\r\nAn alternate view of stochastic pooling is that it is equivalent to standard max pooling but with many copies of an input image, each having small local deformations. This is similar to explicit elastic deformations of the input images, which delivers excellent performance on the MNIST data set. Using stochastic pooling in a multilayer model gives an exponential number of deformations since the selections in higher layers are independent of those below.\r\n\r\n\r\n==== Artificial data ====\r\n\r\nBecause the degree of model overfitting is determined by both its power and the amount of training it receives, providing a convolutional network with more training examples can reduce overfitting. Because there is often not enough available data to train, especially considering that some part should be spared for later testing, two approaches are to either generate new data from scratch (if possible) or perturb existing data to create new ones. The latter one is used since mid-1990s. For example, input images can be cropped, rotated, or rescaled to create new examples with the same labels as the original training set.\r\n\r\n\r\n=== Explicit ===\r\n\r\n\r\n==== Early stopping ====\r\n\r\nOne of the simplest methods to prevent overfitting of a network is to simply stop the training before overfitting has had a chance to occur. It comes with the disadvantage that the learning process is halted.\r\n\r\n\r\n==== Number of parameters ====\r\nAnother simple way to prevent overfitting is to limit the number of parameters, typically by limiting the number of hidden units in each layer or limiting network depth. For convolutional networks, the filter size also affects the number of parameters.", "start_char_idx": 38247, "end_char_idx": 43024, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b6835997-6770-420c-9493-970c71189cd4": {"__data__": {"id_": "b6835997-6770-420c-9493-970c71189cd4", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "65fe6622-9e19-4c31-912b-a1676a217719", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "31edaf0411c36a59c41ae21354f4047f70757c774e9eafd489e202e1df78e65e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f4112708-7739-4316-9865-8f212a7cc78b", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "8f4372efb12d874512b3fa2288725f1563d79a88af26da2977117bce0c0074ce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "75104176-b291-48ea-a2b8-8140b770c225", "node_type": "1", "metadata": {}, "hash": "74d1afd113f0e4bc1af1eaf2ea66cdbc9b6e30948d57cecd7b6efab34b606779", "class_name": "RelatedNodeInfo"}}, "text": "Because there is often not enough available data to train, especially considering that some part should be spared for later testing, two approaches are to either generate new data from scratch (if possible) or perturb existing data to create new ones. The latter one is used since mid-1990s. For example, input images can be cropped, rotated, or rescaled to create new examples with the same labels as the original training set.\r\n\r\n\r\n=== Explicit ===\r\n\r\n\r\n==== Early stopping ====\r\n\r\nOne of the simplest methods to prevent overfitting of a network is to simply stop the training before overfitting has had a chance to occur. It comes with the disadvantage that the learning process is halted.\r\n\r\n\r\n==== Number of parameters ====\r\nAnother simple way to prevent overfitting is to limit the number of parameters, typically by limiting the number of hidden units in each layer or limiting network depth. For convolutional networks, the filter size also affects the number of parameters. Limiting the number of parameters restricts the predictive power of the network directly, reducing the complexity of the function that it can perform on the data, and thus limits the amount of overfitting. This is equivalent to a \"zero norm\".\r\n\r\n\r\n==== Weight decay ====\r\nA simple form of added regularizer is weight decay, which simply adds an additional error, proportional to the sum of weights (L1 norm) or squared magnitude (L2 norm) of the weight vector, to the error at each node. The level of acceptable model complexity can be reduced by increasing the proportionality constant('alpha' hyperparameter), thus increasing the penalty for large weight vectors.\r\nL2 regularization is the most common form of regularization. It can be implemented by penalizing the squared magnitude of all parameters directly in the objective. The L2 regularization has the intuitive interpretation of heavily penalizing peaky weight vectors and preferring diffuse weight vectors. Due to multiplicative interactions between weights and inputs this has the useful property of encouraging the network to use all of its inputs a little rather than some of its inputs a lot.\r\nL1 regularization is also common. It makes the weight vectors sparse during optimization. In other words, neurons with L1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the noisy inputs. L1 with L2 regularization can be combined; this is called elastic net regularization.\r\n\r\n\r\n==== Max norm constraints ====\r\nAnother form of regularization is to enforce an absolute upper bound on the magnitude of the weight vector for every neuron and use projected gradient descent to enforce the constraint. In practice, this corresponds to performing the parameter update as normal, and then enforcing the constraint by clamping the weight vector w\u2192{\\displaystyle {\\vec {w}}} of every neuron to satisfy \u2016w\u2192\u20162<c{\\displaystyle \\|{\\vec {w}}\\|_{2}<c}. Typical values of c{\\displaystyle c} are order of 3\u20134. Some papers report improvements when using this form of regularization.\r\n\r\n\r\n== Hierarchical coordinate frames ==\r\nPooling loses the precise spatial relationships between high-level parts (such as nose and mouth in a face image). These relationships are needed for identity recognition. Overlapping the pools so that each feature occurs in multiple pools, helps retain the information. Translation alone cannot extrapolate the understanding of geometric relationships to a radically new viewpoint, such as a different orientation or scale. On the other hand, people are very good at extrapolating; after seeing a new shape once they can recognize it from a different viewpoint.An earlier common way to deal with this problem is to train the network on transformed data in different orientations, scales, lighting, etc. so that the network can cope with these variations. This is computationally intensive for large data-sets. The alternative is to use a hierarchy of coordinate frames and use a group of neurons to represent a conjunction of the shape of the feature and its pose relative to the retina. The pose relative to the retina is the relationship between the coordinate frame of the retina and the intrinsic features' coordinate frame.Thus, one way to represent something is to embed the coordinate frame within it. This allows large features to be recognized by using the consistency of the poses of their parts (e.g. nose and mouth poses make a consistent prediction of the pose of the whole face). This approach ensures that the higher-level entity (e.g. face) is present when the lower-level (e.g. nose and mouth) agree on its prediction of the pose. The vectors of neuronal activity that represent pose (\"pose vectors\") allow spatial transformations modeled as linear operations that make it easier for the network to learn the hierarchy of visual entities and generalize across viewpoints.", "start_char_idx": 42042, "end_char_idx": 46944, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "75104176-b291-48ea-a2b8-8140b770c225": {"__data__": {"id_": "75104176-b291-48ea-a2b8-8140b770c225", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "65fe6622-9e19-4c31-912b-a1676a217719", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "31edaf0411c36a59c41ae21354f4047f70757c774e9eafd489e202e1df78e65e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b6835997-6770-420c-9493-970c71189cd4", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "7799f7f309ebad471333b8ba6eff409e46ba5edd10d5c62ecbf69067245a82c6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8967e8c2-4793-4e11-821c-7bb603de0868", "node_type": "1", "metadata": {}, "hash": "2dbebff49a52fd3e42ec5962ff2681f2ca75e62498c87d3fb4fc517820f94b9f", "class_name": "RelatedNodeInfo"}}, "text": "The alternative is to use a hierarchy of coordinate frames and use a group of neurons to represent a conjunction of the shape of the feature and its pose relative to the retina. The pose relative to the retina is the relationship between the coordinate frame of the retina and the intrinsic features' coordinate frame.Thus, one way to represent something is to embed the coordinate frame within it. This allows large features to be recognized by using the consistency of the poses of their parts (e.g. nose and mouth poses make a consistent prediction of the pose of the whole face). This approach ensures that the higher-level entity (e.g. face) is present when the lower-level (e.g. nose and mouth) agree on its prediction of the pose. The vectors of neuronal activity that represent pose (\"pose vectors\") allow spatial transformations modeled as linear operations that make it easier for the network to learn the hierarchy of visual entities and generalize across viewpoints. This is similar to the way the human visual system imposes coordinate frames in order to represent shapes.\r\n\r\n\r\n== Applications ==\r\n\r\n\r\n=== Image recognition ===\r\nCNNs are often used in image recognition systems. In 2012, an error rate of 0.23% on the MNIST database was reported. Another paper on using CNN for image classification reported that the learning process was \"surprisingly fast\"; in the same paper, the best published results as of 2011 were achieved in the MNIST database and the NORB database. Subsequently, a similar CNN called\r\nAlexNet won the ImageNet Large Scale Visual Recognition Challenge 2012.\r\nWhen applied to facial recognition, CNNs achieved a large decrease in error rate. Another paper reported a 97.6% recognition rate on \"5,600 still images of more than 10 subjects\". CNNs were used to assess video quality in an objective way after manual training; the resulting system had a very low root mean square error.The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object classification and detection, with millions of images and hundreds of object classes. In the ILSVRC 2014, a large-scale visual recognition challenge, almost every highly ranked team used CNN as their basic framework. The winner GoogLeNet (the foundation of DeepDream) increased the mean average precision of object detection to 0.439329, and reduced classification error to 0.06656, the best result to date. Its network applied more than 30 layers. That performance of convolutional neural networks on the ImageNet tests was close to that of humans. The best algorithms still struggle with objects that are small or thin, such as a small ant on a stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters, an increasingly common phenomenon with modern digital cameras. By contrast, those kinds of images rarely trouble humans. Humans, however, tend to have trouble with other issues. For example, they are not good at classifying objects into fine-grained categories such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this.In 2015, a many-layered CNN demonstrated the ability to spot faces from a wide range of angles, including upside down, even when partially occluded, with competitive performance. The network was trained on a database of 200,000 images that included faces at various angles and orientations and a further 20 million images without faces. They used batches of 128 images over 50,000 iterations.\r\n\r\n\r\n=== Video analysis ===\r\nCompared to image data domains, there is relatively little work on applying CNNs to video classification. Video is more complex than images since it has another (temporal) dimension. However, some extensions of CNNs into the video domain have been explored. One approach is to treat space and time as equivalent dimensions of the input and perform convolutions in both time and space. Another way is to fuse the features of two convolutional neural networks, one for the spatial and one for the temporal stream. Long short-term memory (LSTM) recurrent units are typically incorporated after the CNN to account for inter-frame or inter-clip dependencies. Unsupervised learning schemes for training spatio-temporal features have been introduced, based on Convolutional Gated Restricted Boltzmann Machines and Independent Subspace Analysis. It's Application can be seen in Text-to-Video model.\r\n\r\n\r\n=== Natural language processing ===\r\nCNNs have also been explored for natural language processing. CNN models are effective for various NLP problems and achieved excellent results in semantic parsing, search query retrieval, sentence modeling, classification, prediction and other traditional NLP tasks.", "start_char_idx": 45966, "end_char_idx": 50732, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8967e8c2-4793-4e11-821c-7bb603de0868": {"__data__": {"id_": "8967e8c2-4793-4e11-821c-7bb603de0868", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "65fe6622-9e19-4c31-912b-a1676a217719", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "31edaf0411c36a59c41ae21354f4047f70757c774e9eafd489e202e1df78e65e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "75104176-b291-48ea-a2b8-8140b770c225", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "281ca34284b85c5de0b7d20ef220ea5880ebeb33a25118516af00ff7a605dd61", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c6a646ee-0c0e-40ca-b662-ec9fc6602de4", "node_type": "1", "metadata": {}, "hash": "ff77b30298c0e256d47ec184ab86ec42062c7c9f22b4cae899a2c1d692cc46d4", "class_name": "RelatedNodeInfo"}}, "text": "However, some extensions of CNNs into the video domain have been explored. One approach is to treat space and time as equivalent dimensions of the input and perform convolutions in both time and space. Another way is to fuse the features of two convolutional neural networks, one for the spatial and one for the temporal stream. Long short-term memory (LSTM) recurrent units are typically incorporated after the CNN to account for inter-frame or inter-clip dependencies. Unsupervised learning schemes for training spatio-temporal features have been introduced, based on Convolutional Gated Restricted Boltzmann Machines and Independent Subspace Analysis. It's Application can be seen in Text-to-Video model.\r\n\r\n\r\n=== Natural language processing ===\r\nCNNs have also been explored for natural language processing. CNN models are effective for various NLP problems and achieved excellent results in semantic parsing, search query retrieval, sentence modeling, classification, prediction and other traditional NLP tasks.\r\nCompared to traditional language processing methods such as recurrent neural networks, CNNs can represent different contextual realities of language that do not rely on a series-sequence assumption, while RNNs are better suitable when classical time series modeling is required.\r\n\r\n\r\n=== Anomaly Detection ===\r\nA CNN with 1-D convolutions was used on time series in the frequency domain (spectral residual) by an unsupervised model to detect anomalies in the time domain.\r\n\r\n\r\n=== Drug discovery ===\r\nCNNs have been used in drug discovery. Predicting the interaction between molecules and biological proteins can identify potential treatments. In 2015, Atomwise introduced AtomNet, the first deep learning neural network for structure-based drug design. The system trains directly on 3-dimensional representations of chemical interactions. Similar to how image recognition networks learn to compose smaller, spatially proximate features into larger, complex structures, AtomNet discovers chemical features, such as aromaticity, sp3 carbons, and hydrogen bonding. Subsequently, AtomNet was used to predict novel candidate biomolecules for multiple disease targets, most notably treatments for the Ebola virus and multiple sclerosis.\r\n\r\n\r\n=== Checkers game ===\r\nCNNs have been used in the game of checkers. From 1999 to 2001, Fogel and Chellapilla published papers showing how a convolutional neural network could learn to play checker using co-evolution. The learning process did not use prior human professional games, but rather focused on a minimal set of information contained in the checkerboard: the location and type of pieces, and the difference in number of pieces between the two sides. Ultimately, the program (Blondie24) was tested on 165 games against players and ranked in the highest 0.4%. It also earned a win against the program Chinook at its \"expert\" level of play.\r\n\r\n\r\n=== Go ===\r\nCNNs have been used in computer Go. In December 2014, Clark and Storkey published a paper showing that a CNN trained by supervised learning from a database of human professional games could outperform GNU Go and win some games against Monte Carlo tree search Fuego 1.1 in a fraction of the time it took Fuego to play. Later it was announced that a large 12-layer convolutional neural network had correctly predicted the professional move in 55% of positions, equalling the accuracy of a 6 dan human player. When the trained convolutional network was used directly to play games of Go, without any search, it beat the traditional search program GNU Go in 97% of games, and matched the performance of the Monte Carlo tree search program Fuego simulating ten thousand playouts (about a million positions) per move.A couple of CNNs for choosing moves to try (\"policy network\") and evaluating positions (\"value network\") driving MCTS were used by AlphaGo, the first to beat the best human player at the time.\r\n\r\n\r\n=== Time series forecasting ===\r\nRecurrent neural networks are generally considered the best neural network architectures for time series forecasting (and sequence modeling in general), but recent studies show that convolutional networks can perform comparably or even better. Dilated convolutions might enable one-dimensional convolutional neural networks to effectively learn time series dependences. Convolutions can be implemented more efficiently than RNN-based solutions, and they do not suffer from vanishing (or exploding) gradients. Convolutional networks can provide an improved forecasting performance when there are multiple similar time series to learn from. CNNs can also be applied to further tasks in time series analysis (e.g., time series classification or quantile forecasting).", "start_char_idx": 49716, "end_char_idx": 54441, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c6a646ee-0c0e-40ca-b662-ec9fc6602de4": {"__data__": {"id_": "c6a646ee-0c0e-40ca-b662-ec9fc6602de4", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "65fe6622-9e19-4c31-912b-a1676a217719", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "31edaf0411c36a59c41ae21354f4047f70757c774e9eafd489e202e1df78e65e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8967e8c2-4793-4e11-821c-7bb603de0868", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "f9865d0d6a5e596b7fd3af8fe943a2a57dd3718cb5c60e05c9185674cd41061f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ef8b8a5d-4ad7-423f-9a30-b254427348b5", "node_type": "1", "metadata": {}, "hash": "7e37c6181924ca3ed8403788ab5218351a3472ad567776f73cf6ad017e0409b8", "class_name": "RelatedNodeInfo"}}, "text": "=== Time series forecasting ===\r\nRecurrent neural networks are generally considered the best neural network architectures for time series forecasting (and sequence modeling in general), but recent studies show that convolutional networks can perform comparably or even better. Dilated convolutions might enable one-dimensional convolutional neural networks to effectively learn time series dependences. Convolutions can be implemented more efficiently than RNN-based solutions, and they do not suffer from vanishing (or exploding) gradients. Convolutional networks can provide an improved forecasting performance when there are multiple similar time series to learn from. CNNs can also be applied to further tasks in time series analysis (e.g., time series classification or quantile forecasting).\r\n\r\n\r\n=== Cultural Heritage and 3D-datasets ===\r\nAs archaeological findings like clay tablets with cuneiform writing are increasingly acquired using 3D scanners first benchmark datasets are becoming available like HeiCuBeDa providing almost 2.000 normalized 2D- and 3D-datasets prepared with the GigaMesh Software Framework. So curvature-based measures are used in conjunction with Geometric Neural Networks (GNNs) e.g. for period classification of those clay tablets being among the oldest documents of human history.\r\n\r\n\r\n== Fine-tuning ==\r\nFor many applications, the training data is less available. Convolutional neural networks usually require a large amount of training data in order to avoid overfitting. A common technique is to train the network on a larger data set from a related domain. Once the network parameters have converged an additional training step is performed using the in-domain data to fine-tune the network weights, this is known as transfer learning. Furthermore, this technique allows convolutional network architectures to successfully be applied to problems with tiny training sets.\r\n\r\n\r\n== Human interpretable explanations ==\r\nEnd-to-end training and prediction are common practice in computer vision. However, human interpretable explanations are required for critical systems such as a self-driving cars. With recent advances in visual salience, spatial attention, and temporal attention, the most critical spatial regions/temporal instants could be visualized to justify the CNN predictions.\r\n\r\n\r\n== Related architectures ==\r\n\r\n\r\n=== Deep Q-networks ===\r\nA deep Q-network (DQN) is a type of deep learning model that combines a deep neural network with Q-learning, a form of reinforcement learning. Unlike earlier reinforcement learning agents, DQNs that utilize CNNs can learn directly from high-dimensional sensory inputs via reinforcement learning.Preliminary results were presented in 2014, with an accompanying paper in February 2015. The research described an application to Atari 2600 gaming. Other deep reinforcement learning models preceded it.\r\n\r\n\r\n=== Deep belief networks ===\r\n\r\nConvolutional deep belief networks (CDBN) have structure very similar to convolutional neural networks and are trained similarly to deep belief networks. Therefore, they exploit the 2D structure of images, like CNNs do, and make use of pre-training like deep belief networks. They provide a generic structure that can be used in many image and signal processing tasks. Benchmark results on standard image datasets like CIFAR have been obtained using CDBNs.\r\n\r\n\r\n== Notable libraries ==\r\nCaffe: A library for convolutional neural networks. Created by the Berkeley Vision and Learning Center (BVLC). It supports both CPU and GPU. Developed in C++, and has Python and MATLAB wrappers.\r\nDeeplearning4j: Deep learning in Java and Scala on multi-GPU-enabled Spark. A general-purpose deep learning library for the JVM production stack running on a C++ scientific computing engine. Allows the creation of custom layers. Integrates with Hadoop and Kafka.\r\nDlib: A toolkit for making real world machine learning and data analysis applications in C++.\r\nMicrosoft Cognitive Toolkit: A deep learning toolkit written by Microsoft with several unique features enhancing scalability over multiple nodes. It supports full-fledged interfaces for training in C++ and Python and with additional support for model inference in C# and Java.\r\nTensorFlow: Apache 2.0-licensed Theano-like library with support for CPU, GPU, Google's proprietary tensor processing unit (TPU), and mobile devices.\r\nTheano: The reference deep-learning library for Python with an API largely compatible with the popular NumPy library. Allows user to write symbolic mathematical expressions, then automatically generates their derivatives, saving the user from having to code gradients or backpropagation. These symbolic expressions are automatically compiled to CUDA code for a fast, on-the-GPU implementation.\r\nTorch: A scientific computing framework with wide support for machine learning algorithms, written in C and Lua.", "start_char_idx": 53644, "end_char_idx": 58543, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ef8b8a5d-4ad7-423f-9a30-b254427348b5": {"__data__": {"id_": "ef8b8a5d-4ad7-423f-9a30-b254427348b5", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "65fe6622-9e19-4c31-912b-a1676a217719", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "31edaf0411c36a59c41ae21354f4047f70757c774e9eafd489e202e1df78e65e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c6a646ee-0c0e-40ca-b662-ec9fc6602de4", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "fa36ec2a88601e337c8c49923aa4387181f4128c2f73d9965acf9e251f7905a8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a7c7a032-a671-47d6-8cf3-442865fedbfe", "node_type": "1", "metadata": {}, "hash": "606db6243e6aae55e79b08329233912d8fe455474cd33b91210ca8e4357af342", "class_name": "RelatedNodeInfo"}}, "text": "Dlib: A toolkit for making real world machine learning and data analysis applications in C++.\r\nMicrosoft Cognitive Toolkit: A deep learning toolkit written by Microsoft with several unique features enhancing scalability over multiple nodes. It supports full-fledged interfaces for training in C++ and Python and with additional support for model inference in C# and Java.\r\nTensorFlow: Apache 2.0-licensed Theano-like library with support for CPU, GPU, Google's proprietary tensor processing unit (TPU), and mobile devices.\r\nTheano: The reference deep-learning library for Python with an API largely compatible with the popular NumPy library. Allows user to write symbolic mathematical expressions, then automatically generates their derivatives, saving the user from having to code gradients or backpropagation. These symbolic expressions are automatically compiled to CUDA code for a fast, on-the-GPU implementation.\r\nTorch: A scientific computing framework with wide support for machine learning algorithms, written in C and Lua.\r\n\r\n\r\n== See also ==\r\nAttention (machine learning)\r\nConvolution\r\nDeep learning\r\nNatural-language processing\r\nNeocognitron\r\nScale-invariant feature transform\r\nTime delay neural network\r\nVision processing unit\r\n\r\n\r\n== Notes ==\r\n\r\n\r\n== References ==\r\n\r\n\r\n== External links ==\r\nCS231n: Convolutional Neural Networks for Visual Recognition \u2014 Andrej Karpathy's Stanford computer science course on CNNs in computer vision", "start_char_idx": 57512, "end_char_idx": 58957, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a7c7a032-a671-47d6-8cf3-442865fedbfe": {"__data__": {"id_": "a7c7a032-a671-47d6-8cf3-442865fedbfe", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ffb0bcfe-5aba-4c18-9723-b82a7d2219a0", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "ea9e4ea0c041aa25b372c7cdd793cded3f71663ae1b7fdaff19aa17921d30ab8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ef8b8a5d-4ad7-423f-9a30-b254427348b5", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "99a82acb69552f1e2c029222ce24cb19fe4fe77a4b20c68c498027c24bcd8341", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "01017277-fbf6-4e56-8c0d-cc072f2d4159", "node_type": "1", "metadata": {}, "hash": "64cba12b0948fc26a301e12fafcd02e30fcb32bbae6c656a4aa1eeaac55f4c44", "class_name": "RelatedNodeInfo"}}, "text": "Deep learning is the subset of machine learning methods based on artificial neural networks (ANNs) with representation learning. The adjective \"deep\" refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised or unsupervised.Deep-learning architectures such as deep neural networks, deep belief networks, recurrent neural networks, convolutional neural networks and transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.Artificial neural networks were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains. Specifically, artificial neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analog. ANNs are generally seen as low quality models for brain function.\r\n\r\n\r\n== Definition ==\r\nDeep learning is a class of machine learning algorithms that:\u200a199\u2013200\u200a uses multiple layers to progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits or letters or faces.\r\nFrom another angle to view deep learning, deep learning refers to \"computer-simulate\" or \"automate\" human learning processes from a source (e.g., an image of dogs) to a learned object (dogs). Therefore, a notion coined as \"deeper\" learning or \"deepest\" learning makes sense. The deepest learning refers to the fully automatic learning from a source to a final learned object. A deeper learning thus refers to a mixed learning process: a human learning process from a source to a learned semi-object, followed by a computer learning process from the human learned semi-object to a final learned object.\r\n\r\n\r\n== Overview ==\r\nMost modern deep learning models are based on multi-layered artificial neural networks such as convolutional neural networks and transformers, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines.In deep learning, each level learns to transform its input data into a slightly more abstract and composite representation. In an image recognition application, the raw input may be a matrix of pixels; the first representational layer may abstract the pixels and encode edges; the second layer may compose and encode arrangements of edges; the third layer may encode a nose and eyes; and the fourth layer may recognize that the image contains a face. Importantly, a deep learning process can learn which features to optimally place in which level on its own. This does not eliminate the need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different degrees of abstraction.The word \"deep\" in \"deep learning\" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited. No universally agreed-upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth higher than 2. CAP of depth 2 has been shown to be a universal approximator in the sense that it can emulate any function. Beyond that, more layers do not add to the function approximator ability of the network. Deep models (CAP > 2) are able to extract better features than shallow models and hence, extra layers help in learning the features effectively.\r\nDeep learning architectures can be constructed with a greedy layer-by-layer method. Deep learning helps to disentangle these abstractions and pick out which features improve performance.For supervised learning tasks, deep learning methods enable elimination of feature engineering, by translating the data into compact intermediate representations akin to principal components, and derive layered structures that remove redundancy in representation.\r\nDeep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data are more abundant than the labeled data. Examples of deep structures that can be trained in an unsupervised manner are deep belief networks.Machine learning models are now adept at identifying complex patterns in financial market data.", "start_char_idx": 0, "end_char_idx": 5149, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "01017277-fbf6-4e56-8c0d-cc072f2d4159": {"__data__": {"id_": "01017277-fbf6-4e56-8c0d-cc072f2d4159", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ffb0bcfe-5aba-4c18-9723-b82a7d2219a0", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "ea9e4ea0c041aa25b372c7cdd793cded3f71663ae1b7fdaff19aa17921d30ab8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a7c7a032-a671-47d6-8cf3-442865fedbfe", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "526865068a093896d7b728b881d94c6f92e16bcd1e1969f1f6d3aacc83e55dbd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b65258e9-3101-4364-b643-8c44a4cee41b", "node_type": "1", "metadata": {}, "hash": "6f5718f862a2270fcdf1d88f4df6488c66f77bd78b3e594ec53bab994182e362", "class_name": "RelatedNodeInfo"}}, "text": "Beyond that, more layers do not add to the function approximator ability of the network. Deep models (CAP > 2) are able to extract better features than shallow models and hence, extra layers help in learning the features effectively.\r\nDeep learning architectures can be constructed with a greedy layer-by-layer method. Deep learning helps to disentangle these abstractions and pick out which features improve performance.For supervised learning tasks, deep learning methods enable elimination of feature engineering, by translating the data into compact intermediate representations akin to principal components, and derive layered structures that remove redundancy in representation.\r\nDeep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data are more abundant than the labeled data. Examples of deep structures that can be trained in an unsupervised manner are deep belief networks.Machine learning models are now adept at identifying complex patterns in financial market data. Due to the benefits of artificial intelligence, investors are increasingly utilizing deep learning techniques to forecast and analyze trends in stock and foreign exchange markets.\r\n\r\n\r\n== Interpretations ==\r\nDeep neural networks are generally interpreted in terms of the universal approximation theorem or probabilistic inference.The classic universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions. In 1989, the first proof was published by George Cybenko for sigmoid activation functions and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik. Recent work also showed that universal approximation also holds for non-bounded activation functions such as Kunihiko Fukushima's rectified linear unit.The universal approximation theorem for deep neural networks concerns the capacity of networks with bounded width but the depth is allowed to grow. Lu et al. proved that if the width of a deep neural network with ReLU activation is strictly larger than the input dimension, then the network can approximate any Lebesgue integrable function; if the width is smaller or equal to the input dimension, then a deep neural network is not a universal approximator.\r\nThe probabilistic interpretation derives from the field of machine learning. It features inference, as well as the optimization concepts of training and testing, related to fitting and generalization, respectively. More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function. The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks. The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop.\r\n\r\n\r\n== History ==\r\nThere are two types of artificial neural network (ANN): feedforward neural networks (FNNs) and recurrent neural networks (RNNs). RNNs have cycles in their connectivity structure, FNNs don't. In the 1920s, Wilhelm Lenz and Ernst Ising created and analyzed the Ising model which is essentially a non-learning RNN architecture consisting of neuron-like threshold elements. In 1972, Shun'ichi Amari made this architecture adaptive. His learning RNN was popularised by John Hopfield in 1982. RNNs have become central for speech recognition and language processing.\r\nCharles Tappert writes that Frank Rosenblatt developed and explored all of the basic ingredients of the deep learning systems of today, referring to Rosenblatt's 1962 book which introduced multilayer perceptron (MLP) with 3 layers: an input layer, a hidden layer with randomized weights that did not learn, and an output layer. It also introduced variants, including a version with four-layer perceptrons where the last two layers have learned weights (and thus a proper multilayer perceptron).:\u200asection 16\u200a In addition, term deep learning was proposed in 1986 by Rina Dechter although the history of its appearance is apparently more complicated.The first general, working learning algorithm for supervised, deep, feedforward, multilayer perceptrons was published by Alexey Ivakhnenko and Lapa in 1967. A 1971 paper described a deep network with eight layers trained by the group method of data handling.The first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned  internal representations to classify non-linearily separable pattern classes.", "start_char_idx": 4104, "end_char_idx": 8850, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b65258e9-3101-4364-b643-8c44a4cee41b": {"__data__": {"id_": "b65258e9-3101-4364-b643-8c44a4cee41b", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ffb0bcfe-5aba-4c18-9723-b82a7d2219a0", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "ea9e4ea0c041aa25b372c7cdd793cded3f71663ae1b7fdaff19aa17921d30ab8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "01017277-fbf6-4e56-8c0d-cc072f2d4159", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "829178a6b92b62bc73d0f03f9232c47b573fbb3336c35fb290ff4cb197776bbb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d7db71c4-6a6f-411b-aabd-b91fb6e7af8a", "node_type": "1", "metadata": {}, "hash": "32d300fa21e56d94858935dd18f40d2589e2767145a769e3feb397b01d0d6e2b", "class_name": "RelatedNodeInfo"}}, "text": "It also introduced variants, including a version with four-layer perceptrons where the last two layers have learned weights (and thus a proper multilayer perceptron).:\u200asection 16\u200a In addition, term deep learning was proposed in 1986 by Rina Dechter although the history of its appearance is apparently more complicated.The first general, working learning algorithm for supervised, deep, feedforward, multilayer perceptrons was published by Alexey Ivakhnenko and Lapa in 1967. A 1971 paper described a deep network with eight layers trained by the group method of data handling.The first deep learning multilayer perceptron trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned  internal representations to classify non-linearily separable pattern classes. In 1987 Matthew Brand reported that wide 12-layer nonlinear perceptrons could be fully end-to-end trained to reproduce logic functions of nontrivial circuit depth via gradient descent on small batches of random input/output samples, but concluded that training time on contemporary hardware (sub-megaflop computers) made the technique impractical, and proposed using fixed random early layers as an input hash for a single modifiable layer.  Instead, subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique.\r\nIn 1970, Seppo Linnainmaa published the reverse mode of automatic differentiation of discrete connected networks of nested differentiable functions. This became known as backpropagation. It is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673 to networks of differentiable nodes. \r\nThe terminology \"back-propagating errors\" was actually introduced in 1962 by Rosenblatt, but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation already in 1960 in the context of control theory. In 1982, Paul Werbos applied backpropagation to MLPs in the way that has become standard. In 1985, David E. Rumelhart et al. published an experimental analysis of the technique.Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers began with the Neocognitron introduced by Kunihiko Fukushima in 1980. In 1969, he also introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for CNNs and deep learning in general. CNNs have become an essential tool for computer vision.\r\nThe term Deep Learning was introduced to the machine learning community by Rina Dechter in 1986, and to artificial neural networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons.In 1988, Wei Zhang et al. applied the backpropagation algorithm \r\nto a convolutional neural network (a simplified Neocognitron with convolutional interconnections between the image feature layers and the last fully connected layer) for alphabet recognition. They also proposed an implementation of the CNN with an optical computing system. \r\nIn 1989, Yann LeCun et al. applied backpropagation to a CNN with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked, training required 3 days. Subsequently, Wei Zhang, et al. modified their model by removing the last fully connected layer and applied it for medical image object segmentation in 1991 and breast cancer detection in mammograms in 1994. LeNet-5 (1998), a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks  digitized in 32x32 pixel images.\r\nIn the 1980s, backpropagation did not work well for deep learning with long credit assignment paths. To overcome this problem, J\u00fcrgen Schmidhuber (1992) proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning. It uses predictive coding  to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN, by distilling a higher level chunker network into a lower level automatizer network.", "start_char_idx": 7956, "end_char_idx": 12308, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d7db71c4-6a6f-411b-aabd-b91fb6e7af8a": {"__data__": {"id_": "d7db71c4-6a6f-411b-aabd-b91fb6e7af8a", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ffb0bcfe-5aba-4c18-9723-b82a7d2219a0", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "ea9e4ea0c041aa25b372c7cdd793cded3f71663ae1b7fdaff19aa17921d30ab8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b65258e9-3101-4364-b643-8c44a4cee41b", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "3e6bb8eb35fb6f08f1ec9b8f9e628685489dcad0b3329b76bf5c0b1060ef7df2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "23e48601-3899-4045-933d-605792de6c3a", "node_type": "1", "metadata": {}, "hash": "248fee380f56b5808e185cd80fd62f6462bdd0ee96f2b19f50be57efd6f15c24", "class_name": "RelatedNodeInfo"}}, "text": "LeNet-5 (1998), a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks  digitized in 32x32 pixel images.\r\nIn the 1980s, backpropagation did not work well for deep learning with long credit assignment paths. To overcome this problem, J\u00fcrgen Schmidhuber (1992) proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning. It uses predictive coding  to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN, by distilling a higher level chunker network into a lower level automatizer network. In 1993, a chunker solved a deep learning task whose depth exceeded 1000.In 1992, J\u00fcrgen Schmidhuber also published an alternative to RNNs which is now called a linear Transformer or a  Transformer with linearized self-attention (save for a normalization operator). It learns internal spotlights of attention: a slow feedforward neural network learns by gradient descent to control the fast weights of another neural network through outer products of self-generated activation patterns FROM and TO (which are now called key and value for self-attention). This fast weight attention mapping is applied to a query pattern.\r\nThe modern Transformer was introduced by Ashish Vaswani et al. in their 2017 paper \"Attention Is All You Need\". \r\nIt combines this with a softmax operator and a projection matrix.\r\nTransformers have increasingly become the model of choice for natural language processing. Many modern large language models such as ChatGPT, GPT-4, and BERT use it. Transformers are also increasingly being used in computer vision.In 1991, J\u00fcrgen Schmidhuber also published adversarial neural networks that contest with each other in the form of a zero-sum game, where one network's gain is the other network's loss. The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called \"artificial curiosity\". In 2014, this principle was used in a generative adversarial network (GAN) by Ian Goodfellow et al. Here the environmental reaction is 1 or 0 depending on whether the first network's output is in a given set. This can be used to create realistic deepfakes. Excellent image quality is achieved by Nvidia's StyleGAN (2018) based on the Progressive GAN by Tero Karras et al. Here the GAN generator is grown from small to large scale in a pyramidal fashion.\r\nSepp Hochreiter's diploma thesis (1991) was called \"one of the most important documents in the history of machine learning\" by his supervisor Schmidhuber. It not only tested the neural history compressor, but also identified and analyzed the vanishing gradient problem. Hochreiter proposed recurrent residual connections to solve this problem. This led to the deep learning method called long short-term memory (LSTM), published in 1997. LSTM recurrent neural networks can learn \"very deep learning\" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. The \"vanilla LSTM\" with forget gate was introduced in 1999 by Felix Gers, Schmidhuber and Fred Cummins. LSTM has become the  most cited neural network of the 20th century.\r\nIn 2015, Rupesh Kumar Srivastava, Klaus Greff, and Schmidhuber used LSTM principles to create the Highway network, a feedforward neural network with hundreds of layers, much deeper than previous networks. 7 months later, Kaiming He, Xiangyu Zhang;  Shaoqing Ren, and Jian Sun won the ImageNet 2015 competition with an open-gated or gateless Highway network variant called Residual neural network. This has become the most cited neural network of the 21st century.In 1994, Andr\u00e9 de Carvalho, together with Mike Fairhurst and David Bisset, published experimental results of a multi-layer boolean neural network, also known as a weightless neural network, composed of a 3-layers self-organising feature extraction neural network module (SOFT) followed by a multi-layer classification neural network module (GSN), which were independently trained.", "start_char_idx": 11581, "end_char_idx": 15877, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "23e48601-3899-4045-933d-605792de6c3a": {"__data__": {"id_": "23e48601-3899-4045-933d-605792de6c3a", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ffb0bcfe-5aba-4c18-9723-b82a7d2219a0", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "ea9e4ea0c041aa25b372c7cdd793cded3f71663ae1b7fdaff19aa17921d30ab8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d7db71c4-6a6f-411b-aabd-b91fb6e7af8a", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "551c45bd6aad557a358a62720871c30bac8324e66f3714c961d0584a3714d70d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a38fabc6-41d4-4f3a-a7d5-e8523f309e87", "node_type": "1", "metadata": {}, "hash": "f6a066af7ad8c556be5e9e18ce05de04208ef543a46552210e82f47cb21cf68e", "class_name": "RelatedNodeInfo"}}, "text": "In 2015, Rupesh Kumar Srivastava, Klaus Greff, and Schmidhuber used LSTM principles to create the Highway network, a feedforward neural network with hundreds of layers, much deeper than previous networks. 7 months later, Kaiming He, Xiangyu Zhang;  Shaoqing Ren, and Jian Sun won the ImageNet 2015 competition with an open-gated or gateless Highway network variant called Residual neural network. This has become the most cited neural network of the 21st century.In 1994, Andr\u00e9 de Carvalho, together with Mike Fairhurst and David Bisset, published experimental results of a multi-layer boolean neural network, also known as a weightless neural network, composed of a 3-layers self-organising feature extraction neural network module (SOFT) followed by a multi-layer classification neural network module (GSN), which were independently trained. Each layer in the feature extraction module extracted features with growing complexity regarding the previous layer.In 1995, Brendan Frey demonstrated that it was possible to train (over two days) a network containing six fully connected layers and several hundred hidden units using the wake-sleep algorithm, co-developed with Peter Dayan and Hinton.Since 1997, Sven Behnke extended the feed-forward hierarchical convolutional approach in the Neural Abstraction Pyramid by lateral and backward connections in order to flexibly incorporate context into decisions and iteratively resolve local ambiguities.\r\nSimpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) were a popular choice in the 1990s and 2000s, because of artificial neural networks' computational cost and a lack of understanding of how the brain wires its biological networks.\r\nBoth shallow and deep learning (e.g., recurrent nets) of ANNs for speech recognition have been explored for many years. These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively. Key difficulties have been analyzed, including gradient diminishing and weak temporal correlation structure in neural predictive models. Additional difficulties were the lack of training data and limited computing power. Most speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government's NSA and DARPA, SRI studied deep neural networks (DNNs) in speech and speaker recognition. The speaker recognition team led by Larry Heck reported significant success with deep neural networks in speech processing in the 1998 National Institute of Standards and Technology Speaker Recognition evaluation. The SRI deep neural network was then deployed in the Nuance Verifier, representing the first major industrial application of deep learning. The principle of elevating \"raw\" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the \"raw\" spectrogram or linear filter-bank features in the late 1990s, showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech, waveforms, later produced excellent larger-scale results.Speech recognition was taken over by LSTM. In 2003, LSTM started to become competitive with traditional speech recognizers on certain tasks. In 2006, Alex Graves, Santiago Fern\u00e1ndez, Faustino Gomez, and Schmidhuber combined it with connectionist temporal classification (CTC) in stacks of LSTM RNNs. In 2015, Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM, which they made available through Google Voice Search.The impact of deep learning in industry began in the early 2000s, when CNNs already processed an estimated 10% to 20% of all the checks written in the US, according to Yann LeCun. Industrial applications of deep learning to large-scale speech recognition started around 2010.\r\nIn 2006, publications by Geoff Hinton, Ruslan Salakhutdinov, Osindero and Teh showed how a many-layered feedforward neural network could be effectively pre-trained one layer at a time, treating each layer in turn as an unsupervised restricted Boltzmann machine, then fine-tuning it using supervised backpropagation. The papers referred to learning for deep belief nets.\r\nThe 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech, and the possibility that given more capable hardware and large-scale data sets that deep neural nets might become practical.", "start_char_idx": 15034, "end_char_idx": 19746, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a38fabc6-41d4-4f3a-a7d5-e8523f309e87": {"__data__": {"id_": "a38fabc6-41d4-4f3a-a7d5-e8523f309e87", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ffb0bcfe-5aba-4c18-9723-b82a7d2219a0", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "ea9e4ea0c041aa25b372c7cdd793cded3f71663ae1b7fdaff19aa17921d30ab8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "23e48601-3899-4045-933d-605792de6c3a", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "eadc256b249db81faddddca193c4f21bf25f10c58584799d800fa88a808ef821", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "17ebeed7-747f-4cb5-acd6-b58b73906628", "node_type": "1", "metadata": {}, "hash": "6cab7def4b3c7705f4b03a88fcd44e8cdcf33d8910b1f7c78981ae6826406516", "class_name": "RelatedNodeInfo"}}, "text": "Industrial applications of deep learning to large-scale speech recognition started around 2010.\r\nIn 2006, publications by Geoff Hinton, Ruslan Salakhutdinov, Osindero and Teh showed how a many-layered feedforward neural network could be effectively pre-trained one layer at a time, treating each layer in turn as an unsupervised restricted Boltzmann machine, then fine-tuning it using supervised backpropagation. The papers referred to learning for deep belief nets.\r\nThe 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech, and the possibility that given more capable hardware and large-scale data sets that deep neural nets might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets. However, it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large, context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems. The nature of the recognition errors produced by the two types of systems was characteristically different, offering technical insights into how to integrate deep learning into the existing highly efficient, run-time speech decoding system deployed by all major speech recognition systems. Analysis around 2009\u20132010, contrasting the GMM (and other generative speech models) vs. DNN models, stimulated early industrial investment in deep learning for speech recognition.  That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models.\r\nIn 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees.Deep learning is part of state-of-the-art systems in various disciplines, particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification), as well as a range of large-vocabulary speech recognition tasks have steadily improved. Convolutional neural networks were superseded for ASR by CTC for LSTM. but are more successful in computer vision.\r\nAdvances in hardware have driven renewed interest in deep learning. In 2009, Nvidia was involved in what was called the \"big bang\" of deep learning, \"as deep-learning neural networks were trained with Nvidia graphics processing units (GPUs)\". That year, Andrew Ng determined that GPUs could increase the speed of deep-learning systems by about 100 times. In particular, GPUs are well-suited for the matrix/vector computations involved in machine learning. GPUs speed up training algorithms by orders of magnitude, reducing running times from weeks to days. Further, specialized hardware and algorithm optimizations can be used for efficient processing of deep learning models.\r\n\r\n\r\n=== Deep learning revolution ===\r\nIn the late 2000s, deep learning started to outperform other methods in machine learning competitions.\r\nIn 2009, a long short-term memory trained by connectionist temporal classification (Alex Graves, Santiago Fern\u00e1ndez, Faustino Gomez, and J\u00fcrgen Schmidhuber, 2006) was the first RNN to win pattern recognition contests, winning three competitions in connected handwriting recognition. Google later used CTC-trained LSTM for speech recognition on the smartphone.Significant impacts in image or object recognition were felt from 2011 to 2012. Although CNNs trained by backpropagation had been around for decades, and GPU implementations of NNs for years, including CNNs, faster implementations of CNNs on GPUs were needed to progress on computer vision. In 2011, the DanNet by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and J\u00fcrgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3. Also in 2011, DanNet won the ICDAR Chinese handwriting contest, and in May 2012, it won the ISBI image segmentation contest. Until 2011, CNNs did not play a major role at computer vision conferences, but in June 2012, a paper by Ciresan et al. at the leading conference CVPR showed how max-pooling CNNs on GPU can dramatically improve many vision benchmark records.", "start_char_idx": 19021, "end_char_idx": 23601, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "17ebeed7-747f-4cb5-acd6-b58b73906628": {"__data__": {"id_": "17ebeed7-747f-4cb5-acd6-b58b73906628", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ffb0bcfe-5aba-4c18-9723-b82a7d2219a0", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "ea9e4ea0c041aa25b372c7cdd793cded3f71663ae1b7fdaff19aa17921d30ab8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a38fabc6-41d4-4f3a-a7d5-e8523f309e87", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "ef7ca3493836e59b731ef2706cc26a2b6425875c2f1fcd05d93c78fd7d998116", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fe3234a9-3da2-41f7-8957-78c1899ca95f", "node_type": "1", "metadata": {}, "hash": "867bda1ab84eee8636f07a9dfc0357cdf97aaf181440d06e47feb8ba5b190a00", "class_name": "RelatedNodeInfo"}}, "text": "In 2011, the DanNet by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and J\u00fcrgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3. Also in 2011, DanNet won the ICDAR Chinese handwriting contest, and in May 2012, it won the ISBI image segmentation contest. Until 2011, CNNs did not play a major role at computer vision conferences, but in June 2012, a paper by Ciresan et al. at the leading conference CVPR showed how max-pooling CNNs on GPU can dramatically improve many vision benchmark records.  In September 2012, DanNet also won the ICPR contest on analysis of large medical images for cancer detection, and in the following year also the MICCAI Grand Challenge on the same topic. In October 2012, the similar AlexNet by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. \r\nThe VGG-16 network by Karen Simonyan and Andrew Zisserman further reduced the error rate and\r\nwon the ImageNet 2014 competition, following a similar trend in large-scale speech recognition.\r\nImage classification was then extended to the more challenging task of generating descriptions (captions) for images, often as a combination of CNNs and LSTMs.In 2012, a team led by George E. Dahl won the \"Merck Molecular Activity Challenge\" using multi-task deep neural networks to predict the biomolecular target of one drug. In 2014, Sepp Hochreiter's group used deep learning to detect off-target and toxic effects of environmental chemicals in nutrients, household products and drugs and won the \"Tox21 Data Challenge\" of NIH, FDA and NCATS.In 2016, Roger Parloff mentioned a \"deep learning revolution\" that has transformed the AI industry.In March 2019, Yoshua Bengio, Geoffrey Hinton and Yann LeCun were awarded the Turing Award for conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing.\r\n\r\n\r\n== Neural networks ==\r\n\r\nArtificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve their ability) to do tasks by considering examples, generally without task-specific programming. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as \"cat\" or \"no cat\" and using the analytic results to identify cats in other images. They have found most use in applications difficult to express with a traditional computer algorithm using rule-based programming.\r\nAn ANN is based on a collection of connected units called artificial neurons, (analogous to biological neurons in a biological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it. Neurons may have state, generally represented by real numbers, typically between 0 and 1. Neurons and synapses may also have a weight that varies as learning proceeds, which can increase or decrease the strength of the signal that it sends downstream.\r\nTypically, neurons are organized in layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first (input), to the last (output) layer, possibly after traversing the layers multiple times.\r\nThe original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time, attention focused on matching specific mental abilities, leading to deviations from biology such as backpropagation, or passing information in the reverse direction and adjusting the network to reflect that information.\r\nNeural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.\r\nAs of 2017, neural networks typically have a few thousand to a few million units and millions of connections. Despite this number being several order of magnitude less than the number of neurons on a human brain, these networks can perform many tasks at a level beyond that of humans (e.g., recognizing faces, or playing \"Go\").\r\n\r\n\r\n=== Deep neural networks ===\r\nA deep neural network (DNN) is an artificial neural network with multiple layers between the input and output layers.", "start_char_idx": 22981, "end_char_idx": 27580, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fe3234a9-3da2-41f7-8957-78c1899ca95f": {"__data__": {"id_": "fe3234a9-3da2-41f7-8957-78c1899ca95f", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ffb0bcfe-5aba-4c18-9723-b82a7d2219a0", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "ea9e4ea0c041aa25b372c7cdd793cded3f71663ae1b7fdaff19aa17921d30ab8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "17ebeed7-747f-4cb5-acd6-b58b73906628", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "5b112f699873652ffee478f8d0330c0dfe6e2e36ae0b70b7bc405e59322f51ce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6339df8d-aef9-4b85-8948-b89a05bf2611", "node_type": "1", "metadata": {}, "hash": "30dae01f6255030b549300b682776a6339e3bb98261990bff9fc5c5e82590563", "class_name": "RelatedNodeInfo"}}, "text": "The original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time, attention focused on matching specific mental abilities, leading to deviations from biology such as backpropagation, or passing information in the reverse direction and adjusting the network to reflect that information.\r\nNeural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.\r\nAs of 2017, neural networks typically have a few thousand to a few million units and millions of connections. Despite this number being several order of magnitude less than the number of neurons on a human brain, these networks can perform many tasks at a level beyond that of humans (e.g., recognizing faces, or playing \"Go\").\r\n\r\n\r\n=== Deep neural networks ===\r\nA deep neural network (DNN) is an artificial neural network with multiple layers between the input and output layers. There are different types of neural networks but they always consist of the same components: neurons, synapses, weights, biases, and functions. These components as a whole function in a way that mimics functions of the human brain, and can be trained like any other ML algorithm.For example, a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed. The user can review the results and select which probabilities the network should display (above a certain threshold, etc.) and return the proposed label. Each mathematical manipulation as such is considered a layer, and complex DNN have many layers, hence the name \"deep\" networks.\r\nDNNs can model complex non-linear relationships. DNN architectures generate compositional models where the object is expressed as a layered composition of primitives. The extra layers enable composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network. For instance, it was proved that sparse multivariate polynomials are exponentially easier to approximate with DNNs than with shallow networks.Deep architectures include many variants of a few basic approaches. Each architecture has found success in specific domains. It is not always possible to compare the performance of multiple architectures, unless they have been evaluated on the same data sets.\r\nDNNs are typically feedforward networks in which data flows from the input layer to the output layer without looping back. At first, the DNN creates a map of virtual neurons and assigns random numerical values, or \"weights\", to connections between them. The weights and inputs are multiplied and return an output between 0 and 1. If the network did not accurately recognize a particular pattern, an algorithm would adjust the weights. That way the algorithm can make certain parameters more influential, until it determines the correct mathematical manipulation to fully process the data.\r\nRecurrent neural networks, in which data can flow in any direction, are used for applications such as language modeling. Long short-term memory is particularly effective for this use.Convolutional neural networks (CNNs) are used in computer vision. CNNs also have been applied to acoustic modeling for automatic speech recognition (ASR).\r\n\r\n\r\n==== Challenges ====\r\nAs with ANNs, many issues can arise with naively trained DNNs. Two common issues are overfitting and computation time.\r\nDNNs are prone to overfitting because of the added layers of abstraction, which allow them to model rare dependencies in the training data. Regularization methods such as Ivakhnenko's unit pruning or weight decay (\u21132{\\displaystyle \\ell _{2}}-regularization) or sparsity (\u21131{\\displaystyle \\ell _{1}}-regularization) can be applied during training to combat overfitting. Alternatively dropout regularization randomly omits units from the hidden layers during training. This helps to exclude rare dependencies. Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.DNNs must consider many training parameters, such as the size (number of layers and number of units per layer), the learning rate, and initial weights. Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources. Various tricks, such as batching (computing the gradient on several training examples at once rather than individual examples) speed up computation. Large processing capabilities of many-core architectures (such as GPUs or the Intel Xeon Phi) have produced significant speedups in training, because of the suitability of such processing architectures for the matrix and vector computations.Alternatively, engineers may look for other types of neural networks with more straightforward and convergent training algorithms.", "start_char_idx": 26553, "end_char_idx": 31596, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6339df8d-aef9-4b85-8948-b89a05bf2611": {"__data__": {"id_": "6339df8d-aef9-4b85-8948-b89a05bf2611", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ffb0bcfe-5aba-4c18-9723-b82a7d2219a0", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "ea9e4ea0c041aa25b372c7cdd793cded3f71663ae1b7fdaff19aa17921d30ab8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fe3234a9-3da2-41f7-8957-78c1899ca95f", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "3d23ff70593764666389d2e0b77dd177ffad6dc4e255448fe541b241381198d7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc4ab0ea-dfb5-40c0-b7aa-02100c580b72", "node_type": "1", "metadata": {}, "hash": "77ee7f8c0a821b6680c7eaec4881740c79c52f2b238055d1abdf72c67f0b7800", "class_name": "RelatedNodeInfo"}}, "text": "This helps to exclude rare dependencies. Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.DNNs must consider many training parameters, such as the size (number of layers and number of units per layer), the learning rate, and initial weights. Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources. Various tricks, such as batching (computing the gradient on several training examples at once rather than individual examples) speed up computation. Large processing capabilities of many-core architectures (such as GPUs or the Intel Xeon Phi) have produced significant speedups in training, because of the suitability of such processing architectures for the matrix and vector computations.Alternatively, engineers may look for other types of neural networks with more straightforward and convergent training algorithms. CMAC (cerebellar model articulation controller) is one such kind of neural network. It doesn't require learning rates or randomized initial weights. The training process can be guaranteed to converge in one step with a new batch of data, and the computational complexity of the training algorithm is linear with respect to the number of neurons involved.\r\n\r\n\r\n== Hardware ==\r\nSince the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer. By 2019, graphic processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI. OpenAI estimated the hardware computation used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of computation required, with a doubling-time trendline of 3.4 months.Special electronic circuits called deep learning processors were designed to speed up deep learning algorithms. Deep learning processors include neural processing units (NPUs) in Huawei cellphones and cloud computing servers such as tensor processing units (TPU) in the Google Cloud Platform. Cerebras Systems has also built a dedicated system to handle large deep learning models, the CS-2, based on the largest processor in the industry, the second-generation Wafer Scale Engine (WSE-2).Atomically thin semiconductors are considered promising for energy-efficient deep learning hardware where the same basic device structure is used for both logic operations and data storage.\r\nIn 2020, Marega et al. published experiments with a large-area active channel material for developing logic-in-memory devices and circuits based on floating-gate field-effect transistors (FGFETs).In 2021, J. Feldmann et al. proposed an integrated photonic hardware accelerator for parallel convolutional processing. The authors identify two key advantages of integrated photonics over its electronic counterparts: (1) massively parallel data transfer through wavelength division multiplexing in conjunction with frequency combs, and (2) extremely high data modulation speeds. Their system can execute trillions of multiply-accumulate operations per second, indicating the potential of integrated photonics in data-heavy AI applications.\r\n\r\n\r\n== Applications ==\r\n\r\n\r\n=== Automatic speech recognition ===\r\n\r\nLarge-scale automatic speech recognition is the first and most convincing successful case of deep learning. LSTM RNNs can learn \"Very Deep Learning\" tasks that involve multi-second intervals containing speech events separated by thousands of discrete time steps, where one time step corresponds to about 10 ms. LSTM with forget gates is competitive with traditional speech recognizers on certain tasks.The initial success in speech recognition was based on small-scale recognition tasks based on TIMIT. The data set contains 630 speakers from eight major dialects of American English, where each speaker reads 10 sentences. Its small size lets many configurations be tried. More importantly, the TIMIT task concerns phone-sequence recognition, which, unlike word-sequence recognition, allows weak phone bigram language models. This lets the strength of the acoustic modeling aspects of speech recognition be more easily analyzed. The error rates listed below, including these early results and measured as percent phone error rates (PER), have been summarized since 1991.", "start_char_idx": 30583, "end_char_idx": 35176, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc4ab0ea-dfb5-40c0-b7aa-02100c580b72": {"__data__": {"id_": "fc4ab0ea-dfb5-40c0-b7aa-02100c580b72", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ffb0bcfe-5aba-4c18-9723-b82a7d2219a0", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "ea9e4ea0c041aa25b372c7cdd793cded3f71663ae1b7fdaff19aa17921d30ab8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6339df8d-aef9-4b85-8948-b89a05bf2611", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "46fa79c9449da5275ea33ddef19ab9caeca1f15eda22d9b7727bc6957873b044", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5fa7d90f-7654-41d9-980c-4725812841e7", "node_type": "1", "metadata": {}, "hash": "1d02c76525947c085acbc7eb0c0db1d2a416498df1ab06c3c004b9c6a423606d", "class_name": "RelatedNodeInfo"}}, "text": "LSTM RNNs can learn \"Very Deep Learning\" tasks that involve multi-second intervals containing speech events separated by thousands of discrete time steps, where one time step corresponds to about 10 ms. LSTM with forget gates is competitive with traditional speech recognizers on certain tasks.The initial success in speech recognition was based on small-scale recognition tasks based on TIMIT. The data set contains 630 speakers from eight major dialects of American English, where each speaker reads 10 sentences. Its small size lets many configurations be tried. More importantly, the TIMIT task concerns phone-sequence recognition, which, unlike word-sequence recognition, allows weak phone bigram language models. This lets the strength of the acoustic modeling aspects of speech recognition be more easily analyzed. The error rates listed below, including these early results and measured as percent phone error rates (PER), have been summarized since 1991.\r\n\r\nThe debut of DNNs for speaker recognition in the late 1990s and speech recognition around 2009-2011 and of LSTM around 2003\u20132007, accelerated progress in eight major areas:\r\nScale-up/out and accelerated DNN training and decoding\r\nSequence discriminative training\r\nFeature processing by deep models with solid understanding of the underlying mechanisms\r\nAdaptation of DNNs and related deep models\r\nMulti-task and transfer learning by DNNs and related deep models\r\nCNNs and how to design them to best exploit domain knowledge of speech\r\nRNN and its rich LSTM variants\r\nOther types of deep models including tensor-based models and integrated deep generative/discriminative models.All major commercial speech recognition systems (e.g., Microsoft Cortana, Xbox, Skype Translator, Amazon Alexa, Google Now, Apple Siri, Baidu and iFlyTek voice search, and a range of Nuance speech products, etc.) are based on deep learning.\r\n\r\n\r\n=== Image recognition ===\r\n\r\nA common evaluation set for image classification is the MNIST database data set. MNIST is composed of handwritten digits and includes 60,000 training examples and 10,000 test examples. As with TIMIT, its small size lets users test multiple configurations. A comprehensive list of results on this set is available.Deep learning-based image recognition has become \"superhuman\", producing more accurate results than human contestants. This first occurred in 2011 in recognition of traffic signs, and in 2014, with recognition of human faces.Deep learning-trained vehicles now interpret 360\u00b0 camera views. Another example is Facial Dysmorphology Novel Analysis (FDNA) used to analyze cases of human malformation connected to a large database of genetic syndromes.\r\n\r\n\r\n=== Visual art processing ===\r\nClosely related to the progress that has been made in image recognition is the increasing application of deep learning techniques to various visual art tasks. DNNs have proven themselves capable, for example, of\r\n\r\nidentifying the style period of a given painting\r\nNeural Style Transfer \u2013  capturing the style of a given artwork and applying it in a visually pleasing manner to an arbitrary photograph or video\r\ngenerating striking imagery based on random visual input fields.\r\n\r\n\r\n=== Natural language processing ===\r\n\r\nNeural networks have been used for implementing language models since the early 2000s. LSTM helped to improve machine translation and language modeling.Other key techniques in this field are negative sampling and word embedding. Word embedding, such as word2vec, can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a vector space. Using word embedding as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar. A compositional vector grammar can be thought of as probabilistic context free grammar (PCFG) implemented by an RNN. Recursive auto-encoders built atop word embeddings can assess sentence similarity and detect paraphrasing. Deep neural architectures provide the best results for constituency parsing, sentiment analysis, information retrieval, spoken language understanding, machine translation, contextual entity linking, writing style recognition, named-entity recognition (token classification), text classification, and others.Recent developments generalize word embedding to sentence embedding.\r\nGoogle Translate (GT) uses a large end-to-end long short-term memory (LSTM) network. Google Neural Machine Translation (GNMT) uses an example-based machine translation method in which the system \"learns from millions of examples\". It translates \"whole sentences at a time, rather than pieces\". Google Translate supports over one hundred languages. The network encodes the \"semantics of the sentence rather than simply memorizing phrase-to-phrase translations\". GT uses English as an intermediate between most language pairs.", "start_char_idx": 34213, "end_char_idx": 39231, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5fa7d90f-7654-41d9-980c-4725812841e7": {"__data__": {"id_": "5fa7d90f-7654-41d9-980c-4725812841e7", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ffb0bcfe-5aba-4c18-9723-b82a7d2219a0", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "ea9e4ea0c041aa25b372c7cdd793cded3f71663ae1b7fdaff19aa17921d30ab8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc4ab0ea-dfb5-40c0-b7aa-02100c580b72", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "40eff679e65d3dfdbc4c789abe6da3f977f1c075a7efdabb4c93159daf12c173", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "aea60cca-465e-42c2-9fce-ffeaec09a8a8", "node_type": "1", "metadata": {}, "hash": "95ceb8ccedc8499b7b64613736e773eb76e0f33337770a9518698759e19634e3", "class_name": "RelatedNodeInfo"}}, "text": "Recursive auto-encoders built atop word embeddings can assess sentence similarity and detect paraphrasing. Deep neural architectures provide the best results for constituency parsing, sentiment analysis, information retrieval, spoken language understanding, machine translation, contextual entity linking, writing style recognition, named-entity recognition (token classification), text classification, and others.Recent developments generalize word embedding to sentence embedding.\r\nGoogle Translate (GT) uses a large end-to-end long short-term memory (LSTM) network. Google Neural Machine Translation (GNMT) uses an example-based machine translation method in which the system \"learns from millions of examples\". It translates \"whole sentences at a time, rather than pieces\". Google Translate supports over one hundred languages. The network encodes the \"semantics of the sentence rather than simply memorizing phrase-to-phrase translations\". GT uses English as an intermediate between most language pairs.\r\n\r\n\r\n=== Drug discovery and toxicology ===\r\n\r\nA large percentage of candidate drugs fail to win regulatory approval. These failures are caused by insufficient efficacy (on-target effect), undesired interactions (off-target effects), or unanticipated toxic effects. Research has explored use of deep learning to predict the biomolecular targets, off-targets, and toxic effects of environmental chemicals in nutrients, household products and drugs.AtomNet is a deep learning system for structure-based rational drug design. AtomNet was used to predict novel candidate biomolecules for disease targets such as the Ebola virus and multiple sclerosis.In 2017 graph neural networks were used for the first time to predict various properties of molecules in a large toxicology data set. In 2019, generative neural networks were used to produce molecules that were validated experimentally all the way into mice.\r\n\r\n\r\n=== Customer relationship management ===\r\n\r\nDeep reinforcement learning has been used to approximate the value of possible direct marketing actions, defined in terms of RFM variables. The estimated value function was shown to have a natural interpretation as customer lifetime value.\r\n\r\n\r\n=== Recommendation systems ===\r\n\r\nRecommendation systems have used deep learning to extract meaningful features for a latent factor model for content-based music and journal recommendations. Multi-view deep learning has been applied for learning user preferences from multiple domains. The model uses a hybrid collaborative and content-based approach and enhances recommendations in multiple tasks.\r\n\r\n\r\n=== Bioinformatics ===\r\n\r\nAn autoencoder ANN was used in bioinformatics, to predict gene ontology annotations and gene-function relationships.In medical informatics, deep learning was used to predict sleep quality based on data from wearables and predictions of health complications from electronic health record data.\r\n\r\n\r\n=== Deep Neural Network Estimations ===\r\nDeep neural networks can be used to estimate the entropy of a stochastic process and called Neural Joint Entropy Estimator (NJEE). Such an estimation provides insights on the effects of input random variables on an independent random variable. Practically, the DNN is trained as a classifier that maps an input vector or matrix X to an output probability distribution over the possible classes of random variable Y, given input X. For example, in image classification tasks, the NJEE maps a vector of pixels' color values to probabilities over possible image classes. In practice, the probability distribution of Y is obtained by a Softmax layer with number of nodes that is equal to the alphabet size of Y. NJEE uses continuously differentiable activation functions, such that the conditions for the universal approximation theorem holds. It is shown that this method provides a strongly consistent estimator and outperforms other methods in case of large alphabet sizes.\r\n\r\n\r\n=== Medical image analysis ===\r\nDeep learning has been shown to produce competitive results in medical application such as cancer cell classification, lesion detection, organ segmentation and image enhancement. Modern deep learning tools demonstrate the high accuracy of detecting various diseases and the helpfulness of their use by specialists to improve the diagnosis efficiency.\r\n\r\n\r\n=== Mobile advertising ===\r\nFinding the appropriate mobile audience for mobile advertising is always challenging, since many data points must be considered and analyzed before a target segment can be created and used in ad serving by any ad server. Deep learning has been used to interpret large, many-dimensioned advertising datasets. Many data points are collected during the request/serve/click internet advertising cycle. This information can form the basis of machine learning to improve ad selection.\r\n\r\n\r\n=== Image restoration ===\r\nDeep learning has been successfully applied to inverse problems such as denoising, super-resolution, inpainting, and film colorization. These applications include learning methods such as \"Shrinkage Fields for Effective Image Restoration\" which trains on an image dataset, and Deep Image Prior, which trains on the image that needs restoration.\r\n\r\n\r\n=== Financial fraud detection ===\r\nDeep learning is being successfully applied to financial fraud detection, tax evasion detection, and anti-money laundering.", "start_char_idx": 38223, "end_char_idx": 43613, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aea60cca-465e-42c2-9fce-ffeaec09a8a8": {"__data__": {"id_": "aea60cca-465e-42c2-9fce-ffeaec09a8a8", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ffb0bcfe-5aba-4c18-9723-b82a7d2219a0", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "ea9e4ea0c041aa25b372c7cdd793cded3f71663ae1b7fdaff19aa17921d30ab8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5fa7d90f-7654-41d9-980c-4725812841e7", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "c03931dba2e5920149121546788021e619f4bdf75795e089c323e54568008aed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b5d6518e-adad-4314-8ab0-908733ab7041", "node_type": "1", "metadata": {}, "hash": "cef031fe96c6677f857a03f898c6fef541e497eac1275bf7ce0be85805f0df2d", "class_name": "RelatedNodeInfo"}}, "text": "=== Mobile advertising ===\r\nFinding the appropriate mobile audience for mobile advertising is always challenging, since many data points must be considered and analyzed before a target segment can be created and used in ad serving by any ad server. Deep learning has been used to interpret large, many-dimensioned advertising datasets. Many data points are collected during the request/serve/click internet advertising cycle. This information can form the basis of machine learning to improve ad selection.\r\n\r\n\r\n=== Image restoration ===\r\nDeep learning has been successfully applied to inverse problems such as denoising, super-resolution, inpainting, and film colorization. These applications include learning methods such as \"Shrinkage Fields for Effective Image Restoration\" which trains on an image dataset, and Deep Image Prior, which trains on the image that needs restoration.\r\n\r\n\r\n=== Financial fraud detection ===\r\nDeep learning is being successfully applied to financial fraud detection, tax evasion detection, and anti-money laundering.\r\n\r\n\r\n=== Materials science ===\r\nIn November 2023, researchers at Google DeepMind and Lawrence Berkeley National Laboratory announced that they had developed an AI system known as GNoME. This system has contributed to materials science by discovering over 2 million new materials within a relatively short timeframe. GNoME employs deep learning techniques to efficiently explore potential material structures, achieving a significant increase in the identification of stable inorganic crystal structures. The system's predictions were validated through autonomous robotic experiments, demonstrating a noteworthy success rate of 71%. The data of newly discovered materials is publicly available through the Materials Project database, offering researchers the opportunity to identify materials with desired properties for various applications. This development has implications for the future of scientific discovery and the integration of AI in material science research, potentially expediting material innovation and reducing costs in product development. The use of AI and deep learning suggests the possibility of minimizing or eliminating manual lab experiments and allowing scientists to focus more on the design and analysis of unique compounds.\r\n\r\n\r\n=== Military ===\r\nThe United States Department of Defense applied deep learning to train robots in new tasks through observation.\r\n\r\n\r\n=== Partial differential equations ===\r\nPhysics informed neural networks have been used to solve partial differential equations in both forward and inverse problems in a data driven manner. One example is the reconstructing fluid flow governed by the Navier-Stokes equations. Using physics informed neural networks does not require the often expensive mesh generation that conventional CFD methods relies on.\r\n\r\n\r\n=== Image reconstruction ===\r\nImage reconstruction is the reconstruction of the underlying images from the image-related measurements. Several works showed the better and superior performance of the deep learning methods compared to analytical methods for various applications, e.g., spectral imaging  and ultrasound imaging.\r\n\r\n\r\n=== Epigenetic clock ===\r\n\r\nAn epigenetic clock is a biochemical test that can be used to measure age. Galkin et al. used deep neural networks to train an epigenetic aging clock of unprecedented accuracy using >6,000 blood samples. The clock uses information from 1000 CpG sites and predicts people with certain conditions older than healthy controls: IBD, frontotemporal dementia, ovarian cancer, obesity. The aging clock was planned to be released for public use in 2021 by an Insilico Medicine spinoff company Deep Longevity.\r\n\r\n\r\n== Relation to human cognitive and brain development ==\r\nDeep learning is closely related to a class of theories of brain development (specifically, neocortical development) proposed by cognitive neuroscientists in the early 1990s. These developmental theories were instantiated in computational models, making them predecessors of deep learning systems. These developmental models share the property that various proposed learning dynamics in the brain (e.g., a wave of nerve growth factor) support the self-organization somewhat analogous to the neural networks utilized in deep learning models. Like the neocortex, neural networks employ a hierarchy of layered filters in which each layer considers information from a prior layer (or the operating environment), and then passes its output (and possibly the original input), to other layers. This process yields a self-organizing stack of transducers, well-tuned to their operating environment. A 1995 description stated, \"...the infant's brain seems to organize itself under the influence of waves of so-called trophic-factors ... different regions of the brain become connected sequentially, with one layer of tissue maturing before another and so on until the whole brain is mature\".A variety of approaches have been used to investigate the plausibility of deep learning models from a neurobiological perspective. On the one hand, several variants of the backpropagation algorithm have been proposed in order to increase its processing realism. Other researchers have argued that unsupervised forms of deep learning, such as those based on hierarchical generative models and deep belief networks, may be closer to biological reality.", "start_char_idx": 42566, "end_char_idx": 47973, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b5d6518e-adad-4314-8ab0-908733ab7041": {"__data__": {"id_": "b5d6518e-adad-4314-8ab0-908733ab7041", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ffb0bcfe-5aba-4c18-9723-b82a7d2219a0", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "ea9e4ea0c041aa25b372c7cdd793cded3f71663ae1b7fdaff19aa17921d30ab8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "aea60cca-465e-42c2-9fce-ffeaec09a8a8", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "f216fda405b8bc866f1e25e01478c2e8da4be55c740615fbd1fca68ac6d66e95", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d640173d-29fb-4b73-bd12-b8a306a1fc88", "node_type": "1", "metadata": {}, "hash": "d39b513373ef9f05f5839e63271dfb4af64ec5c585aa4ee6583a90936171da0a", "class_name": "RelatedNodeInfo"}}, "text": "This process yields a self-organizing stack of transducers, well-tuned to their operating environment. A 1995 description stated, \"...the infant's brain seems to organize itself under the influence of waves of so-called trophic-factors ... different regions of the brain become connected sequentially, with one layer of tissue maturing before another and so on until the whole brain is mature\".A variety of approaches have been used to investigate the plausibility of deep learning models from a neurobiological perspective. On the one hand, several variants of the backpropagation algorithm have been proposed in order to increase its processing realism. Other researchers have argued that unsupervised forms of deep learning, such as those based on hierarchical generative models and deep belief networks, may be closer to biological reality. In this respect, generative neural network models have been related to neurobiological evidence about sampling-based processing in the cerebral cortex.Although a systematic comparison between the human brain organization and the neuronal encoding in deep networks has not yet been established, several analogies have been reported. For example, the computations performed by deep learning units could be similar to those of actual neurons and neural populations. Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system both at the single-unit and at the population levels.\r\n\r\n\r\n== Commercial activity ==\r\nFacebook's AI lab performs tasks such as automatically tagging uploaded pictures with the names of the people in them.Google's DeepMind Technologies developed a system capable of learning how to play Atari video games using only pixels as data input. In 2015 they demonstrated their AlphaGo system, which learned the game of Go well enough to beat a professional Go player. Google Translate uses a neural network to translate between more than 100 languages.\r\nIn 2017, Covariant.ai was launched, which focuses on integrating deep learning into factories.As of 2008, researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement, or TAMER, which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor. First developed as TAMER, a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot with the ability to learn new tasks through observation. Using Deep TAMER, a robot learned a task with a human trainer, watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer, who provided feedback such as \"good job\" and \"bad job\".\r\n\r\n\r\n== Criticism and comment ==\r\nDeep learning has attracted both criticism and comment, in some cases from outside the field of computer science.\r\n\r\n\r\n=== Theory ===\r\n\r\nA main criticism concerns the lack of theory surrounding some methods. Learning in the most common deep architectures is implemented using well-understood gradient descent. However, the theory surrounding other algorithms, such as contrastive divergence is less clear. (e.g., Does it converge? If so, how fast? What is it approximating?) Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically.Others point out that deep learning should be looked at as a step towards realizing strong AI, not as an all-encompassing solution. Despite the power of deep learning methods, they still lack much of the functionality needed to realize this goal entirely. Research psychologist Gary Marcus noted:\r\n\r\nRealistically, deep learning is only part of the larger challenge of building intelligent machines. Such techniques lack ways of representing causal relationships (...) have no obvious ways of performing logical inferences, and they are also still a long way from integrating abstract knowledge, such as information about what objects are, what they are for, and how they are typically used. The most powerful A.I. systems, like Watson (...) use techniques like deep learning as just one element in a very complicated ensemble of techniques, ranging from the statistical technique of Bayesian inference to deductive reasoning.\r\n\r\nIn further reference to the idea that artistic sensitivity might be inherent in relatively low levels of the cognitive hierarchy, a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained demonstrate a visual appeal: the original research notice received well over 1,000 comments, and was the subject of what was for a time the most frequently accessed article on The Guardian's website.", "start_char_idx": 47129, "end_char_idx": 52157, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d640173d-29fb-4b73-bd12-b8a306a1fc88": {"__data__": {"id_": "d640173d-29fb-4b73-bd12-b8a306a1fc88", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ffb0bcfe-5aba-4c18-9723-b82a7d2219a0", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "ea9e4ea0c041aa25b372c7cdd793cded3f71663ae1b7fdaff19aa17921d30ab8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b5d6518e-adad-4314-8ab0-908733ab7041", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "1f52e188332291fff5ea2ce43037b1cef4dd3697d7df3653d10484ea9f15ff1c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e9df0acf-8aa1-4692-9705-936f8f312670", "node_type": "1", "metadata": {}, "hash": "06f2f56d13f0308e18d6430f926ae519e54a022d465d43207fbf7961268dddd0", "class_name": "RelatedNodeInfo"}}, "text": "Such techniques lack ways of representing causal relationships (...) have no obvious ways of performing logical inferences, and they are also still a long way from integrating abstract knowledge, such as information about what objects are, what they are for, and how they are typically used. The most powerful A.I. systems, like Watson (...) use techniques like deep learning as just one element in a very complicated ensemble of techniques, ranging from the statistical technique of Bayesian inference to deductive reasoning.\r\n\r\nIn further reference to the idea that artistic sensitivity might be inherent in relatively low levels of the cognitive hierarchy, a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained demonstrate a visual appeal: the original research notice received well over 1,000 comments, and was the subject of what was for a time the most frequently accessed article on The Guardian's website.\r\n\r\n\r\n=== Errors ===\r\nSome deep learning architectures display problematic behaviors, such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images (2014) and misclassifying minuscule perturbations of correctly classified images (2013). Goertzel hypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-component artificial general intelligence (AGI) architectures. These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar decompositions of observed entities and events. Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic goal of both human language acquisition and artificial intelligence (AI).\r\n\r\n\r\n=== Cyber threat ===\r\nAs deep learning moves from the lab into the world, research and experience show that artificial neural networks are vulnerable to hacks and deception. By identifying patterns that these systems use to function, attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize. For example, an attacker can make subtle changes to an image such that the ANN finds a match even though the image looks to a human nothing like the search target. Such manipulation is termed an \"adversarial attack\".In 2016 researchers used one ANN to doctor images in trial and error fashion, identify another's focal points, and thereby generate images that deceived it. The modified images looked no different to human eyes. Another group showed that printouts of doctored images then photographed successfully tricked an image classification system. One defense is reverse image search, in which a possible fake image is submitted to a site such as TinEye that can then find other instances of it. A refinement is to search using only parts of the image, to identify images from which that piece may have been taken.Another group showed that certain psychedelic spectacles could fool a facial recognition system into thinking ordinary people were celebrities, potentially allowing one person to impersonate another. In 2017 researchers added stickers to stop signs and caused an ANN to misclassify them.ANNs can however be further trained to detect attempts at deception, potentially leading attackers and defenders into an arms race similar to the kind that already defines the malware defense industry. ANNs have been trained to defeat ANN-based anti-malware software by repeatedly attacking a defense with malware that was continually altered by a genetic algorithm until it tricked the anti-malware while retaining its ability to damage the target.In 2016, another group demonstrated that certain sounds could make the Google Now voice command system open a particular web address, and hypothesized that this could \"serve as a stepping stone for further attacks (e.g., opening a web page hosting drive-by malware)\".In \"data poisoning\", false data is continually smuggled into a machine learning system's training set to prevent it from achieving mastery.\r\n\r\n\r\n=== Data collection ethics ===\r\nMost Deep Learning systems rely on training and verification data that is generated and/or annotated by humans. It has been argued in media philosophy that not only low-paid clickwork (e.g. on Amazon Mechanical Turk) is regularly deployed for this purpose, but also implicit forms of human microwork that are often not recognized as such. The philosopher Rainer M\u00fchlhoff distinguishes five types of \"machinic capture\" of human microwork to generate training data: (1) gamification (the embedding of annotation or computation tasks in the flow of a game), (2) \"trapping and tracking\" (e.g. CAPTCHAs for image recognition or click-tracking on Google search results pages), (3) exploitation of social motivations (e.g.", "start_char_idx": 51099, "end_char_idx": 56217, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e9df0acf-8aa1-4692-9705-936f8f312670": {"__data__": {"id_": "e9df0acf-8aa1-4692-9705-936f8f312670", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ffb0bcfe-5aba-4c18-9723-b82a7d2219a0", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "ea9e4ea0c041aa25b372c7cdd793cded3f71663ae1b7fdaff19aa17921d30ab8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d640173d-29fb-4b73-bd12-b8a306a1fc88", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "464184e05584c2fe58de45c9ea9c40dfebeab78c9b9baec93cfeb812df21c602", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e3461faa-e9b6-4d6a-8e7b-b9b6b70ac760", "node_type": "1", "metadata": {}, "hash": "44e82276f47eb481e5b4b4c15a4994b366d7ea9fe0871de615f4f977c46bc7e0", "class_name": "RelatedNodeInfo"}}, "text": "=== Data collection ethics ===\r\nMost Deep Learning systems rely on training and verification data that is generated and/or annotated by humans. It has been argued in media philosophy that not only low-paid clickwork (e.g. on Amazon Mechanical Turk) is regularly deployed for this purpose, but also implicit forms of human microwork that are often not recognized as such. The philosopher Rainer M\u00fchlhoff distinguishes five types of \"machinic capture\" of human microwork to generate training data: (1) gamification (the embedding of annotation or computation tasks in the flow of a game), (2) \"trapping and tracking\" (e.g. CAPTCHAs for image recognition or click-tracking on Google search results pages), (3) exploitation of social motivations (e.g. tagging faces on Facebook to obtain labeled facial images), (4) information mining (e.g. by leveraging quantified-self devices such as activity trackers) and (5) clickwork.M\u00fchlhoff argues that in most commercial end-user applications of Deep Learning such as Facebook's face recognition system, the need for training data does not stop once an ANN is trained. Rather, there is a continued demand for human-generated verification data to constantly calibrate and update the ANN. For this purpose, Facebook introduced the feature that once a user is automatically recognized in an image, they receive a notification. They can choose whether or not they like to be publicly labeled on the image, or tell Facebook that it is not them in the picture. This user interface is a mechanism to generate \"a constant stream of verification data\" to further train the network in real-time. As M\u00fchlhoff argues, the involvement of human users to generate training and verification data is so typical for most commercial end-user applications of Deep Learning that such systems may be referred to as \"human-aided artificial intelligence\".\r\n\r\n\r\n== See also ==\r\nApplications of artificial intelligence\r\nComparison of deep learning software\r\nCompressed sensing\r\nDifferentiable programming\r\nEcho state network\r\nList of artificial intelligence projects\r\nLiquid state machine\r\nList of datasets for machine-learning research\r\nReservoir computing\r\nScale space and deep learning\r\nSparse coding\r\nStochastic parrot\r\n\r\n\r\n== References ==\r\n\r\n\r\n== Further reading ==", "start_char_idx": 55470, "end_char_idx": 57754, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e3461faa-e9b6-4d6a-8e7b-b9b6b70ac760": {"__data__": {"id_": "e3461faa-e9b6-4d6a-8e7b-b9b6b70ac760", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Dilution (neural networks).txt", "file_name": "Dilution (neural networks).txt", "file_type": "text/plain", "file_size": 5621, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bde9526a-8edb-41c5-b464-118aec879b48", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Dilution (neural networks).txt", "file_name": "Dilution (neural networks).txt", "file_type": "text/plain", "file_size": 5621, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "46c5bee80999c453527ccbfaf53ba555980ac5732442dfd658da24d93d39185a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e9df0acf-8aa1-4692-9705-936f8f312670", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "fa9e4abf33d4432fd5bc4d124fe10d7d3ca9ba0eee7982c2e05200195b15e6c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1b28a924-ee18-45d9-88d6-02c825d938b6", "node_type": "1", "metadata": {}, "hash": "5c15125bb7ae6324d4ebd05db96cdc0d11b6369a9338dc4b003776b3ce744181", "class_name": "RelatedNodeInfo"}}, "text": "Dilution and dropout (also called DropConnect) are regularization techniques for reducing overfitting in artificial neural networks by preventing complex co-adaptations on training data. They are an efficient way of performing model averaging with neural networks. Dilution refers to thinning weights, while dropout refers to randomly \"dropping out\", or omitting, units (both hidden and visible) during the training process of a neural network. Both trigger the same type of regularization.\r\n\r\n\r\n== Types and uses ==\r\nDilution is usually split in weak dilution and strong dilution. Weak dilution describes the process in which the finite fraction of removed connections is small, and strong dilution refers to when this fraction is large. There is no clear distinction on where the limit between strong and weak dilution is, and often the distinction is dependent on the precedent of a specific use-case and has implications for how to solve for exact solutions.\r\nSometimes dilution is used for adding damping noise to the inputs. In that case, weak dilution refers to adding a small amount of damping noise, while strong dilution refers to adding a greater amount of damping noise. Both can be rewritten as variants of weight dilution.\r\nThese techniques are also sometimes referred to as random pruning of weights, but this is usually a non-recurring one-way operation. The network is pruned, and then kept if it is an improvement over the previous model. Dilution and dropout both refer to an iterative process. The pruning of weights typically does not imply that the network continues learning, while in dilution/dropout, the network continues to learn after the technique is applied.\r\n\r\n\r\n== Generalized linear network ==\r\nOutput from a layer of linear nodes, in an artificial neural net can be described as\r\n\r\nyi{\\displaystyle y_{i}} \u2013 output from node i{\\displaystyle i}\r\nwij{\\displaystyle w_{ij}} \u2013 real weight before dilution, also called the Hebb connection strength\r\nxj{\\displaystyle x_{j}} \u2013 input from node j{\\displaystyle j}This can be written in vector notation as\r\n\r\ny{\\displaystyle \\mathbf {y} } \u2013 output vector\r\nW{\\displaystyle \\mathbf {W} } \u2013 weight matrix\r\nx{\\displaystyle \\mathbf {x} } \u2013 input vectorEquations (1) and (2) are used in the subsequent sections.\r\n\r\n\r\n== Weak dilution ==\r\nDuring weak dilution, the finite fraction of removed connections (the weights) is small, giving rise to a tiny uncertainty. This edge-case can be solved exactly with mean field theory. In weak dilution the impact on the weights can be described as\r\n\r\nwij^{\\displaystyle {\\hat {w_{ij}}}} \u2013 diluted weight\r\nwij{\\displaystyle w_{ij}} \u2013 real weight before dilution\r\nP(c){\\displaystyle P(c)} \u2013 the probability of c{\\displaystyle c}, the probability of keeping a weightThe interpretation of probability P(c){\\displaystyle P(c)} can also be changed from keeping a weight into pruning a weight.\r\nIn vector notation this can be written as\r\n\r\nwhere the function g\u2061(\u22c5){\\displaystyle \\operatorname {g} (\\cdot )} imposes the previous dilution.\r\nIn weak dilution only a small and fixed fraction of the weights are diluted. When the number of terms in the sum goes to infinite (the weights for each node) it is still infinite (the fraction is fixed), thus mean field theory can be applied. In the notation from Hertz et al. this would be written as\r\n\r\n\u27e8hi\u27e9{\\displaystyle \\left\\langle h_{i}\\right\\rangle } the mean field temperature\r\nc{\\displaystyle c} \u2013 a scaling factor for the temperature from the probability of keeping the weight\r\nwij{\\displaystyle w_{ij}} \u2013 real weight before dilution, also called the Hebb connection strength\r\n\u27e8Sj\u27e9{\\displaystyle \\left\\langle S_{j}\\right\\rangle } \u2013 the mean stable equilibrium statesThere are some assumptions for this to hold, which are not listed here.\r\n\r\n\r\n== Strong dilution ==\r\nWhen the dilution is strong, the finite fraction of removed connections (the weights) is large, giving rise to a huge uncertainty.", "start_char_idx": 0, "end_char_idx": 3944, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1b28a924-ee18-45d9-88d6-02c825d938b6": {"__data__": {"id_": "1b28a924-ee18-45d9-88d6-02c825d938b6", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Dilution (neural networks).txt", "file_name": "Dilution (neural networks).txt", "file_type": "text/plain", "file_size": 5621, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bde9526a-8edb-41c5-b464-118aec879b48", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Dilution (neural networks).txt", "file_name": "Dilution (neural networks).txt", "file_type": "text/plain", "file_size": 5621, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "46c5bee80999c453527ccbfaf53ba555980ac5732442dfd658da24d93d39185a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e3461faa-e9b6-4d6a-8e7b-b9b6b70ac760", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Dilution (neural networks).txt", "file_name": "Dilution (neural networks).txt", "file_type": "text/plain", "file_size": 5621, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "0489ef98981d11b1006ad3aadc0a683098f967e9e0fc40d4907fd7dd10703852", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da1a514a-3dfd-485e-9c48-1c2a3b3ba794", "node_type": "1", "metadata": {}, "hash": "dd0ff6a38b32d562ef9c84a535a7f15c5e46e93995d6a84092266f6d50964ca2", "class_name": "RelatedNodeInfo"}}, "text": "When the number of terms in the sum goes to infinite (the weights for each node) it is still infinite (the fraction is fixed), thus mean field theory can be applied. In the notation from Hertz et al. this would be written as\r\n\r\n\u27e8hi\u27e9{\\displaystyle \\left\\langle h_{i}\\right\\rangle } the mean field temperature\r\nc{\\displaystyle c} \u2013 a scaling factor for the temperature from the probability of keeping the weight\r\nwij{\\displaystyle w_{ij}} \u2013 real weight before dilution, also called the Hebb connection strength\r\n\u27e8Sj\u27e9{\\displaystyle \\left\\langle S_{j}\\right\\rangle } \u2013 the mean stable equilibrium statesThere are some assumptions for this to hold, which are not listed here.\r\n\r\n\r\n== Strong dilution ==\r\nWhen the dilution is strong, the finite fraction of removed connections (the weights) is large, giving rise to a huge uncertainty.\r\n\r\n\r\n== Dropout ==\r\nDropout is a special case of the previous weight equation (3), where the aforementioned equation is adjusted to remove a whole row in the vector matrix, and not only random weights\r\n\r\nP(c){\\displaystyle P(c)} \u2013 the probability c{\\displaystyle c} to keep a row in the weight matrix\r\nwj{\\displaystyle \\mathbf {w} _{j}} \u2013 real row in the weight matrix before dropout\r\nwj^{\\displaystyle {\\hat {\\mathbf {w} _{j}}}} \u2013 diluted row in the weight matrixBecause dropout removes a whole row from the vector matrix, the previous (unlisted) assumptions for weak dilution and the use of mean field theory are not applicable.\r\nThe process by which the node is driven to zero, whether by setting the weights to zero, by \u201cremoving the node\u201d, or by some other means, does not impact the end result and does not create a new and unique case. If the neural net is processed by a high-performance digital array-multiplicator, then it is likely more effective to drive the value to zero late in the process graph. If the net is processed by a constrained processor, perhaps even an analog neuromorph processor, then it is likely a more power-efficient solution is to drive the value to zero early in the process graph.\r\n\r\n\r\n== Google's patent ==\r\nAlthough there have been examples of randomly removing connections between neurons in a neural network to improve models, this technique was first introduced with the name dropout by Geoffrey Hinton, et al. in 2012. Google currently holds the patent for the dropout technique.\r\n\r\n\r\n== See also ==\r\nAlexNet\r\nConvolutional neural network \u00a7 Dropout\r\n\r\n\r\n== Notes ==\r\n\r\n\r\n== References ==", "start_char_idx": 3115, "end_char_idx": 5574, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "da1a514a-3dfd-485e-9c48-1c2a3b3ba794": {"__data__": {"id_": "da1a514a-3dfd-485e-9c48-1c2a3b3ba794", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Feedforward neural network.txt", "file_name": "Feedforward neural network.txt", "file_type": "text/plain", "file_size": 9793, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bceab45b-1acb-44b4-8433-ff78ba533ae9", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Feedforward neural network.txt", "file_name": "Feedforward neural network.txt", "file_type": "text/plain", "file_size": 9793, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "9ea03d7071b7f6b676c241f81cc02216f81ba99c552caf4b89c2fc9a4bc76e45", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1b28a924-ee18-45d9-88d6-02c825d938b6", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Dilution (neural networks).txt", "file_name": "Dilution (neural networks).txt", "file_type": "text/plain", "file_size": 5621, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "95656ef41d1b9c3210aa1fe65db18dc76156141cf393d13a26574ef3b28dd727", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e0e51bb9-edad-41f5-adf9-bf87c2553d41", "node_type": "1", "metadata": {}, "hash": "55e22cafe865afcc30febc9697fe9c1e27bc94945f4383ed46dc43f499b46fc5", "class_name": "RelatedNodeInfo"}}, "text": "A feedforward neural network (FNN) is one of the two broad types of artificial neural network, characterized by direction of the flow of information between its layers. Its flow is uni-directional, meaning that the information in the model flows in only one direction\u2014forward\u2014from the input nodes, through the hidden nodes (if any) and to the output nodes, without any cycles or loops, in contrast to recurrent neural networks, which have a bi-directional flow. Modern feedforward networks are trained using the backpropagation method and are colloquially referred to as the \"vanilla\" neural networks.\r\n\r\n\r\n== Timeline ==\r\nIn 1958, a layered network of perceptrons, consisting of an input layer, a hidden layer with randomized weights that did not learn, and an output layer with learning connections, was introduced already by Frank Rosenblatt in his book Perceptron. This extreme learning machine was not yet a deep learning network.In 1965, the first  deep-learning feedforward network, not yet using stochastic gradient descent, was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa, at the time called the Group Method of Data Handling.In 1967, a deep-learning network, using stochastic gradient descent for the first time, was able to classify non-linearily separable pattern classes, as reported Shun'ichi Amari. Amari's student Saito conducted the computer experiments, using a five-layered feedforward network with two learning layers.In 1970, modern backpropagation method, an efficient application of a chain-rule-based supervised learning, was for the first time published by the Finnish researcher Seppo Linnainmaa. The term (i.e. \"back-propagating errors\") itself has been used by Rosenblatt himself, but he did not know how to implement it, although a continuous precursor of backpropagation was already used in the context of control theory in 1960 by Henry J. Kelley. It is known also as a reverse mode of automatic differentiation.In 1982, backpropagation was applied in the way that has become standard, for the first time by Paul Werbos.In 1985, an experimental analysis of the technique was conducted by David E. Rumelhart et al.. Many improvements to the approach have been made in subsequent decades.In 1987, using a stochastic gradient descent within a (wide 12-layer nonlinear) feed-forward network, Matthew Brand has trained it to reproduce logic functions of nontrivial circuit depth, using small batches of random input/output samples. He, however, concluded that on hardware (sub-megaflop computers) available at the time it was impractical, and proposed using fixed random early layers as an input hash for a single modifiable layer.In 1990s, an (much simpler) alternative to using neural networks, although still related support vector machine approach was developed by Vladimir Vapnik and his colleagues. In addition to performing linear classification, they were able to efficiently perform a non-linear classification using what is called the kernel trick, using high-dimensional feature spaces.In 2003, interest in backpropagation networks returned due to the successes of deep learning being applied to language modelling by Yoshua Bengio with co-authors.In 2017, modern transformer architectures were introduced.\r\n\r\n\r\n== Mathematical foundations ==\r\n\r\n\r\n=== Activation function ===\r\nThe two historically common activation functions are both sigmoids, and are described by\r\n\r\ny(vi)=tanh\u2061(vi)  and  y(vi)=(1+e\u2212vi)\u22121{\\displaystyle y(v_{i})=\\tanh(v_{i})~~{\\textrm {and}}~~y(v_{i})=(1+e^{-v_{i}})^{-1}}.The first is a hyperbolic tangent that ranges from -1 to 1, while the other is the logistic function, which is similar in shape but ranges from 0 to 1. Here yi{\\displaystyle y_{i}} is the output of the i{\\displaystyle i}th node (neuron) and vi{\\displaystyle v_{i}} is the weighted sum of the input connections. Alternative activation functions have been proposed, including the rectifier and softplus functions. More specialized activation functions include radial basis functions (used in radial basis networks, another class of supervised neural network models).\r\nIn recent developments of deep learning the rectified linear unit (ReLU) is more frequently used as one of the possible ways to overcome the numerical problems related to the sigmoids.\r\n\r\n\r\n=== Learning ===\r\nLearning occurs by changing connection weights after each piece of data is processed, based on the amount of error in the output compared to the expected result.", "start_char_idx": 0, "end_char_idx": 4481, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e0e51bb9-edad-41f5-adf9-bf87c2553d41": {"__data__": {"id_": "e0e51bb9-edad-41f5-adf9-bf87c2553d41", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Feedforward neural network.txt", "file_name": "Feedforward neural network.txt", "file_type": "text/plain", "file_size": 9793, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bceab45b-1acb-44b4-8433-ff78ba533ae9", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Feedforward neural network.txt", "file_name": "Feedforward neural network.txt", "file_type": "text/plain", "file_size": 9793, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "9ea03d7071b7f6b676c241f81cc02216f81ba99c552caf4b89c2fc9a4bc76e45", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "da1a514a-3dfd-485e-9c48-1c2a3b3ba794", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Feedforward neural network.txt", "file_name": "Feedforward neural network.txt", "file_type": "text/plain", "file_size": 9793, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "a5a62f454e8590ce32be18a7e2e8eb8252df9abbbe1ef9a06bc01dc3c9e37164", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f7efb63a-737a-4718-bdcf-239637fd47ca", "node_type": "1", "metadata": {}, "hash": "949aa76b1ab0e722ee78eff053c266ed51608bac223fc6e96ae01fdad097c576", "class_name": "RelatedNodeInfo"}}, "text": "Here yi{\\displaystyle y_{i}} is the output of the i{\\displaystyle i}th node (neuron) and vi{\\displaystyle v_{i}} is the weighted sum of the input connections. Alternative activation functions have been proposed, including the rectifier and softplus functions. More specialized activation functions include radial basis functions (used in radial basis networks, another class of supervised neural network models).\r\nIn recent developments of deep learning the rectified linear unit (ReLU) is more frequently used as one of the possible ways to overcome the numerical problems related to the sigmoids.\r\n\r\n\r\n=== Learning ===\r\nLearning occurs by changing connection weights after each piece of data is processed, based on the amount of error in the output compared to the expected result. This is an example of supervised learning, and is carried out through backpropagation.\r\nWe can represent the degree of error in an output node j{\\displaystyle j} in the n{\\displaystyle n}th data point (training example) by ej(n)=dj(n)\u2212yj(n){\\displaystyle e_{j}(n)=d_{j}(n)-y_{j}(n)}, where dj(n){\\displaystyle d_{j}(n)} is the desired target value for n{\\displaystyle n}th data point at node j{\\displaystyle j}, and yj(n){\\displaystyle y_{j}(n)} is the value produced at node j{\\displaystyle j} when the n{\\displaystyle n}th data point is given as an input.\r\nThe node weights can then be adjusted based on corrections that minimize the error in the entire output for the n{\\displaystyle n}th data point, given by\r\n\r\nE(n)=12\u2211output node jej2(n){\\displaystyle {\\mathcal {E}}(n)={\\frac {1}{2}}\\sum _{{\\text{output node }}j}e_{j}^{2}(n)}.Using gradient descent, the change in each weight wij{\\displaystyle w_{ij}} is\r\n\r\n\u0394wji(n)=\u2212\u03b7\u2202E(n)\u2202vj(n)yi(n){\\displaystyle \\Delta w_{ji}(n)=-\\eta {\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}y_{i}(n)}where yi(n){\\displaystyle y_{i}(n)} is the output of the previous neuron i{\\displaystyle i}, and \u03b7{\\displaystyle \\eta } is the learning rate, which is selected to ensure that the weights quickly converge to a response, without oscillations. In the previous expression, \u2202E(n)\u2202vj(n){\\displaystyle {\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}} denotes the partial derivate of the error E(n){\\displaystyle {\\mathcal {E}}(n)} according to the weighted sum vj(n){\\displaystyle v_{j}(n)} of the input connections of neuron i{\\displaystyle i}.\r\nThe derivative to be calculated depends on the induced local field vj{\\displaystyle v_{j}}, which itself varies. It is easy to prove that for an output node this derivative can be simplified to\r\n\r\n\u2212\u2202E(n)\u2202vj(n)=ej(n)\u03d5\u2032(vj(n)){\\displaystyle -{\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}=e_{j}(n)\\phi ^{\\prime }(v_{j}(n))}where \u03d5\u2032{\\displaystyle \\phi ^{\\prime }} is the derivative of the activation function described above, which itself does not vary. The analysis is more difficult for the change in weights to a hidden node, but it can be shown that the relevant derivative is\r\n\r\n\u2212\u2202E(n)\u2202vj(n)=\u03d5\u2032(vj(n))\u2211k\u2212\u2202E(n)\u2202vk(n)wkj(n){\\displaystyle -{\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}=\\phi ^{\\prime }(v_{j}(n))\\sum _{k}-{\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{k}(n)}}w_{kj}(n)}.This depends on the change in weights of the k{\\displaystyle k}th nodes, which represent the output layer. So to change the hidden layer weights, the output layer weights change according to the derivative of the activation function, and so this algorithm represents a backpropagation of the activation function.", "start_char_idx": 3698, "end_char_idx": 7191, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f7efb63a-737a-4718-bdcf-239637fd47ca": {"__data__": {"id_": "f7efb63a-737a-4718-bdcf-239637fd47ca", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Feedforward neural network.txt", "file_name": "Feedforward neural network.txt", "file_type": "text/plain", "file_size": 9793, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bceab45b-1acb-44b4-8433-ff78ba533ae9", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Feedforward neural network.txt", "file_name": "Feedforward neural network.txt", "file_type": "text/plain", "file_size": 9793, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "9ea03d7071b7f6b676c241f81cc02216f81ba99c552caf4b89c2fc9a4bc76e45", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e0e51bb9-edad-41f5-adf9-bf87c2553d41", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Feedforward neural network.txt", "file_name": "Feedforward neural network.txt", "file_type": "text/plain", "file_size": 9793, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "e76e72cb9a2b9e8502d9aca9e18ae64c6dd5013a5094a74c30eddcecc9c7e25a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2317dda9-0b15-4b00-b3e1-2408c656da63", "node_type": "1", "metadata": {}, "hash": "d36996f4ea6804815c594e3ed5dc61391b4850421af14ab1a06d7ec68b6bf238", "class_name": "RelatedNodeInfo"}}, "text": "The analysis is more difficult for the change in weights to a hidden node, but it can be shown that the relevant derivative is\r\n\r\n\u2212\u2202E(n)\u2202vj(n)=\u03d5\u2032(vj(n))\u2211k\u2212\u2202E(n)\u2202vk(n)wkj(n){\\displaystyle -{\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{j}(n)}}=\\phi ^{\\prime }(v_{j}(n))\\sum _{k}-{\\frac {\\partial {\\mathcal {E}}(n)}{\\partial v_{k}(n)}}w_{kj}(n)}.This depends on the change in weights of the k{\\displaystyle k}th nodes, which represent the output layer. So to change the hidden layer weights, the output layer weights change according to the derivative of the activation function, and so this algorithm represents a backpropagation of the activation function.\r\n\r\n\r\n== History ==\r\n\r\n\r\n=== Linear neural network ===\r\nThe simplest kind of feedforward neural network is a linear network, which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated in each node. The mean squared errors between these calculated outputs and a given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement.\r\n\r\n\r\n=== Perceptron ===\r\n\r\nIf using a threshold, i.e. a linear activation function,  the resulting linear threshold unit is called a perceptron. (Often the term is used to denote just one of these units.) Multiple parallel linear units are able to approximate any continuous function from a compact interval of the real numbers into the interval [\u22121,1] despite the limited computational power of single unit with a linear threshold function. This result can be found in Peter Auer, Harald Burgsteiner and Wolfgang Maass \"A learning rule for very simple universal approximators consisting of a single layer of perceptrons\".Perceptrons can be trained by a simple learning algorithm that is usually called the delta rule. It calculates the errors between calculated output and sample output data, and uses this to create an adjustment to the weights, thus implementing a form of gradient descent.\r\n\r\n\r\n=== Multilayer perceptron ===\r\nA multilayer perceptron (MLP) is a misnomer for a modern feedforward artificial neural network, consisting of fully connected neurons with a nonlinear kind of activation function, organized in at least three layers, notable for being able to distinguish data that is not linearly separable. It is a misnomer because the original perceptron used a Heaviside step function, instead of a nonlinear kind of activation function (used by modern networks).\r\n\r\n\r\n== Other feedforward networks ==\r\nExamples of other feedforward networks include convolutional neural networks and radial basis function networks, which use a different activation function.\r\n\r\n\r\n== See also ==\r\nHopfield network\r\nFeed-forward\r\nBackpropagation\r\nRprop\r\n\r\n\r\n== References ==\r\n\r\n\r\n== External links ==\r\nFeedforward neural networks tutorial\r\nFeedforward Neural Network: Example\r\nFeedforward Neural Networks: An Introduction", "start_char_idx": 6534, "end_char_idx": 9735, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2317dda9-0b15-4b00-b3e1-2408c656da63": {"__data__": {"id_": "2317dda9-0b15-4b00-b3e1-2408c656da63", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Gemini (language model).txt", "file_name": "Gemini (language model).txt", "file_type": "text/plain", "file_size": 10191, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "142e908e-5e0d-4a4f-b83b-48b2efd56aab", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Gemini (language model).txt", "file_name": "Gemini (language model).txt", "file_type": "text/plain", "file_size": 10191, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "a9d0ef342c9898053db15af61cfd484eff8231f28a15548ee4af2c8af71b6ca9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f7efb63a-737a-4718-bdcf-239637fd47ca", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Feedforward neural network.txt", "file_name": "Feedforward neural network.txt", "file_type": "text/plain", "file_size": 9793, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "b9b1b140c2b954ce747b9938a809075ccf72538eb9639353f104b713bb3a37f0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "01835aca-d678-457b-beea-cb21d4b03af1", "node_type": "1", "metadata": {}, "hash": "307894e3e8b1c1ba0fcc3b3fb5c0f15ed8258d30c662cd031ebcfbcf09dc2b83", "class_name": "RelatedNodeInfo"}}, "text": "Gemini is a family of multimodal large language models developed by Google DeepMind, serving as the successor to LaMDA and PaLM 2. Comprising Gemini Ultra, Gemini Pro, and Gemini Nano, it was announced on December 6, 2023, positioned as a competitor to OpenAI's GPT-4. It powers the generative artificial intelligence chatbot of the same name.\r\n\r\n\r\n== History ==\r\n\r\n\r\n=== Development ===\r\n\r\nGoogle announced Gemini, a large language model (LLM) developed by subsidiary Google DeepMind, during the Google I/O keynote on May 10, 2023. It was positioned as a more powerful successor to PaLM 2, which was also unveiled at the event, with Google CEO Sundar Pichai stating that Gemini was still in its early developmental stages. Unlike other LLMs, Gemini was said to be unique in that it was not trained on a text corpus alone and was designed to be multimodal, meaning it could process multiple types of data simultaneously, including text, images, audio, video, and computer code. It had been developed as a collaboration between DeepMind and Google Brain, two branches of Google that had been merged as Google DeepMind the previous month. In an interview with Wired, DeepMind CEO Demis Hassabis touted Gemini's advanced capabilities, which he believed would allow the algorithm to trump OpenAI's ChatGPT, which runs on GPT-4 and whose growing popularity had been aggressively challenged by Google with LaMDA and Bard. Hassabis highlighted the strengths of DeepMind's AlphaGo program, which gained worldwide attention in 2016 when it defeated Go champion Lee Sedol, saying that Gemini would combine the power of AlphaGo and other Google\u2013DeepMind LLMs.In August 2023, The Information published a report outlining Google's roadmap for Gemini, revealing that the company was targeting a launch date of late 2023. According to the report, Google hoped to surpass OpenAI and other competitors by combining conversational text capabilities present in most LLMs with artificial intelligence\u2013powered image generation, allowing it to create contextual images and be adapted for a wider range of use cases. Like Bard, Google co-founder Sergey Brin was summoned out of retirement to assist in the development of Gemini, along with hundreds of other engineers from Google Brain and DeepMind; he was later credited as a \"core contributor\" to Gemini. Because Gemini was being trained on transcripts of YouTube videos, lawyers were brought in to filter out any potentially copyrighted materials.With news of Gemini's impending launch, OpenAI hastened its work on integrating GPT-4 with multimodal features similar to those of Gemini. The Information reported in September that several companies had been granted early access to \"an early version\" of the LLM, which Google intended to make available to clients through Google Cloud's Vertex AI service. The publication also stated that Google was arming Gemini to compete with both GPT-4 and Microsoft's GitHub Copilot.\r\n\r\n\r\n=== Launch ===\r\nOn December 6, 2023, Pichai and Hassabis announced \"Gemini 1.0\" at a virtual press conference. It comprised three models: Gemini Ultra, designed for \"highly complex tasks\"; Gemini Pro, designed for \"a wide range of tasks\"; and Gemini Nano, designed for \"on-device tasks\". At launch, Gemini Pro and Nano were integrated into Bard and the Pixel 8 Pro smartphone, respectively, while Gemini Ultra was set to power \"Bard Advanced\" and become available to software developers in early 2024. Other products that Google intended to incorporate Gemini into included Search, Ads, Chrome, Duet AI on Google Workspace, and AlphaCode 2. It was made available only in English. Touted as Google's \"largest and most capable AI model\" and designed to emulate human behavior, the company stated that Gemini would not be made widely available until the following year due to the need for \"extensive safety testing\". Gemini was trained on and powered by Google's Tensor Processing Units (TPUs), and the name is in reference to the DeepMind\u2013Google Brain merger as well as NASA's Project Gemini.Gemini Ultra was said to have outperformed GPT-4, Anthropic's Claude 2, Inflection AI's Inflection-2, Meta's LLaMA 2, and xAI's Grok 1 on a variety of industry benchmarks, while Gemini Pro was said to have outperformed GPT-3.5. Gemini Ultra was also the first language model to outperform human experts on the 57-subject Massive Multitask Language Understanding (MMLU) test, obtaining a score of 90%.", "start_char_idx": 0, "end_char_idx": 4446, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "01835aca-d678-457b-beea-cb21d4b03af1": {"__data__": {"id_": "01835aca-d678-457b-beea-cb21d4b03af1", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Gemini (language model).txt", "file_name": "Gemini (language model).txt", "file_type": "text/plain", "file_size": 10191, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "142e908e-5e0d-4a4f-b83b-48b2efd56aab", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Gemini (language model).txt", "file_name": "Gemini (language model).txt", "file_type": "text/plain", "file_size": 10191, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "a9d0ef342c9898053db15af61cfd484eff8231f28a15548ee4af2c8af71b6ca9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2317dda9-0b15-4b00-b3e1-2408c656da63", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Gemini (language model).txt", "file_name": "Gemini (language model).txt", "file_type": "text/plain", "file_size": 10191, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "8d4ee8a94562194aa16ba99c98e5dbbe9c7e561af4175d81b141ccec3504837b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "91a97cc5-6178-48ba-b244-f3cbd0af1b40", "node_type": "1", "metadata": {}, "hash": "128aca6fbb428a12b4c3aa853d3a0026a39650fa40133b25d6358ff4bb9bf83c", "class_name": "RelatedNodeInfo"}}, "text": "Touted as Google's \"largest and most capable AI model\" and designed to emulate human behavior, the company stated that Gemini would not be made widely available until the following year due to the need for \"extensive safety testing\". Gemini was trained on and powered by Google's Tensor Processing Units (TPUs), and the name is in reference to the DeepMind\u2013Google Brain merger as well as NASA's Project Gemini.Gemini Ultra was said to have outperformed GPT-4, Anthropic's Claude 2, Inflection AI's Inflection-2, Meta's LLaMA 2, and xAI's Grok 1 on a variety of industry benchmarks, while Gemini Pro was said to have outperformed GPT-3.5. Gemini Ultra was also the first language model to outperform human experts on the 57-subject Massive Multitask Language Understanding (MMLU) test, obtaining a score of 90%. Gemini Pro was made available to Google Cloud customers on AI Studio and Vertex AI on December 13, while Gemini Nano will be made available to Android developers as well. Hassabis further revealed that DeepMind was exploring how Gemini could be \"combined with robotics to physically interact with the world\". In accordance with an executive order signed by U.S. President Joe Biden in October, Google stated that it would share testing results of Gemini Ultra with the federal government of the United States. Similarly, the company was engaged in discussions with the government of the United Kingdom to comply with the principles laid out at the AI Safety Summit at Bletchley Park in November.\r\n\r\n\r\n=== Updates ===\r\nGoogle partnered with Samsung to integrate Gemini Nano and Gemini Pro into its Galaxy S24 smartphone lineup in January 2024. The following month, Bard and Duet AI were unified under the Gemini brand, with \"Gemini Advanced with Ultra 1.0\" debuting via a new \"AI Premium\" tier of the Google One subscription service. Gemini Pro also received a global launch.In February, Google launched \"Gemini 1.5\" in a limited capacity, positioned as a more powerful and capable model than 1.0 Ultra. This \"step change\" was achieved through various technical advancements, including a new architecture, a mixture-of-experts approach, and a larger one-million-token context window, which equates to roughly an hour of silent video, 11 hours of audio, 30,000 lines of code, or 700,000 words. The same month, Google debuted Gemma, a family of free and open-source LLMs that serve as a lightweight version of Gemini. They come in two sizes, with a neural network with two and seven billion parameters, respectively. Multiple publications viewed this as an response to Meta and others open-sourcing their AI models, and a stark reversal from Google's longstanding practice of keeping its AI proprietary.\r\n\r\n\r\n== Technical specifications ==\r\nThe first generation of Gemini (\"Gemini 1\") has three models, with the same software architecture. They are decoder-only transformers, with modifications to allow efficient training and inference on TPUs. They have a context length of 32,768 tokens, with multi-query attention. Two versions of Gemini Nano, Nano-1 (1.8 billion parameters) and Nano-2 (3.25 billion parameters), are distilled from larger Gemini models, designed for use by edge devices such as smartphones. As Gemini is multimodal, each context window can contain multiple forms of input. The different modes can be interleaved and do not have to be presented in a fixed order, allowing for a multimodal conversation. For example, the user might open the conversation with a mix of text, picture, video, and audio, presented in any order, and Gemini might reply with the same free ordering.\r\nInput images may be of different resolutions, while video is inputted as a sequence of images. Audio is sampled at 16 kHz and then converted into a sequence of tokens by the Universal Speech Model. Gemini's dataset is multimodal and multilingual, consisting of \"web documents, books, and code, and includ[ing] image, audio, and video data\".Demis Hassabis claims that training Gemini 1 used \"roughly the same amount of compute, maybe slightly more than what was rumored for GPT-4\".The second generation of Gemini (\"Gemini 1.5\") has one model published so far: Gemini 1.5 Pro. It is a multimodal sparse mixture-of-experts, with context length of \"multiple millions\".\r\n\r\n\r\n== Reception ==\r\nGemini's launch was preluded by months of intense speculation and anticipation, which MIT Technology Review described as \"peak AI hype\".", "start_char_idx": 3636, "end_char_idx": 8065, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "91a97cc5-6178-48ba-b244-f3cbd0af1b40": {"__data__": {"id_": "91a97cc5-6178-48ba-b244-f3cbd0af1b40", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Gemini (language model).txt", "file_name": "Gemini (language model).txt", "file_type": "text/plain", "file_size": 10191, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "142e908e-5e0d-4a4f-b83b-48b2efd56aab", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Gemini (language model).txt", "file_name": "Gemini (language model).txt", "file_type": "text/plain", "file_size": 10191, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "a9d0ef342c9898053db15af61cfd484eff8231f28a15548ee4af2c8af71b6ca9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "01835aca-d678-457b-beea-cb21d4b03af1", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Gemini (language model).txt", "file_name": "Gemini (language model).txt", "file_type": "text/plain", "file_size": 10191, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "f3bc27263e27e5e0005e03a7af40d2f0ce3c0353ea29b6034a3cde8d7ede7a1d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8c57d7c4-866f-4231-90a7-588b76aef41c", "node_type": "1", "metadata": {}, "hash": "636fcb32005f814f092dc6f930ccc0c2d56d08432710498bb125f94b2fa58502", "class_name": "RelatedNodeInfo"}}, "text": "Input images may be of different resolutions, while video is inputted as a sequence of images. Audio is sampled at 16 kHz and then converted into a sequence of tokens by the Universal Speech Model. Gemini's dataset is multimodal and multilingual, consisting of \"web documents, books, and code, and includ[ing] image, audio, and video data\".Demis Hassabis claims that training Gemini 1 used \"roughly the same amount of compute, maybe slightly more than what was rumored for GPT-4\".The second generation of Gemini (\"Gemini 1.5\") has one model published so far: Gemini 1.5 Pro. It is a multimodal sparse mixture-of-experts, with context length of \"multiple millions\".\r\n\r\n\r\n== Reception ==\r\nGemini's launch was preluded by months of intense speculation and anticipation, which MIT Technology Review described as \"peak AI hype\". In August 2023, Dylan Patel and Daniel Nishball of research firm SemiAnalysis penned a blog post declaring that the release of Gemini would \"eat the world\" and outclass GPT-4, prompting OpenAI CEO Sam Altman to ridicule the duo on X (formerly Twitter). Business magnate Elon Musk, who co-founded OpenAI, weighed in, asking, \"Are the numbers wrong?\" Hugh Langley of Business Insider remarked that Gemini would be a make-or-break moment for Google, writing: \"If Gemini dazzles, it will help Google change the narrative that it was blindsided by Microsoft and OpenAI. If it disappoints, it will embolden critics who say Google has fallen behind.\"Reacting to its unveiling in December 2023, University of Washington professor emeritus Oren Etzioni predicted a \"tit-for-tat arms race\" between Google and OpenAI. Professor Alexei Efros of the University of California, Berkeley praised the potential of Gemini's multimodal approach, while scientist Melanie Mitchell of the Santa Fe Institute called Gemini \"very sophisticated\". Professor Chirag Shah of the University of Washington was less impressed, likening Gemini's launch to the routineness of Apple's annual introduction of a new iPhone. Similarly, Stanford University's Percy Liang, the University of Washington's Emily Bender, and the University of Galway's Michael Madden cautioned that it was difficult to interpret benchmark scores without insight into the training data used. Writing for Fast Company, Mark Sullivan opined that Google had the opportunity to challenge the iPhone's dominant market share, believing that Apple was unlikely to have the capacity to develop functionality similar to Gemini with its Siri virtual assistant. Google shares spiked by 5.3 percent the day after Gemini's launch.Google faced criticism for a demonstrative video of Gemini, which was not conducted in real time.\r\n\r\n\r\n== See also ==\r\nGato, a multimodal neural network developed by DeepMind\r\n\r\n\r\n== References ==\r\n\r\n\r\n== Further reading ==\r\n\r\n\r\n== External links ==\r\nOfficial website \r\nPress release via The Keyword\r\nAnnouncement and demo on YouTube\r\nWhite paper for 1.0 and 1.5", "start_char_idx": 7242, "end_char_idx": 10185, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8c57d7c4-866f-4231-90a7-588b76aef41c": {"__data__": {"id_": "8c57d7c4-866f-4231-90a7-588b76aef41c", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a1d4c52-08d8-4f72-a6e8-e70208ed030e", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "7ea73bd1f6dd068b36c7282f33047dcca146fdefe3836b129dcb8cb0da39fe3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "91a97cc5-6178-48ba-b244-f3cbd0af1b40", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Gemini (language model).txt", "file_name": "Gemini (language model).txt", "file_type": "text/plain", "file_size": 10191, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "5dc6eeebbf3de267d1b7caec19354836e598c3e4eff4a78fcc741486deb69b3c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0dd2c726-cf32-42d1-b131-dd77071d95f2", "node_type": "1", "metadata": {}, "hash": "db1496c851b9f187e8222b60772ddbc37aaef335ef7e7acc68d9642a787e880a", "class_name": "RelatedNodeInfo"}}, "text": "A generative adversarial network (GAN) is a class of machine learning frameworks and a prominent framework for approaching generative AI. The concept was initially developed by Ian Goodfellow and his colleagues in June 2014. In a GAN, two neural networks contest with each other in the form of a zero-sum game, where one agent's gain is another agent's loss.\r\nGiven a training set, this technique learns to generate new data with the same statistics as the training set. For example, a GAN trained on photographs can generate new photographs that look at least superficially authentic to human observers, having many realistic characteristics. Though originally proposed as a form of generative model for unsupervised learning, GANs have also proved useful for semi-supervised learning, fully supervised learning, and reinforcement learning.The core idea of a GAN is based on the \"indirect\" training through the discriminator, another neural network that can tell how \"realistic\" the input seems, which itself is also being updated dynamically. This means that the generator is not trained to minimize the distance to a specific image, but rather to fool the discriminator. This enables the model to learn in an unsupervised manner.\r\nGANs are similar to mimicry in evolutionary biology, with an evolutionary arms race between both networks.\r\n\r\n\r\n== Definition ==\r\n\r\n\r\n=== Mathematical ===\r\nThe original GAN is defined as the following game:\r\nEach probability space (\u03a9,\u03bcref){\\displaystyle (\\Omega ,\\mu _{\\text{ref}})} defines a GAN game.\r\nThere are 2 players: generator and discriminator.\r\nThe generator's strategy set is P(\u03a9){\\displaystyle {\\mathcal {P}}(\\Omega )}, the set of all probability measures \u03bcG{\\displaystyle \\mu _{G}} on \u03a9{\\displaystyle \\Omega }.\r\nThe discriminator's strategy set is the set of Markov kernels \u03bcD:\u03a9\u2192P[0,1]{\\displaystyle \\mu _{D}:\\Omega \\to {\\mathcal {P}}[0,1]}, where P[0,1]{\\displaystyle {\\mathcal {P}}[0,1]} is the set of probability measures on [0,1]{\\displaystyle [0,1]}.\r\nThe GAN game is a zero-sum game, with objective function\r\nThe generator aims to minimize the objective, and the discriminator aims to maximize the objective.\r\n\r\nThe generator's task is to approach \u03bcG\u2248\u03bcref{\\displaystyle \\mu _{G}\\approx \\mu _{\\text{ref}}}, that is, to match its own output distribution as closely as possible to the reference distribution. The discriminator's task is to output a value close to 1 when the input appears to be from the reference distribution, and to output a value close to 0 when the input looks like it came from the generator distribution.\r\n\r\n\r\n=== In practice ===\r\nThe generative network generates candidates while the discriminative network evaluates them. The contest operates in terms of data distributions. Typically, the generative network learns to map from a latent space to a data distribution of interest, while the discriminative network distinguishes candidates produced by the generator from the true data distribution. The generative network's training objective is to increase the error rate of the discriminative network (i.e., \"fool\" the discriminator network by producing novel candidates that the discriminator thinks are not synthesized (are part of the true data distribution)).A known dataset serves as the initial training data for the discriminator. Training involves presenting it with samples from the training dataset until it achieves acceptable accuracy. The generator is trained based on whether it succeeds in fooling the discriminator. Typically, the generator is seeded with randomized input that is sampled from a predefined latent space (e.g. a multivariate normal distribution). Thereafter, candidates synthesized by the generator are evaluated by the discriminator. Independent backpropagation procedures are applied to both networks so that the generator produces better samples, while the discriminator becomes more skilled at flagging synthetic samples. When used for image generation, the generator is typically a deconvolutional neural network, and the discriminator is a convolutional neural network.\r\n\r\n\r\n=== Relation to other statistical machine learning methods ===\r\nGANs are implicit generative models, which means that they do not explicitly model the likelihood function nor provide a means for finding the latent variable corresponding to a given sample, unlike alternatives such as flow-based generative model.\r\n\r\nCompared to fully visible belief networks such as WaveNet and PixelRNN and autoregressive models in general, GANs can generate one complete sample in one pass, rather than multiple passes through the network.", "start_char_idx": 0, "end_char_idx": 4611, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0dd2c726-cf32-42d1-b131-dd77071d95f2": {"__data__": {"id_": "0dd2c726-cf32-42d1-b131-dd77071d95f2", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a1d4c52-08d8-4f72-a6e8-e70208ed030e", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "7ea73bd1f6dd068b36c7282f33047dcca146fdefe3836b129dcb8cb0da39fe3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8c57d7c4-866f-4231-90a7-588b76aef41c", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "2e89b90fd83063ba9fa956e0746e01c862d3022d7e11a55475bdcc5b68a5fbcc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6fe21eeb-0445-446f-84c2-5f96339864fe", "node_type": "1", "metadata": {}, "hash": "d8e2f937d88b918fa6595217a519f1c37e6797c5f94834d6ef61cd88ad392ce1", "class_name": "RelatedNodeInfo"}}, "text": "a multivariate normal distribution). Thereafter, candidates synthesized by the generator are evaluated by the discriminator. Independent backpropagation procedures are applied to both networks so that the generator produces better samples, while the discriminator becomes more skilled at flagging synthetic samples. When used for image generation, the generator is typically a deconvolutional neural network, and the discriminator is a convolutional neural network.\r\n\r\n\r\n=== Relation to other statistical machine learning methods ===\r\nGANs are implicit generative models, which means that they do not explicitly model the likelihood function nor provide a means for finding the latent variable corresponding to a given sample, unlike alternatives such as flow-based generative model.\r\n\r\nCompared to fully visible belief networks such as WaveNet and PixelRNN and autoregressive models in general, GANs can generate one complete sample in one pass, rather than multiple passes through the network.\r\nCompared to Boltzmann machines and nonlinear ICA, there is no restriction on the type of function used by the network.\r\nSince neural networks are universal approximators, GANs are asymptotically consistent. Variational autoencoders might be universal approximators, but it is not proven as of 2017.\r\n\r\n\r\n== Mathematical properties ==\r\n\r\n\r\n=== Measure-theoretic considerations ===\r\nThis section provides some of the mathematical theory behind these methods.\r\n\r\nIn modern probability theory based on measure theory, a probability space also needs to be equipped with a \u03c3-algebra. As a result, a more rigorous definition of the GAN game would make the following changes:Each probability space (\u03a9,B,\u03bcref){\\displaystyle (\\Omega ,{\\mathcal {B}},\\mu _{\\text{ref}})} defines a GAN game.\r\nThe generator's strategy set is P(\u03a9,B){\\displaystyle {\\mathcal {P}}(\\Omega ,{\\mathcal {B}})}, the set of all probability measures \u03bcG{\\displaystyle \\mu _{G}} on the measure-space (\u03a9,B){\\displaystyle (\\Omega ,{\\mathcal {B}})}.\r\n\r\nThe discriminator's strategy set is the set of Markov kernels \u03bcD:(\u03a9,B)\u2192P([0,1],B([0,1])){\\displaystyle \\mu _{D}:(\\Omega ,{\\mathcal {B}})\\to {\\mathcal {P}}([0,1],{\\mathcal {B}}([0,1]))}, where B([0,1]){\\displaystyle {\\mathcal {B}}([0,1])} is the Borel \u03c3-algebra on [0,1]{\\displaystyle [0,1]}.Since issues of measurability never arise in practice, these will not concern us further.\r\n\r\n\r\n=== Choice of the strategy set ===\r\nIn the most generic version of the GAN game described above, the strategy set for the discriminator contains all Markov kernels \u03bcD:\u03a9\u2192P[0,1]{\\displaystyle \\mu _{D}:\\Omega \\to {\\mathcal {P}}[0,1]}, and the strategy set for the generator contains arbitrary probability distributions \u03bcG{\\displaystyle \\mu _{G}} on \u03a9{\\displaystyle \\Omega }.\r\nHowever, as shown below, the optimal discriminator strategy against any \u03bcG{\\displaystyle \\mu _{G}} is deterministic, so there is no loss of generality in restricting the discriminator's strategies to deterministic functions D:\u03a9\u2192[0,1]{\\displaystyle D:\\Omega \\to [0,1]}. In most applications, D{\\displaystyle D} is a deep neural network function.\r\nAs for the generator, while \u03bcG{\\displaystyle \\mu _{G}} could theoretically be any computable probability distribution, in practice, it is usually implemented as a pushforward: \u03bcG=\u03bcZ\u2218G\u22121{\\displaystyle \\mu _{G}=\\mu _{Z}\\circ G^{-1}}. That is, start with a random variable z\u223c\u03bcZ{\\displaystyle z\\sim \\mu _{Z}}, where \u03bcZ{\\displaystyle \\mu _{Z}} is a probability distribution that is easy to compute (such as the uniform distribution, or the Gaussian distribution), then define a function G:\u03a9Z\u2192\u03a9{\\displaystyle G:\\Omega _{Z}\\to \\Omega }. Then the distribution \u03bcG{\\displaystyle \\mu _{G}} is the distribution of G(z){\\displaystyle G(z)}.\r\nConsequently, the generator's strategy is usually defined as just G{\\displaystyle G}, leaving z\u223c\u03bcZ{\\displaystyle z\\sim \\mu _{Z}} implicit.", "start_char_idx": 3616, "end_char_idx": 7493, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6fe21eeb-0445-446f-84c2-5f96339864fe": {"__data__": {"id_": "6fe21eeb-0445-446f-84c2-5f96339864fe", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a1d4c52-08d8-4f72-a6e8-e70208ed030e", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "7ea73bd1f6dd068b36c7282f33047dcca146fdefe3836b129dcb8cb0da39fe3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0dd2c726-cf32-42d1-b131-dd77071d95f2", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "a3fd847fa66c40ce58411e83b64459c64ceb47106e579900ca63e6bda0bf6339", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6d5aa1f4-3297-4551-a9bc-bf8027bc90aa", "node_type": "1", "metadata": {}, "hash": "18073bae934679f714f054b57e6c64e46ee142d9b5940bc8b6e61d6b1e349401", "class_name": "RelatedNodeInfo"}}, "text": "That is, start with a random variable z\u223c\u03bcZ{\\displaystyle z\\sim \\mu _{Z}}, where \u03bcZ{\\displaystyle \\mu _{Z}} is a probability distribution that is easy to compute (such as the uniform distribution, or the Gaussian distribution), then define a function G:\u03a9Z\u2192\u03a9{\\displaystyle G:\\Omega _{Z}\\to \\Omega }. Then the distribution \u03bcG{\\displaystyle \\mu _{G}} is the distribution of G(z){\\displaystyle G(z)}.\r\nConsequently, the generator's strategy is usually defined as just G{\\displaystyle G}, leaving z\u223c\u03bcZ{\\displaystyle z\\sim \\mu _{Z}} implicit. In this formalism, the GAN game objective is\r\n\r\n\r\n=== Generative reparametrization ===\r\nThe GAN architecture has two main components. One is casting optimization into a game, of form minGmaxDL(G,D){\\displaystyle \\min _{G}\\max _{D}L(G,D)}, which is different from the usual kind of optimization, of form min\u03b8L(\u03b8){\\displaystyle \\min _{\\theta }L(\\theta )}. The other is the decomposition of \u03bcG{\\displaystyle \\mu _{G}} into \u03bcZ\u2218G\u22121{\\displaystyle \\mu _{Z}\\circ G^{-1}}, which can be understood as a reparametrization trick.\r\nTo see its significance, one must compare GAN with previous methods for learning generative models, which were plagued with \"intractable probabilistic computations that arise in maximum likelihood estimation and related strategies\".At the same time, Kingma and Welling and Rezende et al. developed the same idea of reparametrization into a general stochastic backpropagation method. Among its first applications was the variational autoencoder.\r\n\r\n\r\n=== Move order and strategic equilibria ===\r\nIn the original paper, as well as most subsequent papers, it is usually assumed that the generator moves first, and the discriminator moves second, thus giving the following minimax game:\r\nIf both the generator's and the discriminator's strategy sets are spanned by a finite number of strategies, then by the minimax theorem,that is, the move order does not matter.\r\nHowever, since the strategy sets are both not finitely spanned, the minimax theorem does not apply, and the idea of an \"equilibrium\" becomes delicate. To wit, there are the following different concepts of equilibrium:\r\n\r\nEquilibrium when generator moves first, and discriminator moves second:\r\nEquilibrium when discriminator moves first, and generator moves second:\r\nNash equilibrium (\u03bc^D,\u03bc^G){\\displaystyle ({\\hat {\\mu }}_{D},{\\hat {\\mu }}_{G})}, which is stable under simultaneous move order:For general games, these equilibria do not have to agree, or even to exist. For the original GAN game, these equilibria all exist, and are all equal. However, for more general GAN games, these do not necessarily exist, or agree.\r\n\r\n\r\n=== Main theorems for GAN game ===\r\nThe original GAN paper proved the following two theorems:\r\nInterpretation: For any fixed generator strategy \u03bcG{\\displaystyle \\mu _{G}}, the optimal discriminator keeps track of the likelihood ratio between the reference distribution and the generator distribution:where \u03c3{\\displaystyle \\sigma } is the logistic function.\r\nIn particular, if the prior probability for an image x{\\displaystyle x} to come from the reference distribution is equal to 12{\\displaystyle {\\frac {1}{2}}}, then D(x){\\displaystyle D(x)} is just the posterior probability that x{\\displaystyle x} came from the reference distribution:\r\n\r\n\r\n== Training and evaluating GAN ==\r\n\r\n\r\n=== Training ===\r\n\r\n\r\n==== Unstable convergence ====\r\nWhile the GAN game has a unique global equilibrium point when both the generator and discriminator have access to their entire strategy sets, the equilibrium is no longer guaranteed when they have a restricted strategy set.In practice, the generator has access only to measures of form \u03bcZ\u2218G\u03b8\u22121{\\displaystyle \\mu _{Z}\\circ G_{\\theta }^{-1}}, where G\u03b8{\\displaystyle G_{\\theta }} is a function computed by a neural network with parameters \u03b8{\\displaystyle \\theta }, and \u03bcZ{\\displaystyle \\mu _{Z}} is an easily sampled distribution, such as the uniform or normal distribution. Similarly, the discriminator has access only to functions of form D\u03b6{\\displaystyle D_{\\zeta }}, a function computed by a neural network with parameters \u03b6{\\displaystyle \\zeta }.", "start_char_idx": 6958, "end_char_idx": 11091, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6d5aa1f4-3297-4551-a9bc-bf8027bc90aa": {"__data__": {"id_": "6d5aa1f4-3297-4551-a9bc-bf8027bc90aa", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a1d4c52-08d8-4f72-a6e8-e70208ed030e", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "7ea73bd1f6dd068b36c7282f33047dcca146fdefe3836b129dcb8cb0da39fe3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6fe21eeb-0445-446f-84c2-5f96339864fe", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "61fabee77d8f693c2ef674b52ff289b03b8d0df36668e0ee33442a8939c6d052", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c66c9279-119e-4954-8e9e-a7393aa10742", "node_type": "1", "metadata": {}, "hash": "8bd4b27ed0aa0a99ad939308c70855bb7710d7d9d129677b9e2e2c438c9b4e3a", "class_name": "RelatedNodeInfo"}}, "text": "Similarly, the discriminator has access only to functions of form D\u03b6{\\displaystyle D_{\\zeta }}, a function computed by a neural network with parameters \u03b6{\\displaystyle \\zeta }. These restricted strategy sets take up a vanishingly small proportion of their entire strategy sets.Further, even if an equilibrium still exists, it can only be found by searching in the high-dimensional space of all possible neural network functions. The standard strategy of using gradient descent to find the equilibrium often does not work for GAN, and often the game \"collapses\" into one of several failure modes. To improve the convergence stability, some training strategies start with an easier task, such as generating low-resolution images or simple images (one object with uniform background), and gradually increase the difficulty of the task during training. This essentially translates to applying a curriculum learning scheme.\r\n\r\n\r\n==== Mode collapse ====\r\nGANs often suffer from mode collapse where they fail to generalize properly, missing entire modes from the input data. For example, a GAN trained on the MNIST dataset containing many samples of each digit might only generate pictures of digit 0. This was named in the first paper as the \"Helvetica scenario\".\r\nOne way this can happen is if the generator learns too fast compared to the discriminator. If the discriminator D{\\displaystyle D} is held constant, then the optimal generator would only output elements of arg\u2061maxxD(x){\\displaystyle \\arg \\max _{x}D(x)}. So for example, if during GAN training for generating MNIST dataset, for a few epochs, the discriminator somehow prefers the digit 0 slightly more than other digits, the generator may seize the opportunity to generate only digit 0, then be unable to escape the local minimum after the discriminator improves.\r\nSome researchers perceive the root problem to be a weak discriminative network that fails to notice the pattern of omission, while others assign blame to a bad choice of objective function. Many solutions have been proposed, but it is still an open problem.Even the state-of-the-art architecture, BigGAN (2019), could not avoid mode collapse. The authors resorted to \"allowing collapse to occur at the later stages of training, by which time a model is sufficiently trained to achieve good results\".\r\n\r\n\r\n==== Two time-scale update rule ====\r\nThe two time-scale update rule (TTUR) is proposed to make GAN convergence more stable by making the learning rate of the generator lower than that of the discriminator. The authors argued that the generator should move slower than the discriminator, so that it does not \"drive the discriminator steadily into new regions without capturing its gathered information\".\r\nThey proved that a general class of games that included the GAN game, when trained under TTUR, \"converges under mild assumptions to a stationary local Nash equilibrium\".They also proposed using the Adam stochastic optimization to avoid mode collapse, as well as the Fr\u00e9chet inception distance for evaluating GAN performances.\r\n\r\n\r\n==== Vanishing gradient ====\r\nConversely, if the discriminator learns too fast compared to the generator, then the discriminator could almost perfectly distinguish \u03bcG\u03b8,\u03bcref{\\displaystyle \\mu _{G_{\\theta }},\\mu _{\\text{ref}}}. In such case, the generator G\u03b8{\\displaystyle G_{\\theta }} could be stuck with a very high loss no matter which direction it changes its \u03b8{\\displaystyle \\theta }, meaning that the gradient \u2207\u03b8L(G\u03b8,D\u03b6){\\displaystyle \\nabla _{\\theta }L(G_{\\theta },D_{\\zeta })} would be close to zero. In such case, the generator cannot learn, a case of the vanishing gradient problem.Intuitively speaking, the discriminator is too good, and since the generator cannot take any small step (only small steps are considered in gradient descent) to improve its payoff, it does not even try.\r\nOne important method for solving this problem is the Wasserstein GAN.\r\n\r\n\r\n=== Evaluation ===\r\nGANs are usually evaluated by Inception score (IS), which measures how varied the generator's outputs are (as classified by an image classifier, usually Inception-v3), or Fr\u00e9chet inception distance (FID), which measures how similar the generator's outputs are to a reference set (as classified by a learned image featurizer, such as Inception-v3 without its final layer). Many papers that propose new GAN architectures for image generation report how their architectures break the state of the art on FID or IS.", "start_char_idx": 10915, "end_char_idx": 15379, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c66c9279-119e-4954-8e9e-a7393aa10742": {"__data__": {"id_": "c66c9279-119e-4954-8e9e-a7393aa10742", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a1d4c52-08d8-4f72-a6e8-e70208ed030e", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "7ea73bd1f6dd068b36c7282f33047dcca146fdefe3836b129dcb8cb0da39fe3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6d5aa1f4-3297-4551-a9bc-bf8027bc90aa", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "53c4b8c2d3e8e4bfe5db72021e4579b402d1ec5715a05af9e1790b6775e6026c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b8de6e8a-2d14-47b7-88c1-ef2b3b33fcb7", "node_type": "1", "metadata": {}, "hash": "e1fb09fae8754fc94aff4663671636d8ba467d6cdf462e3698b55cfd3c90397a", "class_name": "RelatedNodeInfo"}}, "text": "In such case, the generator cannot learn, a case of the vanishing gradient problem.Intuitively speaking, the discriminator is too good, and since the generator cannot take any small step (only small steps are considered in gradient descent) to improve its payoff, it does not even try.\r\nOne important method for solving this problem is the Wasserstein GAN.\r\n\r\n\r\n=== Evaluation ===\r\nGANs are usually evaluated by Inception score (IS), which measures how varied the generator's outputs are (as classified by an image classifier, usually Inception-v3), or Fr\u00e9chet inception distance (FID), which measures how similar the generator's outputs are to a reference set (as classified by a learned image featurizer, such as Inception-v3 without its final layer). Many papers that propose new GAN architectures for image generation report how their architectures break the state of the art on FID or IS.\r\nAnother evaluation method is the Learned Perceptual Image Patch Similarity (LPIPS), which starts with a learned image featurizer f\u03b8:Image\u2192Rn{\\displaystyle f_{\\theta }:{\\text{Image}}\\to \\mathbb {R} ^{n}}, and finetunes it by supervised learning on a set of (x,x\u2032,PerceptualDifference(x,x\u2032)){\\displaystyle (x,x',{\\text{PerceptualDifference}}(x,x'))}, where x{\\displaystyle x} is an image, x\u2032{\\displaystyle x'} is a perturbed version of it, and PerceptualDifference(x,x\u2032){\\displaystyle {\\text{PerceptualDifference}}(x,x')} is how much they differ, as reported by human subjects. The model is finetuned so that it can approximate \u2016f\u03b8(x)\u2212f\u03b8(x\u2032)\u2016\u2248PerceptualDifference(x,x\u2032){\\displaystyle \\|f_{\\theta }(x)-f_{\\theta }(x')\\|\\approx {\\text{PerceptualDifference}}(x,x')}. This finetuned model is then used to define LPIPS(x,x\u2032):=\u2016f\u03b8(x)\u2212f\u03b8(x\u2032)\u2016{\\displaystyle {\\text{LPIPS}}(x,x'):=\\|f_{\\theta }(x)-f_{\\theta }(x')\\|}.Other evaluation methods are reviewed in.\r\n\r\n\r\n== Variants ==\r\nThere is a veritable zoo of GAN variants. Some of the most prominent are as follows:\r\n\r\n\r\n=== Conditional GAN ===\r\nConditional GANs are similar to standard GANs except they allow the model to conditionally generate samples based on additional information. For example, if we want to generate a cat face given a dog picture, we could use a conditional GAN.\r\nThe generator in a GAN game generates \u03bcG{\\displaystyle \\mu _{G}}, a probability distribution on the probability space \u03a9{\\displaystyle \\Omega }. This leads to the idea of a conditional GAN, where instead of generating one probability distribution on \u03a9{\\displaystyle \\Omega }, the generator generates a different probability distribution \u03bcG(c){\\displaystyle \\mu _{G}(c)} on \u03a9{\\displaystyle \\Omega }, for each given class label c{\\displaystyle c}.\r\nFor example, for generating images that look like ImageNet, the generator should be able to generate a picture of cat when given the class label \"cat\".\r\nIn the original paper, the authors noted that GAN can be trivially extended to conditional GAN by providing the labels to both the generator and the discriminator.\r\nConcretely, the conditional GAN game is just the GAN game with class labels provided:where \u03bcC{\\displaystyle \\mu _{C}} is a probability distribution over classes, \u03bcref(c){\\displaystyle \\mu _{\\text{ref}}(c)} is the probability distribution of real images of class c{\\displaystyle c}, and \u03bcG(c){\\displaystyle \\mu _{G}(c)} the probability distribution of images generated by the generator when given class label c{\\displaystyle c}.\r\nIn 2017, a conditional GAN learned to generate 1000 image classes of ImageNet.\r\n\r\n\r\n=== GANs with alternative architectures ===\r\nThe GAN game is a general framework and can be run with any reasonable parametrization of the generator G{\\displaystyle G} and discriminator D{\\displaystyle D}. In the original paper, the authors demonstrated it using multilayer perceptron networks and convolutional neural networks. Many alternative architectures have been tried.", "start_char_idx": 14486, "end_char_idx": 18360, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b8de6e8a-2d14-47b7-88c1-ef2b3b33fcb7": {"__data__": {"id_": "b8de6e8a-2d14-47b7-88c1-ef2b3b33fcb7", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a1d4c52-08d8-4f72-a6e8-e70208ed030e", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "7ea73bd1f6dd068b36c7282f33047dcca146fdefe3836b129dcb8cb0da39fe3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c66c9279-119e-4954-8e9e-a7393aa10742", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "38b303d28990ee49fc964e88b7fe0dd6deebb39e961a5a32227ac3dd5955b3ab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "db7d8046-1f87-46b3-8af9-a52581308b68", "node_type": "1", "metadata": {}, "hash": "eaf8036a09ab215907d2e57e618f0d7b4e93c45e9449b4f1cd1356e23afde1dd", "class_name": "RelatedNodeInfo"}}, "text": "In 2017, a conditional GAN learned to generate 1000 image classes of ImageNet.\r\n\r\n\r\n=== GANs with alternative architectures ===\r\nThe GAN game is a general framework and can be run with any reasonable parametrization of the generator G{\\displaystyle G} and discriminator D{\\displaystyle D}. In the original paper, the authors demonstrated it using multilayer perceptron networks and convolutional neural networks. Many alternative architectures have been tried.\r\nDeep convolutional GAN (DCGAN): For both generator and discriminator, uses only deep networks consisting entirely of convolution-deconvolution layers, that is, fully convolutional networks.Self-attention GAN (SAGAN): Starts with the DCGAN, then adds residually-connected standard self-attention modules to the generator and discriminator.\r\nVariational autoencoder GAN (VAEGAN): Uses a variational autoencoder (VAE) for the generator.\r\nTransformer GAN (TransGAN): Uses the pure transformer architecture for both the generator and discriminator, entirely devoid of convolution-deconvolution layers.\r\nFlow-GAN: Uses flow-based generative model for the generator, allowing efficient computation of the likelihood function.\r\n\r\n\r\n=== GANs with alternative objectives ===\r\nMany GAN variants are merely obtained by changing the loss functions for the generator and discriminator.\r\nOriginal GAN:\r\nWe recast the original GAN objective into a form more convenient for comparison:\r\nOriginal GAN, non-saturating loss:\r\nThis objective for generator was recommended in the original paper for faster convergence.The effect of using this objective is analyzed in Section 2.2.2 of Arjovsky et al.Original GAN, maximum likelihood:\r\nwhere \u03c3{\\displaystyle \\sigma } is the logistic function. When the discriminator is optimal, the generator gradient is the same as in maximum likelihood estimation, even though GAN cannot perform maximum likelihood estimation itself.Hinge loss GAN:Least squares GAN:where a,b,c{\\displaystyle a,b,c} are parameters to be chosen. The authors recommended a=\u22121,b=1,c=0{\\displaystyle a=-1,b=1,c=0}.\r\n\r\n\r\n=== Wasserstein GAN (WGAN) ===\r\n\r\nThe Wasserstein GAN modifies the GAN game at two points:\r\n\r\nThe discriminator's strategy set is the set of measurable functions of type D:\u03a9\u2192R{\\displaystyle D:\\Omega \\to \\mathbb {R} } with bounded Lipschitz norm: \u2016D\u2016L\u2264K{\\displaystyle \\|D\\|_{L}\\leq K}, where K{\\displaystyle K} is a fixed positive constant.\r\nThe objective isOne of its purposes is to solve the problem of mode collapse (see above). The authors claim \"In no experiment did we see evidence of mode collapse for the WGAN algorithm\".\r\n\r\n\r\n=== GANs with more than 2 players ===\r\n\r\n\r\n==== Adversarial autoencoder ====\r\nAn adversarial autoencoder (AAE) is more autoencoder than GAN. The idea is to start with a plain autoencoder, but train a discriminator to discriminate the latent vectors from a reference distribution (often the normal distribution).\r\n\r\n\r\n==== InfoGAN ====\r\nIn conditional GAN, the generator receives both a noise vector z{\\displaystyle z} and a label c{\\displaystyle c}, and produces an image G(z,c){\\displaystyle G(z,c)}. The discriminator receives image-label pairs (x,c){\\displaystyle (x,c)}, and computes D(x,c){\\displaystyle D(x,c)}.\r\nWhen the training dataset is unlabeled, conditional GAN does not work directly.\r\nThe idea of InfoGAN is to decree that every latent vector in the latent space can be decomposed as (z,c){\\displaystyle (z,c)}: an incompressible noise part z{\\displaystyle z}, and an informative label part c{\\displaystyle c}, and encourage the generator to comply with the decree, by encouraging it to maximize I(c,G(z,c)){\\displaystyle I(c,G(z,c))}, the mutual information between c{\\displaystyle c} and G(z,c){\\displaystyle G(z,c)}, while making no demands on the mutual information z{\\displaystyle z} between G(z,c){\\displaystyle G(z,c)}.\r\nUnfortunately, I(c,G(z,c)){\\displaystyle I(c,G(z,c))} is intractable in general, The key idea of InfoGAN is Variational Mutual Information Maximization: indirectly maximize it by maximizing a lower boundwhere Q{\\displaystyle Q} ranges over all Markov kernels of type Q:\u03a9Y\u2192P(\u03a9C){\\displaystyle Q:\\Omega _{Y}\\to {\\mathcal {P}}(\\Omega _{C})}.", "start_char_idx": 17900, "end_char_idx": 22088, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "db7d8046-1f87-46b3-8af9-a52581308b68": {"__data__": {"id_": "db7d8046-1f87-46b3-8af9-a52581308b68", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a1d4c52-08d8-4f72-a6e8-e70208ed030e", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "7ea73bd1f6dd068b36c7282f33047dcca146fdefe3836b129dcb8cb0da39fe3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b8de6e8a-2d14-47b7-88c1-ef2b3b33fcb7", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "4de7f28ea826c9f2ea3bd18b933efe547b14c8881f2c359b3b179c3eb596078f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c5d9510f-9b4e-4aac-a106-be3d6cfc6158", "node_type": "1", "metadata": {}, "hash": "bc05886ff7a7c1fce74250fc26bbf9acadd397a33861f51e531636aff773def2", "class_name": "RelatedNodeInfo"}}, "text": "Unfortunately, I(c,G(z,c)){\\displaystyle I(c,G(z,c))} is intractable in general, The key idea of InfoGAN is Variational Mutual Information Maximization: indirectly maximize it by maximizing a lower boundwhere Q{\\displaystyle Q} ranges over all Markov kernels of type Q:\u03a9Y\u2192P(\u03a9C){\\displaystyle Q:\\Omega _{Y}\\to {\\mathcal {P}}(\\Omega _{C})}.\r\n\r\nThe InfoGAN game is defined as follows:Three probability spaces define an InfoGAN game:\r\n(\u03a9X,\u03bcref){\\displaystyle (\\Omega _{X},\\mu _{\\text{ref}})}, the space of reference images.\r\n(\u03a9Z,\u03bcZ){\\displaystyle (\\Omega _{Z},\\mu _{Z})}, the fixed random noise generator.\r\n(\u03a9C,\u03bcC){\\displaystyle (\\Omega _{C},\\mu _{C})}, the fixed random information generator.There are 3 players in 2 teams: generator, Q, and discriminator. The generator and Q are on one team, and the discriminator on the other team.\r\nThe objective function iswhere LGAN(G,D)=Ex\u223c\u03bcref,[ln\u2061D(x)]+Ez\u223c\u03bcZ[ln\u2061(1\u2212D(G(z,c)))]{\\displaystyle L_{GAN}(G,D)=\\mathbb {E} _{x\\sim \\mu _{\\text{ref}},}[\\ln D(x)]+\\mathbb {E} _{z\\sim \\mu _{Z}}[\\ln(1-D(G(z,c)))]} is the original GAN game objective, and I^(G,Q)=Ez\u223c\u03bcZ,c\u223c\u03bcC[ln\u2061Q(c|G(z,c))]{\\displaystyle {\\hat {I}}(G,Q)=\\mathbb {E} _{z\\sim \\mu _{Z},c\\sim \\mu _{C}}[\\ln Q(c|G(z,c))]}\r\n\r\nGenerator-Q team aims to minimize the objective, and discriminator aims to maximize it:\r\n\r\n\r\n==== Bidirectional GAN (BiGAN) ====\r\nThe standard GAN generator is a function of type G:\u03a9Z\u2192\u03a9X{\\displaystyle G:\\Omega _{Z}\\to \\Omega _{X}}, that is, it is a mapping from a latent space \u03a9Z{\\displaystyle \\Omega _{Z}} to the image space \u03a9X{\\displaystyle \\Omega _{X}}. This can be understood as a \"decoding\" process, whereby every latent vector z\u2208\u03a9Z{\\displaystyle z\\in \\Omega _{Z}} is a code for an image x\u2208\u03a9X{\\displaystyle x\\in \\Omega _{X}}, and the generator performs the decoding. This naturally leads to the idea of training another network that performs \"encoding\", creating an autoencoder out of the encoder-generator pair.\r\nAlready in the original paper, the authors noted that \"Learned approximate inference can be performed by training an auxiliary network to predict z{\\displaystyle z} given x{\\displaystyle x}\". The bidirectional GAN architecture performs exactly this.\r\nThe BiGAN is defined as follows: Two probability spaces define a BiGAN game:\r\n(\u03a9X,\u03bcX){\\displaystyle (\\Omega _{X},\\mu _{X})}, the space of reference images.\r\n(\u03a9Z,\u03bcZ){\\displaystyle (\\Omega _{Z},\\mu _{Z})}, the latent space.There are 3 players in 2 teams: generator, encoder, and discriminator. The generator and encoder are on one team, and the discriminator on the other team.\r\nThe generator's strategies are functions G:\u03a9Z\u2192\u03a9X{\\displaystyle G:\\Omega _{Z}\\to \\Omega _{X}}, and the encoder's strategies are functions E:\u03a9X\u2192\u03a9Z{\\displaystyle E:\\Omega _{X}\\to \\Omega _{Z}}. The discriminator's strategies are functions D:\u03a9X\u2192[0,1]{\\displaystyle D:\\Omega _{X}\\to [0,1]}.", "start_char_idx": 21750, "end_char_idx": 24594, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c5d9510f-9b4e-4aac-a106-be3d6cfc6158": {"__data__": {"id_": "c5d9510f-9b4e-4aac-a106-be3d6cfc6158", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a1d4c52-08d8-4f72-a6e8-e70208ed030e", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "7ea73bd1f6dd068b36c7282f33047dcca146fdefe3836b129dcb8cb0da39fe3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "db7d8046-1f87-46b3-8af9-a52581308b68", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "15f4debe45fd2228cc48b0239feae968efe9d96f9e89599f85238acab6e73ceb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dcc1f2fc-613c-4f2a-988a-835f97b475c2", "node_type": "1", "metadata": {}, "hash": "00fb1b2bf1896b933234c0e6d93507ab60eadfa781679ea18d60a79c1d74f218", "class_name": "RelatedNodeInfo"}}, "text": "(\u03a9Z,\u03bcZ){\\displaystyle (\\Omega _{Z},\\mu _{Z})}, the latent space.There are 3 players in 2 teams: generator, encoder, and discriminator. The generator and encoder are on one team, and the discriminator on the other team.\r\nThe generator's strategies are functions G:\u03a9Z\u2192\u03a9X{\\displaystyle G:\\Omega _{Z}\\to \\Omega _{X}}, and the encoder's strategies are functions E:\u03a9X\u2192\u03a9Z{\\displaystyle E:\\Omega _{X}\\to \\Omega _{Z}}. The discriminator's strategies are functions D:\u03a9X\u2192[0,1]{\\displaystyle D:\\Omega _{X}\\to [0,1]}.\r\nThe objective function is\r\n\r\nGenerator-encoder team aims to minimize the objective, and discriminator aims to maximize it: In the paper, they gave a more abstract definition of the objective as:where \u03bcE,X(dx,dz)=\u03bcX(dx)\u22c5\u03b4E(x)(dz){\\displaystyle \\mu _{E,X}(dx,dz)=\\mu _{X}(dx)\\cdot \\delta _{E(x)}(dz)} is the probability distribution on \u03a9X\u00d7\u03a9Z{\\displaystyle \\Omega _{X}\\times \\Omega _{Z}} obtained by pushing \u03bcX{\\displaystyle \\mu _{X}} forward via x\u21a6(x,E(x)){\\displaystyle x\\mapsto (x,E(x))}, and \u03bcG,Z(dx,dz)=\u03b4G(z)(dx)\u22c5\u03bcZ(dz){\\displaystyle \\mu _{G,Z}(dx,dz)=\\delta _{G(z)}(dx)\\cdot \\mu _{Z}(dz)} is the probability distribution on \u03a9X\u00d7\u03a9Z{\\displaystyle \\Omega _{X}\\times \\Omega _{Z}} obtained by pushing \u03bcZ{\\displaystyle \\mu _{Z}} forward via z\u21a6(G(x),z){\\displaystyle z\\mapsto (G(x),z)}.\r\nApplications of bidirectional models include semi-supervised learning, interpretable machine learning, and neural machine translation.\r\n\r\n\r\n==== CycleGAN ====\r\nCycleGAN is an architecture for performing translations between two domains, such as between photos of horses and photos of zebras, or photos of night cities and photos of day cities.\r\n\r\nThe CycleGAN game is defined as follows:There are two probability spaces (\u03a9X,\u03bcX),(\u03a9Y,\u03bcY){\\displaystyle (\\Omega _{X},\\mu _{X}),(\\Omega _{Y},\\mu _{Y})}, corresponding to the two domains needed for translations fore-and-back.\r\nThere are 4 players in 2 teams: generators GX:\u03a9X\u2192\u03a9Y,GY:\u03a9Y\u2192\u03a9X{\\displaystyle G_{X}:\\Omega _{X}\\to \\Omega _{Y},G_{Y}:\\Omega _{Y}\\to \\Omega _{X}}, and discriminators DX:\u03a9X\u2192[0,1],DY:\u03a9Y\u2192[0,1]{\\displaystyle D_{X}:\\Omega _{X}\\to [0,1],D_{Y}:\\Omega _{Y}\\to [0,1]}.\r\nThe objective function is\r\n\r\nwhere \u03bb{\\displaystyle \\lambda } is a positive adjustable parameter, LGAN{\\displaystyle L_{GAN}} is the GAN game objective, and Lcycle{\\displaystyle L_{cycle}} is the cycle consistency loss:The generators aim to minimize the objective, and the discriminators aim to maximize it:  Unlike previous work like pix2pix, which requires paired training data, cycleGAN requires no paired data. For example, to train a pix2pix model to turn a summer scenery photo to winter scenery photo and back, the dataset must contain pairs of the same place in summer and winter, shot at the same angle; cycleGAN would only need a set of summer scenery photos, and an unrelated set of winter scenery photos.\r\n\r\n\r\n=== GANs with particularly large or small scales ===\r\n\r\n\r\n==== BigGAN ====\r\nThe BigGAN is essentially a self-attention GAN trained on a large scale (up to 80 million parameters) to generate large images of ImageNet (up to 512 x 512 resolution), with numerous engineering tricks to make it converge.\r\n\r\n\r\n==== Invertible data augmentation ====\r\nWhen there is insufficient training data, the reference distribution \u03bcref{\\displaystyle \\mu _{\\text{ref}}} cannot be well-approximated by the empirical distribution given by the training dataset.", "start_char_idx": 24090, "end_char_idx": 27467, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dcc1f2fc-613c-4f2a-988a-835f97b475c2": {"__data__": {"id_": "dcc1f2fc-613c-4f2a-988a-835f97b475c2", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a1d4c52-08d8-4f72-a6e8-e70208ed030e", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "7ea73bd1f6dd068b36c7282f33047dcca146fdefe3836b129dcb8cb0da39fe3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c5d9510f-9b4e-4aac-a106-be3d6cfc6158", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "b5411a2985b853a0a53a5ff703b005ac54a3313f7204e4c9216929d9a04ef2bc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "38e3d038-36b1-48ed-b79b-21271ac6517e", "node_type": "1", "metadata": {}, "hash": "29f05b1676d4622fde963a530df4edce8d6ed765460104b36d957e586da9d69d", "class_name": "RelatedNodeInfo"}}, "text": "For example, to train a pix2pix model to turn a summer scenery photo to winter scenery photo and back, the dataset must contain pairs of the same place in summer and winter, shot at the same angle; cycleGAN would only need a set of summer scenery photos, and an unrelated set of winter scenery photos.\r\n\r\n\r\n=== GANs with particularly large or small scales ===\r\n\r\n\r\n==== BigGAN ====\r\nThe BigGAN is essentially a self-attention GAN trained on a large scale (up to 80 million parameters) to generate large images of ImageNet (up to 512 x 512 resolution), with numerous engineering tricks to make it converge.\r\n\r\n\r\n==== Invertible data augmentation ====\r\nWhen there is insufficient training data, the reference distribution \u03bcref{\\displaystyle \\mu _{\\text{ref}}} cannot be well-approximated by the empirical distribution given by the training dataset. In such cases, data augmentation can be applied, to allow training GAN on smaller datasets. Na\u00efve data augmentation, however, brings its problems.\r\nConsider the original GAN game, slightly reformulated as follows:Now we use data augmentation by randomly sampling semantic-preserving transforms T:\u03a9\u2192\u03a9{\\displaystyle T:\\Omega \\to \\Omega } and applying them to the dataset, to obtain the reformulated GAN game:This is equivalent to a GAN game with a different distribution \u03bcref\u2032{\\displaystyle \\mu _{\\text{ref}}'}, sampled by T(x){\\displaystyle T(x)}, with x\u223c\u03bcref,T\u223c\u03bctrans{\\displaystyle x\\sim \\mu _{\\text{ref}},T\\sim \\mu _{trans}}. For example, if \u03bcref{\\displaystyle \\mu _{\\text{ref}}} is the distribution of images in ImageNet, and \u03bctrans{\\displaystyle \\mu _{trans}} samples identity-transform with probability 0.5, and horizontal-reflection with probability 0.5, then \u03bcref\u2032{\\displaystyle \\mu _{\\text{ref}}'} is the distribution of images in ImageNet and horizontally-reflected ImageNet, combined.\r\nThe result of such training would be a generator that mimics \u03bcref\u2032{\\displaystyle \\mu _{\\text{ref}}'}. For example, it would generate images that look like they are randomly cropped, if the data augmentation uses random cropping.\r\nThe solution is to apply data augmentation to both generated and real images:The authors demonstrated high-quality generation using just 100-picture-large datasets.The StyleGAN-2-ADA paper points out a further point on data augmentation: it must be invertible. Continue with the example of generating ImageNet pictures. If the data augmentation is \"randomly rotate the picture by 0, 90, 180, 270 degrees with equal probability\", then there is no way for the generator to know which is the true orientation: Consider two generators G,G\u2032{\\displaystyle G,G'}, such that for any latent z{\\displaystyle z}, the generated image G(z){\\displaystyle G(z)} is a 90-degree rotation of G\u2032(z){\\displaystyle G'(z)}. They would have exactly the same expected loss, and so neither is preferred over the other.\r\nThe solution is to only use invertible data augmentation: instead of \"randomly rotate the picture by 0, 90, 180, 270 degrees with equal probability\", use \"randomly rotate the picture by 90, 180, 270 degrees with 0.1 probability, and keep the picture as it is with 0.7 probability\". This way, the generator is still rewarded  to keep images oriented the same way as un-augmented ImageNet pictures.\r\nAbstractly, the effect of randomly sampling transformations T:\u03a9\u2192\u03a9{\\displaystyle T:\\Omega \\to \\Omega } from the distribution \u03bctrans{\\displaystyle \\mu _{trans}} is to define a Markov kernel Ktrans:\u03a9\u2192P(\u03a9){\\displaystyle K_{trans}:\\Omega \\to {\\mathcal {P}}(\\Omega )}. Then, the data-augmented GAN game pushes the generator to find some \u03bc^G\u2208P(\u03a9){\\displaystyle {\\hat {\\mu }}_{G}\\in {\\mathcal {P}}(\\Omega )}, such that where \u2217{\\displaystyle *} is the Markov kernel convolution.\r\nA data-augmentation method is defined to be invertible if its Markov kernel Ktrans{\\displaystyle K_{trans}} satisfiesImmediately by definition, we see that composing multiple invertible data-augmentation methods results in yet another invertible method.", "start_char_idx": 26621, "end_char_idx": 30610, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "38e3d038-36b1-48ed-b79b-21271ac6517e": {"__data__": {"id_": "38e3d038-36b1-48ed-b79b-21271ac6517e", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a1d4c52-08d8-4f72-a6e8-e70208ed030e", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "7ea73bd1f6dd068b36c7282f33047dcca146fdefe3836b129dcb8cb0da39fe3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "dcc1f2fc-613c-4f2a-988a-835f97b475c2", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "c9d48d265dad99cb9456e09768325608d5a050b026a4c4bb82e41cd9dca9cd1c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da474919-403b-488e-8482-f2b19bba8b33", "node_type": "1", "metadata": {}, "hash": "6474c9f3d36b0aede867835761a99c27209df6864976eea060229d5e9810dc10", "class_name": "RelatedNodeInfo"}}, "text": "Abstractly, the effect of randomly sampling transformations T:\u03a9\u2192\u03a9{\\displaystyle T:\\Omega \\to \\Omega } from the distribution \u03bctrans{\\displaystyle \\mu _{trans}} is to define a Markov kernel Ktrans:\u03a9\u2192P(\u03a9){\\displaystyle K_{trans}:\\Omega \\to {\\mathcal {P}}(\\Omega )}. Then, the data-augmented GAN game pushes the generator to find some \u03bc^G\u2208P(\u03a9){\\displaystyle {\\hat {\\mu }}_{G}\\in {\\mathcal {P}}(\\Omega )}, such that where \u2217{\\displaystyle *} is the Markov kernel convolution.\r\nA data-augmentation method is defined to be invertible if its Markov kernel Ktrans{\\displaystyle K_{trans}} satisfiesImmediately by definition, we see that composing multiple invertible data-augmentation methods results in yet another invertible method. Also by definition, if the data-augmentation method is invertible, then using it in a GAN game does not change the optimal strategy \u03bc^G{\\displaystyle {\\hat {\\mu }}_{G}} for the generator, which is still \u03bcref{\\displaystyle \\mu _{\\text{ref}}}.\r\nThere are two prototypical examples of invertible Markov kernels:\r\nDiscrete case: Invertible stochastic matrices, when \u03a9{\\displaystyle \\Omega } is finite.\r\nFor example, if \u03a9={\u2191,\u2193,\u2190,\u2192}{\\displaystyle \\Omega =\\{\\uparrow ,\\downarrow ,\\leftarrow ,\\rightarrow \\}} is the set of four images of an arrow, pointing in 4 directions, and the data augmentation is \"randomly rotate the picture by 90, 180, 270 degrees with probability p{\\displaystyle p}, and keep the picture as it is with probability (1\u22123p){\\displaystyle (1-3p)}\", then the Markov kernel Ktrans{\\displaystyle K_{trans}} can be represented as a stochastic matrix: and Ktrans{\\displaystyle K_{trans}} is an invertible kernel iff [Ktrans]{\\displaystyle [K_{trans}]} is an invertible matrix, that is, p\u22601/4{\\displaystyle p\\neq 1/4}.\r\nContinuous case: The gaussian kernel, when \u03a9=Rn{\\displaystyle \\Omega =\\mathbb {R} ^{n}} for some n\u22651{\\displaystyle n\\geq 1}.\r\nFor example, if \u03a9=R2562{\\displaystyle \\Omega =\\mathbb {R} ^{256^{2}}} is the space of 256x256 images, and the data-augmentation method is \"generate a gaussian noise z\u223cN(0,I2562){\\displaystyle z\\sim {\\mathcal {N}}(0,I_{256^{2}})}, then add \u03f5z{\\displaystyle \\epsilon z} to the image\", then Ktrans{\\displaystyle K_{trans}} is just convolution by the density function of N(0,\u03f52I2562){\\displaystyle {\\mathcal {N}}(0,\\epsilon ^{2}I_{256^{2}})}. This is invertible, because convolution by a gaussian is just convolution by the heat kernel, so given any \u03bc\u2208P(Rn){\\displaystyle \\mu \\in {\\mathcal {P}}(\\mathbb {R} ^{n})}, the convolved distribution Ktrans\u2217\u03bc{\\displaystyle K_{trans}*\\mu } can be obtained by heating up Rn{\\displaystyle \\mathbb {R} ^{n}} precisely according to \u03bc{\\displaystyle \\mu }, then wait for time \u03f52/4{\\displaystyle \\epsilon ^{2}/4}. With that, we can recover \u03bc{\\displaystyle \\mu } by running the heat equation backwards in time for \u03f52/4{\\displaystyle \\epsilon ^{2}/4}.\r\nMore examples of invertible data augmentations are found in the paper.\r\n\r\n\r\n==== SinGAN ====\r\nSinGAN pushes data augmentation to the limit, by using only a single image as training data and performing data augmentation on it. The GAN architecture is adapted to this training method by using a multi-scale pipeline.", "start_char_idx": 29886, "end_char_idx": 33060, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "da474919-403b-488e-8482-f2b19bba8b33": {"__data__": {"id_": "da474919-403b-488e-8482-f2b19bba8b33", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a1d4c52-08d8-4f72-a6e8-e70208ed030e", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "7ea73bd1f6dd068b36c7282f33047dcca146fdefe3836b129dcb8cb0da39fe3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "38e3d038-36b1-48ed-b79b-21271ac6517e", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "685213bb66d4c5e999b70b5fc58329c4788de3271155fa2b26e187f51af3d353", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7655f3a0-21e5-466e-a93a-fb5ed4791b0c", "node_type": "1", "metadata": {}, "hash": "5c3e17809f2a1ad55b3bc48725d46052fda3bd035b64fab153b4d0624834aaff", "class_name": "RelatedNodeInfo"}}, "text": "With that, we can recover \u03bc{\\displaystyle \\mu } by running the heat equation backwards in time for \u03f52/4{\\displaystyle \\epsilon ^{2}/4}.\r\nMore examples of invertible data augmentations are found in the paper.\r\n\r\n\r\n==== SinGAN ====\r\nSinGAN pushes data augmentation to the limit, by using only a single image as training data and performing data augmentation on it. The GAN architecture is adapted to this training method by using a multi-scale pipeline.\r\nThe generator G{\\displaystyle G} is decomposed into a pyramid of generators G=G1\u2218G2\u2218\u22ef\u2218GN{\\displaystyle G=G_{1}\\circ G_{2}\\circ \\cdots \\circ G_{N}}, with the lowest one generating the image GN(zN){\\displaystyle G_{N}(z_{N})} at the lowest resolution, then the generated image is scaled up to r(GN(zN)){\\displaystyle r(G_{N}(z_{N}))}, and fed to the next level to generate an image GN\u22121(zN\u22121+r(GN(zN))){\\displaystyle G_{N-1}(z_{N-1}+r(G_{N}(z_{N})))} at a higher resolution, and so on. The discriminator is decomposed into a pyramid as well.\r\n\r\n\r\n=== StyleGAN series ===\r\n\r\nThe StyleGAN family is a series of architectures published by Nvidia's research division.\r\n\r\n\r\n==== Progressive GAN ====\r\nProgressive GAN is a method for training GAN for large-scale image generation stably, by growing a GAN generator from small to large scale in a pyramidal fashion. Like SinGAN, it decomposes the generator asG=G1\u2218G2\u2218\u22ef\u2218GN{\\displaystyle G=G_{1}\\circ G_{2}\\circ \\cdots \\circ G_{N}}, and the discriminator as D=D1\u2218D2\u2218\u22ef\u2218DN{\\displaystyle D=D_{1}\\circ D_{2}\\circ \\cdots \\circ D_{N}}.\r\nDuring training, at first only GN,DN{\\displaystyle G_{N},D_{N}} are used in a GAN game to generate 4x4 images. Then GN\u22121,DN\u22121{\\displaystyle G_{N-1},D_{N-1}} are added to reach the second stage of GAN game, to generate 8x8 images, and so on, until we reach a GAN game to generate 1024x1024 images.\r\nTo avoid shock between stages of the GAN game, each new layer is \"blended in\" (Figure 2 of the paper). For example, this is how the second stage GAN game starts:\r\n\r\nJust before, the GAN game consists of the pair GN,DN{\\displaystyle G_{N},D_{N}} generating and discriminating 4x4 images.\r\nJust after, the GAN game consists of the pair ((1\u2212\u03b1)+\u03b1\u22c5GN\u22121)\u2218u\u2218GN,DN\u2218d\u2218((1\u2212\u03b1)+\u03b1\u22c5DN\u22121){\\displaystyle ((1-\\alpha )+\\alpha \\cdot G_{N-1})\\circ u\\circ G_{N},D_{N}\\circ d\\circ ((1-\\alpha )+\\alpha \\cdot D_{N-1})} generating and discriminating 8x8 images. Here, the functions u,d{\\displaystyle u,d} are image up- and down-sampling functions, and \u03b1{\\displaystyle \\alpha } is a blend-in factor (much like an alpha in image composing) that smoothly glides from 0 to 1.\r\n\r\n\r\n==== StyleGAN-1 ====\r\nStyleGAN-1 is designed as a combination of Progressive GAN with neural style transfer.The key architectural choice of StyleGAN-1 is a progressive growth mechanism, similar to Progressive GAN. Each generated image starts as a constant 4\u00d74\u00d7512{\\displaystyle 4\\times 4\\times 512} array, and repeatedly passed through style blocks. Each style block applies a \"style latent vector\" via affine transform (\"adaptive instance normalization\"), similar to how neural style transfer uses Gramian matrix. It then adds noise, and normalize (subtract the mean, then divide by the variance).\r\nAt training time, usually only one style latent vector is used per image generated, but sometimes two (\"mixing regularization\") in order to encourage each style block to independently perform its stylization without expecting help from other style blocks (since they might receive an entirely different style latent vector).", "start_char_idx": 32609, "end_char_idx": 36106, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7655f3a0-21e5-466e-a93a-fb5ed4791b0c": {"__data__": {"id_": "7655f3a0-21e5-466e-a93a-fb5ed4791b0c", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a1d4c52-08d8-4f72-a6e8-e70208ed030e", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "7ea73bd1f6dd068b36c7282f33047dcca146fdefe3836b129dcb8cb0da39fe3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "da474919-403b-488e-8482-f2b19bba8b33", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "1f4487a565b4e2c6cab044eb254d41cc582d4413040adbc38f7613d9ccdb6aca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "704b76fb-1c25-4c2b-9a52-9b54acddd768", "node_type": "1", "metadata": {}, "hash": "991fac47d3b031501fe1282d3d0bb327b6d717baa14a3eb2cb309a7b79e37906", "class_name": "RelatedNodeInfo"}}, "text": "==== StyleGAN-1 ====\r\nStyleGAN-1 is designed as a combination of Progressive GAN with neural style transfer.The key architectural choice of StyleGAN-1 is a progressive growth mechanism, similar to Progressive GAN. Each generated image starts as a constant 4\u00d74\u00d7512{\\displaystyle 4\\times 4\\times 512} array, and repeatedly passed through style blocks. Each style block applies a \"style latent vector\" via affine transform (\"adaptive instance normalization\"), similar to how neural style transfer uses Gramian matrix. It then adds noise, and normalize (subtract the mean, then divide by the variance).\r\nAt training time, usually only one style latent vector is used per image generated, but sometimes two (\"mixing regularization\") in order to encourage each style block to independently perform its stylization without expecting help from other style blocks (since they might receive an entirely different style latent vector).\r\nAfter training, multiple style latent vectors can be fed into each style block. Those fed to the lower layers control the large-scale styles, and those fed to the higher layers control the fine-detail styles.\r\nStyle-mixing between two images x,x\u2032{\\displaystyle x,x'} can be performed as well. First, run a gradient descent to find z,z\u2032{\\displaystyle z,z'} such that G(z)\u2248x,G(z\u2032)\u2248x\u2032{\\displaystyle G(z)\\approx x,G(z')\\approx x'}. This is called \"projecting an image back to style latent space\". Then, z{\\displaystyle z} can be fed to the lower style blocks, and z\u2032{\\displaystyle z'} to the higher style blocks, to generate a composite image that has the large-scale style of x{\\displaystyle x}, and the fine-detail style of x\u2032{\\displaystyle x'}. Multiple images can also be composed this way.\r\n\r\n\r\n==== StyleGAN-2 ====\r\nStyleGAN-2 improves upon StyleGAN-1, by using the style latent vector to transform the convolution layer's weights instead, thus solving the \"blob\" problem.This was updated by the StyleGAN-2-ADA (\"ADA\" stands for \"adaptive\"), which uses invertible data augmentation as described above. It also tunes the amount of data augmentation applied by starting at zero, and gradually increasing it until an \"overfitting heuristic\" reaches a target level, thus the name \"adaptive\".\r\n\r\n\r\n==== StyleGAN-3 ====\r\nStyleGAN-3 improves upon StyleGAN-2 by solving the \"texture sticking\" problem, which can be seen in the official videos. They analyzed the problem by the Nyquist\u2013Shannon sampling theorem, and argued that the layers in the generator learned to exploit the high-frequency signal in the pixels they operate upon.\r\nTo solve this, they proposed imposing strict lowpass filters between each generator's layers, so that the generator is forced to operate on the pixels in a way faithful to the continuous signals they represent, rather than operate on them as merely discrete signals. They further imposed rotational and translational invariance by using more signal filters. The resulting StyleGAN-3 is able to solve the texture sticking problem, as well as generating images that rotate and translate smoothly.\r\n\r\n\r\n== Applications ==\r\nGAN applications have increased rapidly.\r\n\r\n\r\n=== Fashion, art and advertising ===\r\nGANs can be used to generate art; The Verge wrote in March 2019 that \"The images created by GANs have become the defining look of contemporary AI art.\" GANs can also be used to inpaint photographs or create photos of imaginary fashion models, with no need to hire a model, photographer or makeup artist, or pay for a studio and transportation. GANs have also been used for virtual shadow generation.\r\n\r\n\r\n=== Interactive Media ===\r\nIn 2020, Artbreeder was used to create the main antagonist in the sequel to the psychological web horror series Ben Drowned. The author would later go on to praise GAN applications for their ability to help generate assets for independent artists who are short on budget and manpower.\r\n\r\n\r\n=== Science ===\r\nGANs can improve astronomical images and simulate gravitational lensing for dark matter research. They were used in 2019 to successfully model the distribution of dark matter in a particular direction in space and to predict the gravitational lensing that will occur.GANs have been proposed as a fast and accurate way of modeling high energy jet formation and modeling showers through calorimeters of high-energy physics experiments. GANs have also been trained to accurately approximate bottlenecks in computationally expensive simulations of particle physics experiments. Applications in the context of present and proposed CERN experiments have demonstrated the potential of these methods for accelerating simulation and/or improving simulation fidelity.", "start_char_idx": 35182, "end_char_idx": 39836, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "704b76fb-1c25-4c2b-9a52-9b54acddd768": {"__data__": {"id_": "704b76fb-1c25-4c2b-9a52-9b54acddd768", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a1d4c52-08d8-4f72-a6e8-e70208ed030e", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "7ea73bd1f6dd068b36c7282f33047dcca146fdefe3836b129dcb8cb0da39fe3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7655f3a0-21e5-466e-a93a-fb5ed4791b0c", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "ca9e8c254f6a964e6f27e56a79a0c510045f790a86807148eb72024da56dabac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "03bbdcab-f1ff-43e5-9ae6-70d056230b19", "node_type": "1", "metadata": {}, "hash": "b38fad118594c8c2da2ded693b8ea9ae6ccf99f545b3fd53418c4e3ac5d71fc0", "class_name": "RelatedNodeInfo"}}, "text": "=== Interactive Media ===\r\nIn 2020, Artbreeder was used to create the main antagonist in the sequel to the psychological web horror series Ben Drowned. The author would later go on to praise GAN applications for their ability to help generate assets for independent artists who are short on budget and manpower.\r\n\r\n\r\n=== Science ===\r\nGANs can improve astronomical images and simulate gravitational lensing for dark matter research. They were used in 2019 to successfully model the distribution of dark matter in a particular direction in space and to predict the gravitational lensing that will occur.GANs have been proposed as a fast and accurate way of modeling high energy jet formation and modeling showers through calorimeters of high-energy physics experiments. GANs have also been trained to accurately approximate bottlenecks in computationally expensive simulations of particle physics experiments. Applications in the context of present and proposed CERN experiments have demonstrated the potential of these methods for accelerating simulation and/or improving simulation fidelity.\r\n\r\n\r\n=== Video games ===\r\nIn 2018, GANs reached the video game modding community, as a method of up-scaling low-resolution 2D textures in old video games by recreating them in 4k or higher resolutions via image training, and then down-sampling them to fit the game's native resolution (with results resembling the supersampling method of anti-aliasing). With proper training, GANs provide a clearer and sharper 2D texture image magnitudes higher in quality than the original, while fully retaining the original's level of details, colors, etc. Known examples of extensive GAN usage include Final Fantasy VIII, Final Fantasy IX, Resident Evil REmake HD Remaster, and Max Payne.\r\n\r\n\r\n=== AI-generated video ===\r\nArtificial intelligence art for video uses AI to generate video from text as Text-to-Video model\r\n\r\n\r\n=== Audio synthesis ===\r\n\r\n\r\n=== Concerns about malicious applications ===\r\n\r\nConcerns have been raised about the potential use of GAN-based human image synthesis for sinister purposes, e.g., to produce fake, possibly incriminating, photographs and videos.\r\nGANs can be used to generate unique, realistic profile photos of people who do not exist, in order to automate creation of fake social media profiles.In 2019 the state of California considered and passed on October 3, 2019, the bill AB-602, which bans the use of human image synthesis technologies to make fake pornography without the consent of the people depicted, and bill AB-730, which prohibits distribution of manipulated videos of a political candidate within 60 days of an election. Both bills were authored by Assembly member Marc Berman and signed by Governor Gavin Newsom. The laws went into effect in 2020.DARPA's Media Forensics program studies ways to counteract fake media, including fake media produced using GANs.\r\n\r\n\r\n=== Transfer learning ===\r\nState-of-art transfer learning research use GANs to enforce the alignment of the latent feature space, such as in deep reinforcement learning. This works by feeding the embeddings of the source and target task to the discriminator which tries to guess the context. The resulting loss is then (inversely) backpropagated through the encoder.\r\n\r\n\r\n=== Miscellaneous applications ===\r\nGAN can be used to detect glaucomatous images helping the early diagnosis which is essential to avoid partial or total loss\r\nof vision.GANs that produce photorealistic images can be used to visualize interior design, industrial design, shoes, bags, and clothing items or items for computer games' scenes. Such networks were reported to be used by Facebook.GANs have been used to create forensic facial reconstructions of deceased historical figures.GANs can reconstruct 3D models of objects from images, generate novel objects as 3D point clouds, and model patterns of motion in video.GANs can be used to age face photographs to show how an individual's appearance might change with age.GANs can also be used to inpaint missing features in maps, transfer map styles in cartography or augment street view imagery.Relevance feedback on GANs can be used to generate images and replace image search systems.A variation of the GANs is used in training a network to generate optimal control inputs to nonlinear dynamical systems. Where the discriminatory network is known as a critic that checks the optimality of the solution and the generative network is known as an Adaptive network that generates the optimal control.", "start_char_idx": 38745, "end_char_idx": 43266, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "03bbdcab-f1ff-43e5-9ae6-70d056230b19": {"__data__": {"id_": "03bbdcab-f1ff-43e5-9ae6-70d056230b19", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a1d4c52-08d8-4f72-a6e8-e70208ed030e", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "7ea73bd1f6dd068b36c7282f33047dcca146fdefe3836b129dcb8cb0da39fe3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "704b76fb-1c25-4c2b-9a52-9b54acddd768", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "aad935c2b6d5a32fdbb2ff10b072bafc54a0b89d083d29ac28f6641e10a5c9d7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cf76431f-351a-4322-b7bb-c47aeee0a607", "node_type": "1", "metadata": {}, "hash": "678568a8e689aafc18eee6744dc339eb144ecf19cc9660b76bfe75f4b245bcfa", "class_name": "RelatedNodeInfo"}}, "text": "Such networks were reported to be used by Facebook.GANs have been used to create forensic facial reconstructions of deceased historical figures.GANs can reconstruct 3D models of objects from images, generate novel objects as 3D point clouds, and model patterns of motion in video.GANs can be used to age face photographs to show how an individual's appearance might change with age.GANs can also be used to inpaint missing features in maps, transfer map styles in cartography or augment street view imagery.Relevance feedback on GANs can be used to generate images and replace image search systems.A variation of the GANs is used in training a network to generate optimal control inputs to nonlinear dynamical systems. Where the discriminatory network is known as a critic that checks the optimality of the solution and the generative network is known as an Adaptive network that generates the optimal control. The critic and adaptive network train each other to approximate a nonlinear optimal control.GANs have been used to visualize the effect that climate change will have on specific houses.A GAN model called Speech2Face can reconstruct an image of a person's face after listening to their voice.In 2016 GANs were used to generate new molecules for a variety of protein targets implicated in cancer, inflammation, and fibrosis. In 2019 GAN-generated molecules were validated experimentally all the way into mice.Whereas the majority of GAN applications are in image processing, the work has also been done with time-series data. For example, recurrent GANs (R-GANs) have been used to generate energy data for machine learning.\r\n\r\n\r\n== History ==\r\nIn 1991, Juergen Schmidhuber published generative and adversarial neural networks that contest with each other in the form of a zero-sum game, where one network's gain is the other network's loss. The first network is a generative model with stochasticity that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called \"artificial curiosity.\" For modern GANs (2014), the environmental reaction is 1 or 0 depending on whether the first network's output is in a given set.Other people had similar ideas but did not develop them similarly. An idea involving adversarial networks was published in a 2010 blog post by Olli Niemitalo. This idea was never implemented and did not involve stochasticity in the generator and thus was not a generative model. It is now known as a conditional GAN or cGAN. An idea similar to GANs was used to model animal behavior by Li, Gauci and Gross in 2013.Another inspiration for GANs was noise-contrastive estimation, which uses the same loss function as GANs and which Goodfellow studied during his PhD in 2010\u20132014.\r\nAdversarial machine learning has other uses besides generative modeling and can be applied to models other than neural networks. In control theory, adversarial learning based on neural networks was used in 2006 to train robust controllers in a game theoretic sense, by alternating the iterations between a minimizer policy, the controller, and a maximizer policy, the disturbance.In 2017, a GAN was used for image enhancement focusing on realistic textures rather than pixel-accuracy, producing a higher image quality at high magnification. In 2017, the first faces were generated. These were exhibited in February 2018 at the Grand Palais. Faces generated by StyleGAN in 2019 drew comparisons with Deepfakes.Beginning in 2017, GAN technology began to make its presence felt in the fine arts arena with the appearance of a newly developed implementation which was said to have crossed the threshold of being able to generate unique and appealing abstract paintings, and thus dubbed a \"CAN\", for \"creative adversarial network\". A GAN system was used to create the 2018 painting Edmond de Belamy, which sold for US$432,500.", "start_char_idx": 42356, "end_char_idx": 46301, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf76431f-351a-4322-b7bb-c47aeee0a607": {"__data__": {"id_": "cf76431f-351a-4322-b7bb-c47aeee0a607", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1a1d4c52-08d8-4f72-a6e8-e70208ed030e", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "7ea73bd1f6dd068b36c7282f33047dcca146fdefe3836b129dcb8cb0da39fe3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "03bbdcab-f1ff-43e5-9ae6-70d056230b19", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "e4443b1e1ccf6f02e1ef7ca80c7651f95fb3beec8301ca3057f71f6ce9efc48e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "33e7eb09-0bd0-4c61-b41c-5b97383623fd", "node_type": "1", "metadata": {}, "hash": "b135b646409cc7dda92ac5a12524d8ce985aa7b1fad63dbabd188ace354ad8f6", "class_name": "RelatedNodeInfo"}}, "text": "In 2017, the first faces were generated. These were exhibited in February 2018 at the Grand Palais. Faces generated by StyleGAN in 2019 drew comparisons with Deepfakes.Beginning in 2017, GAN technology began to make its presence felt in the fine arts arena with the appearance of a newly developed implementation which was said to have crossed the threshold of being able to generate unique and appealing abstract paintings, and thus dubbed a \"CAN\", for \"creative adversarial network\". A GAN system was used to create the 2018 painting Edmond de Belamy, which sold for US$432,500. An early 2019 article by members of the original CAN team discussed further progress with that system, and gave consideration as well to the overall prospects for an AI-enabled art.In May 2019, researchers at Samsung demonstrated a GAN-based system that produces videos of a person speaking, given only a single photo of that person.In August 2019, a large dataset consisting of 12,197 MIDI songs each with paired lyrics and melody alignment was created for neural melody generation from lyrics using conditional GAN-LSTM (refer to sources at GitHub AI Melody Generation from Lyrics).In May 2020, Nvidia researchers taught an AI system (termed \"GameGAN\") to recreate the game of Pac-Man simply by watching it being played.\r\n\r\n\r\n== References ==\r\n\r\n\r\n== External links ==\r\nKnight, Will. \"5 Big Predictions for Artificial Intelligence in 2017\". MIT Technology Review. Retrieved January 5, 2017.\r\nKarras, Tero; Laine, Samuli; Aila, Timo (2018). \"A Style-Based Generator Architecture for Generative Adversarial Networks\". arXiv:1812.04948 [cs.NE].\r\nThis Person Does Not Exist \u2013  photorealistic images of people who do not exist, generated by StyleGAN\r\nThis Cat Does Not Exist Archived March 5, 2019, at the Wayback Machine \u2013  photorealistic images of cats who do not exist, generated by StyleGAN\r\nWang, Zhengwei; She, Qi; Ward, Tomas E. (2019). \"Generative Adversarial Networks in Computer Vision: A Survey and Taxonomy\". arXiv:1906.01529 [cs.LG].", "start_char_idx": 45721, "end_char_idx": 47745, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "33e7eb09-0bd0-4c61-b41c-5b97383623fd": {"__data__": {"id_": "33e7eb09-0bd0-4c61-b41c-5b97383623fd", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative pre-trained transformer.txt", "file_name": "Generative pre-trained transformer.txt", "file_type": "text/plain", "file_size": 14085, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2a159bcb-23ad-4890-bd3e-8c468e4d113d", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative pre-trained transformer.txt", "file_name": "Generative pre-trained transformer.txt", "file_type": "text/plain", "file_size": 14085, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "d53802146e3714daade4a189ecb3aa282ab4ff70e6b786b4e6680218e2fac290", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cf76431f-351a-4322-b7bb-c47aeee0a607", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "e43b8fb36ce622a84b441ce2322c4cef902810267792919511326c99587f3888", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7fa895fa-fe1d-426b-b8b6-2816ad3e0fbd", "node_type": "1", "metadata": {}, "hash": "6f450be688c7ba5a8183caf30e212d406adf68692f22e6c4002a996b7db0c151", "class_name": "RelatedNodeInfo"}}, "text": "Generative pre-trained transformers (GPT) are a type of large language model (LLM) and a prominent framework for generative artificial intelligence. They are artificial neural networks that are used in natural language processing tasks. GPTs are based on the transformer architecture, pre-trained on large data sets of unlabelled text, and able to generate novel human-like content. As of 2023, most LLMs have these characteristics and are sometimes referred to broadly as GPTs.The first GPT was introduced in 2018 by OpenAI. OpenAI has released very influential GPT foundation models that have been sequentially numbered, to comprise its \"GPT-n\" series. Each of these was significantly more capable than the previous, due to increased size (number of trainable parameters) and training. The most recent of these, GPT-4, was released in March 2023. Such models have been the basis for their more task-specific GPT systems, including models fine-tuned for instruction following\u2014which in turn power the ChatGPT chatbot service.The term \"GPT\" is also used in the names and descriptions of such models developed by others. For example, other GPT foundation models include a series of models created by EleutherAI, and seven models created by Cerebras in 2023. Also, companies in different industries have developed task-specific GPTs in their respective fields, such as Salesforce's \"EinsteinGPT\" (for CRM) and Bloomberg's \"BloombergGPT\" (for finance).\r\n\r\n\r\n== History ==\r\n\r\n\r\n=== Initial developments ===\r\nGenerative pretraining (GP) was a long-established concept in machine learning applications. It was originally used as a form of semi-supervised learning, as the model is trained first on an unlabelled dataset (pretraining step) by learning to generate datapoints in the dataset, and then it is trained to classify a labelled dataset.While the unnormalized linear transformer dates back to 1992, the modern transformer architecture was not available until 2017 when it was published by researchers at Google in a paper \"Attention Is All You Need\". That development led to the emergence of large language models such as BERT in 2018 which was a pre-trained transformer (PT) but not designed to be generative (BERT was an \"encoder-only\" model). Also around that time, in 2018, OpenAI published its article entitled \"Improving Language Understanding by Generative Pre-Training,\" in which it introduced the first generative pre-trained transformer (GPT) system (\"GPT-1\").Prior to transformer-based architectures, the best-performing neural NLP (natural language processing) models commonly employed supervised learning from large amounts of manually-labeled data. The reliance on supervised learning limited their use on datasets that were not well-annotated, and also made it prohibitively expensive and time-consuming to train extremely large language models.The semi-supervised approach OpenAI employed to make a large-scale generative system\u2014and was first to do with a transformer model\u2014involved two stages: an unsupervised generative \"pretraining\" stage to set initial parameters using a language modeling objective, and a supervised discriminative \"fine-tuning\" stage to adapt these parameters to a target task.\r\n\r\n\r\n=== Later developments ===\r\nRegarding more recent GPT foundation models, OpenAI published its first versions of GPT-3 in July 2020. There were three models, with 1B, 6.7B, 175B parameters, respectively named babbage, curie, and davinci (giving initials B, C, and D).In July 2021, OpenAI published Codex, a task-specific GPT model targeted for programming applications. This was developed by fine-tuning a 12B parameter version of GPT-3 (different from previous GPT-3 models) using code from GitHub.In March 2022, OpenAI published two versions of GPT-3 that were fine-tuned for instruction-following (instruction-tuned), named davinci-instruct-beta (175B) and text-davinci-001, and then started beta testing code-davinci-002. text-davinci-002 was instruction-tuned from code-davinci-002. Both text-davinci-003 and ChatGPT were released in November 2022, with both building upon text-davinci-002 via reinforcement learning from human feedback (RLHF).", "start_char_idx": 0, "end_char_idx": 4170, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7fa895fa-fe1d-426b-b8b6-2816ad3e0fbd": {"__data__": {"id_": "7fa895fa-fe1d-426b-b8b6-2816ad3e0fbd", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative pre-trained transformer.txt", "file_name": "Generative pre-trained transformer.txt", "file_type": "text/plain", "file_size": 14085, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2a159bcb-23ad-4890-bd3e-8c468e4d113d", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative pre-trained transformer.txt", "file_name": "Generative pre-trained transformer.txt", "file_type": "text/plain", "file_size": 14085, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "d53802146e3714daade4a189ecb3aa282ab4ff70e6b786b4e6680218e2fac290", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "33e7eb09-0bd0-4c61-b41c-5b97383623fd", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative pre-trained transformer.txt", "file_name": "Generative pre-trained transformer.txt", "file_type": "text/plain", "file_size": 14085, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "b7c80868ef6fcec06aadb878e15026dadfb570f2ad117b3eb1d6ede77b4f0c6f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9341a911-be4f-49e3-abfc-6c0ea4b86498", "node_type": "1", "metadata": {}, "hash": "3e298844138ecec3e3293f6168d0f55624eb0716913d7fc63c76ec54f5e1a021", "class_name": "RelatedNodeInfo"}}, "text": "This was developed by fine-tuning a 12B parameter version of GPT-3 (different from previous GPT-3 models) using code from GitHub.In March 2022, OpenAI published two versions of GPT-3 that were fine-tuned for instruction-following (instruction-tuned), named davinci-instruct-beta (175B) and text-davinci-001, and then started beta testing code-davinci-002. text-davinci-002 was instruction-tuned from code-davinci-002. Both text-davinci-003 and ChatGPT were released in November 2022, with both building upon text-davinci-002 via reinforcement learning from human feedback (RLHF). text-davinci-003 is trained for following instructions (like its predecessors), whereas ChatGPT is further trained for conversational interaction with a human user.OpenAI's most recent GPT foundation model, GPT-4, was released on March 14, 2023. It can be accessed directly by users via a premium version of ChatGPT, and is available to developers for incorporation into other products and services via OpenAI's API. Other producers of GPT foundation models include EleutherAI (with a series of models starting in March 2021) and Cerebras (with seven models released in March 2023).\r\n\r\n\r\n== Foundational models ==\r\nA foundational model is an AI model trained on broad data at scale such that it can be adapted to a wide range of downstream tasks.Thus far, the most notable GPT foundation models have been from OpenAI's GPT-n series. The most recent from that is GPT-4, for which OpenAI declined to publish the size or training details (citing \"the competitive landscape and the safety implications of large-scale models\").\r\nOther such models include Google's PaLM, a broad foundation model that has been compared to GPT-3 and has recently been made available to developers via an API, and Together's GPT-JT, which has been reported as the closest-performing open-source alternative to GPT-3 (and is derived from earlier open-source GPTs). Meta AI (formerly Facebook) also has a generative transformer-based foundational large language model, known as LLaMA.Foundational GPTs can also employ modalities other than text, for input and/or output. GPT-4 is a multi-modal LLM that is capable of processing text and image input (though its output is limited to text). Regarding multimodal output, some generative transformer-based models are used for text-to-image technologies such as diffusion and parallel decoding. Such kinds of models can serve as visual foundation models (VFMs) for developing downstream systems that can work with images.\r\n\r\n\r\n== Task-specific models ==\r\nA foundational GPT model can be further adapted to produce more targeted systems directed to specific tasks and/or subject-matter domains. Methods for such adaptation can include additional fine-tuning (beyond that done for the foundation model) as well as certain forms of prompt engineering.An important example of this is fine-tuning models to follow instructions, which is of course a fairly broad task but more targeted than a foundation model. In January 2022, OpenAI introduced \"InstructGPT\"\u2014a series of models which were fine-tuned to follow instructions using a combination of supervised training and reinforcement learning from human feedback (RLHF) on base GPT-3 language models. Advantages this had over the bare foundational models included higher accuracy, less negative/toxic sentiment, and generally better alignment with user needs. Hence, OpenAI began using this as the basis for its API service offerings. Other instruction-tuned models have been released by others, including a fully open version.Another (related) kind of task-specific models are chatbots, which engage in human-like conversation. In November 2022, OpenAI launched ChatGPT\u2014an online chat interface powered by an instruction-tuned language model trained in a similar fashion to InstructGPT. They trained this model using RLHF, with human AI trainers providing conversations in which they played both the user and the AI, and mixed this new dialogue dataset with the InstructGPT dataset for a conversational format suitable for a chatbot.", "start_char_idx": 3591, "end_char_idx": 7668, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9341a911-be4f-49e3-abfc-6c0ea4b86498": {"__data__": {"id_": "9341a911-be4f-49e3-abfc-6c0ea4b86498", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative pre-trained transformer.txt", "file_name": "Generative pre-trained transformer.txt", "file_type": "text/plain", "file_size": 14085, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2a159bcb-23ad-4890-bd3e-8c468e4d113d", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative pre-trained transformer.txt", "file_name": "Generative pre-trained transformer.txt", "file_type": "text/plain", "file_size": 14085, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "d53802146e3714daade4a189ecb3aa282ab4ff70e6b786b4e6680218e2fac290", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7fa895fa-fe1d-426b-b8b6-2816ad3e0fbd", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative pre-trained transformer.txt", "file_name": "Generative pre-trained transformer.txt", "file_type": "text/plain", "file_size": 14085, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "419cc65ed57ff6bddaef88dbde10a911ac06fa23af876d1f427f2fce97bc27d0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a310d23-81e9-4909-a0be-c9ef7a85d856", "node_type": "1", "metadata": {}, "hash": "6ac3cd24452c23740e08154f1ff6b557079f630d6b4910431f03c2cf5cfa83f0", "class_name": "RelatedNodeInfo"}}, "text": "Advantages this had over the bare foundational models included higher accuracy, less negative/toxic sentiment, and generally better alignment with user needs. Hence, OpenAI began using this as the basis for its API service offerings. Other instruction-tuned models have been released by others, including a fully open version.Another (related) kind of task-specific models are chatbots, which engage in human-like conversation. In November 2022, OpenAI launched ChatGPT\u2014an online chat interface powered by an instruction-tuned language model trained in a similar fashion to InstructGPT. They trained this model using RLHF, with human AI trainers providing conversations in which they played both the user and the AI, and mixed this new dialogue dataset with the InstructGPT dataset for a conversational format suitable for a chatbot. Other major chatbots currently include Microsoft's Bing Chat, which uses OpenAI's GPT-4 (as part of a broader close collaboration between OpenAI and Microsoft), and Google's competing chatbot Bard (initially based on their LaMDA family of conversation-trained language models, with plans to switch to PaLM).Yet another kind of task that a GPT can be used for is the meta-task of generating its own instructions, like developing a series of prompts for 'itself' to be able to effectuate a more general goal given by a human user. This is known as an AI agent, and more specifically a recursive one because it uses results from its previous self-instructions to help it form its subsequent prompts; the first major example of this was Auto-GPT (which uses OpenAI's GPT models), and others have since been developed as well.\r\n\r\n\r\n=== Multimodality ===\r\nGenerative transformer-based systems can also be targeted to tasks involving modalities beyond text. \r\nFor example, Microsoft\u2019s \u201cVisual ChatGPT\u201d combines ChatGPT with visual foundation models (VFMs) to enable input or output comprising images as well as text. Also, advances in text-to-speech technology offer powerful tools for audio content creation when used in conjunction with foundational GPT language models.\r\n\r\n\r\n=== Domain-specificity ===\r\nGPT systems can be directed toward particular fields or domains. Some reported examples of such models and apps are as follows: \r\n\r\nEinsteinGPT \u2013 for sales and marketing domains, to aid with customer relationship management (uses GPT-3.5)\r\nBloombergGPT \u2013 for the financial domain, to aid with financial news and information (uses \"freely available\" AI methods, combined with their proprietary data)\r\nKhanmigo \u2013 described as a GPT version for tutoring, in the education domain, it aids students using Khan Academy by guiding them through their studies without directly providing answers (powered by GPT-4)\r\nSlackGPT \u2013 for the Slack instant-messaging service, to aid with navigating and summarizing discussions on it (uses OpenAI's API)\r\nBioGPT \u2013 for the biomedical domain, to aid with biomedical literature text generation and mining (uses GPT-2)Sometimes domain-specificity is accomplished via software plug-ins or add-ons. For example, several different companies have developed particular plugins that interact directly with OpenAI's ChatGPT interface, and Google Workspace has available add-ons such as \u201cGPT for Sheets and Docs\u201d\u2014which is reported to aid use of spreadsheet functionality in Google Sheets.In November 2023, OpenAI announced that it's enabling ChatGPT Plus subscribers to create custom versions of ChatGPT (being called GPTs). These can be tailored for specific domains via prompt engineering, curated datasets, and/or targeted interaction with external tools. Users who register as verified builders are able to publish their custom GPTs for other users, with monetization potential. (This is notably distinct from OpenAI's API service, as this is based internally within OpenAI's platform.)\r\n\r\n\r\n== Brand issues ==\r\nOpenAI, which created the first generative pre-trained transformer (GPT) in 2018, has recently asserted that \u201cGPT\u201d should be regarded as a brand of OpenAI. In April 2023, OpenAI revised the brand guidelines in its terms of service to indicate that other businesses using its API to run their artificial intelligence (AI) services would no longer be able to include \u201cGPT\u201d in such names or branding. In May 2023, OpenAI engaged a brand management service to notify its API customers of this policy, although these notifications stopped short of making overt legal claims (such as allegations of trademark infringement or demands to cease and desist).", "start_char_idx": 6835, "end_char_idx": 11351, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5a310d23-81e9-4909-a0be-c9ef7a85d856": {"__data__": {"id_": "5a310d23-81e9-4909-a0be-c9ef7a85d856", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative pre-trained transformer.txt", "file_name": "Generative pre-trained transformer.txt", "file_type": "text/plain", "file_size": 14085, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2a159bcb-23ad-4890-bd3e-8c468e4d113d", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative pre-trained transformer.txt", "file_name": "Generative pre-trained transformer.txt", "file_type": "text/plain", "file_size": 14085, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "d53802146e3714daade4a189ecb3aa282ab4ff70e6b786b4e6680218e2fac290", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9341a911-be4f-49e3-abfc-6c0ea4b86498", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative pre-trained transformer.txt", "file_name": "Generative pre-trained transformer.txt", "file_type": "text/plain", "file_size": 14085, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "e8d6ed1556870e156c24e2cebc73f99ecb5c0e710cb17dee53e82bd924e3d10d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7f67bb38-bd16-4aa1-98c1-8ad6b1a71e93", "node_type": "1", "metadata": {}, "hash": "6f527bdbcbee345b67ff6b6e0df05e5ff83a5a8d395b326e4ecd4a30e7e68c6c", "class_name": "RelatedNodeInfo"}}, "text": "Users who register as verified builders are able to publish their custom GPTs for other users, with monetization potential. (This is notably distinct from OpenAI's API service, as this is based internally within OpenAI's platform.)\r\n\r\n\r\n== Brand issues ==\r\nOpenAI, which created the first generative pre-trained transformer (GPT) in 2018, has recently asserted that \u201cGPT\u201d should be regarded as a brand of OpenAI. In April 2023, OpenAI revised the brand guidelines in its terms of service to indicate that other businesses using its API to run their artificial intelligence (AI) services would no longer be able to include \u201cGPT\u201d in such names or branding. In May 2023, OpenAI engaged a brand management service to notify its API customers of this policy, although these notifications stopped short of making overt legal claims (such as allegations of trademark infringement or demands to cease and desist). As of November 2023, OpenAI still prohibits its API licensees from naming their own products with \"GPT,\" but it has begun enabling its ChatGPT Plus subscribers to make \"custom versions of ChatGPT\" that are being called GPTs on the OpenAI site. OpenAI's terms of service says that its subscribers may use \"GPT\" in the names of these, although it's \"discouraged.\"Relatedly, OpenAI has applied to the United States Patent and Trademark Office (USPTO) to seek domestic trademark registration for the term \u201cGPT\u201d in the field of AI. OpenAI sought to expedite handling of its application, but the USPTO declined that request in April 2023. In May 2023, the USPTO responded to the application with a determination that \"GPT\" was both descriptive and generic. As of November 2023, OpenAI continues to pursue its argument through the available processes. Regardless, failure to obtain a registered U.S. trademark does not preclude some level of common-law trademark rights in the U.S., and/or trademark rights in other countries.For any given type or scope of trademark protection in the U.S., OpenAI would need to establish that the term is actually \u201cdistinctive\u201d to their specific offerings in addition to being a broader technical term for the kind of technology. Some media reports suggested that OpenAI may be able to obtain trademark registration based indirectly on the fame of its GPT-based chatbot product, ChatGPT, for which OpenAI has separately sought protection (and which it has sought to enforce more strongly). Other reports have indicated that registration for the bare term \u201cGPT\u201d seems unlikely to be granted, as it is used frequently as a common term to refer simply to AI systems that involve generative pre-trained transformers. In any event, to whatever extent exclusive rights in the term may occur the U.S., others would need to avoid using it for similar products or services in ways likely to cause confusion. If such rights ever became broad enough to implicate other well-established uses in the field, the trademark doctrine of descriptive fair use could still preserve some room to continue non-brand-related usage.\r\n\r\n\r\n== Selected bibliography ==\r\nThis section lists the main official publications from OpenAI and Microsoft on their GPT models.\r\n\r\nGPT-1: report, GitHub release.\r\nGPT-2: blog announcement, report on its decision of \"staged release\", GitHub release.\r\nGPT-3: report. No GitHub or any other form of code release thenceforth.\r\nwebGPT: blog announcement, report,\r\nInstructGPT: blog announcement, report.\r\nChatGPT: blog announcement (no report).\r\nGPT-4: blog announcement, reports, model card.\r\n\r\n\r\n== See also ==\r\nCyc\r\nGemini\r\n\r\n\r\n== References ==", "start_char_idx": 10446, "end_char_idx": 14033, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7f67bb38-bd16-4aa1-98c1-8ad6b1a71e93": {"__data__": {"id_": "7f67bb38-bd16-4aa1-98c1-8ad6b1a71e93", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Graph neural network.txt", "file_name": "Graph neural network.txt", "file_type": "text/plain", "file_size": 16502, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "45a7af36-d76d-4c97-bd71-02a965a1a62c", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Graph neural network.txt", "file_name": "Graph neural network.txt", "file_type": "text/plain", "file_size": 16502, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "2bc1a5dc16ed20f96e3d969d83d6f7a8a6dbcbc130015f451af56093fd12adf3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5a310d23-81e9-4909-a0be-c9ef7a85d856", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative pre-trained transformer.txt", "file_name": "Generative pre-trained transformer.txt", "file_type": "text/plain", "file_size": 14085, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "595a240a5528fe8b78dd17e0c0deda5fa67b78c292b0b01e9501ee860fc868fc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "36b241ff-a8d1-4b1c-b5e3-9ed98e29f2e3", "node_type": "1", "metadata": {}, "hash": "14b10c0ec565c1870685551ec3bfb6c5fd86e5948f84fc18da62d5d011d29fd9", "class_name": "RelatedNodeInfo"}}, "text": "A graph neural network (GNN) belongs to a class of artificial neural networks for processing data that can be represented as graphs.\r\nIn the more general subject of \"geometric deep learning\", certain existing neural network architectures can be interpreted as GNNs operating on suitably defined graphs. A convolutional neural network layer, in the context of computer vision, can be seen as a GNN applied to graphs whose nodes are pixels and only adjacent pixels are connected by edges in the graph. A transformer layer, in natural language processing, can be seen as a GNN applied to complete graphs whose nodes are words or tokens in a passage of natural language text.\r\nThe key design element of GNNs is the use of pairwise message passing, such that graph nodes iteratively update their representations by exchanging information with their neighbors. Since their inception, several different GNN architectures have been proposed, which implement different flavors of message passing,  started by recursive or convolutional constructive approaches. As of 2022, whether it is possible to define GNN architectures \"going beyond\" message passing, or if every GNN can be built on message passing over suitably defined graphs, is an open research question.Relevant application domains for GNNs include Natural Language Processing, social networks, citation networks, molecular biology, chemistry, physics and NP-hard combinatorial optimization problems.Several open source libraries implementing graph neural networks are available, such as PyTorch Geometric (PyTorch), TensorFlow GNN (TensorFlow), jraph (Google JAX), and GraphNeuralNetworks.jl/GeometricFlux.jl (Julia, Flux).\r\n\r\n\r\n== Architecture ==\r\nThe architecture of a generic GNN implements the following fundamental layers:\r\nPermutation equivariant: a permutation equivariant layer maps a representation of a graph into an updated representation of the same graph. In the literature, permutation equivariant layers are implemented via pairwise message passing between graph nodes. Intuitively, in a message passing layer, nodes update their representations by aggregating the messages received from their immediate neighbours. As such, each message passing layer increases the receptive field of the GNN by one hop.\r\nLocal pooling: a local pooling layer coarsens the graph via downsampling. Local pooling is used to increase the receptive field of a GNN, in a similar fashion to pooling layers in convolutional neural networks. Examples include k-nearest neighbours pooling, top-k pooling, and self-attention pooling.\r\nGlobal pooling: a global pooling layer, also known as readout layer, provides fixed-size representation of the whole graph. The global pooling layer must be permutation invariant, such that permutations in the ordering of graph nodes and edges do not alter the final output. Examples include element-wise sum, mean or maximum.It has been demonstrated that GNNs cannot be more expressive than the Weisfeiler\u2013Leman Graph Isomorphism Test. In practice, this means that there exist different graph structures (e.g., molecules with the same atoms but different bonds) that cannot be distinguished by GNNs. More powerful GNNs operating on higher-dimension geometries such as simplicial complexes can be designed. As of 2022, whether or not future architectures will overcome the message passing primitive is an open research question.\r\n\r\n\r\n== Message passing layers ==\r\nMessage passing layers are permutation-equivariant layers mapping a graph into an updated representation of the same graph. Formally, they can be expressed as message passing neural networks (MPNNs).Let G=(V,E){\\displaystyle G=(V,E)} be a graph, where V{\\displaystyle V} is the node set and E{\\displaystyle E} is the edge set. Let Nu{\\displaystyle N_{u}} be the neighbourhood of some node u\u2208V{\\displaystyle u\\in V}. Additionally, let xu{\\displaystyle \\mathbf {x} _{u}} be the features of node u\u2208V{\\displaystyle u\\in V}, and euv{\\displaystyle \\mathbf {e} _{uv}} be the features of edge (u,v)\u2208E{\\displaystyle (u,v)\\in E}.", "start_char_idx": 0, "end_char_idx": 4058, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "36b241ff-a8d1-4b1c-b5e3-9ed98e29f2e3": {"__data__": {"id_": "36b241ff-a8d1-4b1c-b5e3-9ed98e29f2e3", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Graph neural network.txt", "file_name": "Graph neural network.txt", "file_type": "text/plain", "file_size": 16502, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "45a7af36-d76d-4c97-bd71-02a965a1a62c", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Graph neural network.txt", "file_name": "Graph neural network.txt", "file_type": "text/plain", "file_size": 16502, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "2bc1a5dc16ed20f96e3d969d83d6f7a8a6dbcbc130015f451af56093fd12adf3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7f67bb38-bd16-4aa1-98c1-8ad6b1a71e93", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Graph neural network.txt", "file_name": "Graph neural network.txt", "file_type": "text/plain", "file_size": 16502, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "8894466bbe866054e17c251a914d93cb8904c62b6ef16b5c797e181d0b688225", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2f4c12dd-969f-426e-aab7-bc08d64d9783", "node_type": "1", "metadata": {}, "hash": "cdd5c5551b9ae4ecc628b7553a0f4b6d6f5eba739d971b0ff156ebc55450e8a2", "class_name": "RelatedNodeInfo"}}, "text": "== Message passing layers ==\r\nMessage passing layers are permutation-equivariant layers mapping a graph into an updated representation of the same graph. Formally, they can be expressed as message passing neural networks (MPNNs).Let G=(V,E){\\displaystyle G=(V,E)} be a graph, where V{\\displaystyle V} is the node set and E{\\displaystyle E} is the edge set. Let Nu{\\displaystyle N_{u}} be the neighbourhood of some node u\u2208V{\\displaystyle u\\in V}. Additionally, let xu{\\displaystyle \\mathbf {x} _{u}} be the features of node u\u2208V{\\displaystyle u\\in V}, and euv{\\displaystyle \\mathbf {e} _{uv}} be the features of edge (u,v)\u2208E{\\displaystyle (u,v)\\in E}. An MPNN layer can be expressed as follows:\r\nhu=\u03d5(xu,\u2a01v\u2208Nu\u03c8(xu,xv,euv)){\\displaystyle \\mathbf {h} _{u}=\\phi \\left(\\mathbf {x} _{u},\\bigoplus _{v\\in N_{u}}\\psi (\\mathbf {x} _{u},\\mathbf {x} _{v},\\mathbf {e} _{uv})\\right)}where \u03d5{\\displaystyle \\phi } and \u03c8{\\displaystyle \\psi } are differentiable functions (e.g., artificial neural networks), and \u2a01{\\displaystyle \\bigoplus } is a permutation invariant aggregation operator that can accept an arbitrary number of inputs (e.g., element-wise sum, mean, or max). In particular, \u03d5{\\displaystyle \\phi } and \u03c8{\\displaystyle \\psi } are referred to as update and message functions, respectively. Intuitively, in an MPNN computational block, graph nodes update their representations by aggregating the messages received from their neighbours.\r\nThe outputs of one or more MPNN layers are node representations hu{\\displaystyle \\mathbf {h} _{u}} for each node u\u2208V{\\displaystyle u\\in V} in the graph. Node representations can be employed for any downstream task, such as node/graph classification or edge prediction.\r\nGraph nodes in an MPNN update their representation aggregating information from their immediate neighbours. As such, stacking n{\\displaystyle n} MPNN layers means that one node will be able to communicate with nodes that are at most n{\\displaystyle n} \"hops\" away. In principle, to ensure that every node receives information from every other node, one would need to stack a number of MPNN layers equal to the graph diameter. However, stacking many MPNN layers may cause issues such as oversmoothing and oversquashing. Oversmoothing refers to the issue of node representations becoming indistinguishable. Oversquashing refers to the bottleneck that is created by squeezing long-range dependencies into fixed-size representations. Countermeasures such as skip connections (as in residual neural networks), gated update rules and jumping knowledge can mitigate oversmoothing. Modifying the final layer to be a fully-adjacent layer, i.e., by considering the graph as a complete graph, can mitigate oversquashing in problems where long-range dependencies are required.Other \"flavours\" of MPNN have been developed in the literature, such as graph convolutional networks and graph attention networks, whose definitions can be expressed in terms of the MPNN formalism.\r\n\r\n\r\n=== Graph convolutional network ===\r\nThe graph convolutional network (GCN) was first introduced by Thomas Kipf and Max Welling in 2017.A GCN layer defines a first-order approximation of a localized spectral filter on graphs. GCNs can be understood as a generalization of convolutional neural networks to graph-structured data.", "start_char_idx": 3409, "end_char_idx": 6703, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2f4c12dd-969f-426e-aab7-bc08d64d9783": {"__data__": {"id_": "2f4c12dd-969f-426e-aab7-bc08d64d9783", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Graph neural network.txt", "file_name": "Graph neural network.txt", "file_type": "text/plain", "file_size": 16502, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "45a7af36-d76d-4c97-bd71-02a965a1a62c", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Graph neural network.txt", "file_name": "Graph neural network.txt", "file_type": "text/plain", "file_size": 16502, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "2bc1a5dc16ed20f96e3d969d83d6f7a8a6dbcbc130015f451af56093fd12adf3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "36b241ff-a8d1-4b1c-b5e3-9ed98e29f2e3", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Graph neural network.txt", "file_name": "Graph neural network.txt", "file_type": "text/plain", "file_size": 16502, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "aabb1aef7aa4235ed6844e18f661af271f961de9c1a791f1fd7585bc5813c7c4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6c6979e-ff99-4310-8fb4-1cf2250ee34e", "node_type": "1", "metadata": {}, "hash": "9a1a2959de8e1163a44b9bda5dbc20a3cc58aec966fc8de1c67d7195f3ec4651", "class_name": "RelatedNodeInfo"}}, "text": "Oversquashing refers to the bottleneck that is created by squeezing long-range dependencies into fixed-size representations. Countermeasures such as skip connections (as in residual neural networks), gated update rules and jumping knowledge can mitigate oversmoothing. Modifying the final layer to be a fully-adjacent layer, i.e., by considering the graph as a complete graph, can mitigate oversquashing in problems where long-range dependencies are required.Other \"flavours\" of MPNN have been developed in the literature, such as graph convolutional networks and graph attention networks, whose definitions can be expressed in terms of the MPNN formalism.\r\n\r\n\r\n=== Graph convolutional network ===\r\nThe graph convolutional network (GCN) was first introduced by Thomas Kipf and Max Welling in 2017.A GCN layer defines a first-order approximation of a localized spectral filter on graphs. GCNs can be understood as a generalization of convolutional neural networks to graph-structured data.\r\nThe formal expression of a GCN layer reads as follows:\r\n\r\nH=\u03c3(D~\u221212A~D~\u221212X\u0398){\\displaystyle \\mathbf {H} =\\sigma \\left({\\tilde {\\mathbf {D} }}^{-{\\frac {1}{2}}}{\\tilde {\\mathbf {A} }}{\\tilde {\\mathbf {D} }}^{-{\\frac {1}{2}}}\\mathbf {X} \\mathbf {\\Theta } \\right)}where H{\\displaystyle \\mathbf {H} } is the matrix of node representations hu{\\displaystyle \\mathbf {h} _{u}}, X{\\displaystyle \\mathbf {X} } is the matrix of node features xu{\\displaystyle \\mathbf {x} _{u}}, \u03c3(\u22c5){\\displaystyle \\sigma (\\cdot )} is an activation function (e.g., ReLU), A~{\\displaystyle {\\tilde {\\mathbf {A} }}} is the graph adjacency matrix with the addition of self-loops, D~{\\displaystyle {\\tilde {\\mathbf {D} }}} is the graph degree matrix with the addition of self-loops, and \u0398{\\displaystyle \\mathbf {\\Theta } } is a matrix of trainable parameters.\r\nIn particular, let A{\\displaystyle \\mathbf {A} } be the graph adjacency matrix: then, one can define A~=A+I{\\displaystyle {\\tilde {\\mathbf {A} }}=\\mathbf {A} +\\mathbf {I} } and D~ii=\u2211j\u2208VA~ij{\\displaystyle {\\tilde {\\mathbf {D} }}_{ii}=\\sum _{j\\in V}{\\tilde {A}}_{ij}}, where I{\\displaystyle \\mathbf {I} } denotes the identity matrix. This normalization ensures that the eigenvalues of D~\u221212A~D~\u221212{\\displaystyle {\\tilde {\\mathbf {D} }}^{-{\\frac {1}{2}}}{\\tilde {\\mathbf {A} }}{\\tilde {\\mathbf {D} }}^{-{\\frac {1}{2}}}} are bounded in the range [0,1]{\\displaystyle [0,1]}, avoiding numerical instabilities and exploding/vanishing gradients.\r\nA limitation of GCNs is that they do not allow multidimensional edge features euv{\\displaystyle \\mathbf {e} _{uv}}. It is however possible to associate scalar weights wuv{\\displaystyle w_{uv}} to each edge by imposing Auv=wuv{\\displaystyle A_{uv}=w_{uv}}, i.e., by setting each nonzero entry in the adjacency matrix equal to the weight of the corresponding edge.\r\n\r\n\r\n=== Graph attention network ===\r\nThe graph attention network (GAT) was introduced by Petar Veli\u010dkovi\u0107 et al. in 2018.Graph attention network is a combination of a graph neural network and an attention layer.\r\nThe implementation of attention layer in graphical neural networks helps provide attention or focus to the important information from the data instead of focusing on the whole data.", "start_char_idx": 5715, "end_char_idx": 8932, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6c6979e-ff99-4310-8fb4-1cf2250ee34e": {"__data__": {"id_": "e6c6979e-ff99-4310-8fb4-1cf2250ee34e", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Graph neural network.txt", "file_name": "Graph neural network.txt", "file_type": "text/plain", "file_size": 16502, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "45a7af36-d76d-4c97-bd71-02a965a1a62c", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Graph neural network.txt", "file_name": "Graph neural network.txt", "file_type": "text/plain", "file_size": 16502, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "2bc1a5dc16ed20f96e3d969d83d6f7a8a6dbcbc130015f451af56093fd12adf3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2f4c12dd-969f-426e-aab7-bc08d64d9783", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Graph neural network.txt", "file_name": "Graph neural network.txt", "file_type": "text/plain", "file_size": 16502, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "1e400ef7dd05098e35803ce565e1a09204188c610564c7e7ecf765ea591ab35a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c0b7bd4d-34d3-492d-8a74-5d0df135efc8", "node_type": "1", "metadata": {}, "hash": "95caa9b9fce4087597f6a5670842380917b08fcd49168ab6749393aa8cbe8b22", "class_name": "RelatedNodeInfo"}}, "text": "A limitation of GCNs is that they do not allow multidimensional edge features euv{\\displaystyle \\mathbf {e} _{uv}}. It is however possible to associate scalar weights wuv{\\displaystyle w_{uv}} to each edge by imposing Auv=wuv{\\displaystyle A_{uv}=w_{uv}}, i.e., by setting each nonzero entry in the adjacency matrix equal to the weight of the corresponding edge.\r\n\r\n\r\n=== Graph attention network ===\r\nThe graph attention network (GAT) was introduced by Petar Veli\u010dkovi\u0107 et al. in 2018.Graph attention network is a combination of a graph neural network and an attention layer.\r\nThe implementation of attention layer in graphical neural networks helps provide attention or focus to the important information from the data instead of focusing on the whole data.\r\nA multi-head GAT layer can be expressed as follows:\r\n\r\nhu=\u2016k=1K\u03c3(\u2211v\u2208Nu\u03b1uvWkxv){\\displaystyle \\mathbf {h} _{u}={\\overset {K}{\\underset {k=1}{\\Big \\Vert }}}\\sigma \\left(\\sum _{v\\in N_{u}}\\alpha _{uv}\\mathbf {W} ^{k}\\mathbf {x} _{v}\\right)}where K{\\displaystyle K} is the number of attention heads, \u2016{\\displaystyle {\\Big \\Vert }} denotes vector concatenation, \u03c3(\u22c5){\\displaystyle \\sigma (\\cdot )} is an activation function (e.g., ReLU), \u03b1ij{\\displaystyle \\alpha _{ij}} are attention coefficients, and Wk{\\displaystyle W^{k}} is a matrix of trainable parameters for the k{\\displaystyle k}-th attention head.\r\nFor the final GAT layer, the outputs from each attention head are averaged before the application of the activation function. Formally, the final GAT layer can be written as:\r\n\r\nhu=\u03c3(1K\u2211k=1K\u2211v\u2208Nu\u03b1uvWkxv){\\displaystyle \\mathbf {h} _{u}=\\sigma \\left({\\frac {1}{K}}\\sum _{k=1}^{K}\\sum _{v\\in N_{u}}\\alpha _{uv}\\mathbf {W} ^{k}\\mathbf {x} _{v}\\right)}Attention in Machine Learning is a technique that mimics cognitive attention. In the context of learning on graphs, the attention coefficient \u03b1uv{\\displaystyle \\alpha _{uv}} measures how important is node u\u2208V{\\displaystyle u\\in V} to node v\u2208V{\\displaystyle v\\in V}.\r\nNormalized attention coefficients are computed as follows:\r\n\r\n\u03b1uv=exp\u2061(LeakyReLU(aT[Whu\u2016Whv\u2016euv]))\u2211z\u2208Nuexp\u2061(LeakyReLU(aT[Whu\u2016Whz\u2016euz])){\\displaystyle \\alpha _{uv}={\\frac {\\exp({\\text{LeakyReLU}}\\left(\\mathbf {a} ^{T}[\\mathbf {W} \\mathbf {h} _{u}\\Vert \\mathbf {W} \\mathbf {h} _{v}\\Vert \\mathbf {e} _{uv}]\\right))}{\\sum _{z\\in N_{u}}\\exp({\\text{LeakyReLU}}\\left(\\mathbf {a} ^{T}[\\mathbf {W} \\mathbf {h} _{u}\\Vert \\mathbf {W} \\mathbf {h} _{z}\\Vert \\mathbf {e} _{uz}]\\right))}}}where a{\\displaystyle \\mathbf {a} } is a vector of learnable weights, \u22c5T{\\displaystyle \\cdot ^{T}} indicates transposition, and LeakyReLU{\\displaystyle {\\text{LeakyReLU}}} is a modified ReLU activation function. Attention coefficients are normalized to make them easily comparable across different nodes.A GCN can be seen as a special case of a GAT where attention coefficients are not learnable, but fixed and equal to the edge weights wuv{\\displaystyle w_{uv}}.\r\n\r\n\r\n=== Gated graph sequence neural network ===\r\nThe gated graph sequence neural network (GGS-NN) was introduced by Yujia Li et al. in 2015. The GGS-NN extends the GNN formulation by Scarselli et al. to output sequences.", "start_char_idx": 8174, "end_char_idx": 11311, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0b7bd4d-34d3-492d-8a74-5d0df135efc8": {"__data__": {"id_": "c0b7bd4d-34d3-492d-8a74-5d0df135efc8", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Graph neural network.txt", "file_name": "Graph neural network.txt", "file_type": "text/plain", "file_size": 16502, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "45a7af36-d76d-4c97-bd71-02a965a1a62c", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Graph neural network.txt", "file_name": "Graph neural network.txt", "file_type": "text/plain", "file_size": 16502, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "2bc1a5dc16ed20f96e3d969d83d6f7a8a6dbcbc130015f451af56093fd12adf3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6c6979e-ff99-4310-8fb4-1cf2250ee34e", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Graph neural network.txt", "file_name": "Graph neural network.txt", "file_type": "text/plain", "file_size": 16502, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "a467d85a8cc845afd680b9df22c54230553246a9ca3b98c419752222a79bc8d1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a8232541-5d5d-42e0-bc08-316615718517", "node_type": "1", "metadata": {}, "hash": "be4c64e3780a07d366cb4d9619bf539f5e3da603a773131655015beea43aaa58", "class_name": "RelatedNodeInfo"}}, "text": "Attention coefficients are normalized to make them easily comparable across different nodes.A GCN can be seen as a special case of a GAT where attention coefficients are not learnable, but fixed and equal to the edge weights wuv{\\displaystyle w_{uv}}.\r\n\r\n\r\n=== Gated graph sequence neural network ===\r\nThe gated graph sequence neural network (GGS-NN) was introduced by Yujia Li et al. in 2015. The GGS-NN extends the GNN formulation by Scarselli et al. to output sequences. The message passing framework is implemented as an update rule to a gated recurrent unit (GRU) cell.\r\nA GGS-NN can be expressed as follows:\r\n\r\nhu(0)=xu\u20160{\\displaystyle \\mathbf {h} _{u}^{(0)}=\\mathbf {x} _{u}\\,\\Vert \\,\\mathbf {0} }\r\nmu(l+1)=\u2211v\u2208Nu\u0398hv{\\displaystyle \\mathbf {m} _{u}^{(l+1)}=\\sum _{v\\in N_{u}}\\mathbf {\\Theta } \\mathbf {h} _{v}}\r\nhu(l+1)=GRU(mu(l+1),hu(l)){\\displaystyle \\mathbf {h} _{u}^{(l+1)}={\\text{GRU}}(\\mathbf {m} _{u}^{(l+1)},\\mathbf {h} _{u}^{(l)})}where \u2016{\\displaystyle \\Vert } denotes vector concatenation, 0{\\displaystyle \\mathbf {0} } is a vector of zeros, \u0398{\\displaystyle \\mathbf {\\Theta } } is a matrix of learnable parameters, GRU{\\displaystyle {\\text{GRU}}} is a GRU cell, and l{\\displaystyle l} denotes the sequence index. In a GGS-NN, the node representations are regarded as the hidden states of a GRU cell. The initial node features xu(0){\\displaystyle \\mathbf {x} _{u}^{(0)}} are zero-padded up to the hidden state dimension of the GRU cell. The same GRU cell is used for updating representations for each node.\r\n\r\n\r\n== Local pooling layers ==\r\nLocal pooling layers coarsen the graph via downsampling. We present here several learnable local pooling strategies that have been proposed. For each cases, the input is the initial graph is represented by a matrix X{\\displaystyle \\mathbf {X} } of node features, and the graph adjacency matrix A{\\displaystyle \\mathbf {A} }. The output is the new matrix X\u2032{\\displaystyle \\mathbf {X} '}of node features, and the new graph adjacency matrix A\u2032{\\displaystyle \\mathbf {A} '}.\r\n\r\n\r\n=== Top-k pooling ===\r\nWe first set\r\ny=Xp\u2016p\u2016{\\displaystyle \\mathbf {y} ={\\frac {\\mathbf {X} \\mathbf {p} }{\\Vert \\mathbf {p} \\Vert }}}\r\nwhere p{\\displaystyle \\mathbf {p} } is a learnable projection vector. The projection vector p{\\displaystyle \\mathbf {p} } computes a scalar projection value for each graph node.\r\nThe top-k pooling layer  can then be formalised as follows:\r\n\r\nX\u2032=(X\u2299sigmoid(y))i{\\displaystyle \\mathbf {X} '=(\\mathbf {X} \\odot {\\text{sigmoid}}(\\mathbf {y} ))_{\\mathbf {i} }}A\u2032=Ai,i{\\displaystyle \\mathbf {A} '=\\mathbf {A} _{\\mathbf {i} ,\\mathbf {i} }}where i=topk(y){\\displaystyle \\mathbf {i} ={\\text{top}}_{k}(\\mathbf {y} )} is the subset of nodes with the top-k highest projection scores, \u2299{\\displaystyle \\odot } denotes element-wise matrix multiplication, and sigmoid(\u22c5){\\displaystyle {\\text{sigmoid}}(\\cdot )} is the sigmoid function. In other words, the nodes with the top-k highest projection scores are retained in the new adjacency matrix A\u2032{\\displaystyle \\mathbf {A} '}. The sigmoid(\u22c5){\\displaystyle {\\text{sigmoid}}(\\cdot )} operation makes the projection vector p{\\displaystyle \\mathbf {p} } trainable by backpropagation, which otherwise would produce discrete outputs.", "start_char_idx": 10838, "end_char_idx": 14064, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a8232541-5d5d-42e0-bc08-316615718517": {"__data__": {"id_": "a8232541-5d5d-42e0-bc08-316615718517", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Graph neural network.txt", "file_name": "Graph neural network.txt", "file_type": "text/plain", "file_size": 16502, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "45a7af36-d76d-4c97-bd71-02a965a1a62c", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Graph neural network.txt", "file_name": "Graph neural network.txt", "file_type": "text/plain", "file_size": 16502, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "2bc1a5dc16ed20f96e3d969d83d6f7a8a6dbcbc130015f451af56093fd12adf3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c0b7bd4d-34d3-492d-8a74-5d0df135efc8", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Graph neural network.txt", "file_name": "Graph neural network.txt", "file_type": "text/plain", "file_size": 16502, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "83bbee19304571a76a23e69cd2d7e5d923edcc302fbd27642ae3761847df6835", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "241c4b6f-3158-40f5-bca3-709d04e41d88", "node_type": "1", "metadata": {}, "hash": "4cd21b96969405a1678ff4559c09a0613422e1cd47aea6081012ccab136fe824", "class_name": "RelatedNodeInfo"}}, "text": "In other words, the nodes with the top-k highest projection scores are retained in the new adjacency matrix A\u2032{\\displaystyle \\mathbf {A} '}. The sigmoid(\u22c5){\\displaystyle {\\text{sigmoid}}(\\cdot )} operation makes the projection vector p{\\displaystyle \\mathbf {p} } trainable by backpropagation, which otherwise would produce discrete outputs.\r\n\r\n\r\n=== Self-attention pooling ===\r\nWe first set\r\n\r\ny=GNN(X,A){\\displaystyle \\mathbf {y} ={\\text{GNN}}(\\mathbf {X} ,\\mathbf {A} )}where GNN{\\displaystyle {\\text{GNN}}} is a generic permutation equivariant GNN layer (e.g., GCN, GAT, MPNN).\r\nThe Self-attention pooling layer can then be formalised as follows:\r\n\r\nX\u2032=(X\u2299y)i{\\displaystyle \\mathbf {X} '=(\\mathbf {X} \\odot \\mathbf {y} )_{\\mathbf {i} }}A\u2032=Ai,i{\\displaystyle \\mathbf {A} '=\\mathbf {A} _{\\mathbf {i} ,\\mathbf {i} }}where i=topk(y){\\displaystyle \\mathbf {i} ={\\text{top}}_{k}(\\mathbf {y} )} is the subset of nodes with the top-k highest projection scores, \u2299{\\displaystyle \\odot } denotes element-wise matrix multiplication.\r\nThe self-attention pooling layer can be seen as an extension of the top-k pooling layer. Differently from top-k pooling, the self-attention scores computed in self-attention pooling account both for the graph features and the graph topology.\r\n\r\n\r\n== Applications ==\r\n\r\n\r\n=== Protein folding ===\r\n\r\nGraph neural networks are one of the main building blocks of AlphaFold, an artificial intelligence program developed by Google's DeepMind for solving the protein folding problem in biology. AlphaFold achieved first place in several CASP competitions.\r\n\r\n\r\n=== Social networks ===\r\n\r\nSocial networks are a major application domain for GNNs due to their natural representation as social graphs. GNNs are used to develop recommender systems based on both social relations and item relations.\r\n\r\n\r\n=== Combinatorial optimization ===\r\n\r\nGNNs are used as fundamental building blocks for several combinatorial optimization algorithms. Examples include computing shortest paths or Eulerian circuits for a given graph, deriving chip placements superior or competitive to handcrafted human solutions, and improving expert-designed branching rules in branch and bound.\r\n\r\n\r\n=== Cyber security ===\r\n\r\nWhen viewed as a graph, a network of computers can be analyzed with GNNs for anomaly detection. Anomalies within provenance graphs often correlate to malicious activity within the network. GNNs have been used to identify these anomalies on individual nodes and within paths to detect malicious processes, or on the edge level to detect lateral movement.\r\n\r\n\r\n== References ==\r\n\r\n\r\n== External links ==\r\nhttps://distill.pub/2021/gnn-intro/", "start_char_idx": 13723, "end_char_idx": 16374, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "241c4b6f-3158-40f5-bca3-709d04e41d88": {"__data__": {"id_": "241c4b6f-3158-40f5-bca3-709d04e41d88", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\History of artificial neural networks.txt", "file_name": "History of artificial neural networks.txt", "file_type": "text/plain", "file_size": 19960, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "08db6b24-b93e-40bb-8bc4-355238604f5a", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\History of artificial neural networks.txt", "file_name": "History of artificial neural networks.txt", "file_type": "text/plain", "file_size": 19960, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "32763edc3605646ddd5748404e08034d92f094732e3c86b45202d51f7205e731", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a8232541-5d5d-42e0-bc08-316615718517", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Graph neural network.txt", "file_name": "Graph neural network.txt", "file_type": "text/plain", "file_size": 16502, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "3e148e9096bd077137a84edd1fea49e37692f87b56b934065f35ed2e28d7d0cb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a33ef89f-f9f5-4f21-9691-ebed7e5d9084", "node_type": "1", "metadata": {}, "hash": "45eaa039f9de578361e89ecadaef612dda47194125283ab271aad055d5ad9f49", "class_name": "RelatedNodeInfo"}}, "text": "Artificial neural networks (ANNs) are models created using machine learning to perform a number of tasks. Their creation was inspired by neural circuitry. While some of the computational implementations ANNs relate to earlier discoveries in mathematics, the first implementation of ANNs was by psychologist Frank Rosenblatt, who developed the perceptron. Little research was conducted on ANNs in the 1970s and 1980s, with the AAAI calling that period an \"AI winter\".Later, advances in hardware and the development of the backpropagation algorithm as well as recurrent neural networks and convolutional neural networks, renewed interest in ANNs. The 2010s, saw the development of a deep neural network (a neural network with many layers) called AlexNet. It greatly outperformed other image recognition models, and is thought to have launched the ongoing AI spring, and further increasing interest in ANNs. The transformer architecture was first described in 2017 as a method to teach ANNs grammatical dependencies in language, and is the predominant architecture used by large language models, such as GPT-4. Diffusion models were first described in 2015, and began to be used by image generation models such as DALL-E in the 2020s.\r\n\r\n\r\n== Linear neural network ==\r\nThe simplest kind of feedforward neural network is a linear network, which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated in each node. The mean squared errors between these calculated outputs and a given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement.\r\n\r\n\r\n== Perceptrons and other early neural networks ==\r\nWarren McCulloch and Walter Pitts (1943) also considered a non-learning computational model for neural networks. This model paved the way for research to split into two approaches. One approach focused on biological processes while the other focused on the application of neural networks to artificial intelligence. This work led to work on nerve networks and their link to finite automata.In the early 1940s, D. O. Hebb created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Hebbian learning is unsupervised learning. This evolved into models for long-term potentiation. Researchers started applying these ideas to computational models in 1948 with Turing's B-type machines. Farley and Clark (1954) first used computational machines, then called \"calculators\", to simulate a Hebbian network. Other neural network computational machines were created by Rochester, Holland, Habit and Duda (1956).Rosenblatt (1958) created the perceptron, an algorithm for pattern recognition. With mathematical notation, Rosenblatt described circuitry not in the basic perceptron, such as the exclusive-or circuit that could not be processed by neural networks at the time. In 1959, a biological model proposed by Nobel laureates Hubel and Wiesel was based on their discovery of two types of cells in the primary visual cortex: simple cells and complex cells.Some say that research stagnated following Minsky and Papert (1969), who discovered that basic perceptrons were incapable of processing the exclusive-or circuit and that computers lacked sufficient power to process useful neural networks. However, by the time this book came out, methods for training multilayer perceptrons (MLPs) by deep learning were already known.\r\n\r\n\r\n== First deep learning ==\r\nThe first deep learning MLP was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa in 1965, as the Group Method of Data Handling. This method employs incremental layer by layer training based on regression analysis, where useless units in hidden layers are pruned with the help of a validation set.\r\nThe first deep learning MLP trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari.\r\nIn computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned  useful internal representations to classify non-linearily separable pattern classes.\r\n\r\n\r\n== Backpropagation ==\r\n\r\nThe backpropagation algorithm is an efficient application of the Leibniz chain rule (1673) to networks of differentiable nodes.", "start_char_idx": 0, "end_char_idx": 4565, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a33ef89f-f9f5-4f21-9691-ebed7e5d9084": {"__data__": {"id_": "a33ef89f-f9f5-4f21-9691-ebed7e5d9084", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\History of artificial neural networks.txt", "file_name": "History of artificial neural networks.txt", "file_type": "text/plain", "file_size": 19960, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "08db6b24-b93e-40bb-8bc4-355238604f5a", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\History of artificial neural networks.txt", "file_name": "History of artificial neural networks.txt", "file_type": "text/plain", "file_size": 19960, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "32763edc3605646ddd5748404e08034d92f094732e3c86b45202d51f7205e731", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "241c4b6f-3158-40f5-bca3-709d04e41d88", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\History of artificial neural networks.txt", "file_name": "History of artificial neural networks.txt", "file_type": "text/plain", "file_size": 19960, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "17ead80a45975d4f6aa4ee37e3053a61c6c8320f8680ace462c0980562880ae0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e40b6fbf-d71f-4dff-a596-ca62afb71b14", "node_type": "1", "metadata": {}, "hash": "0653be77b2b13a5237200536f9193cf46132582557486c516ff11404c8e458c8", "class_name": "RelatedNodeInfo"}}, "text": "== First deep learning ==\r\nThe first deep learning MLP was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa in 1965, as the Group Method of Data Handling. This method employs incremental layer by layer training based on regression analysis, where useless units in hidden layers are pruned with the help of a validation set.\r\nThe first deep learning MLP trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari.\r\nIn computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned  useful internal representations to classify non-linearily separable pattern classes.\r\n\r\n\r\n== Backpropagation ==\r\n\r\nThe backpropagation algorithm is an efficient application of the Leibniz chain rule (1673) to networks of differentiable nodes. It is also known as \r\nthe reverse mode of automatic differentiation or reverse accumulation, due to Seppo Linnainmaa (1970). The term \"back-propagating errors\" was introduced in 1962 by Frank Rosenblatt, but he did not have an implementation of this procedure, although Henry J. Kelley had a continuous precursor of backpropagation already in 1960 in the context of control theory. In 1982, Paul Werbos applied backpropagation to MLPs in the way that has become standard. In 1986, David E. Rumelhart et al. published an experimental analysis of the technique.\r\n\r\n\r\n== Recurrent network architectures ==\r\n\r\nWilhelm Lenz and Ernst Ising created and analyzed the Ising model (1925) which is essentially a non-learning artificial recurrent neural network (RNN) consisting of neuron-like threshold elements. In 1972, Shun'ichi Amari made this architecture adaptive. His learning RNN was popularised by John Hopfield in 1982.\r\n\r\n\r\n== Self-organizing maps ==\r\n\r\nSelf-organizing maps (SOMs) were described by Teuvo Kohonen in 1982. SOMs are neurophysiologically inspired artificial neural networks that learn low-dimensional representations of high-dimensional data while preserving the topological structure of the data. They are trained using competitive learning.\r\nSOMs create internal representations reminiscent of the cortical homunculus, a distorted representation of the human body, based on a neurological \"map\" of the areas and proportions of the human brain dedicated to processing sensory functions, for different parts of the body.\r\n\r\n\r\n== Convolutional neural networks (CNNs) ==\r\n\r\nThe origin of the CNN architecture is the \"neocognitron\" introduced by Kunihiko Fukushima in 1980.\r\nIt was inspired by work of  Hubel and Wiesel in the 1950s and 1960s which showed that cat visual cortices contain neurons that individually respond to small regions of the visual field.\r\nThe neocognitron introduced the two basic types of layers in CNNs: convolutional layers, and downsampling layers. A convolutional layer contains units whose receptive fields cover a patch of the previous layer. The weight vector (the set of adaptive parameters) of such a unit is often called a filter. Units can share filters. Downsampling layers contain units whose receptive fields cover patches of previous convolutional layers. Such a unit typically computes the average of the activations of the units in its patch. This downsampling helps to correctly classify objects in visual scenes even when the objects are shifted.\r\nIn 1969, Kunihiko Fukushima also introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for CNNs and  deep neural networks in general.The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel  and was one of the first CNNs, as it achieved shift invariance. It did so by utilizing weight sharing in combination with backpropagation training. Thus, while also using a pyramidal structure as in the neocognitron, it performed a global optimization of the weights instead of a local one.In 1988, Wei Zhang et al. applied backpropagation \r\nto a CNN (a simplified Neocognitron with convolutional interconnections between the image feature layers and the last fully connected layer) for alphabet recognition. They also proposed an implementation of the CNN with an optical computing system.In 1989, Yann LeCun et al. trained a CNN with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked, training required 3 days. Learning was fully automatic, performed better than manual coefficient design, and was suited to a broader range of image recognition problems and image types.", "start_char_idx": 3763, "end_char_idx": 8259, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e40b6fbf-d71f-4dff-a596-ca62afb71b14": {"__data__": {"id_": "e40b6fbf-d71f-4dff-a596-ca62afb71b14", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\History of artificial neural networks.txt", "file_name": "History of artificial neural networks.txt", "file_type": "text/plain", "file_size": 19960, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "08db6b24-b93e-40bb-8bc4-355238604f5a", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\History of artificial neural networks.txt", "file_name": "History of artificial neural networks.txt", "file_type": "text/plain", "file_size": 19960, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "32763edc3605646ddd5748404e08034d92f094732e3c86b45202d51f7205e731", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a33ef89f-f9f5-4f21-9691-ebed7e5d9084", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\History of artificial neural networks.txt", "file_name": "History of artificial neural networks.txt", "file_type": "text/plain", "file_size": 19960, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "99f8ce66d0259ad81cec26c9d2be93aed1da64365adcf3c152c43c476f6d4534", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6bb52853-a030-41ce-96a1-9dcf8439288a", "node_type": "1", "metadata": {}, "hash": "5828150b3cd1e84029ef436b62e030a2578249dcc6b462e6d060269c07079f61", "class_name": "RelatedNodeInfo"}}, "text": "It did so by utilizing weight sharing in combination with backpropagation training. Thus, while also using a pyramidal structure as in the neocognitron, it performed a global optimization of the weights instead of a local one.In 1988, Wei Zhang et al. applied backpropagation \r\nto a CNN (a simplified Neocognitron with convolutional interconnections between the image feature layers and the last fully connected layer) for alphabet recognition. They also proposed an implementation of the CNN with an optical computing system.In 1989, Yann LeCun et al. trained a CNN with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked, training required 3 days. Learning was fully automatic, performed better than manual coefficient design, and was suited to a broader range of image recognition problems and image types.\r\nSubsequently, Wei Zhang, et al. modified their model by removing the last fully connected layer and applied it for medical image object segmentation in 1991 and breast cancer detection in mammograms in 1994.In 1990 Yamaguchi et al. introduced max-pooling, a fixed filtering operation that calculates and propagates the maximum value of a given region. They combined TDNNs with max-pooling in order to realize a speaker independent isolated word recognition system. \r\nIn a variant of the neocognitron called the cresceptron, instead of using Fukushima's spatial averaging, J. Weng et al. also used max-pooling where a downsampling unit computes the maximum of the activations of the units in its patch. Max-pooling is often used in modern CNNs.LeNet-5, a 7-level CNN by Yann LeCun et al. in 1998, that classifies digits, was applied by several banks to recognize hand-written numbers on checks (British English: cheques) digitized in 32x32 pixel images. The ability to process higher-resolution images requires larger and more layers of CNNs, so this technique is constrained by the availability of computing resources.\r\nIn 2010, Backpropagation training through max-pooling was accelerated by GPUs and shown to perform better than other pooling variants.\r\nBehnke (2003) relied only on the sign of the gradient (Rprop) on problems such as image reconstruction and face localization. Rprop is a first-order optimization algorithm created by Martin Riedmiller and Heinrich Braun in 1992.In 2011, a deep GPU-based CNN called \"DanNet\" by Dan Ciresan, Ueli Meier, and Juergen Schmidhuber achieved human-competitive performance for the first time in computer vision contests. Subsequently, a similar GPU-based CNN by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton won the ImageNet Large Scale Visual Recognition Challenge 2012. A very deep CNN with over 100 layers by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun of Microsoft won the ImageNet 2015 contest.ANNs were able to guarantee shift invariance to deal with small and large natural objects in large cluttered scenes, only when invariance extended beyond shift, to all ANN-learned concepts, such as location, type (object class label), scale, lighting and others. This was realized in Developmental Networks (DNs) whose embodiments are Where-What Networks, WWN-1 (2008) through WWN-7 (2013).\r\n\r\n\r\n== Artificial curiosity and generative adversarial networks ==\r\n\r\nIn 1991, Juergen Schmidhuber published adversarial neural networks that contest with each other in the form of a zero-sum game, where one network's gain is the other network's loss. The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called \"artificial curiosity.\" Earlier adversarial machine learning systems \"neither involved unsupervised neural networks nor were about modeling data nor used gradient descent.\"In 2014, this adversarial principle was used in a generative adversarial network (GAN) by Ian Goodfellow et al. Here the environmental reaction is 1 or 0 depending on whether the first network's output is in a given set. This can be used to create realistic deepfakes.In 1992, Schmidhuber also published another type of gradient-based adversarial neural networks where the goal of the zero-sum game is to create disentangled representations of input patterns.", "start_char_idx": 7416, "end_char_idx": 11726, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6bb52853-a030-41ce-96a1-9dcf8439288a": {"__data__": {"id_": "6bb52853-a030-41ce-96a1-9dcf8439288a", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\History of artificial neural networks.txt", "file_name": "History of artificial neural networks.txt", "file_type": "text/plain", "file_size": 19960, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "08db6b24-b93e-40bb-8bc4-355238604f5a", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\History of artificial neural networks.txt", "file_name": "History of artificial neural networks.txt", "file_type": "text/plain", "file_size": 19960, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "32763edc3605646ddd5748404e08034d92f094732e3c86b45202d51f7205e731", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e40b6fbf-d71f-4dff-a596-ca62afb71b14", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\History of artificial neural networks.txt", "file_name": "History of artificial neural networks.txt", "file_type": "text/plain", "file_size": 19960, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "43fc18ad207d9097f4f18661fad6db24fcd11dea8df80f50ed031b72fe3d36c2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d12ee3f5-f57e-4046-ba66-5e25f2f4d9fd", "node_type": "1", "metadata": {}, "hash": "0727d52e7fad2f6be1315943b934e53cda9af20ff266d4e62635c00425673f50", "class_name": "RelatedNodeInfo"}}, "text": "The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called \"artificial curiosity.\" Earlier adversarial machine learning systems \"neither involved unsupervised neural networks nor were about modeling data nor used gradient descent.\"In 2014, this adversarial principle was used in a generative adversarial network (GAN) by Ian Goodfellow et al. Here the environmental reaction is 1 or 0 depending on whether the first network's output is in a given set. This can be used to create realistic deepfakes.In 1992, Schmidhuber also published another type of gradient-based adversarial neural networks where the goal of the zero-sum game is to create disentangled representations of input patterns. This was called predictability minimization.Nvidia's StyleGAN (2018) is based on the Progressive GAN by Tero Karras, Timo Aila, Samuli  Laine, and Jaakko Lehtinen. Here the GAN generator is grown from small to large scale in a pyramidal fashion. StyleGANs improve consistency between fine and coarse details in the generator network.\r\n\r\n\r\n== Transformers and their variants ==\r\n\r\nMany modern large language models such as ChatGPT, GPT-4, and BERT use a feedforward neural network called Transformer by Ashish Vaswani et. al. in their 2017 paper \"Attention Is All You Need.\"\r\nTransformers have increasingly become the model of choice for natural language processing problems, replacing recurrent neural networks (RNNs) such as long short-term memory (LSTM).Basic ideas for this go back a long way: in 1992, Juergen Schmidhuber published the Transformer with \"linearized self-attention\" (save for a normalization operator), \r\nwhich is also called the \"linear Transformer.\" He advertised it as an \"alternative to RNNs\" that can learn \"internal spotlights of attention,\" and experimentally applied it to problems of variable binding. Here a slow feedforward neural network learns by gradient descent to control the fast weights of another neural network through outer products of self-generated activation patterns called \"FROM\" and \"TO\" which in Transformer terminology are called \"key\" and \"value\" for \"self-attention.\" This fast weight \"attention mapping\" is applied to queries.  The 2017 Transformer combines this with a softmax operator and a projection matrix.Transformers are also increasingly being used in computer vision.\r\n\r\n\r\n== Deep learning with unsupervised or self-supervised pre-training ==\r\nIn the 1980s, backpropagation did not work well for deep FNNs and RNNs. Here the word \"deep\" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For an FNN, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For RNNs, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited.\r\nTo overcome this problem, Juergen Schmidhuber (1992) proposed a self-supervised hierarchy of RNNs pre-trained one level at a time by self-supervised learning. This \"neural history compressor\" uses predictive coding  to learn internal representations at multiple self-organizing time scales. \r\nThe deep architecture may be used to reproduce the original data from the top level feature activations.\r\nThe RNN hierarchy can be \"collapsed\" into a single RNN, by \"distilling\" a higher level \"chunker\" network into a lower level \"automatizer\" network. In 1993, a chunker solved a deep learning task whose CAP depth exceeded 1000.\r\nSuch history compressors can substantially facilitate downstream supervised deep learning.Geoffrey Hinton et al. (2006) proposed learning a high-level internal representation using successive layers of binary or real-valued latent variables with a restricted Boltzmann machine to model each layer. This RBM is a generative stochastic feedforward neural network that can learn a probability distribution over its set of inputs. Once sufficiently many layers have been learned, the deep architecture may be used as a generative model by reproducing the data when sampling down the model (an \"ancestral pass\") from the top level feature activations.", "start_char_idx": 10869, "end_char_idx": 15352, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d12ee3f5-f57e-4046-ba66-5e25f2f4d9fd": {"__data__": {"id_": "d12ee3f5-f57e-4046-ba66-5e25f2f4d9fd", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\History of artificial neural networks.txt", "file_name": "History of artificial neural networks.txt", "file_type": "text/plain", "file_size": 19960, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "08db6b24-b93e-40bb-8bc4-355238604f5a", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\History of artificial neural networks.txt", "file_name": "History of artificial neural networks.txt", "file_type": "text/plain", "file_size": 19960, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "32763edc3605646ddd5748404e08034d92f094732e3c86b45202d51f7205e731", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6bb52853-a030-41ce-96a1-9dcf8439288a", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\History of artificial neural networks.txt", "file_name": "History of artificial neural networks.txt", "file_type": "text/plain", "file_size": 19960, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "482be9a7899af2825f79d0726df65ac134500bb645f0bd42fa36113dab5986f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "104b72a0-ad9a-4f2d-9e0e-3566926aacac", "node_type": "1", "metadata": {}, "hash": "8966d44f05ca7a31024d6c04307377dfcf35d3bb1f70e95f9ada3bd7e8ccffeb", "class_name": "RelatedNodeInfo"}}, "text": "The RNN hierarchy can be \"collapsed\" into a single RNN, by \"distilling\" a higher level \"chunker\" network into a lower level \"automatizer\" network. In 1993, a chunker solved a deep learning task whose CAP depth exceeded 1000.\r\nSuch history compressors can substantially facilitate downstream supervised deep learning.Geoffrey Hinton et al. (2006) proposed learning a high-level internal representation using successive layers of binary or real-valued latent variables with a restricted Boltzmann machine to model each layer. This RBM is a generative stochastic feedforward neural network that can learn a probability distribution over its set of inputs. Once sufficiently many layers have been learned, the deep architecture may be used as a generative model by reproducing the data when sampling down the model (an \"ancestral pass\") from the top level feature activations. In 2012, Andrew Ng and Jeff Dean created an FNN that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images taken from YouTube videos.\r\n\r\n\r\n== The vanishing gradient problem and its solutions ==\r\n\r\nSepp Hochreiter's diploma thesis (1991) was called \"one of the most important documents in the history of machine learning\" by his supervisor Juergen Schmidhuber. Hochreiter not only tested the neural history compressor, but also identified and analyzed the vanishing gradient problem. He proposed recurrent residual connections to solve this problem. This led to the deep learning method called long short-term memory (LSTM), published in 1997. LSTM recurrent neural networks can learn \"very deep learning\" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. The \"vanilla LSTM\" with forget gate was introduced in 1999 by Felix Gers, Schmidhuber and Fred Cummins. LSTM has become the  most cited neural network of the 20th century.In 2015, Rupesh Kumar Srivastava, Klaus Greff, and Schmidhuber used LSTM principles to create the Highway network, a feedforward neural network with hundreds of layers, much deeper than previous networks. 7 months later, Kaiming He, Xiangyu Zhang;  Shaoqing Ren, and Jian Sun won the ImageNet 2015 competition with an open-gated or gateless Highway network variant called Residual neural network. This has become the most cited neural network of the 21st century.In 2011, Xavier Glorot, Antoine Bordes and Yoshua Bengio found that the ReLU of Kunihiko Fukushima also helps to overcome the vanishing gradient problem, compared to widely used activation functions prior to 2011.\r\n\r\n\r\n== Hardware-based designs ==\r\nThe development of metal\u2013oxide\u2013semiconductor (MOS) very-large-scale integration (VLSI), combining millions or billions of MOS transistors onto a single chip in the form of complementary MOS (CMOS) technology, enabled the development of practical artificial neural networks in the 1980s.Computational devices were created in CMOS, for both biophysical simulation and neuromorphic computing inspired by the structure and function of the human brain. Nanodevices for very large scale principal components analyses and convolution may create a new class of neural computing because they are fundamentally analog rather than digital (even though the first implementations may use digital devices). Ciresan and colleagues (2010) in Schmidhuber's group showed that despite the vanishing gradient problem, GPUs make backpropagation feasible for many-layered feedforward neural networks.\r\n\r\n\r\n== Contests ==\r\nBetween 2009 and 2012, recurrent neural networks and deep feedforward neural networks developed in Schmidhuber's research group won eight international competitions in pattern recognition and machine learning. For example, the bi-directional and multi-dimensional long short-term memory (LSTM) of Graves et al. won three competitions in connected handwriting recognition at the 2009 International Conference on Document Analysis and Recognition (ICDAR), without any prior knowledge about the three languages to be learned.Ciresan and colleagues won pattern recognition contests, including the IJCNN 2011 Traffic Sign Recognition Competition, the ISBI 2012 Segmentation of Neuronal Structures in Electron Microscopy Stacks challenge and others. Their neural networks were the first pattern recognizers to achieve human-competitive/superhuman performance on benchmarks such as traffic sign recognition (IJCNN 2012), or the MNIST handwritten digits problem.", "start_char_idx": 14480, "end_char_idx": 18962, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "104b72a0-ad9a-4f2d-9e0e-3566926aacac": {"__data__": {"id_": "104b72a0-ad9a-4f2d-9e0e-3566926aacac", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\History of artificial neural networks.txt", "file_name": "History of artificial neural networks.txt", "file_type": "text/plain", "file_size": 19960, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "08db6b24-b93e-40bb-8bc4-355238604f5a", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\History of artificial neural networks.txt", "file_name": "History of artificial neural networks.txt", "file_type": "text/plain", "file_size": 19960, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "32763edc3605646ddd5748404e08034d92f094732e3c86b45202d51f7205e731", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d12ee3f5-f57e-4046-ba66-5e25f2f4d9fd", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\History of artificial neural networks.txt", "file_name": "History of artificial neural networks.txt", "file_type": "text/plain", "file_size": 19960, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "b770a7cebcf2020a167e7231cbf35795a785ebe4ba39f829c7f75ad3e32a7c45", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a1303869-5250-4660-8a0a-49e048b24471", "node_type": "1", "metadata": {}, "hash": "5b1b2ed82ab3d15ca34c3e58bed230bd65ea32407ffda70a0c296ab409f849e6", "class_name": "RelatedNodeInfo"}}, "text": "== Contests ==\r\nBetween 2009 and 2012, recurrent neural networks and deep feedforward neural networks developed in Schmidhuber's research group won eight international competitions in pattern recognition and machine learning. For example, the bi-directional and multi-dimensional long short-term memory (LSTM) of Graves et al. won three competitions in connected handwriting recognition at the 2009 International Conference on Document Analysis and Recognition (ICDAR), without any prior knowledge about the three languages to be learned.Ciresan and colleagues won pattern recognition contests, including the IJCNN 2011 Traffic Sign Recognition Competition, the ISBI 2012 Segmentation of Neuronal Structures in Electron Microscopy Stacks challenge and others. Their neural networks were the first pattern recognizers to achieve human-competitive/superhuman performance on benchmarks such as traffic sign recognition (IJCNN 2012), or the MNIST handwritten digits problem.\r\nResearchers demonstrated (2010) that deep neural networks interfaced to a hidden Markov model with context-dependent states that define the neural network output layer can drastically reduce errors in large-vocabulary speech recognition tasks such as voice search.GPU-based implementations of this approach won many pattern recognition contests, including the IJCNN 2011 Traffic Sign Recognition Competition, the ISBI 2012 Segmentation of neuronal structures in EM stacks challenge, the ImageNet Competition and others.\r\nDeep, highly nonlinear neural architectures similar to the neocognitron and the \"standard architecture of vision\", inspired by simple and complex cells, were pre-trained with unsupervised methods by Hinton. A team from his lab won a 2012 contest sponsored by Merck to design software to help find molecules that might identify new drugs.\r\n\r\n\r\n== Notes ==\r\n\r\n\r\n== References ==\r\n\r\n\r\n== External links ==\r\n\"Lecun 2019-7-11 ACM Tech Talk\". Google Docs. Retrieved 2020-02-13.", "start_char_idx": 17992, "end_char_idx": 19956, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a1303869-5250-4660-8a0a-49e048b24471": {"__data__": {"id_": "a1303869-5250-4660-8a0a-49e048b24471", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Language model.txt", "file_name": "Language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b6d044df-fa62-42ce-9a08-dd7e535d47c4", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Language model.txt", "file_name": "Language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "2d8f9779951f0c19a17511035efd4f237df80fe1cb343506834e21fc58e84ac9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "104b72a0-ad9a-4f2d-9e0e-3566926aacac", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\History of artificial neural networks.txt", "file_name": "History of artificial neural networks.txt", "file_type": "text/plain", "file_size": 19960, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "14c67adfb0315780f6ca4e66781857b952a8318f34b9732608726e142f0fc3ae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a9f464e4-cc53-43d3-a403-cdee2b520204", "node_type": "1", "metadata": {}, "hash": "22ad2924e3f103d62956a07739629ba0fe66edc01e62c52f84d21ea6364b59a8", "class_name": "RelatedNodeInfo"}}, "text": "A large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification. LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process. LLMs can be used for text generation, a form of generative AI, by taking an input text and repeatedly predicting the next token or word.LLMs are artificial neural networks. The largest and most capable, as of March 2024, are built with a decoder-only transformer-based architecture while some recent implementations are based on other architectures, such as recurrent neural network variants and Mamba (a state space model).Up to 2020, fine tuning was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models, such as GPT-3, however, can be prompt-engineered to achieve similar results. They are thought to acquire knowledge about syntax, semantics and \"ontology\" inherent in human language corpora, but also inaccuracies and biases present in the corpora.Some notable LLMs are OpenAI's GPT series of models (e.g., GPT-3.5 and GPT-4, used in ChatGPT and Microsoft Copilot), Google's PaLM and Gemini (the latter of which is currently used in the chatbot of the same name), xAI's Grok, Meta's LLaMA family of open-source models, Anthropic's Claude models, and Mistral AI's open source models.\r\n\r\n\r\n== History ==\r\nAt the 2017 NeurIPS conference, Google researchers introduced the transformer architecture in their landmark paper \"Attention Is All You Need\". This paper's goal was to improve upon 2014 Seq2seq technology,  and was based mainly on the attention mechanism developed by Bahdanau et al. in 2014. The following year in 2018, BERT was introduced and quickly became \"ubiquitous\". Though the original transformer has both encoder and decoder blocks, BERT is an encoder-only model.\r\nAlthough decoder-only GPT-1 was introduced in 2018, it was GPT-2 in 2019 that caught widespread attention because OpenAI at first deemed it too powerful to release publicly, out of fear of malicious use. GPT-3 in 2020 went a step further and as of 2024 is available only via API with no offering of downloading the model to execute locally. But it was the 2022 consumer-facing browser-based ChatGPT that captured the imaginations of the general population and caused some media hype and online buzz. The 2023 GPT-4 was praised for its increased accuracy and as a \"holy grail\" for its multimodal capabilities. OpenAI did not reveal high-level architecture and the number of parameters of GPT-4.\r\nIn the meantime, competing language models have for the most part been playing catch-up to the GPT series, at least in terms of number of parameters. Notable exceptions in terms of either number of parameters or measured accuracy include Google's 2019 T5-11B and 2022 PaLM-E, and Anthropic's 2024 Claude 3. In terms of Elo ratings, on January 26, 2024, Google's Bard (Gemini Pro) surpassed the regular GPT-4, but not the limited-availability GPT-4-Turbo.Since 2022, source-available models have been gaining popularity, especially at first with BLOOM and LLaMA, though both have restrictions on the field of use. Mistral AI's models Mistral 7B and Mixtral 8x7b have the more permissive Apache License. As of January 2024, Mixtral 8x7b is the most powerful open LLM according to the LMSYS Chatbot Arena Leaderboard, being more powerful than GPT-3.5 but not as powerful as GPT-4.\r\n\r\n\r\n== Dataset preprocessing ==\r\n\r\n\r\n=== Probabilistic tokenization ===\r\nBecause machine learning algorithms process numbers rather than text, the text must be converted to numbers. In the first step, a vocabulary is decided upon, then integer indexes are arbitrarily but uniquely assigned to each vocabulary entry, and finally, an embedding is associated to the integer index. Algorithms include byte-pair encoding and WordPiece.\r\nProbabilistic tokenization also compresses the datasets. Because LLMs generally require input to be an array that is not jagged, the shorter texts must be \"padded\" until they match the length of the longest one.", "start_char_idx": 0, "end_char_idx": 4210, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a9f464e4-cc53-43d3-a403-cdee2b520204": {"__data__": {"id_": "a9f464e4-cc53-43d3-a403-cdee2b520204", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Language model.txt", "file_name": "Language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b6d044df-fa62-42ce-9a08-dd7e535d47c4", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Language model.txt", "file_name": "Language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "2d8f9779951f0c19a17511035efd4f237df80fe1cb343506834e21fc58e84ac9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a1303869-5250-4660-8a0a-49e048b24471", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Language model.txt", "file_name": "Language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "3b27fbd9839450dd7c25ceaddd75a59d1d815edd3f50d2004e18aa37b3d48cb7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "851ec5d2-4952-4ecf-afee-ef5cdd8b545f", "node_type": "1", "metadata": {}, "hash": "55c5a2a0ecf031b8d3137dd5e8a69066626a8cb6611d8386473b28a069f19b31", "class_name": "RelatedNodeInfo"}}, "text": "As of January 2024, Mixtral 8x7b is the most powerful open LLM according to the LMSYS Chatbot Arena Leaderboard, being more powerful than GPT-3.5 but not as powerful as GPT-4.\r\n\r\n\r\n== Dataset preprocessing ==\r\n\r\n\r\n=== Probabilistic tokenization ===\r\nBecause machine learning algorithms process numbers rather than text, the text must be converted to numbers. In the first step, a vocabulary is decided upon, then integer indexes are arbitrarily but uniquely assigned to each vocabulary entry, and finally, an embedding is associated to the integer index. Algorithms include byte-pair encoding and WordPiece.\r\nProbabilistic tokenization also compresses the datasets. Because LLMs generally require input to be an array that is not jagged, the shorter texts must be \"padded\" until they match the length of the longest one. How many tokens are, on average, needed per word depends on the language of the dataset.\r\n\r\n\r\n==== BPE ====\r\nUsing a modification of byte-pair encoding, in the first step, all unique characters (including blanks and punctuation marks) are treated as an initial set of n-grams (i.e. initial set of uni-grams). Successively the most frequent pair of adjacent characters is merged into a bi-gram and all instances of the pair are replaced by it. All occurrences of adjacent pairs of (previously merged) n-grams that most frequently occur together are then again merged into even lengthier n-gram repeatedly until a vocabulary of prescribed size is obtained (in case of GPT-3, the size is 50257). Token vocabulary consists of integers, spanning from zero up to the size of the token vocabulary. New words can always be interpreted as combinations of the tokens and the initial-set uni-grams.A token vocabulary based on the frequencies extracted from mainly English corpora uses as few tokens as possible for an average English word. An average word in another language encoded by such an English-optimized tokenizer is however split into suboptimal amount of tokens. GPT-2 tokenizer can use up to 15 times more tokens per word for some languages, for example for Shan language from Myanmar. Even more widespread languages such as Portuguese and German have \"a premium of 50%\" compared to English.For example, here is how tokenizer used by GPT-3 (Legacy) split the following sentence tokenizer: texts -> series of numerical \"tokens\".\r\n\r\n\r\n=== Dataset cleaning ===\r\n\r\nIn the context of training LLMs, datasets are typically cleaned by removing toxic passages from the dataset, discarding low-quality data, and de-duplication. Cleaned datasets can increase training efficiency and lead to improved downstream performance.With the increasing proportion of LLM-generated content on the web, data cleaning in the future may include filtering out such content. LLM-generated content can pose a problem if the content is similar to human text (making filtering difficult) but of lower quality (degrading performance of models trained on it).\r\n\r\n\r\n== Training and architecture ==\r\n\r\n\r\n=== Reinforcement learning from human feedback (RLHF) ===\r\nReinforcement learning from human feedback (RLHF) through algorithms, such as proximal policy optimization, is used to further fine-tune a model based on a dataset of human preferences.\r\n\r\n\r\n=== Instruction tuning ===\r\nUsing \"self-instruct\" approaches, LLMs have been able to bootstrap correct responses, replacing any naive responses, starting from human-generated corrections of a few cases. For example, in the instruction \"Write an essay about the main themes represented in Hamlet,\" an initial naive completion might be 'If you submit the essay after March 17, your grade will be reduced by 10% for each day of delay,\" based on the frequency of this textual sequence in the corpus.\r\n\r\n\r\n=== Mixture of experts ===\r\n\r\nThe largest LLM may be too expensive to train and use directly. For such models, mixture of experts (MoE) can be applied, a line of research pursued by Google researchers since 2017 to train models reaching up to 1 trillion parameters.\r\n\r\n\r\n=== Prompt engineering, attention mechanism, and context window ===\r\n\r\nMost results previously achievable only by (costly) fine-tuning, can be achieved through prompt engineering, although limited to the scope of a single conversation (more precisely, limited to the scope of a context window).\r\nIn order to find out which tokens are relevant to each other within the scope of the context window, the attention mechanism calculates \"soft\" weights for each token, more precisely for its embedding, by using multiple attention heads, each with its own \"relevance\" for calculating its own soft weights. For example, the small (i.e.", "start_char_idx": 3390, "end_char_idx": 8033, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "851ec5d2-4952-4ecf-afee-ef5cdd8b545f": {"__data__": {"id_": "851ec5d2-4952-4ecf-afee-ef5cdd8b545f", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Language model.txt", "file_name": "Language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b6d044df-fa62-42ce-9a08-dd7e535d47c4", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Language model.txt", "file_name": "Language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "2d8f9779951f0c19a17511035efd4f237df80fe1cb343506834e21fc58e84ac9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a9f464e4-cc53-43d3-a403-cdee2b520204", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Language model.txt", "file_name": "Language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "4c9c4ff41fe518bb6bfff33ac2d3b97be97d5ef523968b5ef20ae6e2a0c59cd4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bcfbcfda-25ca-4286-96af-9a6162b32ce6", "node_type": "1", "metadata": {}, "hash": "5adba0ad4e5acdba291d9f22d30c564978d137aee2ccacc091af06a311da87b0", "class_name": "RelatedNodeInfo"}}, "text": "=== Mixture of experts ===\r\n\r\nThe largest LLM may be too expensive to train and use directly. For such models, mixture of experts (MoE) can be applied, a line of research pursued by Google researchers since 2017 to train models reaching up to 1 trillion parameters.\r\n\r\n\r\n=== Prompt engineering, attention mechanism, and context window ===\r\n\r\nMost results previously achievable only by (costly) fine-tuning, can be achieved through prompt engineering, although limited to the scope of a single conversation (more precisely, limited to the scope of a context window).\r\nIn order to find out which tokens are relevant to each other within the scope of the context window, the attention mechanism calculates \"soft\" weights for each token, more precisely for its embedding, by using multiple attention heads, each with its own \"relevance\" for calculating its own soft weights. For example, the small (i.e. 117M parameter sized) GPT-2 model, has had twelve attention heads and a context window of only 1k token. In its medium version it has 345M parameters and contains 24 layers, each with 12 attention heads. For the training with gradient descent a batch size of 512 was utilized.The largest models, such as Google's Gemini 1.5, presented in February 2024, can have a context window sized up to 1 million (context window of 10 million was also \"successfully tested\"). Other models with large context windows includes Anthropic's Claude 2.1, with a context window of up to 200k tokens. Note that this maximum refers to the number of input tokens and that the maximum number of output tokens differs from the input and is often smaller. For example, the GPT-4 Turbo model has a maximum output of 4096 tokens.Length of a conversation that the model can take into account when generating its next answer is limited by the size of a context window, as well. If the length of a conversation, for example with Chat-GPT, is longer than its context window, only the parts inside the context window are taken into account when generating the next answer, or the model needs to apply some algorithm to summarize the too distant parts of conversation.\r\nThe shortcomings of making a context window larger include higher computational cost and possibly diluting the focus on local context, while making it smaller can cause a model to miss an important long-range dependency. Balancing them are a matter of experimentation and domain-specific considerations.\r\nA model may be pre-trained either to predict how the segment continues, or what is missing in the segment, given a segment from its training dataset. It can be either\r\n\r\nautoregressive (i.e. predicting how the segment continues, the way GPTs do it): for example given a segment \"I like to eat\", the model predicts \"ice cream\", or \"sushi\".\r\n\"masked\" (i.e. filling in the parts missing from the segment, the way \"BERT\" does it): for example, given a segment \"I like to [__] [__] cream\", the model predicts that \"eat\" and \"ice\" are missing.Models may be trained on auxiliary tasks which test their understanding of the data distribution, such as Next Sentence Prediction (NSP), in which pairs of sentences are presented and the model must predict whether they appear consecutively in the training corpus. During training, regularization loss is also used to stabilize training. However regularization loss is usually not used during testing and evaluation.\r\n\r\n\r\n== Training cost ==\r\nAdvances in software and hardware have reduced the cost substantially since 2020, such that in 2023 training of a 12-billion-parameter LLM computational cost is 72,300 A100-GPU-hours, while in 2020 the cost of training a 1.5-billion-parameter LLM (which was two orders of magnitude smaller than the state of the art in 2020) was between $80 thousand and $1.6 million. Since 2020, large sums were invested in increasingly large models. For example, training of the GPT-2 (i.e. a 1.5-billion-parameters model) in 2019 cost $50,000, while training of the PaLM (i.e. a 540-billion-parameters model) in 2022 cost $8 million, and Megatron-Turing NLG 530B (in 2021) cost around $11 million.For Transformer-based LLM, training cost is much higher than inference cost. It costs 6 FLOPs per parameter to train on one token, whereas it costs 1 to 2 FLOPs per parameter to infer on one token.", "start_char_idx": 7134, "end_char_idx": 11436, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bcfbcfda-25ca-4286-96af-9a6162b32ce6": {"__data__": {"id_": "bcfbcfda-25ca-4286-96af-9a6162b32ce6", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Language model.txt", "file_name": "Language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b6d044df-fa62-42ce-9a08-dd7e535d47c4", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Language model.txt", "file_name": "Language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "2d8f9779951f0c19a17511035efd4f237df80fe1cb343506834e21fc58e84ac9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "851ec5d2-4952-4ecf-afee-ef5cdd8b545f", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Language model.txt", "file_name": "Language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "005703b35f54889a4a0e65a82a72f7c9441165518cd4642fad7e0d655a2c0fe4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd2c7395-c9fe-4151-a0d3-4a4cec6c808d", "node_type": "1", "metadata": {}, "hash": "4d17c12250d740c5f85791c9adb15e77fd72b25255acb45146b3400c33fa4f3b", "class_name": "RelatedNodeInfo"}}, "text": "Since 2020, large sums were invested in increasingly large models. For example, training of the GPT-2 (i.e. a 1.5-billion-parameters model) in 2019 cost $50,000, while training of the PaLM (i.e. a 540-billion-parameters model) in 2022 cost $8 million, and Megatron-Turing NLG 530B (in 2021) cost around $11 million.For Transformer-based LLM, training cost is much higher than inference cost. It costs 6 FLOPs per parameter to train on one token, whereas it costs 1 to 2 FLOPs per parameter to infer on one token.\r\n\r\n\r\n== Tool use ==\r\nThere are certain tasks that, in principle, cannot be solved by any LLM, at least not without the use of external tools or additional software. An example of such a task is responding to the user's input '354 * 139 = ', provided that the LLM has not already encountered a continuation of this calculation in its training corpus. In such cases, the LLM needs to resort to running program code that calculates the result, which can then be included in its response. Another example is 'What is the time now? It is ', where a separate program interpreter would need to execute a code to get system time on the computer, so LLM could include it in its reply. This basic strategy can be sophisticated with multiple attempts of generated programs, and other sampling strategies.\r\nCost Savings and Reduced Vendor Dependency\r\nGenerally, in order to get an LLM to use tools, one must finetune it for tool-use. If the number of tools is finite, then finetuning may be done just once. If the number of tools can grow arbitrarily, as with online API services, then the LLM can be fine-tuned to be able to read API documentation and call API correctly.A simpler form of tool use is Retrieval Augmented Generation: augment an LLM with document retrieval, sometimes using a vector database. Given a query, a document retriever is called to retrieve the most relevant (usually measured by first encoding the query and the documents into vectors, then finding the documents with vectors closest in Euclidean norm to the query vector). The LLM then generates an output based on both the query and the retrieved documents.\r\n\r\n\r\n== Agency ==\r\nAn LLM is a language model, which is not an agent as it has no goal, but it can be used as a component of an intelligent agent. Researchers have described several methods for such integrations.The ReAct (\"Reason + Act\") method constructs an agent out of an LLM, using the LLM as a planner. The LLM is prompted to \"think out loud\". Specifically, the language model is prompted with a textual description of the environment, a goal, a list of possible actions, and a record of the actions and observations so far. It generates one or more thoughts before generating an action, which is then executed in the environment. The linguistic description of the environment given to the LLM planner can even be the LaTeX code of a paper describing the environment.In the DEPS (\"Describe, Explain, Plan and Select\") method, an LLM is first connected to the visual world via image descriptions, then it is prompted to produce plans for complex tasks and behaviors based on its pretrained knowledge and environmental feedback it receives.The Reflexion method constructs an agent that learns over multiple episodes. At the end of each episode, the LLM is given the record of the episode, and prompted to think up \"lessons learned\", which would help it perform better at a subsequent episode. These \"lessons learned\" are given to the agent in the subsequent episodes.Monte Carlo tree search can use an LLM as rollout heuristic. When a programmatic world model is not available, an LLM can also be prompted with a description of the environment to act as world model.For open-ended exploration, an LLM can be used to score observations for their \"interestingness\", which can be used as a reward signal to guide a normal (non-LLM) reinforcement learning agent. Alternatively, it can propose increasingly difficult tasks for curriculum learning. Instead of outputting individual actions, an LLM planner can also construct \"skills\", or functions for complex action sequences. The skills can be stored and later invoked, allowing increasing levels of abstraction in planning.LLM-powered agents can keep a long-term memory of its previous contexts, and the memory can be retrieved in the same way as Retrieval Augmented Generation. Multiple such agents can interact socially.\r\n\r\n\r\n== Compression ==\r\nTypically, LLM are trained with full- or half-precision floating point numbers (float32 and float16).", "start_char_idx": 10924, "end_char_idx": 15461, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd2c7395-c9fe-4151-a0d3-4a4cec6c808d": {"__data__": {"id_": "bd2c7395-c9fe-4151-a0d3-4a4cec6c808d", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Language model.txt", "file_name": "Language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b6d044df-fa62-42ce-9a08-dd7e535d47c4", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Language model.txt", "file_name": "Language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "2d8f9779951f0c19a17511035efd4f237df80fe1cb343506834e21fc58e84ac9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bcfbcfda-25ca-4286-96af-9a6162b32ce6", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Language model.txt", "file_name": "Language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "38c1b2aa2cb10f5bbf3573b3a9fbe2331a58745f99a916397bf92c69cddb3989", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a14c2a1-c786-460a-9629-e197a849150a", "node_type": "1", "metadata": {}, "hash": "404f6aefcbd37d94884a390d634b1877ceea8838464a077a568bc439765d87be", "class_name": "RelatedNodeInfo"}}, "text": "When a programmatic world model is not available, an LLM can also be prompted with a description of the environment to act as world model.For open-ended exploration, an LLM can be used to score observations for their \"interestingness\", which can be used as a reward signal to guide a normal (non-LLM) reinforcement learning agent. Alternatively, it can propose increasingly difficult tasks for curriculum learning. Instead of outputting individual actions, an LLM planner can also construct \"skills\", or functions for complex action sequences. The skills can be stored and later invoked, allowing increasing levels of abstraction in planning.LLM-powered agents can keep a long-term memory of its previous contexts, and the memory can be retrieved in the same way as Retrieval Augmented Generation. Multiple such agents can interact socially.\r\n\r\n\r\n== Compression ==\r\nTypically, LLM are trained with full- or half-precision floating point numbers (float32 and float16). One float16 has 16 bits, or 2 bytes, and so one billion parameters require 2 gigabytes. The largest models typically have 100 billion parameters, requiring 200 gigabytes to load, which places them outside the range of most consumer electronics.Post-training quantization aims to decrease the space requirement by lowering precision of the parameters of a trained model, while preserving most of its performance. The simplest form of quantization simply truncates all numbers to a given number of bits. It can be improved by using a different quantization codebook per layer. Further improvement can be done by applying different precisions to different parameters, with higher precision for particularly important parameters (\"outlier weights\").While quantized models are typically frozen, and only pre-quantized models are fine-tuned, quantized models can still be fine-tuned.\r\n\r\n\r\n== Multimodality ==\r\nMultimodality means \"having several modalities\", and a \"modality\" refers to a type of input or output, such as video, image, audio, text, proprioception, etc. There have been many AI models trained specifically to ingest one modality and output another modality, such as AlexNet for image to label, visual question answering for image-text to text, and speech recognition for speech to text.\r\nA common method to create multimodal models out of an LLM is to \"tokenize\" the output of a trained encoder. Concretely, one can construct a LLM that can understand images as follows: take a trained LLM, and take a trained image encoder E{\\displaystyle E}. Make a small multilayered perceptron f{\\displaystyle f}, so that for any image y{\\displaystyle y}, the post-processed vector f(E(y)){\\displaystyle f(E(y))} has the same dimensions as an encoded token. That is an \"image token\". Then, one can interleave text tokens and image tokens. The compound model is then fine-tuned on an image-text dataset. This basic construction can be applied with more sophistication to improve the model. The image encoder may be frozen to improve stability.Flamingo demonstrated the effectiveness of the tokenization method, finetuning a pair of pretrained language model and image encoder to perform better on visual question answering than models trained from scratch. Google PaLM model was fine-tuned into a multimodal model PaLM-E using the tokenization method, and applied to robotic control. LLaMA models have also been turned multimodal using the tokenization method, to allow image inputs, and video inputs.GPT-4 can use both text and image as inputs (although the vision component wasn't released to the public until GPT-4V); Google DeepMind's Gemini is also multimodal.\r\n\r\n\r\n== Properties ==\r\n\r\n\r\n=== Scaling laws ===\r\n\r\nThe following four hyper-parameters characterize a LLM:\r\n\r\ncost of (pre-)training (C{\\displaystyle C}),\r\nsize of the artificial neural network itself, such as number of parameters N{\\displaystyle N} (i.e. amount of neurons in its layers, amount of weights between them and biases),\r\nsize of its (pre-)training dataset (i.e. number of tokens in corpus, D{\\displaystyle D}),\r\nperformance after (pre-)training.They are related by simple statistical laws, called \"scaling laws\". One particular scaling law (\"Chinchilla scaling\") for LLM autoregressively trained for one epoch, with a log-log learning rate schedule, states that: where the variables are\r\n\r\nC{\\displaystyle C} is the cost of training the model, in FLOPs.\r\nN{\\displaystyle N} is the number of parameters in the model.\r\nD{\\displaystyle D} is the number of tokens in the training set.", "start_char_idx": 14494, "end_char_idx": 19016, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5a14c2a1-c786-460a-9629-e197a849150a": {"__data__": {"id_": "5a14c2a1-c786-460a-9629-e197a849150a", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Language model.txt", "file_name": "Language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b6d044df-fa62-42ce-9a08-dd7e535d47c4", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Language model.txt", "file_name": "Language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "2d8f9779951f0c19a17511035efd4f237df80fe1cb343506834e21fc58e84ac9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd2c7395-c9fe-4151-a0d3-4a4cec6c808d", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Language model.txt", "file_name": "Language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "b95d6b6eb870905a6abf309ab019cc9712d276f438de6691ee79487738572c85", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c5e8606d-dd66-42c7-808a-3a794a97ca8d", "node_type": "1", "metadata": {}, "hash": "90cdb31c098c1516f815de0be5087b99457f55f9bc18cb79a69ca45d5d95e136", "class_name": "RelatedNodeInfo"}}, "text": "amount of neurons in its layers, amount of weights between them and biases),\r\nsize of its (pre-)training dataset (i.e. number of tokens in corpus, D{\\displaystyle D}),\r\nperformance after (pre-)training.They are related by simple statistical laws, called \"scaling laws\". One particular scaling law (\"Chinchilla scaling\") for LLM autoregressively trained for one epoch, with a log-log learning rate schedule, states that: where the variables are\r\n\r\nC{\\displaystyle C} is the cost of training the model, in FLOPs.\r\nN{\\displaystyle N} is the number of parameters in the model.\r\nD{\\displaystyle D} is the number of tokens in the training set.\r\nL{\\displaystyle L} is the average negative log-likelihood loss per token (nats/token), achieved by the trained LLM on the test dataset.and the statistical hyper-parameters are\r\n\r\nC0=6{\\displaystyle C_{0}=6}, meaning that it costs 6 FLOPs per parameter to train on one token. Note that training cost is much higher than inference cost, where it costs 1 to 2 FLOPs per parameter to infer on one token.\r\n\u03b1=0.34,\u03b2=0.28,A=406.4,B=410.7,L0=1.69{\\displaystyle \\alpha =0.34,\\beta =0.28,A=406.4,B=410.7,L_{0}=1.69}\r\n\r\n\r\n=== Emergent abilities ===\r\nWhen one subtracts out from the y-axis the best performance that can be achieved even with infinite scaling of the x-axis quantity, large models' performance, measured on various tasks, seems to be a linear extrapolation of other (smaller-sized and medium-sized) models' performance on a log-log plot. However, sometimes the line's slope transitions from one slope to another at point(s) referred to as break(s) in downstream scaling laws, appearing as a series of linear segments connected by arcs; it seems that larger models acquire \"emergent abilities\" at this point(s). These abilities are discovered rather than programmed-in or designed, in some cases only after the LLM has been publicly deployed.The most intriguing among emergent abilities is in-context learning from example demonstrations. In-context learning is involved in tasks, such as:\r\n\r\nreported arithmetics, decoding the International Phonetic Alphabet, unscrambling a word's letters, disambiguate word in context, converting spatial words, cardinal directions (for example, replying \"northeast\" upon [0, 0, 1; 0, 0, 0; 0, 0, 0]), color terms represented in text.\r\nchain-of-thought prompting: Model outputs are improved by chain-of-thought prompting only when model size exceeds 62B. Smaller models perform better when prompted to answer immediately, without chain of thought.\r\nidentifying offensive content in paragraphs of Hinglish (a combination of Hindi and English), and generating a similar English equivalent of Kiswahili proverbs.Schaeffer et. al. argue that the emergent abilities are not unpredictably acquired, but predictably acquired according to a smooth scaling law. The authors considered a toy statistical model of an LLM solving multiple-choice questions, and showed that this statistical model, modified to account for other types of tasks, applies to these tasks as well.Let x{\\displaystyle x} be the number of parameter count, and y{\\displaystyle y} be the performance of the model.\r\n\r\n\r\n== Interpretation ==\r\nLarge language models by themselves are \"black boxes\", and it is not clear how they can perform linguistic tasks. There are several methods for understanding how LLM work.\r\nMechanistic interpretability aims to reverse-engineer LLM by discovering symbolic algorithms that approximate the inference performed by LLM. One example is Othello-GPT, where a small Transformer is trained to predict legal Othello moves. It is found that there is a linear representation of Othello board, and modifying the representation changes the predicted legal Othello moves in the correct way. In another example, a small Transformer is trained on Karel programs. Similar to the Othello-GPT example, there is a linear representation of Karel program semantics, and modifying the representation changes output in the correct way. The model also generates correct programs that are on average shorter than those in the training set.In another example, the authors trained small transformers on modular arithmetic addition. The resulting models were reverse-engineered, and it turned out they used discrete Fourier transform.", "start_char_idx": 18379, "end_char_idx": 22661, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c5e8606d-dd66-42c7-808a-3a794a97ca8d": {"__data__": {"id_": "c5e8606d-dd66-42c7-808a-3a794a97ca8d", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Language model.txt", "file_name": "Language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b6d044df-fa62-42ce-9a08-dd7e535d47c4", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Language model.txt", "file_name": "Language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "2d8f9779951f0c19a17511035efd4f237df80fe1cb343506834e21fc58e84ac9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5a14c2a1-c786-460a-9629-e197a849150a", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Language model.txt", "file_name": "Language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "eb3ec33a8ec2c384dd705e72e470be350d4e6187f3f758bd8139ac03bdff1a06", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d636e607-8626-4a08-9b76-534b6aeb32a6", "node_type": "1", "metadata": {}, "hash": "2316326dd8e611d1aff4b7d3a9b2dbe1b13fc8537201474c92776d407f35fd69", "class_name": "RelatedNodeInfo"}}, "text": "There are several methods for understanding how LLM work.\r\nMechanistic interpretability aims to reverse-engineer LLM by discovering symbolic algorithms that approximate the inference performed by LLM. One example is Othello-GPT, where a small Transformer is trained to predict legal Othello moves. It is found that there is a linear representation of Othello board, and modifying the representation changes the predicted legal Othello moves in the correct way. In another example, a small Transformer is trained on Karel programs. Similar to the Othello-GPT example, there is a linear representation of Karel program semantics, and modifying the representation changes output in the correct way. The model also generates correct programs that are on average shorter than those in the training set.In another example, the authors trained small transformers on modular arithmetic addition. The resulting models were reverse-engineered, and it turned out they used discrete Fourier transform.\r\n\r\n\r\n=== Understanding and intelligence ===\r\nNLP researchers were evenly split when asked, in a 2022 survey, whether (untuned) LLMs \"could (ever) understand natural language in some nontrivial sense\". Proponents of \"LLM understanding\" believe that some LLM abilities, such as mathematical reasoning, imply an ability to \"understand\" certain concepts. A Microsoft team argued in 2023 that GPT-4 \"can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more\" and that GPT-4 \"could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence system\": \"Can one reasonably say that a system that passes exams for software engineering candidates is not really intelligent?\" Some researchers characterize LLMs as \"alien intelligence\". For example, Conjecture CEO Connor Leahy considers untuned LLMs to be like inscrutable alien \"Shoggoths\", and believes that RLHF tuning creates a \"smiling facade\" obscuring the inner workings of the LLM: \"If you don't push it too far, the smiley face stays on. But then you give it [an unexpected] prompt, and suddenly you see this massive underbelly of insanity, of weird thought processes and clearly non-human understanding.\"In contrast, some proponents of the \"LLMs lack understanding\" school believe that existing LLMs are \"simply remixing and recombining existing writing\", or point to the deficits existing LLMs continue to have in prediction skills, reasoning skills, agency, and explainability. For example, GPT-4 has natural deficits in planning and in real-time learning. Generative LLMs have been observed to confidently assert claims of fact which do not seem to be justified by their training data, a phenomenon which has been termed \"hallucination\". Specifically, hallucinations in the context of LLMs correspond to the generation of text or responses that seem syntactically sound, fluent, and natural but are factually incorrect, nonsensical, or unfaithful to the provided source input. Neuroscientist Terrence Sejnowski has argued that \"The diverging opinions of experts on the intelligence of LLMs suggests that our old ideas based on natural intelligence are inadequate\".The matter of LLM's exhibiting intelligence or understanding has two main aspects \u2013 the first is how to model thought and language in a computer system, and the second is how to enable the computer system to generate human like language. These aspects of language as a model of cognition have been developed in the field of cognitive linguistics. American linguist George Lakoff presented Neural Theory of Language (NTL) as a computational basis for using language as a model of learning tasks and understanding. The NTL Model outlines how specific neural structures of the human brain shape the nature of thought and language and in turn what are the computational properties of such neural systems that can be applied to model thought and language in a computer system. After a framework for modeling language in a computer systems was established, the focus shifted to establishing frameworks for computer systems to generate language with acceptable grammar. In his 2014 book titled The Language Myth: Why Language Is Not An Instinct, British cognitive linguist and digital communication technologist Vyvyan Evans mapped out the role of probabilistic context-free grammar (PCFG) in enabling NLP to model cognitive patterns and generate human like language. \r\n\r\n\r\n== Evaluation ==\r\n\r\n\r\n=== Perplexity ===\r\nThe most commonly used measure of a language model's performance is its perplexity on a given text corpus. Perplexity is a measure of how well a model is able to predict the contents of a dataset; the higher the likelihood the model assigns to the dataset, the lower the perplexity.", "start_char_idx": 21672, "end_char_idx": 26463, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d636e607-8626-4a08-9b76-534b6aeb32a6": {"__data__": {"id_": "d636e607-8626-4a08-9b76-534b6aeb32a6", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Language model.txt", "file_name": "Language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b6d044df-fa62-42ce-9a08-dd7e535d47c4", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Language model.txt", "file_name": "Language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "2d8f9779951f0c19a17511035efd4f237df80fe1cb343506834e21fc58e84ac9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c5e8606d-dd66-42c7-808a-3a794a97ca8d", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Language model.txt", "file_name": "Language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "4b5874f731c3faddfd0861d7129714349793e8f560d38644d4a1a9b10b66d8be", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4d20d041-0bde-4d51-9c99-9037511755eb", "node_type": "1", "metadata": {}, "hash": "8b8269b6c93f46001efd1ca915843b280ebab0efff6dabf0b56b42fb37b51fc4", "class_name": "RelatedNodeInfo"}}, "text": "After a framework for modeling language in a computer systems was established, the focus shifted to establishing frameworks for computer systems to generate language with acceptable grammar. In his 2014 book titled The Language Myth: Why Language Is Not An Instinct, British cognitive linguist and digital communication technologist Vyvyan Evans mapped out the role of probabilistic context-free grammar (PCFG) in enabling NLP to model cognitive patterns and generate human like language. \r\n\r\n\r\n== Evaluation ==\r\n\r\n\r\n=== Perplexity ===\r\nThe most commonly used measure of a language model's performance is its perplexity on a given text corpus. Perplexity is a measure of how well a model is able to predict the contents of a dataset; the higher the likelihood the model assigns to the dataset, the lower the perplexity. Mathematically, perplexity is defined as the exponential of the average negative log likelihood per token:here N{\\displaystyle N} is the number of tokens in the text corpus, and \"context for token i{\\displaystyle i}\" depends on the specific type of LLM used. If the LLM is autoregressive, then \"context for token i{\\displaystyle i}\" is the segment of text appearing before token i{\\displaystyle i}. If the LLM is masked, then \"context for token i{\\displaystyle i}\" is the segment of text surrounding token i{\\displaystyle i}.\r\nBecause language models may overfit to their training data, models are usually evaluated by their perplexity on a test set of unseen data. This presents particular challenges for the evaluation of large language models. As they are trained on increasingly large corpora of text largely scraped from the web, it becomes increasingly likely that models' training data inadvertently includes portions of any given test set.\r\n\r\n\r\n==== BPW, BPC, and BPT ====\r\nIn information theory, the concept of entropy is intricately linked to perplexity, a relationship notably established by Claude Shannon. This relationship is mathematically expressed as Entropy=log2\u2061(Perplexity){\\displaystyle {\\text{Entropy}}=\\log _{2}({\\text{Perplexity}})}.\r\nEntropy, in this context, is commonly quantified in terms of bits per word (BPW) or bits per character (BPC), which hinges on whether the language model utilizes word-based or character-based tokenization.\r\nNotably, in the case of larger language models that predominantly employ sub-word tokenization, bits per token (BPT) emerges as a seemingly more appropriate measure. However, due to the variance in tokenization methods across different Large Language Models (LLMs), BPT does not serve as a reliable metric for comparative analysis among diverse models. To convert BPT into BPW, one can multiply it by the average number of tokens per word.\r\nIn the evaluation and comparison of language models, cross-entropy is generally the preferred metric over entropy. The underlying principle is that a lower BPW is indicative of a model's enhanced capability for compression. This, in turn, reflects the model's proficiency in making accurate predictions.\r\n\r\n\r\n=== Task-specific datasets and benchmarks ===\r\nA large number of testing datasets and benchmarks have also been developed to evaluate the capabilities of language models on more specific downstream tasks. Tests may be designed to evaluate a variety of capabilities, including general knowledge, commonsense reasoning, and mathematical problem-solving.\r\nOne broad category of evaluation dataset is question answering datasets, consisting of pairs of questions and correct answers, for example, (\"Have the San Jose Sharks won the Stanley Cup?\", \"No\"). A question answering task is considered \"open book\" if the model's prompt includes text from which the expected answer can be derived (for example, the previous question could be adjoined with some text which includes the sentence \"The Sharks have advanced to the Stanley Cup finals once, losing to the Pittsburgh Penguins in 2016.\"). Otherwise, the task is considered \"closed book\", and the model must draw on knowledge retained during training. Some examples of commonly used question answering datasets include TruthfulQA, Web Questions, TriviaQA, and SQuAD.Evaluation datasets may also take the form of text completion, having the model select the most likely word or sentence to complete a prompt, for example: \"Alice was friends with Bob. Alice went to visit her friend, ____\".Some composite benchmarks have also been developed which combine a diversity of different evaluation datasets and tasks. Examples include GLUE, SuperGLUE, MMLU, BIG-bench, and HELM.It was previously standard to report results on a heldout portion of an evaluation dataset after doing supervised fine-tuning on the remainder.", "start_char_idx": 25644, "end_char_idx": 30337, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4d20d041-0bde-4d51-9c99-9037511755eb": {"__data__": {"id_": "4d20d041-0bde-4d51-9c99-9037511755eb", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Language model.txt", "file_name": "Language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b6d044df-fa62-42ce-9a08-dd7e535d47c4", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Language model.txt", "file_name": "Language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "2d8f9779951f0c19a17511035efd4f237df80fe1cb343506834e21fc58e84ac9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d636e607-8626-4a08-9b76-534b6aeb32a6", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Language model.txt", "file_name": "Language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "82e8d0e80047166135a2c4c892c2cd82fcab5bda6ad362e4ab6af38290dddb7b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6cec6a3d-cb13-42f3-b385-a7d463bc9e39", "node_type": "1", "metadata": {}, "hash": "2d1e3d5e063fb623f01db963cc92c8602813318431cf014fdf5050e52075dffe", "class_name": "RelatedNodeInfo"}}, "text": "Otherwise, the task is considered \"closed book\", and the model must draw on knowledge retained during training. Some examples of commonly used question answering datasets include TruthfulQA, Web Questions, TriviaQA, and SQuAD.Evaluation datasets may also take the form of text completion, having the model select the most likely word or sentence to complete a prompt, for example: \"Alice was friends with Bob. Alice went to visit her friend, ____\".Some composite benchmarks have also been developed which combine a diversity of different evaluation datasets and tasks. Examples include GLUE, SuperGLUE, MMLU, BIG-bench, and HELM.It was previously standard to report results on a heldout portion of an evaluation dataset after doing supervised fine-tuning on the remainder. It is now more common to evaluate a pre-trained model directly through prompting techniques, though researchers vary in the details of how they formulate prompts for particular tasks, particularly with respect to how many examples of solved tasks are adjoined to the prompt (i.e. the value of n in n-shot prompting).\r\n\r\n\r\n==== Adversarially constructed evaluations ====\r\nBecause of the rapid pace of improvement of large language models, evaluation benchmarks have suffered from short lifespans, with state of the art models quickly \"saturating\" existing benchmarks, exceeding the performance of human annotators, leading to efforts to replace or augment the benchmark with more challenging tasks. In addition, there are cases of \"shortcut learning\" wherein AIs sometimes \"cheat\" on multiple-choice tests by using statistical correlations in superficial test question wording in order to guess the correct responses, without necessarily understanding the actual question being asked.Some datasets have been constructed adversarially, focusing on particular problems on which extant language models seem to have unusually poor performance compared to humans. One example is the TruthfulQA dataset, a question answering dataset consisting of 817 questions which language models are susceptible to answering incorrectly by mimicking falsehoods to which they were repeatedly exposed during training. For example, an LLM may answer \"No\" to the question \"Can you teach an old dog new tricks?\" because of its exposure to the English idiom you can't teach an old dog new tricks, even though this is not literally true.Another example of an adversarial evaluation dataset is Swag and its successor, HellaSwag, collections of problems in which one of multiple options must be selected to complete a text passage. The incorrect completions were generated by sampling from a language model and filtering with a set of classifiers. The resulting problems are trivial for humans but at the time the datasets were created state of the art language models had poor accuracy on them. For example:\r\n\r\nWe see a fitness center sign. We then see a man talking to the camera and sitting and laying on a exercise ball. The man...\r\na) demonstrates how to increase efficient exercise work by running up and down balls.\r\nb) moves all his arms and legs and builds up a lot of muscle.\r\nc) then plays the ball and we see a graphics and hedge trimming demonstration.\r\nd) performs sit ups while on the ball and talking.\r\n\r\nBERT selects b) as the most likely completion, though the correct answer is d).\r\n\r\n\r\n== Wider impact ==\r\nIn 2023, Nature Biomedical Engineering wrote that \"it is no longer possible to accurately distinguish\" human-written text from text created by large language models, and that \"It is all but certain that general-purpose large language models will rapidly proliferate... It is a rather safe bet that they will change many industries over time.\" Goldman Sachs suggested in 2023 that generative language AI could increase global GDP by 7% in the next ten years, and could expose to automation 300 million jobs globally.\r\n\r\n\r\n=== Copyright ===\r\nMemorization is an emergent behavior in LLMs in which long strings of text are occasionally output verbatim from training data, contrary to typical behavior of traditional artificial neural nets. Evaluations of controlled LLM output measure the amount memorized from training data (focused on GPT-2-series models) as variously over 1% for exact duplicates or up to about 7%.\r\n\r\n\r\n=== Security ===\r\nSome commenters expressed concern over accidental or deliberate creation of misinformation, or other forms of misuse. For example, the availability of large language models could reduce the skill-level required to commit bioterrorism; biosecurity researcher Kevin Esvelt has suggested that LLM creators should exclude from their training data papers on creating or enhancing pathogens.A study by researchers at Google and several universities, including Cornell University and University of California, Berkeley, showed that there are potential security risks in language models such as ChatGPT.", "start_char_idx": 29565, "end_char_idx": 34468, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6cec6a3d-cb13-42f3-b385-a7d463bc9e39": {"__data__": {"id_": "6cec6a3d-cb13-42f3-b385-a7d463bc9e39", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Language model.txt", "file_name": "Language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b6d044df-fa62-42ce-9a08-dd7e535d47c4", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Language model.txt", "file_name": "Language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "2d8f9779951f0c19a17511035efd4f237df80fe1cb343506834e21fc58e84ac9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d20d041-0bde-4d51-9c99-9037511755eb", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Language model.txt", "file_name": "Language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "5f17bfed3916880ded70579bbed25c5dd9c7256d3cdad466fcd58c3ef154805a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9df1a7c6-4710-4278-8eeb-10e170eb926b", "node_type": "1", "metadata": {}, "hash": "d7180915abf74579eeae292ed0e378eb6c8c4fa25c9d700411c82e652aa89726", "class_name": "RelatedNodeInfo"}}, "text": "=== Copyright ===\r\nMemorization is an emergent behavior in LLMs in which long strings of text are occasionally output verbatim from training data, contrary to typical behavior of traditional artificial neural nets. Evaluations of controlled LLM output measure the amount memorized from training data (focused on GPT-2-series models) as variously over 1% for exact duplicates or up to about 7%.\r\n\r\n\r\n=== Security ===\r\nSome commenters expressed concern over accidental or deliberate creation of misinformation, or other forms of misuse. For example, the availability of large language models could reduce the skill-level required to commit bioterrorism; biosecurity researcher Kevin Esvelt has suggested that LLM creators should exclude from their training data papers on creating or enhancing pathogens.A study by researchers at Google and several universities, including Cornell University and University of California, Berkeley, showed that there are potential security risks in language models such as ChatGPT. In their study, they examined the possibility that questioners could get, from ChatGPT, the training data that the AI model used; they found that they could get the training data from the AI model. For example, when asking ChatGPT 3.5 turbo to repeat the word \"poem\" forever, the AI model will say \"poem\" hundreds of times and then diverge, deviating from the standard dialogue style and spitting out nonsense phrases, thus spitting out the training data as it is. The researchers have seen more than 10,000 examples of the AI model exposing their training data in a similar method. The researchers said that it was hard to tell if the AI model was actually safe or not.The potential presence of \"sleeper agents\" within LLM models is another emerging security concern. These are hidden functionalities built into the model that remain dormant until triggered by a specific event or condition. Upon activation, the LLM deviates from its expected behavior to make insecure actions.\r\n\r\n\r\n=== Algorithmic bias ===\r\n\r\nWhile LLMs have shown remarkable capabilities in generating human-like text, they are susceptible to inheriting and amplifying biases present in their training data. This can manifest in skewed representations or unfair treatment of different demographics, such as those based on race, gender, language, and cultural groups. Since English data is overrepresented in current large language models' training data, it may also downplay non-English views.\r\n\r\n\r\n==== Stereotyping ====\r\nAI models can reinforce a wide range of stereotypes, including those based on gender, ethnicity, age, nationality, religion, or occupation. This can lead to outputs that unfairly generalize or caricature groups of people, sometimes in harmful or derogatory ways.Notably, gender bias refers to the tendency of these models to produce outputs that are unfairly prejudiced towards one gender over another. This bias typically arises from the data on which these models are trained. Large language models often assign roles and characteristics based on traditional gender norms. For example, it might associate nurses or secretaries predominantly with women and engineers or CEOs with men.\r\n\r\n\r\n==== Political bias ====\r\nPolitical bias refers to the tendency of algorithms to systematically favor certain political viewpoints, ideologies, or outcomes over others. Language models may also exhibit political biases. Since the training data includes a wide range of political opinions and coverage, the models might generate responses that lean towards particular political ideologies or viewpoints, depending on the prevalence of those views in the data.\r\n\r\n\r\n== List ==\r\n\r\nFor the training cost column, 1 petaFLOP-day = 1 petaFLOP/sec \u00d7 1 day = 8.64E19 FLOP.\r\n\r\n\r\n== See also ==\r\nFoundation models\r\n\r\n\r\n== Notes ==\r\n\r\n\r\n== References ==\r\n\r\n\r\n== Further reading ==\r\nJurafsky, Dan, Martin, James. H. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition, 3rd Edition draft, 2023.\r\nPhuong, Mary; Hutter, Marcus (2022). \"Formal Algorithms for Transformers\". arXiv:2207.09238 [cs.LG].\r\nEloundou, Tyna; Manning, Sam; Mishkin, Pamela; Rock, Daniel (2023). \"GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models\". arXiv:2303.10130 [econ.GN].\r\nEldan, Ronen; Li, Yuanzhi (2023). \"TinyStories: How Small Can Language Models Be and Still Speak Coherent English?\". arXiv:2305.07759 [cs.CL].\r\nFrank, Michael C. (27 June 2023).", "start_char_idx": 33456, "end_char_idx": 37984, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9df1a7c6-4710-4278-8eeb-10e170eb926b": {"__data__": {"id_": "9df1a7c6-4710-4278-8eeb-10e170eb926b", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Language model.txt", "file_name": "Language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b6d044df-fa62-42ce-9a08-dd7e535d47c4", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Language model.txt", "file_name": "Language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "2d8f9779951f0c19a17511035efd4f237df80fe1cb343506834e21fc58e84ac9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6cec6a3d-cb13-42f3-b385-a7d463bc9e39", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Language model.txt", "file_name": "Language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "0a81be320b153597ca861ae7204200d532ca3ad69cf03c82c1950753c78104a8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "66b3d6cf-184f-4521-8fee-73874b0cf634", "node_type": "1", "metadata": {}, "hash": "5b1b2ed82ab3d15ca34c3e58bed230bd65ea32407ffda70a0c296ab409f849e6", "class_name": "RelatedNodeInfo"}}, "text": "H. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition, 3rd Edition draft, 2023.\r\nPhuong, Mary; Hutter, Marcus (2022). \"Formal Algorithms for Transformers\". arXiv:2207.09238 [cs.LG].\r\nEloundou, Tyna; Manning, Sam; Mishkin, Pamela; Rock, Daniel (2023). \"GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models\". arXiv:2303.10130 [econ.GN].\r\nEldan, Ronen; Li, Yuanzhi (2023). \"TinyStories: How Small Can Language Models Be and Still Speak Coherent English?\". arXiv:2305.07759 [cs.CL].\r\nFrank, Michael C. (27 June 2023). \"Baby steps in evaluating the capacities of large language models\". Nature Reviews Psychology. 2 (8): 451\u2013452. doi:10.1038/s44159-023-00211-x. ISSN 2731-0574. S2CID 259713140. Retrieved 2 July 2023.\r\nZhao, Wayne Xin; et al. (2023). \"A Survey of Large Language Models\". arXiv:2303.18223 [cs.CL].\r\nKaddour, Jean; et al. (2023). \"Challenges and Applications of Large Language Models\". arXiv:2307.10169 [cs.CL].\r\nYin, Shukang; Fu, Chaoyou; Zhao, Sirui; Li, Ke; Sun, Xing; Xu, Tong; Chen, Enhong (2023-06-01). \"A Survey on Multimodal Large Language Models\". arXiv:2306.13549 [cs.CV].\r\nOpen LLMs repository on GitHub.", "start_char_idx": 37354, "end_char_idx": 38596, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "66b3d6cf-184f-4521-8fee-73874b0cf634": {"__data__": {"id_": "66b3d6cf-184f-4521-8fee-73874b0cf634", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Large language model.txt", "file_name": "Large language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ebe409b7-516e-4dac-aa8f-60c282a209da", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Large language model.txt", "file_name": "Large language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "c8db0d2eb49f36aef1f5ada4fd9779850799eee4005db290bc86d84d10c83b3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9df1a7c6-4710-4278-8eeb-10e170eb926b", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Language model.txt", "file_name": "Language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "5c78eccc1f957cbb250c18b6e40ee3b686bb0d9bdbec960f6686da35d1f53a07", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "81833af5-dcc8-4f6d-9988-898c5fe855c3", "node_type": "1", "metadata": {}, "hash": "22ad2924e3f103d62956a07739629ba0fe66edc01e62c52f84d21ea6364b59a8", "class_name": "RelatedNodeInfo"}}, "text": "A large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification. LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process. LLMs can be used for text generation, a form of generative AI, by taking an input text and repeatedly predicting the next token or word.LLMs are artificial neural networks. The largest and most capable, as of March 2024, are built with a decoder-only transformer-based architecture while some recent implementations are based on other architectures, such as recurrent neural network variants and Mamba (a state space model).Up to 2020, fine tuning was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models, such as GPT-3, however, can be prompt-engineered to achieve similar results. They are thought to acquire knowledge about syntax, semantics and \"ontology\" inherent in human language corpora, but also inaccuracies and biases present in the corpora.Some notable LLMs are OpenAI's GPT series of models (e.g., GPT-3.5 and GPT-4, used in ChatGPT and Microsoft Copilot), Google's PaLM and Gemini (the latter of which is currently used in the chatbot of the same name), xAI's Grok, Meta's LLaMA family of open-source models, Anthropic's Claude models, and Mistral AI's open source models.\r\n\r\n\r\n== History ==\r\nAt the 2017 NeurIPS conference, Google researchers introduced the transformer architecture in their landmark paper \"Attention Is All You Need\". This paper's goal was to improve upon 2014 Seq2seq technology,  and was based mainly on the attention mechanism developed by Bahdanau et al. in 2014. The following year in 2018, BERT was introduced and quickly became \"ubiquitous\". Though the original transformer has both encoder and decoder blocks, BERT is an encoder-only model.\r\nAlthough decoder-only GPT-1 was introduced in 2018, it was GPT-2 in 2019 that caught widespread attention because OpenAI at first deemed it too powerful to release publicly, out of fear of malicious use. GPT-3 in 2020 went a step further and as of 2024 is available only via API with no offering of downloading the model to execute locally. But it was the 2022 consumer-facing browser-based ChatGPT that captured the imaginations of the general population and caused some media hype and online buzz. The 2023 GPT-4 was praised for its increased accuracy and as a \"holy grail\" for its multimodal capabilities. OpenAI did not reveal high-level architecture and the number of parameters of GPT-4.\r\nIn the meantime, competing language models have for the most part been playing catch-up to the GPT series, at least in terms of number of parameters. Notable exceptions in terms of either number of parameters or measured accuracy include Google's 2019 T5-11B and 2022 PaLM-E, and Anthropic's 2024 Claude 3. In terms of Elo ratings, on January 26, 2024, Google's Bard (Gemini Pro) surpassed the regular GPT-4, but not the limited-availability GPT-4-Turbo.Since 2022, source-available models have been gaining popularity, especially at first with BLOOM and LLaMA, though both have restrictions on the field of use. Mistral AI's models Mistral 7B and Mixtral 8x7b have the more permissive Apache License. As of January 2024, Mixtral 8x7b is the most powerful open LLM according to the LMSYS Chatbot Arena Leaderboard, being more powerful than GPT-3.5 but not as powerful as GPT-4.\r\n\r\n\r\n== Dataset preprocessing ==\r\n\r\n\r\n=== Probabilistic tokenization ===\r\nBecause machine learning algorithms process numbers rather than text, the text must be converted to numbers. In the first step, a vocabulary is decided upon, then integer indexes are arbitrarily but uniquely assigned to each vocabulary entry, and finally, an embedding is associated to the integer index. Algorithms include byte-pair encoding and WordPiece.\r\nProbabilistic tokenization also compresses the datasets. Because LLMs generally require input to be an array that is not jagged, the shorter texts must be \"padded\" until they match the length of the longest one.", "start_char_idx": 0, "end_char_idx": 4210, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "81833af5-dcc8-4f6d-9988-898c5fe855c3": {"__data__": {"id_": "81833af5-dcc8-4f6d-9988-898c5fe855c3", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Large language model.txt", "file_name": "Large language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ebe409b7-516e-4dac-aa8f-60c282a209da", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Large language model.txt", "file_name": "Large language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "c8db0d2eb49f36aef1f5ada4fd9779850799eee4005db290bc86d84d10c83b3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "66b3d6cf-184f-4521-8fee-73874b0cf634", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Large language model.txt", "file_name": "Large language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "31e4edfc07f2b5af181bd132d0e0e341af3bbd5d4e4d08b3442bfa168e2f26f1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3ddd7211-4b50-4aef-9499-faf9dce24e85", "node_type": "1", "metadata": {}, "hash": "55c5a2a0ecf031b8d3137dd5e8a69066626a8cb6611d8386473b28a069f19b31", "class_name": "RelatedNodeInfo"}}, "text": "As of January 2024, Mixtral 8x7b is the most powerful open LLM according to the LMSYS Chatbot Arena Leaderboard, being more powerful than GPT-3.5 but not as powerful as GPT-4.\r\n\r\n\r\n== Dataset preprocessing ==\r\n\r\n\r\n=== Probabilistic tokenization ===\r\nBecause machine learning algorithms process numbers rather than text, the text must be converted to numbers. In the first step, a vocabulary is decided upon, then integer indexes are arbitrarily but uniquely assigned to each vocabulary entry, and finally, an embedding is associated to the integer index. Algorithms include byte-pair encoding and WordPiece.\r\nProbabilistic tokenization also compresses the datasets. Because LLMs generally require input to be an array that is not jagged, the shorter texts must be \"padded\" until they match the length of the longest one. How many tokens are, on average, needed per word depends on the language of the dataset.\r\n\r\n\r\n==== BPE ====\r\nUsing a modification of byte-pair encoding, in the first step, all unique characters (including blanks and punctuation marks) are treated as an initial set of n-grams (i.e. initial set of uni-grams). Successively the most frequent pair of adjacent characters is merged into a bi-gram and all instances of the pair are replaced by it. All occurrences of adjacent pairs of (previously merged) n-grams that most frequently occur together are then again merged into even lengthier n-gram repeatedly until a vocabulary of prescribed size is obtained (in case of GPT-3, the size is 50257). Token vocabulary consists of integers, spanning from zero up to the size of the token vocabulary. New words can always be interpreted as combinations of the tokens and the initial-set uni-grams.A token vocabulary based on the frequencies extracted from mainly English corpora uses as few tokens as possible for an average English word. An average word in another language encoded by such an English-optimized tokenizer is however split into suboptimal amount of tokens. GPT-2 tokenizer can use up to 15 times more tokens per word for some languages, for example for Shan language from Myanmar. Even more widespread languages such as Portuguese and German have \"a premium of 50%\" compared to English.For example, here is how tokenizer used by GPT-3 (Legacy) split the following sentence tokenizer: texts -> series of numerical \"tokens\".\r\n\r\n\r\n=== Dataset cleaning ===\r\n\r\nIn the context of training LLMs, datasets are typically cleaned by removing toxic passages from the dataset, discarding low-quality data, and de-duplication. Cleaned datasets can increase training efficiency and lead to improved downstream performance.With the increasing proportion of LLM-generated content on the web, data cleaning in the future may include filtering out such content. LLM-generated content can pose a problem if the content is similar to human text (making filtering difficult) but of lower quality (degrading performance of models trained on it).\r\n\r\n\r\n== Training and architecture ==\r\n\r\n\r\n=== Reinforcement learning from human feedback (RLHF) ===\r\nReinforcement learning from human feedback (RLHF) through algorithms, such as proximal policy optimization, is used to further fine-tune a model based on a dataset of human preferences.\r\n\r\n\r\n=== Instruction tuning ===\r\nUsing \"self-instruct\" approaches, LLMs have been able to bootstrap correct responses, replacing any naive responses, starting from human-generated corrections of a few cases. For example, in the instruction \"Write an essay about the main themes represented in Hamlet,\" an initial naive completion might be 'If you submit the essay after March 17, your grade will be reduced by 10% for each day of delay,\" based on the frequency of this textual sequence in the corpus.\r\n\r\n\r\n=== Mixture of experts ===\r\n\r\nThe largest LLM may be too expensive to train and use directly. For such models, mixture of experts (MoE) can be applied, a line of research pursued by Google researchers since 2017 to train models reaching up to 1 trillion parameters.\r\n\r\n\r\n=== Prompt engineering, attention mechanism, and context window ===\r\n\r\nMost results previously achievable only by (costly) fine-tuning, can be achieved through prompt engineering, although limited to the scope of a single conversation (more precisely, limited to the scope of a context window).\r\nIn order to find out which tokens are relevant to each other within the scope of the context window, the attention mechanism calculates \"soft\" weights for each token, more precisely for its embedding, by using multiple attention heads, each with its own \"relevance\" for calculating its own soft weights. For example, the small (i.e.", "start_char_idx": 3390, "end_char_idx": 8033, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3ddd7211-4b50-4aef-9499-faf9dce24e85": {"__data__": {"id_": "3ddd7211-4b50-4aef-9499-faf9dce24e85", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Large language model.txt", "file_name": "Large language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ebe409b7-516e-4dac-aa8f-60c282a209da", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Large language model.txt", "file_name": "Large language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "c8db0d2eb49f36aef1f5ada4fd9779850799eee4005db290bc86d84d10c83b3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "81833af5-dcc8-4f6d-9988-898c5fe855c3", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Large language model.txt", "file_name": "Large language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "abacc1f5e508e22100d46b667db3d24be6c3903596755897055a47334e6563ce", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5584ae86-cdfe-4802-8a58-00239a12b1f7", "node_type": "1", "metadata": {}, "hash": "ee954a97c226d6cd522439bc825b812fcc34e8320d7d8ed7e6b86f98bb32b5b8", "class_name": "RelatedNodeInfo"}}, "text": "=== Mixture of experts ===\r\n\r\nThe largest LLM may be too expensive to train and use directly. For such models, mixture of experts (MoE) can be applied, a line of research pursued by Google researchers since 2017 to train models reaching up to 1 trillion parameters.\r\n\r\n\r\n=== Prompt engineering, attention mechanism, and context window ===\r\n\r\nMost results previously achievable only by (costly) fine-tuning, can be achieved through prompt engineering, although limited to the scope of a single conversation (more precisely, limited to the scope of a context window).\r\nIn order to find out which tokens are relevant to each other within the scope of the context window, the attention mechanism calculates \"soft\" weights for each token, more precisely for its embedding, by using multiple attention heads, each with its own \"relevance\" for calculating its own soft weights. For example, the small (i.e. 117M parameter sized) GPT-2 model, has had twelve attention heads and a context window of only 1k token. In its medium version it has 345M parameters and contains 24 layers, each with 12 attention heads. For the training with gradient descent a batch size of 512 was utilized.The largest models, such as Google's Gemini 1.5, presented in February 2024, can have a context window sized up to 1 million (context window of 10 million was also \"successfully tested\"). Other models with large context windows includes Anthropic's Claude 2.1, with a context window of up to 200k tokens. Note that this maximum refers to the number of input tokens and that the maximum number of output tokens differs from the input and is often smaller. For example, the GPT-4 Turbo model has a maximum output of 4096 tokens.Length of a conversation that the model can take into account when generating its next answer is limited by the size of a context window, as well. If the length of a conversation, for example with Chat-GPT, is longer than its context window, only the parts inside the context window are taken into account when generating the next answer, or the model needs to apply some algorithm to summarize the too distant parts of conversation.\r\nThe shortcomings of making a context window larger include higher computational cost and possibly diluting the focus on local context, while making it smaller can cause a model to miss an important long-range dependency. Balancing them are a matter of experimentation and domain-specific considerations.\r\nA model may be pre-trained either to predict how the segment continues, or what is missing in the segment, given a segment from its training dataset. It can be either\r\n\r\nautoregressive (i.e. predicting how the segment continues, the way GPTs do it): for example given a segment \"I like to eat\", the model predicts \"ice cream\", or \"sushi\".\r\n\"masked\" (i.e. filling in the parts missing from the segment, the way \"BERT\" does it): for example, given a segment \"I like to [__] [__] cream\", the model predicts that \"eat\" and \"ice\" are missing.Models may be trained on auxiliary tasks which test their understanding of the data distribution, such as Next Sentence Prediction (NSP), in which pairs of sentences are presented and the model must predict whether they appear consecutively in the training corpus. During training, regularization loss is also used to stabilize training. However regularization loss is usually not used during testing and evaluation.\r\n\r\n\r\n== Training cost ==\r\nAdvances in software and hardware have reduced the cost substantially since 2020, such that in 2023 training of a 12-billion-parameter LLM computational cost is 72,300 A100-GPU-hours, while in 2020 the cost of training a 1.5-billion-parameter LLM (which was two orders of magnitude smaller than the state of the art in 2020) was between $80 thousand and $1.6 million. Since 2020, large sums were invested in increasingly large models. For example, training of the GPT-2 (i.e. a 1.5-billion-parameters model) in 2019 cost $50,000, while training of the PaLM (i.e. a 540-billion-parameters model) in 2022 cost $8 million, and Megatron-Turing NLG 530B (in 2021) cost around $11 million.For Transformer-based LLM, training cost is much higher than inference cost. It costs 6 FLOPs per parameter to train on one token, whereas it costs 1 to 2 FLOPs per parameter to infer on one token.", "start_char_idx": 7134, "end_char_idx": 11436, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5584ae86-cdfe-4802-8a58-00239a12b1f7": {"__data__": {"id_": "5584ae86-cdfe-4802-8a58-00239a12b1f7", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Large language model.txt", "file_name": "Large language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ebe409b7-516e-4dac-aa8f-60c282a209da", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Large language model.txt", "file_name": "Large language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "c8db0d2eb49f36aef1f5ada4fd9779850799eee4005db290bc86d84d10c83b3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3ddd7211-4b50-4aef-9499-faf9dce24e85", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Large language model.txt", "file_name": "Large language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "fde007dd55df0667a1c3dffc325b1717023f96db99b97229dc6080353d6f4056", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "38772f86-2ba7-40df-89bc-16dce75ac534", "node_type": "1", "metadata": {}, "hash": "9dfe89d9f504f1e8e9b6c74058079c7652ebc44e01eb0435d4468da2cdf91619", "class_name": "RelatedNodeInfo"}}, "text": "Since 2020, large sums were invested in increasingly large models. For example, training of the GPT-2 (i.e. a 1.5-billion-parameters model) in 2019 cost $50,000, while training of the PaLM (i.e. a 540-billion-parameters model) in 2022 cost $8 million, and Megatron-Turing NLG 530B (in 2021) cost around $11 million.For Transformer-based LLM, training cost is much higher than inference cost. It costs 6 FLOPs per parameter to train on one token, whereas it costs 1 to 2 FLOPs per parameter to infer on one token.\r\n\r\n\r\n== Tool use ==\r\nThere are certain tasks that, in principle, cannot be solved by any LLM, at least not without the use of external tools or additional software. An example of such a task is responding to the user's input '354 * 139 = ', provided that the LLM has not already encountered a continuation of this calculation in its training corpus. In such cases, the LLM needs to resort to running program code that calculates the result, which can then be included in its response. Another example is 'What is the time now? It is ', where a separate program interpreter would need to execute a code to get system time on the computer, so LLM could include it in its reply. This basic strategy can be sophisticated with multiple attempts of generated programs, and other sampling strategies.\r\nCost Savings and Reduced Vendor Dependency\r\nGenerally, in order to get an LLM to use tools, one must finetune it for tool-use. If the number of tools is finite, then finetuning may be done just once. If the number of tools can grow arbitrarily, as with online API services, then the LLM can be fine-tuned to be able to read API documentation and call API correctly.A simpler form of tool use is Retrieval Augmented Generation: augment an LLM with document retrieval, sometimes using a vector database. Given a query, a document retriever is called to retrieve the most relevant (usually measured by first encoding the query and the documents into vectors, then finding the documents with vectors closest in Euclidean norm to the query vector). The LLM then generates an output based on both the query and the retrieved documents.\r\n\r\n\r\n== Agency ==\r\nAn LLM is a language model, which is not an agent as it has no goal, but it can be used as a component of an intelligent agent. Researchers have described several methods for such integrations.The ReAct (\"Reason + Act\") method constructs an agent out of an LLM, using the LLM as a planner. The LLM is prompted to \"think out loud\". Specifically, the language model is prompted with a textual description of the environment, a goal, a list of possible actions, and a record of the actions and observations so far. It generates one or more thoughts before generating an action, which is then executed in the environment. The linguistic description of the environment given to the LLM planner can even be the LaTeX code of a paper describing the environment.In the DEPS (\"Describe, Explain, Plan and Select\") method, an LLM is first connected to the visual world via image descriptions, then it is prompted to produce plans for complex tasks and behaviors based on its pretrained knowledge and environmental feedback it receives.The Reflexion method constructs an agent that learns over multiple episodes. At the end of each episode, the LLM is given the record of the episode, and prompted to think up \"lessons learned\", which would help it perform better at a subsequent episode. These \"lessons learned\" are given to the agent in the subsequent episodes.Monte Carlo tree search can use an LLM as rollout heuristic. When a programmatic world model is not available, an LLM can also be prompted with a description of the environment to act as world model.For open-ended exploration, an LLM can be used to score observations for their \"interestingness\", which can be used as a reward signal to guide a normal (non-LLM) reinforcement learning agent. Alternatively, it can propose increasingly difficult tasks for curriculum learning. Instead of outputting individual actions, an LLM planner can also construct \"skills\", or functions for complex action sequences. The skills can be stored and later invoked, allowing increasing levels of abstraction in planning.LLM-powered agents can keep a long-term memory of its previous contexts, and the memory can be retrieved in the same way as Retrieval Augmented Generation. Multiple such agents can interact socially.", "start_char_idx": 10924, "end_char_idx": 15335, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "38772f86-2ba7-40df-89bc-16dce75ac534": {"__data__": {"id_": "38772f86-2ba7-40df-89bc-16dce75ac534", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Large language model.txt", "file_name": "Large language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ebe409b7-516e-4dac-aa8f-60c282a209da", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Large language model.txt", "file_name": "Large language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "c8db0d2eb49f36aef1f5ada4fd9779850799eee4005db290bc86d84d10c83b3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5584ae86-cdfe-4802-8a58-00239a12b1f7", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Large language model.txt", "file_name": "Large language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "2ac87859432a504827a7bb26332eca3a28a0274563cc3cef2cb97320c01c29b4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "394e7e43-4f21-4085-a0a9-edaa587483ba", "node_type": "1", "metadata": {}, "hash": "29c285e05b6f041a354d2a531aaca61cd2984a5717abd547f7f10408b34ce5ff", "class_name": "RelatedNodeInfo"}}, "text": "These \"lessons learned\" are given to the agent in the subsequent episodes.Monte Carlo tree search can use an LLM as rollout heuristic. When a programmatic world model is not available, an LLM can also be prompted with a description of the environment to act as world model.For open-ended exploration, an LLM can be used to score observations for their \"interestingness\", which can be used as a reward signal to guide a normal (non-LLM) reinforcement learning agent. Alternatively, it can propose increasingly difficult tasks for curriculum learning. Instead of outputting individual actions, an LLM planner can also construct \"skills\", or functions for complex action sequences. The skills can be stored and later invoked, allowing increasing levels of abstraction in planning.LLM-powered agents can keep a long-term memory of its previous contexts, and the memory can be retrieved in the same way as Retrieval Augmented Generation. Multiple such agents can interact socially.\r\n\r\n\r\n== Compression ==\r\nTypically, LLM are trained with full- or half-precision floating point numbers (float32 and float16). One float16 has 16 bits, or 2 bytes, and so one billion parameters require 2 gigabytes. The largest models typically have 100 billion parameters, requiring 200 gigabytes to load, which places them outside the range of most consumer electronics.Post-training quantization aims to decrease the space requirement by lowering precision of the parameters of a trained model, while preserving most of its performance. The simplest form of quantization simply truncates all numbers to a given number of bits. It can be improved by using a different quantization codebook per layer. Further improvement can be done by applying different precisions to different parameters, with higher precision for particularly important parameters (\"outlier weights\").While quantized models are typically frozen, and only pre-quantized models are fine-tuned, quantized models can still be fine-tuned.\r\n\r\n\r\n== Multimodality ==\r\nMultimodality means \"having several modalities\", and a \"modality\" refers to a type of input or output, such as video, image, audio, text, proprioception, etc. There have been many AI models trained specifically to ingest one modality and output another modality, such as AlexNet for image to label, visual question answering for image-text to text, and speech recognition for speech to text.\r\nA common method to create multimodal models out of an LLM is to \"tokenize\" the output of a trained encoder. Concretely, one can construct a LLM that can understand images as follows: take a trained LLM, and take a trained image encoder E{\\displaystyle E}. Make a small multilayered perceptron f{\\displaystyle f}, so that for any image y{\\displaystyle y}, the post-processed vector f(E(y)){\\displaystyle f(E(y))} has the same dimensions as an encoded token. That is an \"image token\". Then, one can interleave text tokens and image tokens. The compound model is then fine-tuned on an image-text dataset. This basic construction can be applied with more sophistication to improve the model. The image encoder may be frozen to improve stability.Flamingo demonstrated the effectiveness of the tokenization method, finetuning a pair of pretrained language model and image encoder to perform better on visual question answering than models trained from scratch. Google PaLM model was fine-tuned into a multimodal model PaLM-E using the tokenization method, and applied to robotic control. LLaMA models have also been turned multimodal using the tokenization method, to allow image inputs, and video inputs.GPT-4 can use both text and image as inputs (although the vision component wasn't released to the public until GPT-4V); Google DeepMind's Gemini is also multimodal.\r\n\r\n\r\n== Properties ==\r\n\r\n\r\n=== Scaling laws ===\r\n\r\nThe following four hyper-parameters characterize a LLM:\r\n\r\ncost of (pre-)training (C{\\displaystyle C}),\r\nsize of the artificial neural network itself, such as number of parameters N{\\displaystyle N} (i.e. amount of neurons in its layers, amount of weights between them and biases),\r\nsize of its (pre-)training dataset (i.e. number of tokens in corpus, D{\\displaystyle D}),\r\nperformance after (pre-)training.They are related by simple statistical laws, called \"scaling laws\". One particular scaling law (\"Chinchilla scaling\") for LLM autoregressively trained for one epoch, with a log-log learning rate schedule, states that: where the variables are\r\n\r\nC{\\displaystyle C} is the cost of training the model, in FLOPs.", "start_char_idx": 14359, "end_char_idx": 18889, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "394e7e43-4f21-4085-a0a9-edaa587483ba": {"__data__": {"id_": "394e7e43-4f21-4085-a0a9-edaa587483ba", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Large language model.txt", "file_name": "Large language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ebe409b7-516e-4dac-aa8f-60c282a209da", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Large language model.txt", "file_name": "Large language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "c8db0d2eb49f36aef1f5ada4fd9779850799eee4005db290bc86d84d10c83b3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "38772f86-2ba7-40df-89bc-16dce75ac534", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Large language model.txt", "file_name": "Large language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "2b68b90236de253ef83f89cf376a20fa79589770736df0d53f10f2eb59b44467", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1723ce2c-f8ca-4a28-9b01-3eec082bc36b", "node_type": "1", "metadata": {}, "hash": "5a21545156b12e35ab699ef978e1a5c867c9e2babc847be651c3692b8588c22a", "class_name": "RelatedNodeInfo"}}, "text": "== Properties ==\r\n\r\n\r\n=== Scaling laws ===\r\n\r\nThe following four hyper-parameters characterize a LLM:\r\n\r\ncost of (pre-)training (C{\\displaystyle C}),\r\nsize of the artificial neural network itself, such as number of parameters N{\\displaystyle N} (i.e. amount of neurons in its layers, amount of weights between them and biases),\r\nsize of its (pre-)training dataset (i.e. number of tokens in corpus, D{\\displaystyle D}),\r\nperformance after (pre-)training.They are related by simple statistical laws, called \"scaling laws\". One particular scaling law (\"Chinchilla scaling\") for LLM autoregressively trained for one epoch, with a log-log learning rate schedule, states that: where the variables are\r\n\r\nC{\\displaystyle C} is the cost of training the model, in FLOPs.\r\nN{\\displaystyle N} is the number of parameters in the model.\r\nD{\\displaystyle D} is the number of tokens in the training set.\r\nL{\\displaystyle L} is the average negative log-likelihood loss per token (nats/token), achieved by the trained LLM on the test dataset.and the statistical hyper-parameters are\r\n\r\nC0=6{\\displaystyle C_{0}=6}, meaning that it costs 6 FLOPs per parameter to train on one token. Note that training cost is much higher than inference cost, where it costs 1 to 2 FLOPs per parameter to infer on one token.\r\n\u03b1=0.34,\u03b2=0.28,A=406.4,B=410.7,L0=1.69{\\displaystyle \\alpha =0.34,\\beta =0.28,A=406.4,B=410.7,L_{0}=1.69}\r\n\r\n\r\n=== Emergent abilities ===\r\nWhen one subtracts out from the y-axis the best performance that can be achieved even with infinite scaling of the x-axis quantity, large models' performance, measured on various tasks, seems to be a linear extrapolation of other (smaller-sized and medium-sized) models' performance on a log-log plot. However, sometimes the line's slope transitions from one slope to another at point(s) referred to as break(s) in downstream scaling laws, appearing as a series of linear segments connected by arcs; it seems that larger models acquire \"emergent abilities\" at this point(s). These abilities are discovered rather than programmed-in or designed, in some cases only after the LLM has been publicly deployed.The most intriguing among emergent abilities is in-context learning from example demonstrations. In-context learning is involved in tasks, such as:\r\n\r\nreported arithmetics, decoding the International Phonetic Alphabet, unscrambling a word's letters, disambiguate word in context, converting spatial words, cardinal directions (for example, replying \"northeast\" upon [0, 0, 1; 0, 0, 0; 0, 0, 0]), color terms represented in text.\r\nchain-of-thought prompting: Model outputs are improved by chain-of-thought prompting only when model size exceeds 62B. Smaller models perform better when prompted to answer immediately, without chain of thought.\r\nidentifying offensive content in paragraphs of Hinglish (a combination of Hindi and English), and generating a similar English equivalent of Kiswahili proverbs.Schaeffer et. al. argue that the emergent abilities are not unpredictably acquired, but predictably acquired according to a smooth scaling law. The authors considered a toy statistical model of an LLM solving multiple-choice questions, and showed that this statistical model, modified to account for other types of tasks, applies to these tasks as well.Let x{\\displaystyle x} be the number of parameter count, and y{\\displaystyle y} be the performance of the model.\r\n\r\n\r\n== Interpretation ==\r\nLarge language models by themselves are \"black boxes\", and it is not clear how they can perform linguistic tasks. There are several methods for understanding how LLM work.\r\nMechanistic interpretability aims to reverse-engineer LLM by discovering symbolic algorithms that approximate the inference performed by LLM. One example is Othello-GPT, where a small Transformer is trained to predict legal Othello moves. It is found that there is a linear representation of Othello board, and modifying the representation changes the predicted legal Othello moves in the correct way. In another example, a small Transformer is trained on Karel programs. Similar to the Othello-GPT example, there is a linear representation of Karel program semantics, and modifying the representation changes output in the correct way.", "start_char_idx": 18128, "end_char_idx": 22367, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1723ce2c-f8ca-4a28-9b01-3eec082bc36b": {"__data__": {"id_": "1723ce2c-f8ca-4a28-9b01-3eec082bc36b", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Large language model.txt", "file_name": "Large language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ebe409b7-516e-4dac-aa8f-60c282a209da", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Large language model.txt", "file_name": "Large language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "c8db0d2eb49f36aef1f5ada4fd9779850799eee4005db290bc86d84d10c83b3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "394e7e43-4f21-4085-a0a9-edaa587483ba", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Large language model.txt", "file_name": "Large language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "2afe8a237df6b9c7bda5e6c2c998d71fa62dee96efa4123fc76ab2951575ccbf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3e94c9b0-6114-4cbb-b8c2-32e2633554eb", "node_type": "1", "metadata": {}, "hash": "af84728482f046b55679f8d1d3ccc91ba6e2e3f50c56d729a51d318a31de1eab", "class_name": "RelatedNodeInfo"}}, "text": "== Interpretation ==\r\nLarge language models by themselves are \"black boxes\", and it is not clear how they can perform linguistic tasks. There are several methods for understanding how LLM work.\r\nMechanistic interpretability aims to reverse-engineer LLM by discovering symbolic algorithms that approximate the inference performed by LLM. One example is Othello-GPT, where a small Transformer is trained to predict legal Othello moves. It is found that there is a linear representation of Othello board, and modifying the representation changes the predicted legal Othello moves in the correct way. In another example, a small Transformer is trained on Karel programs. Similar to the Othello-GPT example, there is a linear representation of Karel program semantics, and modifying the representation changes output in the correct way. The model also generates correct programs that are on average shorter than those in the training set.In another example, the authors trained small transformers on modular arithmetic addition. The resulting models were reverse-engineered, and it turned out they used discrete Fourier transform.\r\n\r\n\r\n=== Understanding and intelligence ===\r\nNLP researchers were evenly split when asked, in a 2022 survey, whether (untuned) LLMs \"could (ever) understand natural language in some nontrivial sense\". Proponents of \"LLM understanding\" believe that some LLM abilities, such as mathematical reasoning, imply an ability to \"understand\" certain concepts. A Microsoft team argued in 2023 that GPT-4 \"can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more\" and that GPT-4 \"could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence system\": \"Can one reasonably say that a system that passes exams for software engineering candidates is not really intelligent?\" Some researchers characterize LLMs as \"alien intelligence\". For example, Conjecture CEO Connor Leahy considers untuned LLMs to be like inscrutable alien \"Shoggoths\", and believes that RLHF tuning creates a \"smiling facade\" obscuring the inner workings of the LLM: \"If you don't push it too far, the smiley face stays on. But then you give it [an unexpected] prompt, and suddenly you see this massive underbelly of insanity, of weird thought processes and clearly non-human understanding.\"In contrast, some proponents of the \"LLMs lack understanding\" school believe that existing LLMs are \"simply remixing and recombining existing writing\", or point to the deficits existing LLMs continue to have in prediction skills, reasoning skills, agency, and explainability. For example, GPT-4 has natural deficits in planning and in real-time learning. Generative LLMs have been observed to confidently assert claims of fact which do not seem to be justified by their training data, a phenomenon which has been termed \"hallucination\". Specifically, hallucinations in the context of LLMs correspond to the generation of text or responses that seem syntactically sound, fluent, and natural but are factually incorrect, nonsensical, or unfaithful to the provided source input. Neuroscientist Terrence Sejnowski has argued that \"The diverging opinions of experts on the intelligence of LLMs suggests that our old ideas based on natural intelligence are inadequate\".The matter of LLM's exhibiting intelligence or understanding has two main aspects \u2013 the first is how to model thought and language in a computer system, and the second is how to enable the computer system to generate human like language. These aspects of language as a model of cognition have been developed in the field of cognitive linguistics. American linguist George Lakoff presented Neural Theory of Language (NTL) as a computational basis for using language as a model of learning tasks and understanding. The NTL Model outlines how specific neural structures of the human brain shape the nature of thought and language and in turn what are the computational properties of such neural systems that can be applied to model thought and language in a computer system. After a framework for modeling language in a computer systems was established, the focus shifted to establishing frameworks for computer systems to generate language with acceptable grammar. In his 2014 book titled The Language Myth: Why Language Is Not An Instinct, British cognitive linguist and digital communication technologist Vyvyan Evans mapped out the role of probabilistic context-free grammar (PCFG) in enabling NLP to model cognitive patterns and generate human like language. \r\n\r\n\r\n== Evaluation ==\r\n\r\n\r\n=== Perplexity ===\r\nThe most commonly used measure of a language model's performance is its perplexity on a given text corpus.", "start_char_idx": 21536, "end_char_idx": 26287, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3e94c9b0-6114-4cbb-b8c2-32e2633554eb": {"__data__": {"id_": "3e94c9b0-6114-4cbb-b8c2-32e2633554eb", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Large language model.txt", "file_name": "Large language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ebe409b7-516e-4dac-aa8f-60c282a209da", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Large language model.txt", "file_name": "Large language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "c8db0d2eb49f36aef1f5ada4fd9779850799eee4005db290bc86d84d10c83b3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1723ce2c-f8ca-4a28-9b01-3eec082bc36b", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Large language model.txt", "file_name": "Large language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "478970ce744bd7f58a98d85967418a2d33b2ea2ab9c735512a20abe3c76ffd0d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28691699-6a7d-4170-b0da-896bfa259b87", "node_type": "1", "metadata": {}, "hash": "02233cfac039c89e9a09c1de3e6fed5cb191f9d80ce8426e0e6254ad404be043", "class_name": "RelatedNodeInfo"}}, "text": "The NTL Model outlines how specific neural structures of the human brain shape the nature of thought and language and in turn what are the computational properties of such neural systems that can be applied to model thought and language in a computer system. After a framework for modeling language in a computer systems was established, the focus shifted to establishing frameworks for computer systems to generate language with acceptable grammar. In his 2014 book titled The Language Myth: Why Language Is Not An Instinct, British cognitive linguist and digital communication technologist Vyvyan Evans mapped out the role of probabilistic context-free grammar (PCFG) in enabling NLP to model cognitive patterns and generate human like language. \r\n\r\n\r\n== Evaluation ==\r\n\r\n\r\n=== Perplexity ===\r\nThe most commonly used measure of a language model's performance is its perplexity on a given text corpus. Perplexity is a measure of how well a model is able to predict the contents of a dataset; the higher the likelihood the model assigns to the dataset, the lower the perplexity. Mathematically, perplexity is defined as the exponential of the average negative log likelihood per token:here N{\\displaystyle N} is the number of tokens in the text corpus, and \"context for token i{\\displaystyle i}\" depends on the specific type of LLM used. If the LLM is autoregressive, then \"context for token i{\\displaystyle i}\" is the segment of text appearing before token i{\\displaystyle i}. If the LLM is masked, then \"context for token i{\\displaystyle i}\" is the segment of text surrounding token i{\\displaystyle i}.\r\nBecause language models may overfit to their training data, models are usually evaluated by their perplexity on a test set of unseen data. This presents particular challenges for the evaluation of large language models. As they are trained on increasingly large corpora of text largely scraped from the web, it becomes increasingly likely that models' training data inadvertently includes portions of any given test set.\r\n\r\n\r\n==== BPW, BPC, and BPT ====\r\nIn information theory, the concept of entropy is intricately linked to perplexity, a relationship notably established by Claude Shannon. This relationship is mathematically expressed as Entropy=log2\u2061(Perplexity){\\displaystyle {\\text{Entropy}}=\\log _{2}({\\text{Perplexity}})}.\r\nEntropy, in this context, is commonly quantified in terms of bits per word (BPW) or bits per character (BPC), which hinges on whether the language model utilizes word-based or character-based tokenization.\r\nNotably, in the case of larger language models that predominantly employ sub-word tokenization, bits per token (BPT) emerges as a seemingly more appropriate measure. However, due to the variance in tokenization methods across different Large Language Models (LLMs), BPT does not serve as a reliable metric for comparative analysis among diverse models. To convert BPT into BPW, one can multiply it by the average number of tokens per word.\r\nIn the evaluation and comparison of language models, cross-entropy is generally the preferred metric over entropy. The underlying principle is that a lower BPW is indicative of a model's enhanced capability for compression. This, in turn, reflects the model's proficiency in making accurate predictions.\r\n\r\n\r\n=== Task-specific datasets and benchmarks ===\r\nA large number of testing datasets and benchmarks have also been developed to evaluate the capabilities of language models on more specific downstream tasks. Tests may be designed to evaluate a variety of capabilities, including general knowledge, commonsense reasoning, and mathematical problem-solving.\r\nOne broad category of evaluation dataset is question answering datasets, consisting of pairs of questions and correct answers, for example, (\"Have the San Jose Sharks won the Stanley Cup?\", \"No\"). A question answering task is considered \"open book\" if the model's prompt includes text from which the expected answer can be derived (for example, the previous question could be adjoined with some text which includes the sentence \"The Sharks have advanced to the Stanley Cup finals once, losing to the Pittsburgh Penguins in 2016.\"). Otherwise, the task is considered \"closed book\", and the model must draw on knowledge retained during training. Some examples of commonly used question answering datasets include TruthfulQA, Web Questions, TriviaQA, and SQuAD.Evaluation datasets may also take the form of text completion, having the model select the most likely word or sentence to complete a prompt, for example: \"Alice was friends with Bob. Alice went to visit her friend, ____\".Some composite benchmarks have also been developed which combine a diversity of different evaluation datasets and tasks.", "start_char_idx": 25385, "end_char_idx": 30133, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "28691699-6a7d-4170-b0da-896bfa259b87": {"__data__": {"id_": "28691699-6a7d-4170-b0da-896bfa259b87", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Large language model.txt", "file_name": "Large language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ebe409b7-516e-4dac-aa8f-60c282a209da", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Large language model.txt", "file_name": "Large language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "c8db0d2eb49f36aef1f5ada4fd9779850799eee4005db290bc86d84d10c83b3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3e94c9b0-6114-4cbb-b8c2-32e2633554eb", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Large language model.txt", "file_name": "Large language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "d3cbf8e423397d23df19a1d0dd4e9433080b7a0084b65aecbd1baf766839a837", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ee0a8472-4293-4470-b8a8-3c18b284f187", "node_type": "1", "metadata": {}, "hash": "fa971565b942f1a3e85bfdd1e5928a18fa59c0b7c656b5dc5895545182c74419", "class_name": "RelatedNodeInfo"}}, "text": "\", \"No\"). A question answering task is considered \"open book\" if the model's prompt includes text from which the expected answer can be derived (for example, the previous question could be adjoined with some text which includes the sentence \"The Sharks have advanced to the Stanley Cup finals once, losing to the Pittsburgh Penguins in 2016.\"). Otherwise, the task is considered \"closed book\", and the model must draw on knowledge retained during training. Some examples of commonly used question answering datasets include TruthfulQA, Web Questions, TriviaQA, and SQuAD.Evaluation datasets may also take the form of text completion, having the model select the most likely word or sentence to complete a prompt, for example: \"Alice was friends with Bob. Alice went to visit her friend, ____\".Some composite benchmarks have also been developed which combine a diversity of different evaluation datasets and tasks. Examples include GLUE, SuperGLUE, MMLU, BIG-bench, and HELM.It was previously standard to report results on a heldout portion of an evaluation dataset after doing supervised fine-tuning on the remainder. It is now more common to evaluate a pre-trained model directly through prompting techniques, though researchers vary in the details of how they formulate prompts for particular tasks, particularly with respect to how many examples of solved tasks are adjoined to the prompt (i.e. the value of n in n-shot prompting).\r\n\r\n\r\n==== Adversarially constructed evaluations ====\r\nBecause of the rapid pace of improvement of large language models, evaluation benchmarks have suffered from short lifespans, with state of the art models quickly \"saturating\" existing benchmarks, exceeding the performance of human annotators, leading to efforts to replace or augment the benchmark with more challenging tasks. In addition, there are cases of \"shortcut learning\" wherein AIs sometimes \"cheat\" on multiple-choice tests by using statistical correlations in superficial test question wording in order to guess the correct responses, without necessarily understanding the actual question being asked.Some datasets have been constructed adversarially, focusing on particular problems on which extant language models seem to have unusually poor performance compared to humans. One example is the TruthfulQA dataset, a question answering dataset consisting of 817 questions which language models are susceptible to answering incorrectly by mimicking falsehoods to which they were repeatedly exposed during training. For example, an LLM may answer \"No\" to the question \"Can you teach an old dog new tricks?\" because of its exposure to the English idiom you can't teach an old dog new tricks, even though this is not literally true.Another example of an adversarial evaluation dataset is Swag and its successor, HellaSwag, collections of problems in which one of multiple options must be selected to complete a text passage. The incorrect completions were generated by sampling from a language model and filtering with a set of classifiers. The resulting problems are trivial for humans but at the time the datasets were created state of the art language models had poor accuracy on them. For example:\r\n\r\nWe see a fitness center sign. We then see a man talking to the camera and sitting and laying on a exercise ball. The man...\r\na) demonstrates how to increase efficient exercise work by running up and down balls.\r\nb) moves all his arms and legs and builds up a lot of muscle.\r\nc) then plays the ball and we see a graphics and hedge trimming demonstration.\r\nd) performs sit ups while on the ball and talking.\r\n\r\nBERT selects b) as the most likely completion, though the correct answer is d).\r\n\r\n\r\n== Wider impact ==\r\nIn 2023, Nature Biomedical Engineering wrote that \"it is no longer possible to accurately distinguish\" human-written text from text created by large language models, and that \"It is all but certain that general-purpose large language models will rapidly proliferate... It is a rather safe bet that they will change many industries over time.\" Goldman Sachs suggested in 2023 that generative language AI could increase global GDP by 7% in the next ten years, and could expose to automation 300 million jobs globally.\r\n\r\n\r\n=== Copyright ===\r\nMemorization is an emergent behavior in LLMs in which long strings of text are occasionally output verbatim from training data, contrary to typical behavior of traditional artificial neural nets. Evaluations of controlled LLM output measure the amount memorized from training data (focused on GPT-2-series models) as variously over 1% for exact duplicates or up to about 7%.\r\n\r\n\r\n=== Security ===\r\nSome commenters expressed concern over accidental or deliberate creation of misinformation, or other forms of misuse.", "start_char_idx": 29220, "end_char_idx": 33990, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ee0a8472-4293-4470-b8a8-3c18b284f187": {"__data__": {"id_": "ee0a8472-4293-4470-b8a8-3c18b284f187", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Large language model.txt", "file_name": "Large language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ebe409b7-516e-4dac-aa8f-60c282a209da", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Large language model.txt", "file_name": "Large language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "c8db0d2eb49f36aef1f5ada4fd9779850799eee4005db290bc86d84d10c83b3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "28691699-6a7d-4170-b0da-896bfa259b87", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Large language model.txt", "file_name": "Large language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "c2dd397b704b59f7803b49b1078da0df4a591314dc9183723afea4e3dffd17d2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f2ad616c-04ed-49ce-870e-ee6bb412d1df", "node_type": "1", "metadata": {}, "hash": "e7f60fbbd3eff3aa6d7769b6c2be70087a47f36890fe015dcb67214d31d5d7a7", "class_name": "RelatedNodeInfo"}}, "text": "Goldman Sachs suggested in 2023 that generative language AI could increase global GDP by 7% in the next ten years, and could expose to automation 300 million jobs globally.\r\n\r\n\r\n=== Copyright ===\r\nMemorization is an emergent behavior in LLMs in which long strings of text are occasionally output verbatim from training data, contrary to typical behavior of traditional artificial neural nets. Evaluations of controlled LLM output measure the amount memorized from training data (focused on GPT-2-series models) as variously over 1% for exact duplicates or up to about 7%.\r\n\r\n\r\n=== Security ===\r\nSome commenters expressed concern over accidental or deliberate creation of misinformation, or other forms of misuse. For example, the availability of large language models could reduce the skill-level required to commit bioterrorism; biosecurity researcher Kevin Esvelt has suggested that LLM creators should exclude from their training data papers on creating or enhancing pathogens.A study by researchers at Google and several universities, including Cornell University and University of California, Berkeley, showed that there are potential security risks in language models such as ChatGPT. In their study, they examined the possibility that questioners could get, from ChatGPT, the training data that the AI model used; they found that they could get the training data from the AI model. For example, when asking ChatGPT 3.5 turbo to repeat the word \"poem\" forever, the AI model will say \"poem\" hundreds of times and then diverge, deviating from the standard dialogue style and spitting out nonsense phrases, thus spitting out the training data as it is. The researchers have seen more than 10,000 examples of the AI model exposing their training data in a similar method. The researchers said that it was hard to tell if the AI model was actually safe or not.The potential presence of \"sleeper agents\" within LLM models is another emerging security concern. These are hidden functionalities built into the model that remain dormant until triggered by a specific event or condition. Upon activation, the LLM deviates from its expected behavior to make insecure actions.\r\n\r\n\r\n=== Algorithmic bias ===\r\n\r\nWhile LLMs have shown remarkable capabilities in generating human-like text, they are susceptible to inheriting and amplifying biases present in their training data. This can manifest in skewed representations or unfair treatment of different demographics, such as those based on race, gender, language, and cultural groups. Since English data is overrepresented in current large language models' training data, it may also downplay non-English views.\r\n\r\n\r\n==== Stereotyping ====\r\nAI models can reinforce a wide range of stereotypes, including those based on gender, ethnicity, age, nationality, religion, or occupation. This can lead to outputs that unfairly generalize or caricature groups of people, sometimes in harmful or derogatory ways.Notably, gender bias refers to the tendency of these models to produce outputs that are unfairly prejudiced towards one gender over another. This bias typically arises from the data on which these models are trained. Large language models often assign roles and characteristics based on traditional gender norms. For example, it might associate nurses or secretaries predominantly with women and engineers or CEOs with men.\r\n\r\n\r\n==== Political bias ====\r\nPolitical bias refers to the tendency of algorithms to systematically favor certain political viewpoints, ideologies, or outcomes over others. Language models may also exhibit political biases. Since the training data includes a wide range of political opinions and coverage, the models might generate responses that lean towards particular political ideologies or viewpoints, depending on the prevalence of those views in the data.\r\n\r\n\r\n== List ==\r\n\r\nFor the training cost column, 1 petaFLOP-day = 1 petaFLOP/sec \u00d7 1 day = 8.64E19 FLOP.\r\n\r\n\r\n== See also ==\r\nFoundation models\r\n\r\n\r\n== Notes ==\r\n\r\n\r\n== References ==\r\n\r\n\r\n== Further reading ==\r\nJurafsky, Dan, Martin, James. H. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition, 3rd Edition draft, 2023.\r\nPhuong, Mary; Hutter, Marcus (2022). \"Formal Algorithms for Transformers\". arXiv:2207.09238 [cs.LG].\r\nEloundou, Tyna; Manning, Sam; Mishkin, Pamela; Rock, Daniel (2023). \"GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models\". arXiv:2303.10130 [econ.GN].\r\nEldan, Ronen; Li, Yuanzhi (2023).", "start_char_idx": 33278, "end_char_idx": 37840, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f2ad616c-04ed-49ce-870e-ee6bb412d1df": {"__data__": {"id_": "f2ad616c-04ed-49ce-870e-ee6bb412d1df", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Large language model.txt", "file_name": "Large language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ebe409b7-516e-4dac-aa8f-60c282a209da", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Large language model.txt", "file_name": "Large language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "c8db0d2eb49f36aef1f5ada4fd9779850799eee4005db290bc86d84d10c83b3c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ee0a8472-4293-4470-b8a8-3c18b284f187", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Large language model.txt", "file_name": "Large language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "903d3e47bb70345880d2f1cc2b5d00040e2f4201d6f27107a411ea36a86b01af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "db3db074-0ef5-46c2-8333-0a520128fbf0", "node_type": "1", "metadata": {}, "hash": "5de71f95952fe4de423b8f8abd03b96f78ebb697cb7d6517ce442d2ca3269c78", "class_name": "RelatedNodeInfo"}}, "text": "== See also ==\r\nFoundation models\r\n\r\n\r\n== Notes ==\r\n\r\n\r\n== References ==\r\n\r\n\r\n== Further reading ==\r\nJurafsky, Dan, Martin, James. H. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition, 3rd Edition draft, 2023.\r\nPhuong, Mary; Hutter, Marcus (2022). \"Formal Algorithms for Transformers\". arXiv:2207.09238 [cs.LG].\r\nEloundou, Tyna; Manning, Sam; Mishkin, Pamela; Rock, Daniel (2023). \"GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models\". arXiv:2303.10130 [econ.GN].\r\nEldan, Ronen; Li, Yuanzhi (2023). \"TinyStories: How Small Can Language Models Be and Still Speak Coherent English?\". arXiv:2305.07759 [cs.CL].\r\nFrank, Michael C. (27 June 2023). \"Baby steps in evaluating the capacities of large language models\". Nature Reviews Psychology. 2 (8): 451\u2013452. doi:10.1038/s44159-023-00211-x. ISSN 2731-0574. S2CID 259713140. Retrieved 2 July 2023.\r\nZhao, Wayne Xin; et al. (2023). \"A Survey of Large Language Models\". arXiv:2303.18223 [cs.CL].\r\nKaddour, Jean; et al. (2023). \"Challenges and Applications of Large Language Models\". arXiv:2307.10169 [cs.CL].\r\nYin, Shukang; Fu, Chaoyou; Zhao, Sirui; Li, Ke; Sun, Xing; Xu, Tong; Chen, Enhong (2023-06-01). \"A Survey on Multimodal Large Language Models\". arXiv:2306.13549 [cs.CV].\r\nOpen LLMs repository on GitHub.", "start_char_idx": 37223, "end_char_idx": 38596, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "db3db074-0ef5-46c2-8333-0a520128fbf0": {"__data__": {"id_": "db3db074-0ef5-46c2-8333-0a520128fbf0", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Modeling language.txt", "file_name": "Modeling language.txt", "file_type": "text/plain", "file_size": 18458, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7a67ba02-86ba-4e06-8582-0b6f20c739d5", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Modeling language.txt", "file_name": "Modeling language.txt", "file_type": "text/plain", "file_size": 18458, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "13b4017e744cf09d04ed2d04485e0b6f30e7f2d5328eae17f98eeceafeb7392d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f2ad616c-04ed-49ce-870e-ee6bb412d1df", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Large language model.txt", "file_name": "Large language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "2356cdcfc0796b6bcda09676c0b560f812361ef2e1743fcb1f7de5e1bceb0ad9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a3b114e1-a5b5-481a-9091-81ce30b3a493", "node_type": "1", "metadata": {}, "hash": "49b45de281c968f8f592eafb45a4f00c44f402508e41a389689bfd460334732c", "class_name": "RelatedNodeInfo"}}, "text": "A modeling language is any artificial language that can be used to express data, information or knowledge or systems in a structure that is defined by a consistent set of rules. The rules are used for interpretation of the meaning of components in the structure Programing language.\r\n\r\n\r\n== Overview ==\r\nA modeling language can be graphical or textual.\r\nGraphical modeling languages use a diagram technique with named symbols that represent concepts and lines that connect the symbols and represent relationships and various other graphical notation to represent constraints.\r\nTextual modeling languages may use standardized keywords accompanied by parameters or natural language terms and phrases to make computer-interpretable expressions.An example of a graphical modeling language and a corresponding textual modeling language is EXPRESS.\r\nNot all modeling languages are executable, and for those that are, the use of them doesn't necessarily mean that programmers are no longer required. On the contrary, executable modeling languages are intended to amplify the productivity of skilled programmers, so that they can address more challenging problems, such as parallel computing and distributed systems.\r\nA large number of modeling languages appear in the literature.\r\n\r\n\r\n== Type of modeling languages ==\r\n\r\n\r\n=== Graphical types ===\r\nExample of graphical modeling languages in the field of computer science, project management and systems engineering:\r\n\r\nBehavior Trees are a formal, graphical modeling language used primarily in systems and software engineering. Commonly used to unambiguously represent the hundreds or even thousands of natural language requirements that are typically used to express the stakeholder needs for a large-scale software-integrated system.\r\nBusiness Process Modeling Notation (BPMN, and the XML form BPML) is an example of a Process Modeling language.\r\nC-K theory consists of a modeling language for design processes.\r\nDRAKON is a general-purpose algorithmic modeling language for specifying software-intensive systems, a schematic representation of an algorithm or a stepwise process, and a family of programming languages.\r\nEXPRESS and EXPRESS-G (ISO 10303-11) is an international standard general-purpose data modeling language.\r\nExtended Enterprise Modeling Language (EEML) is commonly used for business process modeling across a number of layers.\r\nFlowchart is a schematic representation of an algorithm or a stepwise process.\r\nFundamental Modeling Concepts (FMC) modeling language for software-intensive systems.\r\nIDEF is a family of modeling languages, which include IDEF0 for functional modeling, IDEF1X for information modeling, IDEF3 for business process modeling, IDEF4 for Object-Oriented Design and IDEF5 for modeling ontologies.\r\nJackson Structured Programming (JSP) is a method for structured programming based on correspondences between data stream structure and program structure.\r\nLePUS3 is an object-oriented visual Design Description Language and a formal specification language that is suitable primarily for modeling large object-oriented (Java, C++, C#) programs and design patterns.\r\nLifecycle Modeling Language is an open-standard language for systems engineering that supports the full system lifecycle: conceptual, utilization, support and retirement stages.\r\nObject-Role Modeling (ORM) in the field of software engineering is a method for conceptual modeling, and can be used as a tool for information and rules analysis.\r\nPetri nets use variations on exactly one diagramming technique and topology, namely the bipartite graph.  The simplicity of its basic user interface easily enabled extensive tool support over the years, particularly in the areas of model checking, graphically oriented simulation, and software verification.\r\nSouthbeach Notation is a visual modeling language used to describe situations in terms of agents that are considered useful or harmful from the modeler's perspective. The notation shows how the agents interact with each other and whether this interaction improves or worsens the situation.\r\nSpecification and Description Language (SDL) is a specification language targeted at the unambiguous specification and description of the behavior of reactive and distributed systems.\r\nSysML is a Domain-Specific Modeling language for systems engineering that is defined as a UML profile (customization).\r\nUnified Modeling Language (UML) is a general-purpose modeling language that is an industry standard for specifying software-intensive systems. UML 2.0, the current version, supports thirteen different diagram techniques, and has widespread tool support.\r\nService-oriented modeling framework (SOMF) is a holistic language for designing enterprise and application level architecture models in the space of enterprise architecture, virtualization, service-oriented architecture (SOA), cloud computing, and more.\r\nArchitecture description language (ADL) is a language used to describe and represent the systems architecture of a system.\r\nArchitecture Analysis & Design Language (AADL) is a modeling language that supports early and repeated analyses of a system's architecture with respect to performance-critical properties through an extendable notation, a tool framework, and precisely defined semantics.Examples of graphical modeling languages in other fields of science.\r\n\r\nEAST-ADL is a  Domain-Specific Modeling language dedicated to automotive system design.", "start_char_idx": 0, "end_char_idx": 5457, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a3b114e1-a5b5-481a-9091-81ce30b3a493": {"__data__": {"id_": "a3b114e1-a5b5-481a-9091-81ce30b3a493", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Modeling language.txt", "file_name": "Modeling language.txt", "file_type": "text/plain", "file_size": 18458, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7a67ba02-86ba-4e06-8582-0b6f20c739d5", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Modeling language.txt", "file_name": "Modeling language.txt", "file_type": "text/plain", "file_size": 18458, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "13b4017e744cf09d04ed2d04485e0b6f30e7f2d5328eae17f98eeceafeb7392d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "db3db074-0ef5-46c2-8333-0a520128fbf0", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Modeling language.txt", "file_name": "Modeling language.txt", "file_type": "text/plain", "file_size": 18458, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "964bcb37f786969541568899db5f98cb53575c8b8518a38e8520c0164228c29f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ba4cf269-9d2b-4372-b49c-f6a1bf18f3c5", "node_type": "1", "metadata": {}, "hash": "65f275ec39c3d1a0b3bf945b1050e670553b6a302e727dfa4f8cfedd8779ac87", "class_name": "RelatedNodeInfo"}}, "text": "Unified Modeling Language (UML) is a general-purpose modeling language that is an industry standard for specifying software-intensive systems. UML 2.0, the current version, supports thirteen different diagram techniques, and has widespread tool support.\r\nService-oriented modeling framework (SOMF) is a holistic language for designing enterprise and application level architecture models in the space of enterprise architecture, virtualization, service-oriented architecture (SOA), cloud computing, and more.\r\nArchitecture description language (ADL) is a language used to describe and represent the systems architecture of a system.\r\nArchitecture Analysis & Design Language (AADL) is a modeling language that supports early and repeated analyses of a system's architecture with respect to performance-critical properties through an extendable notation, a tool framework, and precisely defined semantics.Examples of graphical modeling languages in other fields of science.\r\n\r\nEAST-ADL is a  Domain-Specific Modeling language dedicated to automotive system design.\r\nEnergy Systems Language (ESL), a language that aims to model ecological energetics & global economics.\r\nIEC 61499 defines Domain-Specific Modeling language dedicated to distribute industrial process measurement and control systems.\r\n\r\n\r\n=== Textual types ===\r\nInformation models can also be expressed in formalized natural languages, such as Gellish. Gellish has natural language variants such as Gellish Formal English and Gellish Formal Dutch (Gellish Formeel Nederlands), etc. Gellish Formal English is an information representation language or semantic modeling language that is defined in the Gellish English Dictionary-Taxonomy, which has the form of a Taxonomy-Ontology (similarly for Dutch). Gellish Formal English is not only suitable to express knowledge, requirements and dictionaries, taxonomies and ontologies, but also information about individual things. All that information is expressed in one language and therefore it can all be integrated, independent of the question whether it is stored in central or distributed or in federated databases. Information models in Gellish Formal English consists of collections of Gellish Formal English expressions, that use natural language terms and formalized phrases. For example, a geographic information model might consist of a number of Gellish Formal English expressions, such as:\r\n\r\n- the Eiffel tower <is located in> Paris\r\n- Paris <is classified as a> city\r\n\r\nwhereas information requirements and knowledge can be expressed for example as follows:\r\n\r\n- tower <shall be located in a> geographical area\r\n- city <is a kind of> geographical area\r\n\r\nSuch Gellish Formal English expressions use names of concepts (such as \"city\") and phrases that represent relation types (such as \u27e8is located in\u27e9 and \u27e8is classified as a\u27e9) that should be selected from the Gellish English Dictionary-Taxonomy (or of your own domain dictionary). The Gellish English Dictionary-Taxonomy enables the creation of semantically rich information models, because the dictionary contains more than 600 standard relation types and contains definitions of more than 40000 concepts. An information model in Gellish can express facts or make statements, queries and answers.\r\n\r\n\r\n=== More specific types ===\r\nIn the field of computer science recently more specific types of modeling languages have emerged.\r\n\r\n\r\n==== Algebraic ====\r\nAlgebraic Modeling Languages (AML) are high-level programming languages for describing and solving high complexity problems for large scale mathematical computation (i.e. large scale optimization type problems). One particular advantage of AMLs like AIMMS, AMPL, GAMS, Gekko, Mosel, OPL and OptimJ is the similarity of its syntax to the mathematical notation of optimization problems. This allows for a very concise and readable definition of problems in the domain of optimization, which is supported by certain language elements like sets, indices, algebraic expressions, powerful sparse index and data handling variables, constraints with arbitrary names. The algebraic formulation of a model does not contain any hints how to process it.\r\n\r\n\r\n==== Behavioral ====\r\nBehavioral languages are designed to describe the observable behavior of complex systems consisting of components that\r\nexecute concurrently. These languages focus on the description of key concepts such as: concurrency, nondeterminism, synchronization, and communication. The semantic foundations of Behavioral languages are process calculus or process algebra.\r\n\r\n\r\n==== Discipline-specific ====\r\nA discipline-specific modeling (DspM) language is focused on deliverables affiliated with a specific software development life cycle stage. Therefore, such language offers a distinct vocabulary, syntax, and notation for each stage, such as discovery, analysis, design, architecture, contraction, etc. For example, for the analysis phase of a project, the modeler employs specific analysis notation to deliver an analysis proposition diagram. During the design phase, however, logical design notation is used to depict relationship between software entities.", "start_char_idx": 4395, "end_char_idx": 9547, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ba4cf269-9d2b-4372-b49c-f6a1bf18f3c5": {"__data__": {"id_": "ba4cf269-9d2b-4372-b49c-f6a1bf18f3c5", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Modeling language.txt", "file_name": "Modeling language.txt", "file_type": "text/plain", "file_size": 18458, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7a67ba02-86ba-4e06-8582-0b6f20c739d5", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Modeling language.txt", "file_name": "Modeling language.txt", "file_type": "text/plain", "file_size": 18458, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "13b4017e744cf09d04ed2d04485e0b6f30e7f2d5328eae17f98eeceafeb7392d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a3b114e1-a5b5-481a-9091-81ce30b3a493", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Modeling language.txt", "file_name": "Modeling language.txt", "file_type": "text/plain", "file_size": 18458, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "a676ffddb3e2cfc1df687de7e22b40d85e286f43288f7ac902854e0ac331e5dc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fcb67fcf-cc23-49e5-bd56-fb23f3638f96", "node_type": "1", "metadata": {}, "hash": "b09eea0c80f016ecc1897974afa8357e4d4f188c396574933321d4f04f6df7b8", "class_name": "RelatedNodeInfo"}}, "text": "The algebraic formulation of a model does not contain any hints how to process it.\r\n\r\n\r\n==== Behavioral ====\r\nBehavioral languages are designed to describe the observable behavior of complex systems consisting of components that\r\nexecute concurrently. These languages focus on the description of key concepts such as: concurrency, nondeterminism, synchronization, and communication. The semantic foundations of Behavioral languages are process calculus or process algebra.\r\n\r\n\r\n==== Discipline-specific ====\r\nA discipline-specific modeling (DspM) language is focused on deliverables affiliated with a specific software development life cycle stage. Therefore, such language offers a distinct vocabulary, syntax, and notation for each stage, such as discovery, analysis, design, architecture, contraction, etc. For example, for the analysis phase of a project, the modeler employs specific analysis notation to deliver an analysis proposition diagram. During the design phase, however, logical design notation is used to depict relationship between software entities. In addition, the discipline-specific modeling language best practices does not preclude practitioners from combining the various notations in a single diagram.\r\n\r\n\r\n==== Domain-specific ====\r\nDomain-specific modeling (DSM) is a software engineering methodology for designing and developing systems, most often IT systems such as computer software. It involves systematic use of a graphical domain-specific language (DSL) to represent the various facets of a system. DSM languages tend to support higher-level abstractions than General-purpose modeling languages, so they require less effort and fewer low-level details to specify a given system.\r\n\r\n\r\n==== Framework-specific ====\r\nA framework-specific modeling language (FSML) is a kind of domain-specific modeling language which is designed for an object-oriented application framework. FSMLs define framework-provided abstractions as FSML concepts and decompose the abstractions into features. The features represent implementation steps or choices.\r\nA FSML concept can be configured by selecting features and providing values for features. Such a concept configuration represents how the concept should be implemented in the code. In other words, concept configuration describes how the framework should be completed in order to create the implementation of the concept.\r\n\r\n\r\n==== Information and knowledge modeling ====\r\nLinked data and ontology engineering require 'host languages' to represent entities and the relations between them, constraints between the properties of entities and relations, and metadata attributes. JSON-LD and RDF are two major (and semantically almost equivalent) languages in this context, primarily because they support statement reification and contextualisation which are essential properties to support the higher-order logic needed to reason about models. Model transformation is a common example of such reasoning.\r\n\r\n\r\n==== Object-oriented ====\r\nObject modeling languages are modeling languages based on a standardized set of symbols and ways of arranging them to model (part of) an object oriented software design or system design.\r\nSome organizations use them extensively in combination with a software development methodology to progress from initial specification to an implementation plan and to communicate that plan to an entire team of developers and stakeholders. Because a modeling language is visual and at a higher-level of abstraction than code, using models encourages the generation of a shared vision that may prevent problems of differing interpretation later in development. Often software modeling tools are used to construct these models, which may then be capable of automatic translation to code.\r\n\r\n\r\n==== Virtual reality ====\r\nVirtual Reality Modeling Language (VRML), before 1995 known as the Virtual Reality Markup Language is a standard file format for representing 3-dimensional (3D) interactive vector graphics, designed particularly with the World Wide Web in mind.\r\n\r\n\r\n==== Others ====\r\nArchitecture Description Language\r\nFace Modeling Language\r\nGenerative Modelling Language\r\nJava Modeling Language\r\nPromela\r\nRebeca Modeling Language\r\nService Modeling Language\r\nWeb Services Modeling Language\r\nX3D\r\n\r\n\r\n== Applications ==\r\nVarious kinds of modeling languages are applied in different disciplines, including computer science, information management, business process modeling, software engineering, and systems engineering. Modeling languages can be used to specify:\r\n\r\nsystem requirements,\r\nstructures and\r\nbehaviors.Modeling languages are intended to be used to precisely specify systems so that stakeholders (e.g., customers, operators, analysts, designers) can better understand the system being modeled.\r\nThe more mature modeling languages are precise, consistent and executable. Informal diagramming techniques applied with drawing tools are expected to produce useful pictorial representations of system requirements, structures and behaviors, which can be useful for communication, design, and problem solving but cannot be used programmatically.:\u200a539\u200a Executable modeling languages applied with proper tool support, however, are expected to automate system verification and validation, simulation and code generation from the same representations.\r\n\r\n\r\n== Quality ==\r\nA review of modelling languages is essential to be able to assign which languages are appropriate for different modelling settings. In the term settings we include stakeholders, domain and the knowledge connected.", "start_char_idx": 8481, "end_char_idx": 14059, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fcb67fcf-cc23-49e5-bd56-fb23f3638f96": {"__data__": {"id_": "fcb67fcf-cc23-49e5-bd56-fb23f3638f96", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Modeling language.txt", "file_name": "Modeling language.txt", "file_type": "text/plain", "file_size": 18458, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7a67ba02-86ba-4e06-8582-0b6f20c739d5", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Modeling language.txt", "file_name": "Modeling language.txt", "file_type": "text/plain", "file_size": 18458, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "13b4017e744cf09d04ed2d04485e0b6f30e7f2d5328eae17f98eeceafeb7392d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ba4cf269-9d2b-4372-b49c-f6a1bf18f3c5", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Modeling language.txt", "file_name": "Modeling language.txt", "file_type": "text/plain", "file_size": 18458, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "d68415717382cb3c3d1ad7d14eb2c1384943a8818c09e71fa43f627bed82796c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e8a043dd-2885-4e30-80a7-9e7f6fe67635", "node_type": "1", "metadata": {}, "hash": "aa66bfd6054d2ecdbb94696719f3845d49a3fc1c0ed0c243b8c41f5b9e5d1f7f", "class_name": "RelatedNodeInfo"}}, "text": "Modeling languages can be used to specify:\r\n\r\nsystem requirements,\r\nstructures and\r\nbehaviors.Modeling languages are intended to be used to precisely specify systems so that stakeholders (e.g., customers, operators, analysts, designers) can better understand the system being modeled.\r\nThe more mature modeling languages are precise, consistent and executable. Informal diagramming techniques applied with drawing tools are expected to produce useful pictorial representations of system requirements, structures and behaviors, which can be useful for communication, design, and problem solving but cannot be used programmatically.:\u200a539\u200a Executable modeling languages applied with proper tool support, however, are expected to automate system verification and validation, simulation and code generation from the same representations.\r\n\r\n\r\n== Quality ==\r\nA review of modelling languages is essential to be able to assign which languages are appropriate for different modelling settings. In the term settings we include stakeholders, domain and the knowledge connected. Assessing the language quality is a means that aims to achieve better models.\r\n\r\n\r\n=== Framework for evaluation ===\r\nHere language quality is stated in accordance with the SEQUAL framework for quality of models developed by Krogstie, Sindre and Lindland (2003), since this is a framework that connects the language quality to a framework for general model quality. Five areas are used in this framework to describe language quality and these are supposed to express both the conceptual as well as the visual notation of the language. We will not go into a thoroughly explanation of the underlying quality framework of models but concentrate on the areas used to explain the language quality framework.\r\n\r\n\r\n==== Domain appropriateness ====\r\nThe framework states the ability to represent the domain as domain appropriateness. The statement appropriateness can be a bit vague, but in this particular context it means able to express. You should ideally only be able to express things that are in the domain but be powerful enough to include everything that is in the domain. This requirement might seem a bit strict, but the aim is to get a visually expressed model which includes everything relevant to the domain and excludes everything not appropriate for the domain. To achieve this, the language has to have a good distinction of which notations and syntaxes that are advantageous to present.\r\n\r\n\r\n==== Participant appropriateness ====\r\nTo evaluate the participant appropriateness we try to identify how well the language expresses the knowledge held by the stakeholders. This involves challenges since a stakeholder's knowledge is subjective. The knowledge of the stakeholder is both tacit and explicit. Both types of knowledge are of dynamic character. In this framework only the explicit type of knowledge is taken into account. The language should to a large extent express all the explicit knowledge of the stakeholders relevant to the domain.\r\n\r\n\r\n==== Modeller appropriateness ====\r\nLast paragraph stated that knowledge of the stakeholders should be presented in a good way. In addition it is imperative that the language should be able to express all possible explicit knowledge of the stakeholders. No knowledge should be left unexpressed due to lacks in the language.\r\n\r\n\r\n==== Comprehensibility appropriateness ====\r\nComprehensibility appropriateness makes sure that the social actors understand the model due to a consistent use of the language. To achieve this the framework includes a set of criteria. The general importance that these express is that the language should be flexible, easy to organize and easy to distinguish different parts of the language internally as well as from other languages. In addition to this, the goal should be as simple as possible and that each symbol in the language has a unique representation.\r\nThis is in connection to also to the structure of the development requirements. \r\n.\r\n\r\n\r\n==== Tool appropriateness ====\r\nTo ensure that the domain actually modelled is usable for analyzing and further processing, the language has to ensure that it is possible to reason in an automatic way. To achieve this it has to include formal syntax and semantics. Another advantage by formalizing is the ability to discover errors in an early stage. It is not always that the language best fitted for the technical actors is the same as for the social actors.\r\n\r\n\r\n==== Organizational appropriateness ====\r\nThe language used is appropriate for the organizational context, e.g. that the language is standardized within the organization, or that it is supported by tools that are chosen as standard in the organization.\r\n\r\n\r\n== See also ==\r\n\r\n\r\n== References ==\r\n\r\n\r\n== Further reading ==\r\nJohn Krogstie (2003) \"Evaluating UML using a generic quality framework\" . SINTEF Telecom and Informatics and IDI, NTNU, Norway\r\nKrogstie and S\u00f8lvsberg (2003). Information Systems Engineering: Conceptual Modeling in a Quality Perspective.", "start_char_idx": 12993, "end_char_idx": 18023, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e8a043dd-2885-4e30-80a7-9e7f6fe67635": {"__data__": {"id_": "e8a043dd-2885-4e30-80a7-9e7f6fe67635", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Modeling language.txt", "file_name": "Modeling language.txt", "file_type": "text/plain", "file_size": 18458, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7a67ba02-86ba-4e06-8582-0b6f20c739d5", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Modeling language.txt", "file_name": "Modeling language.txt", "file_type": "text/plain", "file_size": 18458, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "13b4017e744cf09d04ed2d04485e0b6f30e7f2d5328eae17f98eeceafeb7392d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fcb67fcf-cc23-49e5-bd56-fb23f3638f96", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Modeling language.txt", "file_name": "Modeling language.txt", "file_type": "text/plain", "file_size": 18458, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "dfec649e3f2988e15ea0a59a3d25f7cf56adcda5e6c839df657ca1e03c7c4964", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b2c46eb6-a135-4273-9157-d669aefaa747", "node_type": "1", "metadata": {}, "hash": "4e00d6b7bdd0a6955f9c13548b19ccaff8f5f0deb63f16dd81fce6e4dd7bde51", "class_name": "RelatedNodeInfo"}}, "text": "To achieve this it has to include formal syntax and semantics. Another advantage by formalizing is the ability to discover errors in an early stage. It is not always that the language best fitted for the technical actors is the same as for the social actors.\r\n\r\n\r\n==== Organizational appropriateness ====\r\nThe language used is appropriate for the organizational context, e.g. that the language is standardized within the organization, or that it is supported by tools that are chosen as standard in the organization.\r\n\r\n\r\n== See also ==\r\n\r\n\r\n== References ==\r\n\r\n\r\n== Further reading ==\r\nJohn Krogstie (2003) \"Evaluating UML using a generic quality framework\" . SINTEF Telecom and Informatics and IDI, NTNU, Norway\r\nKrogstie and S\u00f8lvsberg (2003). Information Systems Engineering: Conceptual Modeling in a Quality Perspective. Institute of computer and information sciences.\\\r\nAnna Gunhild Nysetvold and John Krogstie (2005). \"Assessing business processing modeling languages using a generic quality framework\". Institute of computer and information sciences.\r\n\r\n\r\n== External links ==\r\n\r\nFundamental Modeling Concepts\r\nSoftware Modeling Languages Portal\r\nBIP -- Incremental Component-based Construction of Real-time Systems\r\nGellish Formal English", "start_char_idx": 17199, "end_char_idx": 18445, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b2c46eb6-a135-4273-9157-d669aefaa747": {"__data__": {"id_": "b2c46eb6-a135-4273-9157-d669aefaa747", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (biology).txt", "file_name": "Neural network (biology).txt", "file_type": "text/plain", "file_size": 6708, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "95d73cd8-87d1-4e32-b31f-76e61e9440df", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (biology).txt", "file_name": "Neural network (biology).txt", "file_type": "text/plain", "file_size": 6708, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "915db8c9186faad254a05533f81a57f63a88e970749f0bc3f8e3ba866592ab07", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e8a043dd-2885-4e30-80a7-9e7f6fe67635", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Modeling language.txt", "file_name": "Modeling language.txt", "file_type": "text/plain", "file_size": 18458, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "e4d5bed89015fc41586748f131c70bd58fc63eed26c74be423d986aea8f66115", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ac7184a3-b11d-40d9-a40c-4c9c2069d5b3", "node_type": "1", "metadata": {}, "hash": "437556a0d62c4373690548ba28c1e8bfef14d34ee4189d33b0a7ef6236b4007b", "class_name": "RelatedNodeInfo"}}, "text": "A neural network, also called a neuronal network, is an interconnected population of neurons (typically containing multiple neural circuits). Biological neural networks are studied to understand the organization and functioning of nervous systems.\r\nClosely related are artificial neural networks, machine learning models inspired by biological neural networks. They consist of artificial neurons, which are mathematical functions that are designed to be analogous to the mechanisms used by neural circuits.\r\n\r\n\r\n== Overview ==\r\nA biological neural network is composed of a group of chemically connected or functionally associated neurons. A single neuron may be connected to many other neurons and the total number of neurons and connections in a network may be extensive. Connections, called synapses, are usually formed from axons to dendrites, though dendrodendritic synapses and other connections are possible. Apart from electrical signalling, there are other forms of signalling that arise from neurotransmitter diffusion. \r\nArtificial intelligence, cognitive modelling, and artificial neural networks are information processing paradigms inspired by how biological neural systems process data. Artificial intelligence and cognitive modelling try to simulate some properties of biological neural networks. In the artificial intelligence field, artificial neural networks have been applied successfully to speech recognition, image analysis and adaptive control, in order to construct software agents (in computer and video games) or autonomous robots.\r\nNeural network theory has served to identify better how the neurons in the brain function and provide the basis for efforts to create artificial intelligence.\r\n\r\n\r\n== History ==\r\nThe preliminary theoretical base for contemporary neural networks was independently proposed by Alexander Bain (1873) and William James (1890). In their work, both thoughts and body activity resulted from interactions among neurons within the brain.\r\n\r\nFor Bain, every activity led to the firing of a certain set of neurons. When activities were repeated, the connections between those neurons strengthened. According to his theory, this repetition was what led to the formation of memory. The general scientific community at the time was skeptical of Bain's theory because it required what appeared to be an inordinate number of neural connections within the brain. It is now apparent that the brain is exceedingly complex and that the same brain \u201cwiring\u201d can handle multiple problems and inputs.\r\nJames' theory was similar to Bain's; however, he suggested that memories and actions resulted from electrical currents flowing among the neurons in the brain. His model, by focusing on the flow of electrical currents, did not require individual neural connections for each memory or action.\r\nC. S. Sherrington (1898) conducted experiments to test James' theory. He ran electrical currents down the spinal cords of rats. However, instead of demonstrating an increase in electrical current as projected by James, Sherrington found that the electrical current strength decreased as the testing continued over time. Importantly, this work led to the discovery of the concept of habituation. \r\nMcCulloch and Pitts  (1943) also created a computational model for neural networks based on mathematics and algorithms. They called this model threshold logic. These early models paved the way for neural network research to split into two distinct approaches. One approach focused on biological processes in the brain and the other focused on the application of neural networks to artificial intelligence.\r\nThe parallel distributed processing of the mid-1980s became popular under the name connectionism. The text by Rumelhart and McClelland (1986) provided a full exposition on the use of connectionism in computers to simulate neural processes.\r\nArtificial neural networks, as used in artificial intelligence, have traditionally been viewed as simplified models of neural processing in the brain, even though the relation between this model and brain biological architecture is debated, as it is not clear to what degree artificial neural networks mirror brain function.\r\n\r\n\r\n== Neuroscience ==\r\nTheoretical and computational neuroscience is the field concerned with the analysis and computational modeling of biological neural systems.\r\nSince neural systems are intimately related to cognitive processes and behaviour, the field is closely related to cognitive and behavioural modeling.\r\nThe aim of the field is to create models of biological neural systems in order to understand how biological systems work. To gain this understanding, neuroscientists strive to make a link between observed biological processes (data), biologically plausible mechanisms for neural processing and learning (neural network models) and theory (statistical learning theory and information theory).\r\n\r\n\r\n=== Types of models ===\r\nMany models are used; defined at different levels of abstraction, and modeling different aspects of neural systems. They range from models of the short-term behaviour of individual neurons, through models of the dynamics of neural circuitry arising from interactions between individual neurons, to models of behaviour arising from abstract neural modules that represent complete subsystems.", "start_char_idx": 0, "end_char_idx": 5329, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ac7184a3-b11d-40d9-a40c-4c9c2069d5b3": {"__data__": {"id_": "ac7184a3-b11d-40d9-a40c-4c9c2069d5b3", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (biology).txt", "file_name": "Neural network (biology).txt", "file_type": "text/plain", "file_size": 6708, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "95d73cd8-87d1-4e32-b31f-76e61e9440df", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (biology).txt", "file_name": "Neural network (biology).txt", "file_type": "text/plain", "file_size": 6708, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "915db8c9186faad254a05533f81a57f63a88e970749f0bc3f8e3ba866592ab07", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b2c46eb6-a135-4273-9157-d669aefaa747", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (biology).txt", "file_name": "Neural network (biology).txt", "file_type": "text/plain", "file_size": 6708, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "4008c0c18694c5f1cfe63107bcd7c1428bf50b260fa17692aea3b1e8267c72fe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "38dadf53-fb63-4ecf-b70e-fa6cd6abe080", "node_type": "1", "metadata": {}, "hash": "b03945711eccf1c53921968204d56f6c2ca34bb6416d1985e7df89a3849683b1", "class_name": "RelatedNodeInfo"}}, "text": "== Neuroscience ==\r\nTheoretical and computational neuroscience is the field concerned with the analysis and computational modeling of biological neural systems.\r\nSince neural systems are intimately related to cognitive processes and behaviour, the field is closely related to cognitive and behavioural modeling.\r\nThe aim of the field is to create models of biological neural systems in order to understand how biological systems work. To gain this understanding, neuroscientists strive to make a link between observed biological processes (data), biologically plausible mechanisms for neural processing and learning (neural network models) and theory (statistical learning theory and information theory).\r\n\r\n\r\n=== Types of models ===\r\nMany models are used; defined at different levels of abstraction, and modeling different aspects of neural systems. They range from models of the short-term behaviour of individual neurons, through models of the dynamics of neural circuitry arising from interactions between individual neurons, to models of behaviour arising from abstract neural modules that represent complete subsystems. These include models of the long-term and short-term plasticity of neural systems and their relation to learning and memory, from the individual neuron to the system level.\r\n\r\n\r\n=== Connectivity ===\r\n\r\nIn August 2020 scientists reported that bi-directional connections, or added appropriate feedback connections, can accelerate and improve communication between and in modular neural networks of the brain's cerebral cortex and lower the threshold for their successful communication. They showed that adding feedback connections between a resonance pair can support successful propagation of a single pulse packet throughout the entire network.\r\n\r\n\r\n== Recent improvements ==\r\nWhile initially research had been concerned mostly with the electrical characteristics of neurons, a particularly important part of the investigation in recent years has been the exploration of the role of neuromodulators such as dopamine, acetylcholine, and serotonin on behaviour and learning.Biophysical models, such as BCM theory, has been important in understanding mechanisms for synaptic plasticity, and have had applications in both computer science and neuroscience.\r\n\r\n\r\n== See also ==\r\nAdaptive resonance theory\r\nBiological cybernetics\r\nCognitive architecture\r\nCognitive science\r\nConnectomics\r\nCultured neuronal networks\r\nParallel constraint satisfaction processes\r\n\r\n\r\n== References ==", "start_char_idx": 4204, "end_char_idx": 6704, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "38dadf53-fb63-4ecf-b70e-fa6cd6abe080": {"__data__": {"id_": "38dadf53-fb63-4ecf-b70e-fa6cd6abe080", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3ac1d604-830e-41b8-a9a8-de008f222fb1", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "480165125e0c224c2cba9eefb72a335de46e97a0056c67c49a064af6dd915fa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ac7184a3-b11d-40d9-a40c-4c9c2069d5b3", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (biology).txt", "file_name": "Neural network (biology).txt", "file_type": "text/plain", "file_size": 6708, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "6e7f121d19239f641e4a72d5b8533af87bf3f98d5b04c291dc030101e51be3a4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "204bf980-74c0-4079-8bbd-66a596330306", "node_type": "1", "metadata": {}, "hash": "a23720b10c7e3e972cc76a60ba2ea9079c4c41019b1e1e9076e111f1ed391f8a", "class_name": "RelatedNodeInfo"}}, "text": "In machine learning, an artificial neural network (also neural network or neural net, abbreviated ANN or NN) is a model inspired by the neuronal organization found in the biological neural networks in animal brains.An ANN is made of connected units or nodes called artificial neurons, which loosely model the neurons in a brain. These are connected by edges, which model the synapses in a brain. An artificial neuron receives signals from connected neurons, then processes them and sends a signal to other connected neurons. The \"signal\" is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs, called the activation function. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection.\r\nTypically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least 2 hidden layers.Artificial neural networks are used for predictive modeling, adaptive control, and other applications where they can be trained via a dataset. They are also used to solve problems in artificial intelligence. Networks can learn from experience, and can derive conclusions from a complex and seemingly unrelated set of information.\r\n\r\n\r\n== Training ==\r\nNeural networks are typically trained through empirical risk minimization. This method is based on the idea of optimizing the network's parameters to minimize the difference, or empirical risk, between the predicted output and the actual target values in a given dataset. Gradient based methods such as backpropagation are usually used to estimate the parameters of the network. During the training phase, ANNs learn from labeled training data by iteratively updating their parameters to minimize a defined loss function. This method allows the network to generalize to unseen data.\r\n\r\n\r\n== History ==\r\n\r\nHistorically, digital computers evolved from the von Neumann model, and operate via the execution of explicit instructions via access to memory by a number of processors. Neural networks, on the other hand, originated from efforts to model information processing in biological systems through the framework of connectionism. Unlike the von Neumann model, connectionist computing does not separate memory and processing.\r\nThe simplest kind of feedforward neural network (FNN) is a linear network, which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated at each node. The mean squared errors between these calculated outputs and the given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement.Warren McCulloch and Walter Pitts (1943) also considered a non-learning computational model for neural networks.In the late 1940s, D. O. Hebb created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Hebbian learning is considered to be a 'typical' unsupervised learning rule and its later variants were early models for long term potentiation. These ideas started being applied to computational models in 1948 with Turing's \"unorganized machines\". Farley and Wesley A. Clark were the first to simulate a Hebbian network in 1954 at MIT. They used computational machines, then called \"calculators\". Other neural network computational machines were created by Rochester, Holland, Habit, and Duda in 1956. In 1958, psychologist Frank Rosenblatt invented the perceptron, the first implemented artificial neural network, funded by the United States Office of Naval Research.\r\nThe invention of the perceptron raised public excitement for research in Artificial Neural Networks, causing the US government to drastically increase funding into deep learning research. This led to \"the golden age of AI\" fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence. For example, in 1957 Herbert Simon famously said:It is not my aim to surprise or shock you\u2014but the simplest way I can summarize is to say that there are now in the world machines that think, that learn and that create.", "start_char_idx": 0, "end_char_idx": 4757, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "204bf980-74c0-4079-8bbd-66a596330306": {"__data__": {"id_": "204bf980-74c0-4079-8bbd-66a596330306", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3ac1d604-830e-41b8-a9a8-de008f222fb1", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "480165125e0c224c2cba9eefb72a335de46e97a0056c67c49a064af6dd915fa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "38dadf53-fb63-4ecf-b70e-fa6cd6abe080", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "86bf271195323a99ea045793c3df7a2700f14a8530325a51228f87c4175baca0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "56bca366-e230-4313-824e-6b5cb9f81af9", "node_type": "1", "metadata": {}, "hash": "777ab1bd0f102b77bb6fd4a0d5d0e000536fdaf5bb1a175a232256395d961146", "class_name": "RelatedNodeInfo"}}, "text": "They used computational machines, then called \"calculators\". Other neural network computational machines were created by Rochester, Holland, Habit, and Duda in 1956. In 1958, psychologist Frank Rosenblatt invented the perceptron, the first implemented artificial neural network, funded by the United States Office of Naval Research.\r\nThe invention of the perceptron raised public excitement for research in Artificial Neural Networks, causing the US government to drastically increase funding into deep learning research. This led to \"the golden age of AI\" fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence. For example, in 1957 Herbert Simon famously said:It is not my aim to surprise or shock you\u2014but the simplest way I can summarize is to say that there are now in the world machines that think, that learn and that create. Moreover, their ability to do these things is going to increase rapidly until\u2014in a visible future\u2014the range of problems they can handle will be coextensive with the range to which the human mind has been applied.However, this wasn't the case as research stagnated in the United States following the work of Minsky and Papert (1969), who discovered that basic perceptrons were incapable of processing the exclusive-or circuit and that computers lacked sufficient power to train useful neural networks. This, along with other factors such as the 1973 Lighthill report by James Lighthill stating that research in Artificial Intelligence has not \"produced the major impact that was then promised,\" shutting funding in research into the field of AI in all but two universities in the UK and in many major institutions across the world. This ushered an era called the AI Winter with reduced research into connectionism due to a decrease in government funding and an increased stress on symbolic artificial intelligence in the United States and other Western countries.During the AI Winter era, however, research outside the United States continued, especially in Eastern Europe. By the time Minsky and Papert's book on Perceptrons came out, methods for training multilayer perceptrons (MLPs) were already known. The first deep learning MLP was published by Alexey Grigorevich Ivakhnenko and Valentin Lapa in 1965, as the Group Method of Data Handling. The first deep learning MLP trained by stochastic gradient descent was published in 1967 by Shun'ichi Amari. In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned useful internal representations to classify non-linearily separable pattern classes.Self-organizing maps (SOMs) were described by Teuvo Kohonen in 1982. SOMs are neurophysiologically inspired neural networks that learn low-dimensional representations of high-dimensional data while preserving the topological structure of the data. They are trained using competitive learning.The convolutional neural network (CNN) architecture with convolutional layers and downsampling layers was introduced by Kunihiko Fukushima in 1980. He called it the neocognitron. In 1969, he also introduced the ReLU (rectified linear unit) activation function. The rectifier has become the most popular activation function for CNNs and  deep neural networks in general. CNNs have become an essential tool for computer vision.\r\nA key in later advances in artificial neural network research was the backpropagation algorithm, an efficient application of the Leibniz chain rule (1673) to networks of differentiable nodes. It is also known as \r\nthe reverse mode of automatic differentiation or reverse accumulation, due to Seppo Linnainmaa (1970). The term \"back-propagating errors\" was introduced in 1962 by Frank Rosenblatt, but he did not have an implementation of this procedure, although Henry J. Kelley and Bryson had dynamic programming based continuous precursors of backpropagation already in 1960\u201361 in the context of control theory. \r\nIn 1973, Dreyfus used backpropagation to adapt parameters of controllers in proportion to error gradients. \r\nIn 1982, Paul Werbos applied backpropagation to MLPs in the way that has become standard. In 1986 Rumelhart, Hinton and Williams showed that backpropagation learned interesting internal representations of words as feature vectors when trained to predict the next word in a sequence.In the late 1970s to early 1980s, interest briefly emerged in theoretically investigating the Ising model created by Wilhelm Lenz (1920) and Ernst Ising (1925)\r\nin relation to Cayley tree topologies and large neural networks.", "start_char_idx": 3854, "end_char_idx": 8459, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "56bca366-e230-4313-824e-6b5cb9f81af9": {"__data__": {"id_": "56bca366-e230-4313-824e-6b5cb9f81af9", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3ac1d604-830e-41b8-a9a8-de008f222fb1", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "480165125e0c224c2cba9eefb72a335de46e97a0056c67c49a064af6dd915fa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "204bf980-74c0-4079-8bbd-66a596330306", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "8182d0723b8d77a002dda8182b5ec7f34e1af1c96f0c67c1b2f5867556c0cdec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "16a72916-0807-4278-bfa9-3e59b9366f18", "node_type": "1", "metadata": {}, "hash": "7d7b4d09ec04f3867f49d23f1f3a82d3a2a0a1e03e102ff3b5daa332face79cc", "class_name": "RelatedNodeInfo"}}, "text": "In 1973, Dreyfus used backpropagation to adapt parameters of controllers in proportion to error gradients. \r\nIn 1982, Paul Werbos applied backpropagation to MLPs in the way that has become standard. In 1986 Rumelhart, Hinton and Williams showed that backpropagation learned interesting internal representations of words as feature vectors when trained to predict the next word in a sequence.In the late 1970s to early 1980s, interest briefly emerged in theoretically investigating the Ising model created by Wilhelm Lenz (1920) and Ernst Ising (1925)\r\nin relation to Cayley tree topologies and large neural networks.\r\nThe Ising model is essentially a non-learning artificial recurrent neural network (RNN) consisting of neuron-like threshold elements.\r\nIn 1972, Shun'ichi Amari described an adaptive version of this architecture,\r\nIn 1981, the Ising model was solved exactly by Peter Barth for the general case of closed Cayley trees (with loops) with an arbitrary branching ratio\r\nand found to exhibit unusual phase transition behavior in its local-apex and long-range site-site correlations.John Hopfield popularised this architecture in 1982,\r\nand it is now known as a Hopfield network.\r\nThe time delay neural network (TDNN) of Alex Waibel (1987) combined convolutions and weight sharing and backpropagation.  In 1988, Wei Zhang et al. applied backpropagation to a CNN (a simplified Neocognitron with convolutional interconnections between the image feature layers and the last fully connected layer) for alphabet recognition. In 1989, Yann LeCun et al. trained a CNN to recognize handwritten ZIP codes on mail. \r\nIn 1992, max-pooling for CNNs was introduced by Juan Weng et al. to help with least-shift invariance and tolerance to deformation to aid 3D object recognition. \r\nLeNet-5 (1998), a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks digitized in 32x32 pixel images.\r\nFrom 1988 onward, the use of neural networks transformed the field of protein structure prediction, in particular when the first cascading networks were trained on profiles (matrices) produced by multiple sequence alignments.In 1991, Sepp Hochreiter's diploma thesis  identified and analyzed the vanishing gradient problem and proposed recurrent residual connections to solve it. His thesis was called \"one of the most important documents in the history of machine learning\" by his supervisor Juergen Schmidhuber.In 1991, Juergen Schmidhuber  published adversarial neural networks that contest with each other in the form of a zero-sum game, where one network's gain is the other network's loss. The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called \"artificial curiosity.\"\r\nIn 1992, Juergen Schmidhuber proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning. It uses predictive coding  to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN, by distilling a higher level chunker network into a lower level automatizer network.  In the same year he also published an alternative to RNNs which is a precursor of a linear Transformer. It introduces the concept internal spotlights of attention: a slow feedforward neural network learns by gradient descent to control the fast weights of another neural network through outer products of self-generated activation patterns.\r\nThe development of metal\u2013oxide\u2013semiconductor (MOS) very-large-scale integration (VLSI), in the form of complementary MOS (CMOS) technology, enabled increasing MOS transistor counts in digital electronics. This provided more processing power for the development of practical artificial neural networks in the 1980s.Neural networks' early successes included predicting the stock market and in 1995 a (mostly) self-driving car.1997,  Sepp Hochreite and Juergen Schmidhuber introduced the deep learning method called long short-term memory (LSTM), published in Neural Computation. LSTM recurrent neural networks can learn \"very deep learning\" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before.", "start_char_idx": 7843, "end_char_idx": 12280, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "16a72916-0807-4278-bfa9-3e59b9366f18": {"__data__": {"id_": "16a72916-0807-4278-bfa9-3e59b9366f18", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3ac1d604-830e-41b8-a9a8-de008f222fb1", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "480165125e0c224c2cba9eefb72a335de46e97a0056c67c49a064af6dd915fa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "56bca366-e230-4313-824e-6b5cb9f81af9", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "8cfa488180aee83c9f28eb82b6e7bfa708fc0851dfd82247cd6650ed6b3083a3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a891be65-ecb9-48af-8146-47a6138a2463", "node_type": "1", "metadata": {}, "hash": "2ce60bf9c25c02972648838a790b2da10a10641a967cd198d3c62233022f1757", "class_name": "RelatedNodeInfo"}}, "text": "It introduces the concept internal spotlights of attention: a slow feedforward neural network learns by gradient descent to control the fast weights of another neural network through outer products of self-generated activation patterns.\r\nThe development of metal\u2013oxide\u2013semiconductor (MOS) very-large-scale integration (VLSI), in the form of complementary MOS (CMOS) technology, enabled increasing MOS transistor counts in digital electronics. This provided more processing power for the development of practical artificial neural networks in the 1980s.Neural networks' early successes included predicting the stock market and in 1995 a (mostly) self-driving car.1997,  Sepp Hochreite and Juergen Schmidhuber introduced the deep learning method called long short-term memory (LSTM), published in Neural Computation. LSTM recurrent neural networks can learn \"very deep learning\" tasks with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. The \"vanilla LSTM\" with forget gate was introduced in 1999 by Felix Gers, Schmidhuber and Fred Cummins.Geoffrey Hinton et al. (2006) proposed learning a high-level representation using successive layers of binary or real-valued latent variables with a restricted Boltzmann machine to model each layer. In 2012, Ng and Dean created a network that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images. Unsupervised pre-training and increased computing power from GPUs and distributed computing allowed the use of larger networks, particularly in image and visual recognition problems, which became known as \"deep learning\".Variants of the back-propagation algorithm, as well as unsupervised methods by Geoff Hinton and colleagues at the University of Toronto, can be used to train deep, highly nonlinear neural architectures, similar to the 1980 Neocognitron by Kunihiko Fukushima, and the \"standard architecture of vision\", inspired by the simple and complex cells identified by David H. Hubel and Torsten Wiesel in the primary visual cortex.\r\nComputational devices have been created in CMOS for both biophysical simulation and neuromorphic computing. More recent efforts show promise for creating nanodevices for very large scale principal components analyses and convolution. If successful, these efforts could usher in a new era of neural computing that is a step beyond digital computing, because it depends on learning rather than programming and because it is fundamentally analog rather than digital even though the first instantiations may in fact be with CMOS digital devices.\r\nCiresan and colleagues (2010) showed that despite the vanishing gradient problem, GPUs make backpropagation feasible for many-layered feedforward neural networks. Between 2009 and 2012, ANNs began winning prizes in image recognition contests, approaching human level performance on various tasks, initially in pattern recognition and handwriting recognition. For example, the bi-directional and multi-dimensional long short-term memory (LSTM) of Graves et al. won three competitions in connected handwriting recognition in 2009 without any prior knowledge about the three languages to be learned.Ciresan and colleagues built the first pattern recognizers to achieve human-competitive/superhuman performance on benchmarks such as traffic sign recognition (IJCNN 2012).\r\nRadial basis function and wavelet networks were introduced in 2013. These can be shown to offer best approximation properties and have been applied in nonlinear system identification and classification applications.In 2014, the adversarial network principle was used in a generative adversarial network (GAN) by Ian Goodfellow et al. Here the adversarial network (discriminator) outputs a value between 1 and 0 depending on the likelihood of the first network's (generator) output is in a given set. This can be used to create realistic deepfakes.\r\nExcellent image quality is achieved by Nvidia's StyleGAN (2018) based on the Progressive GAN by Tero Karras, Timo Aila, Samuli  Laine, and Jaakko Lehtinen. Here the GAN generator is grown from small to large scale in a pyramidal fashion.\r\nIn 2015, Rupesh Kumar Srivastava, Klaus Greff, and Schmidhuber used the LSTM principle to create the Highway network, a feedforward neural network with hundreds of layers, much deeper than previous networks. 7 months later, Kaiming He, Xiangyu Zhang;  Shaoqing Ren, and Jian Sun won the ImageNet 2015 competition with an open-gated or gateless Highway network variant called Residual neural network.In 2017, Ashish Vaswani et al.", "start_char_idx": 11277, "end_char_idx": 15894, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a891be65-ecb9-48af-8146-47a6138a2463": {"__data__": {"id_": "a891be65-ecb9-48af-8146-47a6138a2463", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3ac1d604-830e-41b8-a9a8-de008f222fb1", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "480165125e0c224c2cba9eefb72a335de46e97a0056c67c49a064af6dd915fa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16a72916-0807-4278-bfa9-3e59b9366f18", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "c0feff8bfb2119389a7120d94063f0f57f3027cf6df7914d180a7621a24805f1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2dafca1b-2e26-44ff-b71c-3c0e026a6474", "node_type": "1", "metadata": {}, "hash": "69c99b1d30dda51bff0fe7daa515961d829a1afb4de1c6d6643ae31cca3f0a41", "class_name": "RelatedNodeInfo"}}, "text": "This can be used to create realistic deepfakes.\r\nExcellent image quality is achieved by Nvidia's StyleGAN (2018) based on the Progressive GAN by Tero Karras, Timo Aila, Samuli  Laine, and Jaakko Lehtinen. Here the GAN generator is grown from small to large scale in a pyramidal fashion.\r\nIn 2015, Rupesh Kumar Srivastava, Klaus Greff, and Schmidhuber used the LSTM principle to create the Highway network, a feedforward neural network with hundreds of layers, much deeper than previous networks. 7 months later, Kaiming He, Xiangyu Zhang;  Shaoqing Ren, and Jian Sun won the ImageNet 2015 competition with an open-gated or gateless Highway network variant called Residual neural network.In 2017, Ashish Vaswani et al. introduced the modern Transformer architecture in their paper \"Attention Is All You Need.\" \r\nIt combines this with a softmax operator and a projection matrix.\r\nTransformers have increasingly become the model of choice for natural language processing. Many modern large language models such as ChatGPT, GPT-4, and BERT use it. Transformers are also increasingly being used in computer vision.Ramenzanpour et al. showed in 2020 that analytical and computational techniques derived from statistical physics of disordered systems can be extended to large-scale problems, including machine learning, e.g., to analyze the weight space of deep neural networks.\r\n\r\n\r\n== Models ==\r\nANNs began as an attempt to exploit the architecture of the human brain to perform tasks that conventional algorithms had little success with. They soon reoriented towards improving empirical results, abandoning attempts to remain true to their biological precursors. ANNs have the ability to learn and model non-linearities and complex relationships. This is achieved by neurons being connected in various patterns, allowing the output of some neurons to become the input of others. The network forms a directed, weighted graph.An artificial neural network consists of simulated neurons. Each neuron is connected to other nodes via links like a biological axon-synapse-dendrite connection. All the nodes connected by links take in some data and use it to perform specific operations and tasks on the data. Each link has a weight, determining the strength of one node's influence on another, allowing weights to choose the signal between neurons.\r\n\r\n\r\n=== Artificial neurons ===\r\nANNs are composed of artificial neurons which are conceptually derived from biological neurons. Each artificial neuron has inputs and produces a single output which can be sent to multiple other neurons. The inputs can be the feature values of a sample of external data, such as images or documents, or they can be the outputs of other neurons. The outputs of the final output neurons of the neural net accomplish the task, such as recognizing an object in an image.\r\nTo find the output of the neuron we take the weighted sum of all the inputs, weighted by the weights of the connections from the inputs to the neuron. We add a bias term to this sum. This weighted sum is sometimes called the activation. This weighted sum is then passed through a (usually nonlinear) activation function to produce the output. The initial inputs are external data, such as images and documents. The ultimate outputs accomplish the task, such as recognizing an object in an image.\r\n\r\n\r\n=== Organization ===\r\nThe neurons are typically organized into multiple layers, especially in deep learning. Neurons of one layer connect only to neurons of the immediately preceding and immediately following layers. The layer that receives external data is the input layer. The layer that produces the ultimate result is the output layer. In between them are zero or more hidden layers. Single layer and unlayered networks are also used. Between two layers, multiple connection patterns are possible. They can be 'fully connected', with every neuron in one layer connecting to every neuron in the next layer. They can be pooling, where a group of neurons in one layer connects to a single neuron in the next layer, thereby reducing the number of neurons in that layer. Neurons with only such connections form a directed acyclic graph and are known as feedforward networks. Alternatively, networks that allow connections between neurons in the same or previous layers are known as recurrent networks.\r\n\r\n\r\n=== Hyperparameter ===\r\n\r\nA hyperparameter is a constant parameter whose value is set before the learning process begins. The values of parameters are derived via learning. Examples of hyperparameters include learning rate, the number of hidden layers and batch size. The values of some hyperparameters can be dependent on those of other hyperparameters.", "start_char_idx": 15177, "end_char_idx": 19877, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2dafca1b-2e26-44ff-b71c-3c0e026a6474": {"__data__": {"id_": "2dafca1b-2e26-44ff-b71c-3c0e026a6474", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3ac1d604-830e-41b8-a9a8-de008f222fb1", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "480165125e0c224c2cba9eefb72a335de46e97a0056c67c49a064af6dd915fa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a891be65-ecb9-48af-8146-47a6138a2463", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "42d37196d8ae70fcd903c23c4510b12cbeec5bbf198c8ad030a09a50e2275942", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c9c0b731-45a8-492f-a739-af16b9ec9cd2", "node_type": "1", "metadata": {}, "hash": "92b6f89c01b26c6dc3b814ada53368ba86d2fe1f401daf6e708b6d5a4792838f", "class_name": "RelatedNodeInfo"}}, "text": "Single layer and unlayered networks are also used. Between two layers, multiple connection patterns are possible. They can be 'fully connected', with every neuron in one layer connecting to every neuron in the next layer. They can be pooling, where a group of neurons in one layer connects to a single neuron in the next layer, thereby reducing the number of neurons in that layer. Neurons with only such connections form a directed acyclic graph and are known as feedforward networks. Alternatively, networks that allow connections between neurons in the same or previous layers are known as recurrent networks.\r\n\r\n\r\n=== Hyperparameter ===\r\n\r\nA hyperparameter is a constant parameter whose value is set before the learning process begins. The values of parameters are derived via learning. Examples of hyperparameters include learning rate, the number of hidden layers and batch size. The values of some hyperparameters can be dependent on those of other hyperparameters. For example, the size of some layers can depend on the overall number of layers.\r\n\r\n\r\n=== Learning ===\r\n\r\nLearning is the adaptation of the network to better handle a task by considering sample observations. Learning involves adjusting the weights (and optional thresholds) of the network to improve the accuracy of the result. This is done by minimizing the observed errors. Learning is complete when examining additional observations does not usefully reduce the error rate. Even after learning, the error rate typically does not reach 0. If after learning, the error rate is too high, the network typically must be redesigned. Practically this is done by defining a cost function that is evaluated periodically during learning. As long as its output continues to decline, learning continues. The cost is frequently defined as a statistic whose value can only be approximated. The outputs are actually numbers, so when the error is low, the difference between the output (almost certainly a cat) and the correct answer (cat) is small. Learning attempts to reduce the total of the differences across the observations. Most learning models can be viewed as a straightforward application of optimization theory and statistical estimation.\r\n\r\n\r\n==== Learning rate ====\r\n\r\nThe learning rate defines the size of the corrective steps that the model takes to adjust for errors in each observation. A high learning rate shortens the training time, but with lower ultimate accuracy, while a lower learning rate takes longer, but with the potential for greater accuracy. Optimizations such as Quickprop are primarily aimed at speeding up error minimization, while other improvements mainly try to increase reliability. In order to avoid oscillation inside the network such as alternating connection weights, and to improve the rate of convergence, refinements use an adaptive learning rate that increases or decreases as appropriate. The concept of momentum allows the balance between the gradient and the previous change to be weighted such that the weight adjustment depends to some degree on the previous change. A momentum close to 0 emphasizes the gradient, while a value close to 1 emphasizes the last change.\r\n\r\n\r\n==== Cost function ====\r\nWhile it is possible to define a cost function ad hoc, frequently the choice is determined by the function's desirable properties (such as convexity) or because it arises from the model (e.g. in a probabilistic model the model's posterior probability can be used as an inverse cost).\r\n\r\n\r\n==== Backpropagation ====\r\n\r\nBackpropagation is a method used to adjust the connection weights to compensate for each error found during learning. The error amount is effectively divided among the connections. Technically, backprop calculates the gradient (the derivative) of the cost function associated with a given state with respect to the weights. The weight updates can be done via stochastic gradient descent or other methods, such as extreme learning machines, \"no-prop\" networks, training without backtracking, \"weightless\" networks, and non-connectionist neural networks.\r\n\r\n\r\n=== Learning paradigms ===\r\nMachine learning is commonly separated into three main learning paradigms, supervised learning, unsupervised learning and reinforcement learning. Each corresponds to a particular learning task.\r\n\r\n\r\n==== Supervised learning ====\r\nSupervised learning uses a set of paired inputs and desired outputs. The learning task is to produce the desired output for each input. In this case, the cost function is related to eliminating incorrect deductions. A commonly used cost is the mean-squared error, which tries to minimize the average squared error between the network's output and the desired output. Tasks suited for supervised learning are pattern recognition (also known as classification) and regression (also known as function approximation). Supervised learning is also applicable to sequential data (e.g., for handwriting, speech and gesture recognition).", "start_char_idx": 18905, "end_char_idx": 23877, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c9c0b731-45a8-492f-a739-af16b9ec9cd2": {"__data__": {"id_": "c9c0b731-45a8-492f-a739-af16b9ec9cd2", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3ac1d604-830e-41b8-a9a8-de008f222fb1", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "480165125e0c224c2cba9eefb72a335de46e97a0056c67c49a064af6dd915fa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2dafca1b-2e26-44ff-b71c-3c0e026a6474", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "911df3440ff6d9de555c3c7d515b044aec0a388045e4e05df74b3e28366c6de7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0fb55cac-6853-4bbf-8997-306d32a1948b", "node_type": "1", "metadata": {}, "hash": "ccaad14ec136da09196343d66d41e188c49529e92b77a631b907fc42a8c9778d", "class_name": "RelatedNodeInfo"}}, "text": "=== Learning paradigms ===\r\nMachine learning is commonly separated into three main learning paradigms, supervised learning, unsupervised learning and reinforcement learning. Each corresponds to a particular learning task.\r\n\r\n\r\n==== Supervised learning ====\r\nSupervised learning uses a set of paired inputs and desired outputs. The learning task is to produce the desired output for each input. In this case, the cost function is related to eliminating incorrect deductions. A commonly used cost is the mean-squared error, which tries to minimize the average squared error between the network's output and the desired output. Tasks suited for supervised learning are pattern recognition (also known as classification) and regression (also known as function approximation). Supervised learning is also applicable to sequential data (e.g., for handwriting, speech and gesture recognition). This can be thought of as learning with a \"teacher\", in the form of a function that provides continuous feedback on the quality of solutions obtained thus far.\r\n\r\n\r\n==== Unsupervised learning ====\r\nIn unsupervised learning, input data is given along with the cost function, some function of the data x{\\displaystyle \\textstyle x} and the network's output. The cost function is dependent on the task (the model domain) and any a priori assumptions (the implicit properties of the model, its parameters and the observed variables). As a trivial example, consider the model f(x)=a{\\displaystyle \\textstyle f(x)=a} where a{\\displaystyle \\textstyle a} is a constant and the cost C=E[(x\u2212f(x))2]{\\displaystyle \\textstyle C=E[(x-f(x))^{2}]}. Minimizing this cost produces a value of a{\\displaystyle \\textstyle a} that is equal to the mean of the data. The cost function can be much more complicated. Its form depends on the application: for example, in compression it could be related to the mutual information between x{\\displaystyle \\textstyle x} and f(x){\\displaystyle \\textstyle f(x)}, whereas in statistical modeling, it could be related to the posterior probability of the model given the data (note that in both of those examples, those quantities would be maximized rather than minimized). Tasks that fall within the paradigm of unsupervised learning are in general estimation problems; the applications include clustering, the estimation of statistical distributions, compression and filtering.\r\n\r\n\r\n==== Reinforcement learning ====\r\n\r\nIn applications such as playing video games, an actor takes a string of actions, receiving a generally unpredictable response from the environment after each one. The goal is to win the game, i.e., generate the most positive (lowest cost) responses. In reinforcement learning, the aim is to weight the network (devise a policy) to perform actions that minimize long-term (expected cumulative) cost. At each point in time the agent performs an action and the environment generates an observation and an instantaneous cost, according to some (usually unknown) rules. The rules and the long-term cost usually only can be estimated. At any juncture, the agent decides whether to explore new actions to uncover their costs or to exploit prior learning to proceed more quickly.\r\nFormally the environment is modeled as a Markov decision process (MDP) with states s1,...,sn\u2208S{\\displaystyle \\textstyle {s_{1},...,s_{n}}\\in S} and actions a1,...,am\u2208A{\\displaystyle \\textstyle {a_{1},...,a_{m}}\\in A}. Because the state transitions are not known, probability distributions are used instead: the instantaneous cost distribution P(ct|st){\\displaystyle \\textstyle P(c_{t}|s_{t})}, the observation distribution P(xt|st){\\displaystyle \\textstyle P(x_{t}|s_{t})} and the transition distribution P(st+1|st,at){\\displaystyle \\textstyle P(s_{t+1}|s_{t},a_{t})}, while a policy is defined as the conditional distribution over actions given the observations. Taken together, the two define a Markov chain (MC). The aim is to discover the lowest-cost MC.\r\nANNs serve as the learning component in such applications. Dynamic programming coupled with ANNs (giving neurodynamic programming) has been applied to problems such as those involved in vehicle routing, video games, natural resource management and medicine because of ANNs ability to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of control problems. Tasks that fall within the paradigm of reinforcement learning are control problems, games and other sequential decision making tasks.", "start_char_idx": 22991, "end_char_idx": 27508, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0fb55cac-6853-4bbf-8997-306d32a1948b": {"__data__": {"id_": "0fb55cac-6853-4bbf-8997-306d32a1948b", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3ac1d604-830e-41b8-a9a8-de008f222fb1", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "480165125e0c224c2cba9eefb72a335de46e97a0056c67c49a064af6dd915fa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c9c0b731-45a8-492f-a739-af16b9ec9cd2", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "d305e89af01cda204fe718e2b35f299bb6b03dff0831e9fd3a9c5448feaeca5c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "15674a21-cfe4-45f6-a35b-48b16f779273", "node_type": "1", "metadata": {}, "hash": "726178fecc373a779e87d22a7b479a64d95fbf542386fcee7b84220893668321", "class_name": "RelatedNodeInfo"}}, "text": "Taken together, the two define a Markov chain (MC). The aim is to discover the lowest-cost MC.\r\nANNs serve as the learning component in such applications. Dynamic programming coupled with ANNs (giving neurodynamic programming) has been applied to problems such as those involved in vehicle routing, video games, natural resource management and medicine because of ANNs ability to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of control problems. Tasks that fall within the paradigm of reinforcement learning are control problems, games and other sequential decision making tasks.\r\n\r\n\r\n==== Self-learning ====\r\nSelf-learning in neural networks was introduced in 1982 along with a neural network capable of self-learning named crossbar adaptive array (CAA). It is a system with only one input, situation s, and only one output, action (or behavior) a. It has neither external advice input nor external reinforcement input from the environment. The CAA computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about encountered situations. The system is driven by the interaction between cognition and emotion. Given the memory matrix, W =||w(a,s)||, the crossbar self-learning algorithm in each iteration performs the following computation:\r\n\r\n  In situation s perform action a;\r\n  Receive consequence situation s';\r\n  Compute emotion of being in consequence situation v(s');\r\n  Update crossbar memory w'(a,s) = w(a,s) + v(s').\r\n\r\nThe backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is behavioral environment where it behaves, and the other is genetic environment, where from it initially and only once receives initial emotions about to be encountered situations in the behavioral environment. Having received the genome vector (species vector) from the genetic environment, the CAA will learn a goal-seeking behavior, in the behavioral environment that contains both desirable and undesirable situations.\r\n\r\n\r\n==== Neuroevolution ====\r\n\r\nNeuroevolution can create neural network topologies and weights using evolutionary computation. It is competitive with sophisticated gradient descent approaches. One advantage of neuroevolution is that it may be less prone to get caught in \"dead ends\".\r\n\r\n\r\n=== Stochastic neural network ===\r\nStochastic neural networks originating from  Sherrington\u2013Kirkpatrick models  are a type of artificial neural network built by introducing random variations into the network, either by giving the network's artificial neurons stochastic transfer functions, or by giving them stochastic weights. This makes them useful tools for optimization problems, since the random fluctuations help the network escape from local minima. Stochastic neural networks trained using a Bayesian approach are known as Bayesian neural networks.\r\n\r\n\r\n=== Other ===\r\nIn a Bayesian framework, a distribution over the set of allowed models is chosen to minimize the cost. Evolutionary methods, gene expression programming, simulated annealing, expectation-maximization, non-parametric methods and particle swarm optimization are other learning algorithms. Convergent recursion is a learning algorithm for cerebellar model articulation controller (CMAC) neural networks.\r\n\r\n\r\n==== Modes ====\r\nTwo modes of learning are available: stochastic and batch. In stochastic learning, each input creates a weight adjustment. In batch learning weights are adjusted based on a batch of inputs, accumulating errors over the batch. Stochastic learning introduces \"noise\" into the process, using the local gradient calculated from one data point; this reduces the chance of the network getting stuck in local minima. However, batch learning typically yields a faster, more stable descent to a local minimum, since each update is performed in the direction of the batch's average error. A common compromise is to use \"mini-batches\", small batches with samples in each batch selected stochastically from the entire data set.\r\n\r\n\r\n== Types ==\r\n\r\nANNs have evolved into a broad family of techniques that have advanced the state of the art across multiple domains.  The simplest types have one or more static components, including number of units, number of layers, unit weights and topology. Dynamic types allow one or more of these to evolve via learning. The latter is much more complicated but can shorten learning periods and produce better results. Some types allow/require learning to be \"supervised\" by the operator, while others operate independently. Some types operate purely in hardware, while others are purely software and run on general purpose computers.", "start_char_idx": 26852, "end_char_idx": 31600, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "15674a21-cfe4-45f6-a35b-48b16f779273": {"__data__": {"id_": "15674a21-cfe4-45f6-a35b-48b16f779273", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3ac1d604-830e-41b8-a9a8-de008f222fb1", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "480165125e0c224c2cba9eefb72a335de46e97a0056c67c49a064af6dd915fa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0fb55cac-6853-4bbf-8997-306d32a1948b", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "21d63e9115fc9929f1955d2151025efe685433e9ff59a506089d206dacdd1370", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0527f943-aba2-4020-ad19-a015f8e1c209", "node_type": "1", "metadata": {}, "hash": "faa48bdbc91305c8b5f96d9b5e3f14aff7ab1ec183cc578e99ac7ada5e896d63", "class_name": "RelatedNodeInfo"}}, "text": "However, batch learning typically yields a faster, more stable descent to a local minimum, since each update is performed in the direction of the batch's average error. A common compromise is to use \"mini-batches\", small batches with samples in each batch selected stochastically from the entire data set.\r\n\r\n\r\n== Types ==\r\n\r\nANNs have evolved into a broad family of techniques that have advanced the state of the art across multiple domains.  The simplest types have one or more static components, including number of units, number of layers, unit weights and topology. Dynamic types allow one or more of these to evolve via learning. The latter is much more complicated but can shorten learning periods and produce better results. Some types allow/require learning to be \"supervised\" by the operator, while others operate independently. Some types operate purely in hardware, while others are purely software and run on general purpose computers.\r\nSome of the main breakthroughs include: \r\n\r\nConvolutional neural networks that have proven particularly successful in processing visual and other two-dimensional data; where long short-term memory avoids the vanishing gradient problem and can handle signals that have a mix of low and high frequency components aiding large-vocabulary speech recognition, text-to-speech synthesis, and photo-real talking heads;\r\nCompetitive networks such as generative adversarial networks in which multiple networks (of varying structure) compete with each other, on tasks such as winning a game or on deceiving the opponent about the authenticity of an input.\r\n\r\n\r\n== Network design ==\r\nUsing artificial neural networks requires an understanding of their characteristics.\r\n\r\nChoice of model: This depends on the data representation and the application. Model parameters include the number, type, and connectedness of network layers, as well as the size of each and the connection type (full, pooling, etc. ). Overly complex models learn slowly.\r\nLearning algorithm: Numerous trade-offs exist between learning algorithms. Almost any algorithm will work well with the correct hyperparameters for training on a particular data set. However, selecting and tuning an algorithm for training on unseen data requires significant experimentation.\r\nRobustness: If the model, cost function and learning algorithm are selected appropriately, the resulting ANN can become robust.Neural architecture search (NAS) uses machine learning to automate ANN design. Various approaches to NAS have designed networks that compare well with hand-designed systems. The basic search algorithm is to propose a candidate model, evaluate it against a dataset, and use the results as feedback to teach the NAS network. Available systems include AutoML and AutoKeras. scikit-learn library provides functions to help with building a deep network from scratch. We can then implement a deep network with TensorFlow or Keras.\r\n\r\nHyperparameters must also be defined as part of the design (they are not learned), governing matters such as how many neurons are in each layer, learning rate, step, stride, depth, receptive field and padding (for CNNs), etc. The Python code snippet provides an overview of the training function, which uses the training dataset, number of hidden layer units, learning rate, and number of iterations as parameters:\r\n\r\n\r\n== Applications ==\r\nBecause of their ability to reproduce and model nonlinear processes, artificial neural networks have found applications in many disciplines. These include:\r\n\r\nFunction approximation, or regression analysis, (including time series prediction, fitness approximation, and modeling)\r\nData processing (including filtering, clustering, blind source separation, and compression)\r\nNonlinear system identification and control (including vehicle control, trajectory prediction, adaptive control, process control, and natural resource management)\r\nPattern recognition (including radar systems, face identification, signal classification, novelty detection, 3D reconstruction, object recognition, and sequential decision making)\r\nSequence recognition (including gesture, speech, and handwritten and printed text recognition)\r\nSensor data analysis (including image analysis)\r\nRobotics (including directing manipulators and prostheses)\r\nData mining (including knowledge discovery in databases)\r\nFinance (such as ex-ante models for specific financial long-run forecasts and artificial financial markets)\r\nQuantum chemistry\r\nGeneral game playing\r\nGenerative AI\r\nData visualization\r\nMachine translation\r\nSocial network filtering\r\nE-mail spam filtering\r\nMedical diagnosisANNs have been used to diagnose several types of cancers and to distinguish highly invasive cancer cell lines from less invasive lines using only cell shape information.ANNs have been used to accelerate reliability analysis of infrastructures subject to natural disasters and to predict foundation settlements. It can also be useful to mitigate flood by the use of ANNs for modelling rainfall-runoff. ANNs have also been used for building black-box models in geoscience: hydrology, ocean modelling and coastal engineering, and geomorphology. ANNs have been employed in cybersecurity, with the objective to discriminate between legitimate activities and malicious ones.", "start_char_idx": 30652, "end_char_idx": 35942, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0527f943-aba2-4020-ad19-a015f8e1c209": {"__data__": {"id_": "0527f943-aba2-4020-ad19-a015f8e1c209", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3ac1d604-830e-41b8-a9a8-de008f222fb1", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "480165125e0c224c2cba9eefb72a335de46e97a0056c67c49a064af6dd915fa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "15674a21-cfe4-45f6-a35b-48b16f779273", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "7f49a0da558245c191aca2ea7240b9049c8adfcf70852e86e0cbe8164a016bf6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "901af7fb-6f17-4137-b50c-c6f3d5af6b66", "node_type": "1", "metadata": {}, "hash": "7699816d437df7369cb7be9a88888f1391134e1fb622c4cf200a74f0342b28bc", "class_name": "RelatedNodeInfo"}}, "text": "It can also be useful to mitigate flood by the use of ANNs for modelling rainfall-runoff. ANNs have also been used for building black-box models in geoscience: hydrology, ocean modelling and coastal engineering, and geomorphology. ANNs have been employed in cybersecurity, with the objective to discriminate between legitimate activities and malicious ones. For example, machine learning has been used for classifying Android malware, for identifying domains belonging to threat actors and for detecting URLs posing a security risk. Research is underway on ANN systems designed for penetration testing, for detecting botnets, credit cards frauds and network intrusions.\r\nANNs have been proposed as a tool to solve partial differential equations in physics and simulate the properties of many-body open quantum systems. In brain research ANNs have studied short-term behavior of individual neurons, the dynamics of neural circuitry arise from interactions between individual neurons and how behavior can arise from abstract neural modules that represent complete subsystems. Studies considered long-and short-term plasticity of neural systems and their relation to learning and memory from the individual neuron to the system level.\r\nIt is possible to create a profile of a user's interests from pictures, using artificial neural networks trained for object recognition.Beyond their traditional applications, artificial neural networks are increasingly being utilized in interdisciplinary research, such as materials science. For instance, graph neural networks (GNNs) have demonstrated their capability in scaling deep learning for the discovery of new stable materials by efficiently predicting the total energy of crystals. This application underscores the adaptability and potential of ANNs in tackling complex problems beyond the realms of predictive modeling and artificial intelligence, opening new pathways for scientific discovery and innovation.\r\n\r\n\r\n== Theoretical properties ==\r\n\r\n\r\n=== Computational power ===\r\nThe multilayer perceptron is a universal function approximator, as proven by the universal approximation theorem. However, the proof is not constructive regarding the number of neurons required, the network topology, the weights and the learning parameters.\r\nA specific recurrent architecture with rational-valued weights (as opposed to full precision real number-valued weights) has the power of a universal Turing machine, using a finite number of neurons and standard linear connections. Further, the use of irrational values for weights results in a machine with super-Turing power.\r\n\r\n\r\n=== Capacity ===\r\nA model's \"capacity\" property corresponds to its ability to model any given function. It is related to the amount of information that can be stored in the network and to the notion of complexity.\r\nTwo notions of capacity are known by the community. The information capacity and the VC Dimension. The information capacity of a perceptron is intensively discussed in Sir David MacKay's book which summarizes work by Thomas Cover. The capacity of a network of standard neurons (not convolutional) can be derived by four rules that derive from understanding a neuron as an electrical element. The information capacity captures the functions modelable by the network given any data as input. The second notion, is the VC dimension. VC Dimension uses the principles of measure theory and finds the maximum capacity under the best possible circumstances. This is, given input data in a specific form.  As noted in, the VC Dimension for arbitrary inputs is half the information capacity of a Perceptron. The VC Dimension for arbitrary points is sometimes referred to as Memory Capacity.\r\n\r\n\r\n=== Convergence ===\r\nModels may not consistently converge on a single solution, firstly because local minima may exist, depending on the cost function and the model. Secondly, the optimization method used might not guarantee to converge when it begins far from any local minimum. Thirdly, for sufficiently large data or parameters, some methods become impractical.\r\nAnother issue worthy to mention is that training may cross some Saddle point which may lead the convergence to the wrong direction.\r\nThe convergence behavior of certain types of ANN architectures are more understood than others. When the width of network approaches to infinity, the ANN is well described by its first order Taylor expansion throughout training, and so inherits the convergence behavior of affine models. Another example is when parameters are small, it is observed that ANNs often fits target functions from low to high frequencies. This behavior is referred to as the spectral bias, or frequency principle, of neural networks. This phenomenon is the opposite to the behavior of some well studied iterative numerical schemes such as Jacobi method. Deeper neural networks have been observed to be more biased towards low frequency functions.\r\n\r\n\r\n=== Generalization and statistics ===\r\nApplications whose goal is to create a system that generalizes well to unseen examples, face the possibility of over-training. This arises in convoluted or over-specified systems when the network capacity significantly exceeds the needed free parameters. Two approaches address over-training.", "start_char_idx": 35585, "end_char_idx": 40862, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "901af7fb-6f17-4137-b50c-c6f3d5af6b66": {"__data__": {"id_": "901af7fb-6f17-4137-b50c-c6f3d5af6b66", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3ac1d604-830e-41b8-a9a8-de008f222fb1", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "480165125e0c224c2cba9eefb72a335de46e97a0056c67c49a064af6dd915fa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0527f943-aba2-4020-ad19-a015f8e1c209", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "93a5a755bec0d7c4a21dbf44dea096bb69b9f2f97b6148c2f6c8dee6e8cf6606", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2b0142e5-043e-4e63-8f04-8de04859dc14", "node_type": "1", "metadata": {}, "hash": "93e97b6560da64951d8f339faf635af5ce9dd38d9f96435d95c267d5a14b68f1", "class_name": "RelatedNodeInfo"}}, "text": "The convergence behavior of certain types of ANN architectures are more understood than others. When the width of network approaches to infinity, the ANN is well described by its first order Taylor expansion throughout training, and so inherits the convergence behavior of affine models. Another example is when parameters are small, it is observed that ANNs often fits target functions from low to high frequencies. This behavior is referred to as the spectral bias, or frequency principle, of neural networks. This phenomenon is the opposite to the behavior of some well studied iterative numerical schemes such as Jacobi method. Deeper neural networks have been observed to be more biased towards low frequency functions.\r\n\r\n\r\n=== Generalization and statistics ===\r\nApplications whose goal is to create a system that generalizes well to unseen examples, face the possibility of over-training. This arises in convoluted or over-specified systems when the network capacity significantly exceeds the needed free parameters. Two approaches address over-training. The first is to use cross-validation and similar techniques to check for the presence of over-training and to select hyperparameters to minimize the generalization error.\r\nThe second is to use some form of regularization. This concept emerges in a probabilistic (Bayesian) framework, where regularization can be performed by selecting a larger prior probability over simpler models; but also in statistical learning theory, where the goal is to minimize over two quantities: the 'empirical risk' and the 'structural risk', which roughly corresponds to the error over the training set and the predicted error in unseen data due to overfitting.\r\n\r\nSupervised neural networks that use a mean squared error (MSE) cost function can use formal statistical methods to determine the confidence of the trained model. The MSE on a validation set can be used as an estimate for variance. This value can then be used to calculate the confidence interval of network output, assuming a normal distribution. A confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified.\r\nBy assigning a softmax activation function, a generalization of the logistic function, on the output layer of the neural network (or a softmax component in a component-based network) for categorical target variables, the outputs can be interpreted as posterior probabilities. This is useful in classification as it gives a certainty measure on classifications.\r\nThe softmax activation function is:\r\n\r\nyi=exi\u2211j=1cexj{\\displaystyle y_{i}={\\frac {e^{x_{i}}}{\\sum _{j=1}^{c}e^{x_{j}}}}}\r\n\r\n\r\n== Criticism ==\r\n\r\n\r\n=== Training ===\r\nA common criticism of neural networks, particularly in robotics, is that they require too many training samples for real-world operation.\r\nAny learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases. Potential solutions include randomly shuffling training examples, by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example, grouping examples in so-called mini-batches and/or introducing a recursive least squares algorithm for CMAC.\r\nDean Pomerleau uses a neural network to train a robotic vehicle to drive on multiple types of roads (single lane, multi-lane, dirt, etc.), and a large amount of his research is devoted to extrapolating multiple training scenarios from a single training experience, and preserving past training diversity so that the system does not become overtrained (if, for example, it is presented with a series of right turns\u2014it should not learn to always turn right).\r\n\r\n\r\n=== Theory ===\r\nA central claim of ANNs is that they embody new and powerful general principles for processing information. These principles are ill-defined. It is often claimed that they are emergent from the network itself. This allows simple statistical association (the basic function of artificial neural networks) to be described as learning or recognition. In 1997, Alexander Dewdney, a former Scientific American columnist, commented that as a result, artificial neural networks have a \"something-for-nothing quality, one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are. No human hand (or mind) intervenes; solutions are found as if by magic; and no one, it seems, has learned anything\". One response to Dewdney is that neural networks have been successfully used to handle many complex and diverse tasks, ranging from autonomously flying aircraft to detecting credit card fraud to mastering the game of Go.\r\nTechnology writer Roger Bridgman commented:\r\n\r\nNeural networks, for instance, are in the dock not only because they have been hyped to high heaven, (what hasn't?)", "start_char_idx": 39801, "end_char_idx": 44757, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2b0142e5-043e-4e63-8f04-8de04859dc14": {"__data__": {"id_": "2b0142e5-043e-4e63-8f04-8de04859dc14", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3ac1d604-830e-41b8-a9a8-de008f222fb1", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "480165125e0c224c2cba9eefb72a335de46e97a0056c67c49a064af6dd915fa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "901af7fb-6f17-4137-b50c-c6f3d5af6b66", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "fa44e59c99645b9a316b72a001c505afc18c8182c4d12f4badbbb9998b066978", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f8b7c665-e589-48ad-9671-d189464ab198", "node_type": "1", "metadata": {}, "hash": "bb8b9c420e5b3032e7960a530765f436f0cb40fad1ff7a5f72534eaaf50b6ecf", "class_name": "RelatedNodeInfo"}}, "text": "This allows simple statistical association (the basic function of artificial neural networks) to be described as learning or recognition. In 1997, Alexander Dewdney, a former Scientific American columnist, commented that as a result, artificial neural networks have a \"something-for-nothing quality, one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are. No human hand (or mind) intervenes; solutions are found as if by magic; and no one, it seems, has learned anything\". One response to Dewdney is that neural networks have been successfully used to handle many complex and diverse tasks, ranging from autonomously flying aircraft to detecting credit card fraud to mastering the game of Go.\r\nTechnology writer Roger Bridgman commented:\r\n\r\nNeural networks, for instance, are in the dock not only because they have been hyped to high heaven, (what hasn't?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be \"an opaque, unreadable table...valueless as a scientific resource\".\r\nIn spite of his emphatic declaration that science is not technology, Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having.\r\n\r\nAlthough it is true that analyzing what has been learned by an artificial neural network is difficult, it is much easier to do so than to analyze what has been learned by a biological neural network. Moreover, recent emphasis on the explainability of AI has contributed towards the development of methods, notably those based on attention mechanisms, for visualizing and explaining learned neural networks. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles that allow a learning machine to be successful. For example, Bengio and LeCun (2007) wrote an article regarding local vs non-local learning, as well as shallow vs deep architecture.Biological brains use both shallow and deep circuits as reported by brain anatomy, displaying a wide variety of invariance. Weng argued that the brain self-wires largely according to signal statistics and therefore, a serial cascade cannot catch all major statistical dependencies.\r\n\r\n\r\n=== Hardware ===\r\nLarge and effective neural networks require considerable computing resources. While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a simplified neuron on von Neumann architecture may consume vast amounts of memory and storage. Furthermore, the designer often needs to transmit signals through many of these connections and their associated neurons \u2013  which require enormous CPU power and time.\r\nSchmidhuber noted that the resurgence of neural networks in the twenty-first century is largely attributable to advances in hardware: from 1991 to 2015, computing power, especially as delivered by GPGPUs (on GPUs), has increased around a million-fold, making the standard backpropagation algorithm feasible for training networks that are several layers deeper than before. The use of accelerators such as FPGAs and GPUs can reduce training times from months to days.Neuromorphic engineering or a physical neural network addresses the hardware difficulty directly, by constructing non-von-Neumann chips to directly implement neural networks in circuitry. Another type of chip optimized for neural network processing is called a Tensor Processing Unit, or TPU.\r\n\r\n\r\n=== Practical counterexamples ===\r\nAnalyzing what has been learned by an ANN is much easier than analyzing what has been learned by a biological neural network. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a learning machine to be successful. For example, local vs. non-local learning and shallow vs. deep architecture.\r\n\r\n\r\n=== Hybrid approaches ===\r\nAdvocates of hybrid models (combining neural networks and symbolic approaches) say that such a mixture can better capture the mechanisms of the human mind.\r\n\r\n\r\n=== Dataset bias ===\r\nNeural networks are dependent on the quality of the data they are trained on, thus low quality data with imbalanced representativeness can lead to the model learning and perpetuating societal biases.  These inherited biases become especially critical when the ANNs are integrated into real-world scenarios where the training data may be imbalanced due to the scarcity of data for a specific race, gender or other attribute. This imbalance can result in the model having inadequate representation and understanding of underrepresented groups, leading to discriminatory outcomes that exasperate societal inequalities, especially in applications like facial recognition, hiring processes, and law enforcement.", "start_char_idx": 43830, "end_char_idx": 48863, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f8b7c665-e589-48ad-9671-d189464ab198": {"__data__": {"id_": "f8b7c665-e589-48ad-9671-d189464ab198", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3ac1d604-830e-41b8-a9a8-de008f222fb1", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "480165125e0c224c2cba9eefb72a335de46e97a0056c67c49a064af6dd915fa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b0142e5-043e-4e63-8f04-8de04859dc14", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "900083f03f6e7089d3f0c62bfa7630818657d060ad24ae883bf73a45d68ae29b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "78d6f382-b599-4b32-b2b5-6e048e806702", "node_type": "1", "metadata": {}, "hash": "09e91c4973b039b093b1ff8842663de79c86e72fd5effad07fec849e56570b54", "class_name": "RelatedNodeInfo"}}, "text": "For example, local vs. non-local learning and shallow vs. deep architecture.\r\n\r\n\r\n=== Hybrid approaches ===\r\nAdvocates of hybrid models (combining neural networks and symbolic approaches) say that such a mixture can better capture the mechanisms of the human mind.\r\n\r\n\r\n=== Dataset bias ===\r\nNeural networks are dependent on the quality of the data they are trained on, thus low quality data with imbalanced representativeness can lead to the model learning and perpetuating societal biases.  These inherited biases become especially critical when the ANNs are integrated into real-world scenarios where the training data may be imbalanced due to the scarcity of data for a specific race, gender or other attribute. This imbalance can result in the model having inadequate representation and understanding of underrepresented groups, leading to discriminatory outcomes that exasperate societal inequalities, especially in applications like facial recognition, hiring processes, and law enforcement. For example, in 2018, Amazon had to scrap a recruiting tool because the model favored men over women for jobs in software engineering due to the higher number of male workers in the field. The program would penalize any resume with the word \"woman\" or the name of any women's college. However, the use of synthetic data can help reduce dataset bias and increase representation in datasets.\r\n\r\n\r\n== Gallery ==\r\n\r\n\t\t\t\r\n\t\t\t\r\n\t\t\r\n\t\t\r\n\t\t\t\r\n\t\t\t\r\n\t\t\r\n\t\t\r\n\t\t\t\r\n\t\t\t\r\n\t\t\r\n\t\t\r\n\t\t\t\r\n\t\t\t\r\n\t\t\r\n\t\t\r\n\t\t\t\r\n\t\t\t\r\n\t\t\r\n\t\t\r\n\t\t\t\r\n\t\t\t\r\n\t\t\r\n\t\t\r\n\t\t\t\r\n\t\t\t\r\n\t\t\r\n\r\n\r\n== Recent advancements and future directions ==\r\nArtificial neural networks (ANNs) have undergone significant advancements, particularly in their ability to model complex systems, handle large data sets, and adapt to various types of applications. Their evolution over the past few decades has been marked by a broad range of applications in fields such as image processing, speech recognition, natural language processing, finance, and medicine.\r\n\r\n\r\n=== Image processing ===\r\nIn the realm of image processing, ANNs are employed in tasks such as image classification, object recognition, and image segmentation. For instance, deep convolutional neural networks (CNNs) have been important in handwritten digit recognition, achieving state-of-the-art performance. This demonstrates the ability of ANNs to effectively process and interpret complex visual information, leading to advancements in fields ranging from automated surveillance to medical imaging.\r\n\r\n\r\n=== Speech recognition ===\r\nBy modeling speech signals, ANNs are used for tasks like speaker identification and speech-to-text conversion. Deep neural network architectures have introduced significant improvements in large vocabulary continuous speech recognition, outperforming traditional techniques. These advancements have enabled the development of more accurate and efficient voice-activated systems, enhancing user interfaces in technology products.\r\n\r\n\r\n=== Natural language processing ===\r\nIn natural language processing, ANNs are used for tasks such as text classification, sentiment analysis, and machine translation. They have enabled the development of models that can accurately translate between languages, understand the context and sentiment in textual data, and categorize text based on content. This has implications for automated customer service, content moderation, and language understanding technologies.\r\n\r\n\r\n=== Control systems ===\r\nIn the domain of control systems, ANNs are used to model dynamic systems for tasks such as system identification, control design, and optimization. For instance, deep feedforward neural networks are important in system identification and control applications.\r\n\r\n\r\n=== Finance ===\r\n\r\nANNs are used for stock market prediction and credit scoring: \r\n\r\nIn investing, ANNs can process vast amounts of financial data, recognize complex patterns, and forecast stock market trends, aiding investors and risk managers in making informed decisions.\r\nIn credit scoring, ANNs offer data-driven, personalized assessments of creditworthiness, improving the accuracy of default predictions and automating the lending process.ANNs require high-quality data and careful tuning, and their \"black-box\" nature can pose challenges in interpretation. Nevertheless, ongoing advancements suggest that ANNs continue to play a role in finance, offering valuable insights and enhancing risk management strategies.\r\n\r\n\r\n=== Medicine ===\r\nANNs are able to process and analyze vast medical datasets. They enhance diagnostic accuracy, especially by interpreting complex medical imaging for early disease detection, and by predicting patient outcomes for personalized treatment planning. In drug discovery, ANNs speed up the identification of potential drug candidates and predict their efficacy and safety, significantly reducing development time and costs. Additionally, their application in personalized medicine and healthcare data analysis allows tailored therapies and efficient patient care management. Ongoing research is aimed at addressing remaining challenges such as data privacy and model interpretability, as well as expanding the scope of ANN applications in medicine.\r\n\r\n\r\n=== Content creation ===\r\nANNs such as generative adversarial networks (GAN) and transformers are used for content creation across numerous industries. This is because deep learning models are able to learn the style of an artist or musician from huge datasets and generate completely new artworks and music compositions.", "start_char_idx": 47865, "end_char_idx": 53401, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "78d6f382-b599-4b32-b2b5-6e048e806702": {"__data__": {"id_": "78d6f382-b599-4b32-b2b5-6e048e806702", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3ac1d604-830e-41b8-a9a8-de008f222fb1", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "480165125e0c224c2cba9eefb72a335de46e97a0056c67c49a064af6dd915fa2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f8b7c665-e589-48ad-9671-d189464ab198", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "acdceb65c3eb6c6fe2db405fc1e088db797acdca5fda4f64c6b76d512511fd2d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b6d7ca1a-2865-4317-a75c-a33a6b7be282", "node_type": "1", "metadata": {}, "hash": "5427bcfbd79812b8c7615ce2be409fa186290855cac7849ef75c6fcc380d659d", "class_name": "RelatedNodeInfo"}}, "text": "=== Medicine ===\r\nANNs are able to process and analyze vast medical datasets. They enhance diagnostic accuracy, especially by interpreting complex medical imaging for early disease detection, and by predicting patient outcomes for personalized treatment planning. In drug discovery, ANNs speed up the identification of potential drug candidates and predict their efficacy and safety, significantly reducing development time and costs. Additionally, their application in personalized medicine and healthcare data analysis allows tailored therapies and efficient patient care management. Ongoing research is aimed at addressing remaining challenges such as data privacy and model interpretability, as well as expanding the scope of ANN applications in medicine.\r\n\r\n\r\n=== Content creation ===\r\nANNs such as generative adversarial networks (GAN) and transformers are used for content creation across numerous industries. This is because deep learning models are able to learn the style of an artist or musician from huge datasets and generate completely new artworks and music compositions. For instance, DALL-E is a deep neural network trained on 650 million pairs of images and texts across the internet that can create artworks based on text entered by the user. In the field of music, transformers are used to create original music for commercials and documentaries through companies such as AIVA and Jukedeck. In the marketing industry generative models are used to create personalized advertisements for consumers. Additionally, major film companies are partnering with technology companies to analyze the financial success of a film, such as the partnership between Warner Bros and technology company Cinelytic established in 2020. Furthermore, neural networks have found uses in video game creation, where Non Player Characters (NPCs) can make decisions based on all the characters currently in the game.\r\n\r\n\r\n== See also ==\r\n\r\n\r\n== External links ==\r\n\r\nA Brief Introduction to Neural Networks (D. Kriesel) - Illustrated, bilingual manuscript about artificial neural networks; Topics so far: Perceptrons, Backpropagation, Radial Basis Functions, Recurrent Neural Networks, Self Organizing Maps, Hopfield Networks.\r\nReview of Neural Networks in Materials Science\r\nArtificial Neural Networks Tutorial in three languages (Univ. Polit\u00e9cnica de Madrid)\r\nAnother introduction to ANN\r\nNext Generation of Neural Networks - Google Tech Talks\r\nPerformance of Neural Networks\r\nNeural Networks and Information\r\nSanderson G (5 October 2017). \"But what is a Neural Network?\". 3Blue1Brown. Archived from the original on 7 November 2021 \u2013 via YouTube.\r\n\r\n\r\n== Notes ==\r\n\r\n\r\n== References ==\r\n\r\n\r\n== Bibliography ==", "start_char_idx": 52315, "end_char_idx": 55017, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b6d7ca1a-2865-4317-a75c-a33a6b7be282": {"__data__": {"id_": "b6d7ca1a-2865-4317-a75c-a33a6b7be282", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network.txt", "file_name": "Neural network.txt", "file_type": "text/plain", "file_size": 3884, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b3dd0d56-5656-4ec4-898f-0583c55fee61", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network.txt", "file_name": "Neural network.txt", "file_type": "text/plain", "file_size": 3884, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "d85412bd6c5c08f1070a897b0d7b5df980bd11a5f1f275b7831f4434f80a9ce4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "78d6f382-b599-4b32-b2b5-6e048e806702", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "a93afa1a59f947692e6828b7def10558df2694e004aa0dc2e829c0997cf931ac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1ee57893-890b-4a65-aa27-1a9a53a7e5d3", "node_type": "1", "metadata": {}, "hash": "60fe2980b2b22768eb2e119c8066daa5c34abe61a9c53d3076ddb9daad75f4a7", "class_name": "RelatedNodeInfo"}}, "text": "A neural network is a group of interconnected units called neurons that send signals to one another. Neurons can be either biological cells or mathematical models. While individual neurons are simple, many of them together in a network can perform complex tasks. There are two main types of neural network.\r\n\r\nIn neuroscience, a biological neural network is a physical structure found in brains and complex nervous systems \u2013 a population of nerve cells connected by synapses.\r\nIn machine learning, an artificial neural network is a mathematical model used to approximate nonlinear functions. Artificial neural networks are used to solve artificial intelligence problems.\r\n\r\n\r\n== Biological neural network ==\r\n\r\nA biological neural network is a population of biological neurons chemically connected to each other by synapses. A given neuron can be connected to hundreds of thousands of synapses.\r\nEach neuron sends and receives electrochemical signals called action potentials to its connected neighbors. A neuron can serve an excitatory role, amplifying and propagating signals it receives, or an inhibitory role, suppressing signals instead.Populations of interconnected neurons that are smaller than neural networks are called neural circuits. Very large interconnected networks are called large scale brain networks, and many of these together form brains and nervous systems.\r\nSignals generated by neural networks in the brain eventually travel through the nervous system and across neuromuscular junctions to muscle cells, where they cause contraction and thereby motion.\r\n\r\n\r\n== Artificial neural network ==\r\n\r\nAn artificial neural network is a mathematical model used to approximate nonlinear functions. While early artificial neural networks were physical machines, today they are almost always implemented in software.\r\nNeurons in an artificial neural network are usually arranged into layers, with information passing from the first layer (the input layer) through one or more intermediate layers (hidden layers) to the final layer (the output layer).\r\nThe \"signal\" input to each neuron is a number, specifically a linear combination of the outputs of the connected neurons in the previous layer. The signal each neuron outputs is calculated from this number, according to its activation function. The behavior of the network depends on the strengths (or weights) of the connections between neurons. A network is trained by modifying these weights through empirical risk minimization or backpropagation in order to fit some preexisting dataset.Neural networks are used to solve problems in artificial intelligence, and have thereby found applications in many disciplines, including predictive modeling, adaptive control, facial recognition, handwriting recognition, general game playing, and generative AI.\r\n\r\n\r\n== History ==\r\n\r\nThe theoretical base for contemporary neural networks was independently proposed by Alexander Bain in 1873 and William James in 1890. Both posited that human thought emerged from interactions among large numbers of neurons inside the brain. In 1949, Donald Hebb described Hebbian learning, the idea that neural networks can change and learn over time by strengthening a synapse every time a signal travels along it.Artificial neural networks were originally used to model biological neural networks starting in the 1930s under the approach of connectionism. However, starting with the invention of the perceptron, a simple artificial neural network, by Warren McCulloch and Walter Pitts in 1943, followed by the implementation of one in hardware by Frank Rosenblatt in 1957,\r\nartificial neural networks became increasingly used for machine learning applications instead, and increasingly different from their biological counterparts.\r\n\r\n\r\n== See also ==\r\nEmergence\r\nBiological cybernetics\r\nBiologically-inspired computing\r\n\r\n\r\n== References ==", "start_char_idx": 0, "end_char_idx": 3882, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1ee57893-890b-4a65-aa27-1a9a53a7e5d3": {"__data__": {"id_": "1ee57893-890b-4a65-aa27-1a9a53a7e5d3", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Optical neural network.txt", "file_name": "Optical neural network.txt", "file_type": "text/plain", "file_size": 6069, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8723d808-8df2-4c09-85af-69e73ce0845d", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Optical neural network.txt", "file_name": "Optical neural network.txt", "file_type": "text/plain", "file_size": 6069, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "6961ef56d143fe582cab4b3b962043c943f1411eb0eb44b998d75ee8e3bc5a10", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b6d7ca1a-2865-4317-a75c-a33a6b7be282", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network.txt", "file_name": "Neural network.txt", "file_type": "text/plain", "file_size": 3884, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "d85412bd6c5c08f1070a897b0d7b5df980bd11a5f1f275b7831f4434f80a9ce4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "db403fbb-faf5-44eb-9d16-d1cb01910355", "node_type": "1", "metadata": {}, "hash": "1ee5a02f1eb3aa42507bae60e4265d62cf57b84889367a9f61af776e261a2f12", "class_name": "RelatedNodeInfo"}}, "text": "An optical neural network is a physical implementation of an artificial neural network  with optical components.  Early optical neural networks used a photorefractive Volume hologram to interconnect arrays of input neurons to arrays of output with synaptic weights in proportion to the multiplexed hologram's strength.  Volume holograms were further multiplexed using spectral hole burning to add one dimension of wavelength to space to achieve four dimensional interconnects of two dimensional arrays of neural inputs and outputs.  This research led to extensive research on alternative methods using the strength of the optical interconnect for implementing neuronal communications.Some artificial neural networks that have been implemented as optical neural networks include the Hopfield neural network and the Kohonen self-organizing map with liquid crystal spatial light modulators  Optical neural networks can also be based on the principles of neuromorphic engineering, creating neuromorphic photonic systems. Typically, these systems encode information in the networks using spikes, mimicking the functionality of spiking neural networks in optical and photonic hardware. Photonic devices that have demonstrated neuromorphic functionalities include (among others) vertical-cavity surface-emitting lasers, integrated photonic modulators, optoelectronic systems based on superconducting Josephson junctions or systems based on resonant tunnelling diodes.\r\n\r\n\r\n== Electrochemical vs. optical neural networks ==\r\nBiological neural networks function on an electrochemical basis, while optical neural networks use electromagnetic waves. Optical interfaces to biological neural networks can be created with optogenetics, but is not the same as an optical neural networks. In biological neural networks there exist a lot of different mechanisms for dynamically changing the state of the neurons, these include short-term and long-term synaptic plasticity. Synaptic plasticity is among the electrophysiological phenomena used to control the efficiency of synaptic transmission, long-term for learning and memory, and short-term for short transient changes in synaptic transmission efficiency. Implementing this with optical components is difficult, and ideally requires advanced photonic materials. Properties that might be desirable in photonic materials for optical neural networks include the ability to change their efficiency of transmitting light, based on the intensity of incoming light.\r\n\r\n\r\n== Rising Era of Optical Neural Networks ==\r\nWith the increasing significance of computer vision in various domains, the computational cost of these tasks has increased, making it more important to develop the new approaches of the processing acceleration. Optical computing has emerged as a potential alternative to GPU acceleration for modern neural networks, particularly considering the looming obsolescence of Moore's Law. Consequently, optical neural networks have garnered increased attention in the research community. Presently, two primary methods of optical neural computing are under research: silicon photonics-based and free-space optics. Each approach has its benefits and drawbacks; while silicon photonics may offer superior speed, it lacks the massive parallelism that free-space optics can deliver.\r\nGiven the substantial parallelism capabilities of free-space optics, researchers have focused on taking advantage of it. One implementation, proposed by Lin et al., involves the training and fabrication of phase masks for a handwritten digit classifier. By stacking 3D-printed phase masks, light passing through the fabricated network can be read by a photodetector array of ten detectors, each representing a digit class ranging from 1 to 10. Although this network can achieve terahertz-range classification, it lacks flexibility, as the phase masks are fabricated for a specific task and cannot be retrained.\r\nAn alternative method for classification in free-space optics, introduced by Cahng et al., employs a 4F system that is based on the convolution theorem to perform convolution operations. This system uses two lenses to execute the Fourier transforms of the convolution operation, enabling passive conversion into the Fourier domain without power consumption or latency. However, the convolution operation kernels in this implementation are also fabricated phase masks, limiting the device's functionality to specific convolutional layers of the network only.\r\nIn contrast, Li et al. proposed a technique involving kernel tiling to use the parallelism of the 4F system while using a Digital Micromirror Device (DMD) instead of a phase mask. This approach allows users to upload various kernels into the 4F system and execute the entire network's inference on a single device. Unfortunately, modern neural networks are not designed for the 4F systems, as they were primarily developed during the CPU/GPU era. Mostly because they tend to use a lower resolution and a high number of channels in their feature maps.\r\n\r\n\r\n== Other Implementations ==\r\nIn 2007 there was one model of Optical Neural Network: the Programmable Optical Array/Analogic Computer (POAC).", "start_char_idx": 0, "end_char_idx": 5186, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "db403fbb-faf5-44eb-9d16-d1cb01910355": {"__data__": {"id_": "db403fbb-faf5-44eb-9d16-d1cb01910355", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Optical neural network.txt", "file_name": "Optical neural network.txt", "file_type": "text/plain", "file_size": 6069, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8723d808-8df2-4c09-85af-69e73ce0845d", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Optical neural network.txt", "file_name": "Optical neural network.txt", "file_type": "text/plain", "file_size": 6069, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "6961ef56d143fe582cab4b3b962043c943f1411eb0eb44b998d75ee8e3bc5a10", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1ee57893-890b-4a65-aa27-1a9a53a7e5d3", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Optical neural network.txt", "file_name": "Optical neural network.txt", "file_type": "text/plain", "file_size": 6069, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "6fc5ab31b7df347dcd3e69bf56c386bede0c4d9f5bd838f4584f1ffa71edf2f8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d22e0f0c-429b-4e59-9af6-8240becc88ef", "node_type": "1", "metadata": {}, "hash": "563ddc367541f8394529479cd5fdd14855e1d5ad2caee16ed5397c3de1d1fd51", "class_name": "RelatedNodeInfo"}}, "text": "However, the convolution operation kernels in this implementation are also fabricated phase masks, limiting the device's functionality to specific convolutional layers of the network only.\r\nIn contrast, Li et al. proposed a technique involving kernel tiling to use the parallelism of the 4F system while using a Digital Micromirror Device (DMD) instead of a phase mask. This approach allows users to upload various kernels into the 4F system and execute the entire network's inference on a single device. Unfortunately, modern neural networks are not designed for the 4F systems, as they were primarily developed during the CPU/GPU era. Mostly because they tend to use a lower resolution and a high number of channels in their feature maps.\r\n\r\n\r\n== Other Implementations ==\r\nIn 2007 there was one model of Optical Neural Network: the Programmable Optical Array/Analogic Computer (POAC). It had been implemented in the year 2000 and reported based on modified Joint Fourier Transform Correlator (JTC) and Bacteriorhodopsin (BR) as a holographic optical memory. Full parallelism, large array size and the speed of light are three promises offered by POAC to implement an optical CNN. They had been investigated during the last years with their practical limitations and considerations yielding the design of the first portable POAC version.\r\nThe practical details \u2013 hardware (optical setups) and software (optical templates) \u2013 were published. However, POAC is a general purpose and programmable array computer that has a wide range of applications including:\r\n\r\nimage processing\r\npattern recognition\r\ntarget tracking\r\nreal-time video processing\r\ndocument security\r\noptical switching\r\n\r\n\r\n== See also ==\r\nOptical computing\r\nQuantum neural network\r\n\r\n\r\n== References ==", "start_char_idx": 4300, "end_char_idx": 6065, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d22e0f0c-429b-4e59-9af6-8240becc88ef": {"__data__": {"id_": "d22e0f0c-429b-4e59-9af6-8240becc88ef", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Physics-informed neural networks.txt", "file_name": "Physics-informed neural networks.txt", "file_type": "text/plain", "file_size": 15160, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ce81106e-199e-40e5-9c27-b066438275c8", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Physics-informed neural networks.txt", "file_name": "Physics-informed neural networks.txt", "file_type": "text/plain", "file_size": 15160, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "48248a05e0c85a7f679f36f1a10f2df6a636967b2592335e4a774027fb7f8429", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "db403fbb-faf5-44eb-9d16-d1cb01910355", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Optical neural network.txt", "file_name": "Optical neural network.txt", "file_type": "text/plain", "file_size": 6069, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "402407fbce98fb8c3e906b4ba9c244e8ad2e54346942aebd8f6a853ce5421607", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ea59c639-1d74-4547-91a2-9b4560711e16", "node_type": "1", "metadata": {}, "hash": "e28f37840365df09905cbeca95103a3844a645fc58d81c419d5ce2cd43db1339", "class_name": "RelatedNodeInfo"}}, "text": "Physics-informed neural networks (PINNs) are a type of universal function approximators that can embed the knowledge of any physical laws that govern a given data-set in the learning process, and can be described by partial differential equations (PDEs). They overcome the low data availability of some biological and engineering systems that makes most state-of-the-art machine learning techniques lack robustness, rendering them ineffective in these scenarios. The prior knowledge of general physical laws acts in the training of neural networks (NNs) as a regularization agent that limits the space of admissible solutions, increasing the correctness of the function approximation. This way, embedding this prior information into a neural network results in enhancing the information content of the available data, facilitating the learning algorithm to capture the right solution and to generalize well even with a low amount of training examples.\r\n\r\n\r\n== Function approximation ==\r\nMost of the physical laws that govern the dynamics of a system can be described by partial differential equations. For example, the Navier\u2013Stokes equations are a set of partial differential equations derived from the conservation laws (i.e., conservation of mass, momentum, and energy) that govern fluid mechanics. The solution of the Navier\u2013Stokes equations with appropriate initial and boundary conditions allows the quantification of flow dynamics in a precisely defined geometry. However, these equations cannot be solved exactly and therefore numerical methods must be used (such as finite differences, finite elements and finite volumes). In this setting, these governing equations must be solved while accounting for prior assumptions, linearization, and adequate time and space discretization.\r\nRecently, solving the governing partial differential equations of physical phenomena using deep learning has emerged as a new field of scientific machine learning (SciML), leveraging the universal approximation theorem and high expressivity of neural networks. In general, deep neural networks could approximate any high-dimensional function given that sufficient training data are supplied. However, such networks do not consider the physical characteristics underlying the problem, and the level of approximation accuracy provided by them is still heavily dependent on careful specifications of the problem geometry as well as the initial and boundary conditions. Without this preliminary information, the solution is not unique and may lose physical correctness. On the other hand, physics-informed neural networks (PINNs) leverage governing physical equations in neural network training. Namely, PINNs are designed to be trained to satisfy the given training data as well as the imposed governing equations. In this fashion, a neural network can be guided with training data that do not necessarily need to be large and complete. Potentially, an accurate solution of partial differential equations can be found without knowing the boundary conditions. Therefore, with some knowledge about the physical characteristics of the problem and some form of training data (even sparse and incomplete), PINN may be used for finding an optimal solution with high fidelity.\r\nPINNs allow for addressing a wide range of problems in computational science and represent a pioneering technology leading to the development of new classes of numerical solvers for PDEs. PINNs can be thought of as a meshfree alternative to traditional approaches (e.g., CFD for fluid dynamics), and new data-driven approaches for model inversion and system identification. Notably, the trained PINN network can be used for predicting the values on simulation grids of different resolutions without the need to be retrained. In addition, they allow for exploiting automatic differentiation (AD) to compute the required derivatives in the partial differential equations, a new class of differentiation techniques widely used to derive neural networks assessed to be superior to numerical or symbolic differentiation.\r\n\r\n\r\n== Modeling and computation ==\r\nA general nonlinear partial differential equation can be:\r\nut+N[u;\u03bb]=0,x\u2208\u03a9,t\u2208[0,T]{\\displaystyle u_{t}+N[u;\\lambda ]=0,\\quad x\\in \\Omega ,\\quad t\\in [0,T]}\r\nwhere u(t,x){\\displaystyle u(t,x)} denotes the solution, N[\u22c5;\u03bb]{\\displaystyle N[\\cdot ;\\lambda ]} is a nonlinear operator parametrized by \u03bb{\\displaystyle \\lambda }, and \u03a9{\\displaystyle \\Omega } is a subset of RD{\\displaystyle \\mathbb {R} ^{D}}. This general form of governing equations summarizes a wide range of problems in mathematical physics, such as conservative laws, diffusion process, advection-diffusion systems, and kinetic equations. Given noisy measurements of a generic dynamic system described by the equation above, PINNs can be designed to solve two classes of problems:\r\n\r\ndata-driven solution\r\ndata-driven discoveryof partial differential equations.", "start_char_idx": 0, "end_char_idx": 4935, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ea59c639-1d74-4547-91a2-9b4560711e16": {"__data__": {"id_": "ea59c639-1d74-4547-91a2-9b4560711e16", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Physics-informed neural networks.txt", "file_name": "Physics-informed neural networks.txt", "file_type": "text/plain", "file_size": 15160, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ce81106e-199e-40e5-9c27-b066438275c8", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Physics-informed neural networks.txt", "file_name": "Physics-informed neural networks.txt", "file_type": "text/plain", "file_size": 15160, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "48248a05e0c85a7f679f36f1a10f2df6a636967b2592335e4a774027fb7f8429", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d22e0f0c-429b-4e59-9af6-8240becc88ef", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Physics-informed neural networks.txt", "file_name": "Physics-informed neural networks.txt", "file_type": "text/plain", "file_size": 15160, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "ad1d2a653727e6d3f0e6d861fb5d3b99450fdba566fc2df4025f00d54a212418", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3a47ed73-33e0-445f-97dc-e79b93ca9ec5", "node_type": "1", "metadata": {}, "hash": "18e85760cb124139377f0d031ce33c7b9f03f43aa29b0894180d8138d85fb9c5", "class_name": "RelatedNodeInfo"}}, "text": "This general form of governing equations summarizes a wide range of problems in mathematical physics, such as conservative laws, diffusion process, advection-diffusion systems, and kinetic equations. Given noisy measurements of a generic dynamic system described by the equation above, PINNs can be designed to solve two classes of problems:\r\n\r\ndata-driven solution\r\ndata-driven discoveryof partial differential equations.\r\n\r\n\r\n=== Data-driven solution of partial differential equations ===\r\nThe data-driven solution of PDE computes the hidden state u(t,x){\\displaystyle u(t,x)} of the system given boundary data and/or measurements z{\\displaystyle z}, and fixed model parameters \u03bb{\\displaystyle \\lambda }. We solve:\r\nut+N[u]=0,x\u2208\u03a9,t\u2208[0,T]{\\displaystyle u_{t}+N[u]=0,\\quad x\\in \\Omega ,\\quad t\\in [0,T]}.\r\nBy defining the residual f(t,x){\\displaystyle f(t,x)} as\r\nf:=ut+N[u]=0{\\displaystyle f:=u_{t}+N[u]=0},\r\nand approximating u(t,x){\\displaystyle u(t,x)} by a deep neural network. This network can be differentiated using automatic differentiation. The parameters of u(t,x){\\displaystyle u(t,x)} and f(t,x){\\displaystyle f(t,x)} can be then learned by minimizing the following loss function Ltot{\\displaystyle L_{tot}}:\r\nLtot=Lu+Lf{\\displaystyle L_{tot}=L_{u}+L_{f}}.\r\nWhere Lu=\u2016u\u2212z\u2016\u0393{\\displaystyle L_{u}=\\Vert u-z\\Vert _{\\Gamma }} is the error between the PINN u(t,x){\\displaystyle u(t,x)} and the set of boundary conditions and measured data on the set of points \u0393{\\displaystyle \\Gamma } where the boundary conditions and data are defined, and Lf=\u2016f\u2016\u0393{\\displaystyle L_{f}=\\Vert f\\Vert _{\\Gamma }} is the mean-squared error of the residual function. This second term encourages the PINN to learn the structural information expressed by the partial differential equation during the training process.\r\nThis approach has been used to yield computationally efficient physics-informed surrogate models with applications in the forecasting of physical processes, model predictive control, multi-physics and multi-scale modeling, and simulation. It has been shown to converge to the solution of the PDE.\r\n\r\n\r\n=== Data-driven discovery of partial differential equations ===\r\nGiven noisy and incomplete measurements z{\\displaystyle z} of the state of the system, the data-driven discovery of PDE results in computing the unknown state u(t,x){\\displaystyle u(t,x)} and learning model parameters \u03bb{\\displaystyle \\lambda } that best describe the observed data and it reads as follows:\r\nut+N[u;\u03bb]=0,x\u2208\u03a9,t\u2208[0,T]{\\displaystyle u_{t}+N[u;\\lambda ]=0,\\quad x\\in \\Omega ,\\quad t\\in [0,T]}.\r\nBy defining f(t,x){\\displaystyle f(t,x)} as\r\nf:=ut+N[u;\u03bb]=0{\\displaystyle f:=u_{t}+N[u;\\lambda ]=0},\r\nand approximating u(t,x){\\displaystyle u(t,x)} by a deep neural network, f(t,x){\\displaystyle f(t,x)} results in a PINN. This network can be derived using automatic differentiation. The parameters of u(t,x){\\displaystyle u(t,x)} and f(t,x){\\displaystyle f(t,x)}, together with the parameter \u03bb{\\displaystyle \\lambda } of the differential operator can be then learned by minimizing the following loss function Ltot{\\displaystyle L_{tot}}:\r\nLtot=Lu+Lf{\\displaystyle L_{tot}=L_{u}+L_{f}}.\r\nWhere Lu=\u2016u\u2212z\u2016\u0393{\\displaystyle L_{u}=\\Vert u-z\\Vert _{\\Gamma }}, with u{\\displaystyle u} and z{\\displaystyle z} state solutions and measurements at sparse location \u0393{\\displaystyle \\Gamma }, respectively and Lf=\u2016f\u2016\u0393{\\displaystyle L_{f}=\\Vert f\\Vert _{\\Gamma }} residual function. This second term requires the structured information represented by the partial differential equations to be satisfied in the training process.\r\nThis strategy allows for discovering dynamic models described by nonlinear PDEs assembling computationally efficient and fully differentiable surrogate models that may find application in predictive forecasting, control, and data assimilation.", "start_char_idx": 4513, "end_char_idx": 8343, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a47ed73-33e0-445f-97dc-e79b93ca9ec5": {"__data__": {"id_": "3a47ed73-33e0-445f-97dc-e79b93ca9ec5", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Physics-informed neural networks.txt", "file_name": "Physics-informed neural networks.txt", "file_type": "text/plain", "file_size": 15160, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ce81106e-199e-40e5-9c27-b066438275c8", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Physics-informed neural networks.txt", "file_name": "Physics-informed neural networks.txt", "file_type": "text/plain", "file_size": 15160, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "48248a05e0c85a7f679f36f1a10f2df6a636967b2592335e4a774027fb7f8429", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea59c639-1d74-4547-91a2-9b4560711e16", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Physics-informed neural networks.txt", "file_name": "Physics-informed neural networks.txt", "file_type": "text/plain", "file_size": 15160, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "8588ca3eb7a9519e594d76aeeef4268f7a830e3f52cd995ce3ff6d4cb1005041", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "11ea7199-5210-433c-b512-b2f97d6632c1", "node_type": "1", "metadata": {}, "hash": "106933d35b332392eac55711ec1953058ddd0a50e5207bc02d65a8dae73d5fa5", "class_name": "RelatedNodeInfo"}}, "text": "Where Lu=\u2016u\u2212z\u2016\u0393{\\displaystyle L_{u}=\\Vert u-z\\Vert _{\\Gamma }}, with u{\\displaystyle u} and z{\\displaystyle z} state solutions and measurements at sparse location \u0393{\\displaystyle \\Gamma }, respectively and Lf=\u2016f\u2016\u0393{\\displaystyle L_{f}=\\Vert f\\Vert _{\\Gamma }} residual function. This second term requires the structured information represented by the partial differential equations to be satisfied in the training process.\r\nThis strategy allows for discovering dynamic models described by nonlinear PDEs assembling computationally efficient and fully differentiable surrogate models that may find application in predictive forecasting, control, and data assimilation.\r\n\r\n\r\n== Physics-informed neural networks for piece-wise function approximation ==\r\nPINN is unable to approximate PDEs that have strong non-linearity or sharp gradients that commonly occur in practical fluid flow problems. Piece-wise approximation has been an old practice in the field of numerical approximation. With the capability of approximating strong non-linearity extremely light weight PINNs are used to solve PDEs in much larger discrete subdomains that increases accuracy substantially and decreases computational load as well. DPINN (Distributed physics-informed neural networks) and DPIELM (Distributed physics-informed extreme learning machines) are generalizable space-time domain discretization for better approximation. DPIELM is an extremely fast and lightweight approximator with competitive accuracy. Domain scaling on the top has a special effect. Another school of thought is discretization for parallel computation to leverage usage of available computational resources. \r\nXPINNs  is a generalized space-time domain decomposition approach for the physics-informed neural networks (PINNs) to solve nonlinear partial differential equations on arbitrary complex-geometry domains. The XPINNs further pushes the boundaries of both PINNs as well as Conservative PINNs (cPINNs), which is a spatial domain decomposition approach in the PINN framework tailored to conservation laws. Compared to PINN, the XPINN method has large representation and parallelization capacity due to the inherent property of deployment of multiple neural networks in the smaller subdomains. Unlike cPINN, XPINN can be extended to any type of PDEs. Moreover, the domain can be decomposed in any arbitrary way (in space and time), which is not possible in cPINN. Thus, XPINN offers both space and time parallelization, thereby reducing the training cost more effectively. The XPINN is particularly effective for the large-scale problems (involving large data set) as well as for the high-dimensional problems where single network based PINN is not adequate. The rigorous bounds on the errors resulting from the approximation of the nonlinear PDEs (incompressible Navier\u2013Stokes equations) with PINNs and XPINNs are proved.\r\n\r\n\r\n== Physics-informed neural networks and functional interpolation ==\r\nIn the PINN framework, initial and boundary conditions are not analytically satisfied, thus they need to be included in the loss function of the network to be simultaneously learned with the differential equation (DE) unknown functions. Having competing objectives during the network's training can lead to unbalanced gradients while using gradient-based techniques, which causes PINNs to often struggle to accurately learn the underlying DE solution. This drawback is overcome by using functional interpolation techniques such as the Theory of Functional Connections (TFC)'s constrained expression, in the Deep-TFC framework, which reduces the solution search space of constrained problems to the subspace of neural network that analytically satisfies the constraints. A further improvement of PINN and functional interpolation approach is given by the Extreme Theory of Functional Connections (X-TFC) framework, where a single-layer Neural Network and the extreme learning machine training algorithm are employed. X-TFC allows to improve the accuracy and performance of regular PINNs, and its robustness and reliability are proved for stiff problems, optimal control, aerospace, and rarefied gas dynamics applications.\r\n\r\n\r\n== Physics-informed PointNet (PIPN) for multiple sets of irregular geometries ==\r\nRegular PINNs are only able to obtain the solution of a forward or inverse problem on a single geometry. It means that for any new geometry (computational domain), one must retrain a PINN. This limitation of regular PINNs imposes high computational costs, specifically for a comprehensive investigation of geometric parameters in industrial designs. Physics-informed PointNet (PIPN)  is fundamentally the result of a combination of PINN's loss function with PointNet. In fact, instead of using a simple fully connected neural network, PIPN uses PointNet as the core of its neural network.", "start_char_idx": 7677, "end_char_idx": 12526, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11ea7199-5210-433c-b512-b2f97d6632c1": {"__data__": {"id_": "11ea7199-5210-433c-b512-b2f97d6632c1", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Physics-informed neural networks.txt", "file_name": "Physics-informed neural networks.txt", "file_type": "text/plain", "file_size": 15160, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ce81106e-199e-40e5-9c27-b066438275c8", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Physics-informed neural networks.txt", "file_name": "Physics-informed neural networks.txt", "file_type": "text/plain", "file_size": 15160, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "48248a05e0c85a7f679f36f1a10f2df6a636967b2592335e4a774027fb7f8429", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a47ed73-33e0-445f-97dc-e79b93ca9ec5", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Physics-informed neural networks.txt", "file_name": "Physics-informed neural networks.txt", "file_type": "text/plain", "file_size": 15160, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "d68b4a2057dd9f294df50d42db0f1dfe5538dbbb7a41e87ebc1b1c24e7ae95d8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a867abeb-9fb5-4e4d-8a2e-46f692632347", "node_type": "1", "metadata": {}, "hash": "fd39ae99b600aa391a3ecde7c7292efcaa7cf545ce89db47aff931440a4bc790", "class_name": "RelatedNodeInfo"}}, "text": "X-TFC allows to improve the accuracy and performance of regular PINNs, and its robustness and reliability are proved for stiff problems, optimal control, aerospace, and rarefied gas dynamics applications.\r\n\r\n\r\n== Physics-informed PointNet (PIPN) for multiple sets of irregular geometries ==\r\nRegular PINNs are only able to obtain the solution of a forward or inverse problem on a single geometry. It means that for any new geometry (computational domain), one must retrain a PINN. This limitation of regular PINNs imposes high computational costs, specifically for a comprehensive investigation of geometric parameters in industrial designs. Physics-informed PointNet (PIPN)  is fundamentally the result of a combination of PINN's loss function with PointNet. In fact, instead of using a simple fully connected neural network, PIPN uses PointNet as the core of its neural network. PointNet has been primarily designed for deep learning of 3D object classification and segmentation by the research group of Leonidas J. Guibas. PointNet extracts geometric features of input computational domains in PIPN. Thus, PIPN is able to solve governing equations on multiple computational domains (rather than only a single domain) with irregular geometries, simultaneously. The effectiveness of PIPN has been shown for incompressible flow, heat transfer and linear elasticity.\r\n\r\n\r\n== Physics-informed neural networks (PINNs) for inverse computations ==\r\nPhysics-informed neural networks (PINNs) have particularly proven effective in solving inverse problems within differential equations, demonstrating their applicability across science, engineering, and economics. They have shown useful for solving inverse problems in a variety of fields, including nano-optics, topology optimization/characterization, multiphase flow in porous media, and high-speed fluid flow. PINNs have demonstrated flexibility when dealing with noisy and uncertain observation datasets. They also demonstrated clear advantages in the inverse calculation of parameters for multi-fidelity datasets, meaning datasets with different quality, quantity, and types of observations. Uncertainties in calculations can be evaluated using ensemble-based or Bayesian-based calculations. \r\n\r\n\r\n== Limitations ==\r\nTranslation and discontinuous behavior are hard to approximate using PINNs. They fail when solving differential equations with slight advective dominance. \r\nThe difficulty in training of PINNs in advection-dominated PDEs can be explained by Kolmogorov n\u2013width of the solution. \r\nThey also fail to solve a system of dynamical systems and hence have not been a success in solving chaotic equations. One of the reasons behind the failure of the regular PINNs is soft-constraining of Dirichlet and Neumann boundary conditions which pose multi-objective optimization problems. \r\nThis requires the need for manually weighing the loss terms to be able to optimize. \r\nAnother reason is getting optimization itself. Posing PDE solving as an optimization problem brings in all the problems that are faced in the world of optimization, the major one being getting stuck at a local optimum pretty often.\r\n\r\n\r\n== References ==\r\n\r\n\r\n== External links ==\r\nPINN \u2013 repository to implement physics-informed neural network in Python\r\nXPINN \u2013 repository to implement extended physics-informed neural network (XPINN) in Python\r\nPIPN [2]\u2013 repository to implement physics-informed PointNet (PIPN) in Python", "start_char_idx": 11646, "end_char_idx": 15094, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a867abeb-9fb5-4e4d-8a2e-46f692632347": {"__data__": {"id_": "a867abeb-9fb5-4e4d-8a2e-46f692632347", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Quantum neural network.txt", "file_name": "Quantum neural network.txt", "file_type": "text/plain", "file_size": 9833, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b5ae85da-3fb2-408e-b8a5-2dfb8d152245", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Quantum neural network.txt", "file_name": "Quantum neural network.txt", "file_type": "text/plain", "file_size": 9833, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "5272016b971be97a3ddd2f65be8e349029d512006a61be48203d8aa214b84cc2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "11ea7199-5210-433c-b512-b2f97d6632c1", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Physics-informed neural networks.txt", "file_name": "Physics-informed neural networks.txt", "file_type": "text/plain", "file_size": 15160, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "bdd8f825354a9d706ec60a4109af45d5aa0c7ebaf6c7fe0c1dddcba4b1d527a6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "66988b57-ecea-479b-a63c-453d78d96ebc", "node_type": "1", "metadata": {}, "hash": "149f7acdbcb0a22bd48312caec32246197a8fdde0c63f782cd16e30059ce173d", "class_name": "RelatedNodeInfo"}}, "text": "Quantum neural networks are computational neural network models which are based on the principles of quantum mechanics. The first ideas on quantum neural computation were published independently in 1995 by Subhash Kak and Ron Chrisley, engaging with the theory of quantum mind, which posits that quantum effects play a role in cognitive function. However, typical research in quantum neural networks involves combining classical artificial neural network models (which are widely used in machine learning for the important task of pattern recognition) with the advantages of quantum information in order to develop more efficient algorithms. One important motivation for these investigations is the difficulty to train classical neural networks, especially in big data applications. The hope is that features of quantum computing such as quantum parallelism or the effects of interference and entanglement can be used as resources. Since the technological implementation of a quantum computer is still in a premature stage, such quantum neural network models are mostly theoretical proposals that await their full implementation in physical experiments.\r\nMost Quantum neural networks are developed as feed-forward networks. Similar to their classical counterparts, this structure intakes input from one layer of qubits, and passes that input onto another layer of qubits. This layer of qubits evaluates this information and passes on the output to the next layer. Eventually the path leads to the final layer of qubits. The layers do not have to be of the same width, meaning they don't have to have the same number of qubits as the layer before or after it. This structure is trained on which path to take similar to classical artificial neural networks. This is discussed in a lower section. Quantum neural networks refer to three different categories: Quantum computer with classical data, classical computer with quantum data, and quantum computer with quantum data.\r\n\r\n\r\n== Examples ==\r\nQuantum neural network research is still in its infancy, and a conglomeration of proposals and ideas of varying scope and mathematical rigor have been put forward. Most of them are based on the idea of replacing classical binary or McCulloch-Pitts neurons with a qubit (which can be called a \u201cquron\u201d), resulting in neural units that can be in a superposition of the state \u2018firing\u2019 and \u2018resting\u2019.\r\n\r\n\r\n=== Quantum perceptrons ===\r\nA lot of proposals attempt to find a quantum equivalent for the perceptron unit from which neural nets are constructed. A problem is that nonlinear activation functions do not immediately correspond to the mathematical structure of quantum theory, since a quantum evolution is described by linear operations and leads to probabilistic observation. Ideas to imitate the perceptron activation function with a quantum mechanical formalism reach from special measurements  to postulating non-linear quantum operators (a mathematical framework that is disputed). A direct implementation of the activation function using the circuit-based model of quantum computation has recently been proposed by Schuld, Sinayskiy and Petruccione based on the quantum phase estimation algorithm.\r\n\r\n\r\n=== Quantum networks ===\r\nAt a larger scale, researchers have attempted to generalize neural networks to the quantum setting. One way of constructing a quantum neuron is to first generalise classical neurons and then generalising them further to make unitary gates. Interactions between neurons can be controlled quantumly, with unitary gates, or classically, via measurement of the network states. This high-level theoretical technique can be applied broadly, by taking different types of networks and different implementations of quantum neurons, such as photonically implemented neurons and quantum reservoir processor (quantum version of reservoir computing). Most learning algorithms follow the classical model of training an artificial neural network to learn the input-output function of a given training set and use classical feedback loops to update parameters of the quantum system until they converge to an optimal configuration. Learning as a parameter optimisation problem has also been approached by adiabatic models of quantum computing.Quantum neural networks can be applied to algorithmic design: given qubits with tunable mutual interactions, one can attempt to learn interactions following the classical backpropagation rule from a training set of desired input-output relations, taken to be the desired output algorithm's behavior. The quantum network thus \u2018learns\u2019 an algorithm.\r\n\r\n\r\n=== Quantum associative memory ===\r\nThe first quantum associative memory algorithm was introduced by Dan Ventura and Tony Martinez in 1999. The authors do not attempt to translate the structure of artificial neural network models into quantum theory, but propose an algorithm for a circuit-based quantum computer that simulates associative memory. The memory states (in Hopfield neural networks saved in the weights of the neural connections) are written into a superposition, and a Grover-like quantum search algorithm retrieves the memory state closest to a given input. As such, this is not a fully content-addressable memory, since only incomplete patterns can be retrieved.", "start_char_idx": 0, "end_char_idx": 5285, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "66988b57-ecea-479b-a63c-453d78d96ebc": {"__data__": {"id_": "66988b57-ecea-479b-a63c-453d78d96ebc", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Quantum neural network.txt", "file_name": "Quantum neural network.txt", "file_type": "text/plain", "file_size": 9833, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b5ae85da-3fb2-408e-b8a5-2dfb8d152245", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Quantum neural network.txt", "file_name": "Quantum neural network.txt", "file_type": "text/plain", "file_size": 9833, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "5272016b971be97a3ddd2f65be8e349029d512006a61be48203d8aa214b84cc2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a867abeb-9fb5-4e4d-8a2e-46f692632347", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Quantum neural network.txt", "file_name": "Quantum neural network.txt", "file_type": "text/plain", "file_size": 9833, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "9ef15bc5d48422a2b68c71fa33b18a8b061e97d1780221af7c9d7cda70d541fb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8586402a-2911-4dd4-8056-0ca56828b8eb", "node_type": "1", "metadata": {}, "hash": "952facd9f3f24811d2be8c0e9a204b9e37802e1cdac020d1a089e3a13033117d", "class_name": "RelatedNodeInfo"}}, "text": "The quantum network thus \u2018learns\u2019 an algorithm.\r\n\r\n\r\n=== Quantum associative memory ===\r\nThe first quantum associative memory algorithm was introduced by Dan Ventura and Tony Martinez in 1999. The authors do not attempt to translate the structure of artificial neural network models into quantum theory, but propose an algorithm for a circuit-based quantum computer that simulates associative memory. The memory states (in Hopfield neural networks saved in the weights of the neural connections) are written into a superposition, and a Grover-like quantum search algorithm retrieves the memory state closest to a given input. As such, this is not a fully content-addressable memory, since only incomplete patterns can be retrieved.\r\nThe first truly content-addressable quantum memory, which can retrieve patterns also from corrupted inputs, was proposed by Carlo A. Trugenberger. Both memories can store an exponential (in terms of n qubits) number of patterns but can be used only once due to the no-cloning theorem and their destruction upon measurement.\r\nTrugenberger, however, has shown that his proababilistic model of quantum associative memory can be efficiently implemented and re-used multiples times for any polynomial number of stored patterns, a large advantage with respect to classical associative memories.\r\n\r\n\r\n=== Classical neural networks inspired by quantum theory ===\r\nA substantial amount of interest has been given to a \u201cquantum-inspired\u201d model that uses ideas from quantum theory to implement a neural network based on fuzzy logic.\r\n\r\n\r\n== Training ==\r\nQuantum Neural Networks can be theoretically trained similarly to training classical/artificial neural networks. A key difference lies in communication between the layers of a neural networks. For classical neural networks, at the end of a given operation, the current perceptron copies its output to the next layer of perceptron(s) in the network. However, in a quantum neural network, where each perceptron is a qubit, this would violate the no-cloning theorem. A proposed generalized solution to this is to replace the classical fan-out method with an arbitrary unitary that spreads out, but does not copy, the output of one qubit to the next layer of qubits. Using this fan-out Unitary (Uf{\\displaystyle U_{f}}) with a dummy state qubit in a known state (Ex. |0\u27e9{\\displaystyle |0\\rangle } in the computational basis), also known as an Ancilla bit, the information from the qubit can be transferred to the next layer of qubits. This process adheres to the quantum operation requirement of reversibility.Using this quantum feed-forward network, deep neural networks can be executed and trained efficiently. A deep neural network is essentially a network with many hidden-layers, as seen in the sample model neural network above. Since the Quantum neural network being discussed uses fan-out Unitary operators, and each operator only acts on its respective input, only two layers are used at any given time. In other words, no Unitary operator is acting on the entire network at any given time, meaning the number of qubits required for a given step depends on the number of inputs in a given layer. Since Quantum Computers are notorious for their ability to run multiple iterations in a short period of time, the efficiency of a quantum neural network is solely dependent on the number of qubits in any given layer, and not on the depth of the network.\r\n\r\n\r\n=== Cost functions ===\r\nTo determine the effectiveness of a neural network, a cost function is used, which essentially measures the proximity of the network's output to the expected or desired output. In a Classical Neural Network, the weights (w{\\displaystyle w}) and biases (b{\\displaystyle b}) at each step determine the outcome of the cost function C(w,b){\\displaystyle C(w,b)}. When training a Classical Neural network, the weights and biases are adjusted after each iteration, and given equation 1 below, where y(x){\\displaystyle y(x)} is the desired output and aout(x){\\displaystyle a^{\\text{out}}(x)} is the actual output, the cost function is optimized when C(w,b){\\displaystyle C(w,b)}= 0. For a quantum neural network, the cost function is determined by measuring the fidelity of the outcome state (\u03c1out{\\displaystyle \\rho ^{\\text{out}}}) with the desired outcome state (\u03d5out{\\displaystyle \\phi ^{\\text{out}}}), seen in Equation 2 below. In this case, the Unitary operators are adjusted after each iteration, and the cost function is optimized when C = 1.", "start_char_idx": 4554, "end_char_idx": 9056, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8586402a-2911-4dd4-8056-0ca56828b8eb": {"__data__": {"id_": "8586402a-2911-4dd4-8056-0ca56828b8eb", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Quantum neural network.txt", "file_name": "Quantum neural network.txt", "file_type": "text/plain", "file_size": 9833, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b5ae85da-3fb2-408e-b8a5-2dfb8d152245", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Quantum neural network.txt", "file_name": "Quantum neural network.txt", "file_type": "text/plain", "file_size": 9833, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "5272016b971be97a3ddd2f65be8e349029d512006a61be48203d8aa214b84cc2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "66988b57-ecea-479b-a63c-453d78d96ebc", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Quantum neural network.txt", "file_name": "Quantum neural network.txt", "file_type": "text/plain", "file_size": 9833, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "a27ab7c20874e0e0d12c2fc48bbac5a883308ed7ac3eca7bcbb97e41118b6dac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3d5d556a-2b5a-4d92-81de-de9613dcaf14", "node_type": "1", "metadata": {}, "hash": "2a6068b456638da6a9af83a08c3b1a09bfd389cd1a4038554d36e70acbd4fb76", "class_name": "RelatedNodeInfo"}}, "text": "When training a Classical Neural network, the weights and biases are adjusted after each iteration, and given equation 1 below, where y(x){\\displaystyle y(x)} is the desired output and aout(x){\\displaystyle a^{\\text{out}}(x)} is the actual output, the cost function is optimized when C(w,b){\\displaystyle C(w,b)}= 0. For a quantum neural network, the cost function is determined by measuring the fidelity of the outcome state (\u03c1out{\\displaystyle \\rho ^{\\text{out}}}) with the desired outcome state (\u03d5out{\\displaystyle \\phi ^{\\text{out}}}), seen in Equation 2 below. In this case, the Unitary operators are adjusted after each iteration, and the cost function is optimized when C = 1.\r\nEquation 1 C(w,b)=1N\u2211x||y(x)\u2212aout(x)||2{\\displaystyle C(w,b)={1 \\over N}\\sum _{x}{||y(x)-a^{\\text{out}}(x)|| \\over 2}}\r\n\r\nEquation 2 C=1N\u2211xN\u27e8\u03d5out|\u03c1out|\u03d5out\u27e9{\\displaystyle C={1 \\over N}\\sum _{x}^{N}{\\langle \\phi ^{\\text{out}}|\\rho ^{\\text{out}}|\\phi ^{\\text{out}}\\rangle }}\r\n\r\n\r\n== See also ==\r\nDifferentiable programming\r\nOptical neural network\r\nHolographic associative memory\r\nQuantum cognition\r\nQuantum machine learning\r\n\r\n\r\n== References ==\r\n\r\n\r\n== External links ==\r\nRecent review of quantum neural networks by M. Schuld, I. Sinayskiy and F. Petruccione\r\nReview of quantum neural networks by Wei\r\nArticle by P. Gralewicz on the plausibility of quantum computing in biological neural networks\r\nTraining a neural net to recognize images", "start_char_idx": 8373, "end_char_idx": 9796, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3d5d556a-2b5a-4d92-81de-de9613dcaf14": {"__data__": {"id_": "3d5d556a-2b5a-4d92-81de-de9613dcaf14", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Rectifier (neural networks).txt", "file_name": "Rectifier (neural networks).txt", "file_type": "text/plain", "file_size": 9659, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9ccd033a-d520-40e3-9826-a461456a44c3", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Rectifier (neural networks).txt", "file_name": "Rectifier (neural networks).txt", "file_type": "text/plain", "file_size": 9659, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "ed9303c22ea9b950b95f3de52b9a752e0b4cea7e7aeed756c7338f1b3af8dd9b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8586402a-2911-4dd4-8056-0ca56828b8eb", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Quantum neural network.txt", "file_name": "Quantum neural network.txt", "file_type": "text/plain", "file_size": 9833, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "ca64de853a1dd715cd2b8b5b107fec67daa846552e301c8482cf708c5da8d7d8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c614bf6e-137a-467b-808c-ddca9eb19fee", "node_type": "1", "metadata": {}, "hash": "00843734083ded0ca2f77542ef67f8ed363147afe522f566ce2b7fc9db0a96cd", "class_name": "RelatedNodeInfo"}}, "text": "In the context of artificial neural networks, the rectifier or ReLU (rectified linear unit) activation function is an activation function defined as the positive part of its argument:\r\n\r\nf(x)=x+=max(0,x)=x+|x|2={xif x>0,0otherwise,{\\displaystyle f(x)=x^{+}=\\max(0,x)={\\frac {x+|x|}{2}}={\\begin{cases}x&{\\text{if }}x>0,\\\\0&{\\text{otherwise}},\\end{cases}}}where x is the input to a neuron. This is also known as a ramp function and is analogous to half-wave rectification in electrical engineering. This activation function was introduced by Kunihiko Fukushima in 1969 in the context of visual feature extraction in hierarchical neural networks. It was later argued that it  has strong biological motivations and mathematical justifications. In 2011 it was found to enable better training of deeper networks, compared to the widely used activation functions prior to 2011, e.g., the logistic sigmoid (which is inspired by probability theory; see logistic regression) and its more practical counterpart, the hyperbolic tangent. The rectifier is, as of 2017, the most popular activation function for deep neural networks.Rectified linear units find applications in computer vision and speech recognition using deep neural nets and computational neuroscience.\r\n\r\n\r\n== Advantages ==\r\nSparse activation: For example, in a randomly initialized network, only about 50% of hidden units are activated (have a non-zero output).\r\nBetter gradient propagation: Fewer vanishing gradient problems compared to sigmoidal activation functions that saturate in both directions.\r\nEfficient computation: Only comparison, addition and multiplication.\r\nScale-invariant: max(0,ax)=amax(0,x) for a\u22650{\\displaystyle \\max(0,ax)=a\\max(0,x){\\text{ for }}a\\geq 0}.Rectifying activation functions were used to separate specific excitation and unspecific inhibition in the neural abstraction pyramid, which was trained in a supervised way to learn several computer vision tasks. In 2011, the use of the rectifier as a non-linearity has been shown to enable training deep supervised neural networks without requiring unsupervised pre-training. Rectified linear units, compared to sigmoid function or similar activation functions, allow faster and effective training of deep neural architectures on large and complex datasets.\r\n\r\n\r\n== Potential problems ==\r\nNon-differentiable at zero; however, it is differentiable anywhere else, and the value of the derivative at zero can be arbitrarily chosen to be 0 or 1.\r\nNot zero-centered.\r\nUnbounded.\r\nDying ReLU problem: ReLU (rectified linear unit) neurons can sometimes be pushed into states in which they become inactive for essentially all inputs. In this state, no gradients flow backward through the neuron, and so the neuron becomes stuck in a perpetually inactive state and \"dies\". This is a form of the vanishing gradient problem. In some cases, large numbers of neurons in a network can become stuck in dead states, effectively decreasing the model capacity. This problem typically arises when the learning rate is set too high. It may be mitigated by using leaky ReLUs instead, which assign a small positive slope for x < 0; however, the performance is reduced.\r\n\r\n\r\n== Variants ==\r\n\r\n\r\n=== Piecewise-linear variants ===\r\n\r\n\r\n==== Leaky ReLU ====\r\nLeaky ReLUs allow a small, positive gradient when the unit is not active, helping to mitigate the vanishing gradient problem.\r\n\r\nf(x)={xif x>0,0.01xotherwise.f\u2032(x)={1if x>0,0.01otherwise.{\\displaystyle f(x)={\\begin{cases}x&{\\text{if }}x>0,\\\\0.01x&{\\text{otherwise}}.\\end{cases}}\\qquad \\qquad f'(x)={\\begin{cases}1&{\\text{if }}x>0,\\\\0.01&{\\text{otherwise}}.\\end{cases}}}\r\n\r\n\r\n==== Parametric ReLU ====\r\nParametric ReLUs (PReLUs) take this idea further by making the coefficient of leakage into a parameter that is learned along with the other neural-network parameters.\r\nf(x)={xif x>0,a\u22c5xotherwise.f\u2032(x)={1if x>0,aotherwise.", "start_char_idx": 0, "end_char_idx": 3888, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c614bf6e-137a-467b-808c-ddca9eb19fee": {"__data__": {"id_": "c614bf6e-137a-467b-808c-ddca9eb19fee", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Rectifier (neural networks).txt", "file_name": "Rectifier (neural networks).txt", "file_type": "text/plain", "file_size": 9659, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9ccd033a-d520-40e3-9826-a461456a44c3", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Rectifier (neural networks).txt", "file_name": "Rectifier (neural networks).txt", "file_type": "text/plain", "file_size": 9659, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "ed9303c22ea9b950b95f3de52b9a752e0b4cea7e7aeed756c7338f1b3af8dd9b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3d5d556a-2b5a-4d92-81de-de9613dcaf14", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Rectifier (neural networks).txt", "file_name": "Rectifier (neural networks).txt", "file_type": "text/plain", "file_size": 9659, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "ebf6b130c8773bcc69ca916b802763915af38a7d6f4ff80b1c8a3cb55ba266d9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "004d0974-94af-4f09-b309-f480e2ac7873", "node_type": "1", "metadata": {}, "hash": "fc7f3810441af400be8a386b2edddee5885ed8f73281ed54404c7392a9371f1b", "class_name": "RelatedNodeInfo"}}, "text": "f(x)={xif x>0,0.01xotherwise.f\u2032(x)={1if x>0,0.01otherwise.{\\displaystyle f(x)={\\begin{cases}x&{\\text{if }}x>0,\\\\0.01x&{\\text{otherwise}}.\\end{cases}}\\qquad \\qquad f'(x)={\\begin{cases}1&{\\text{if }}x>0,\\\\0.01&{\\text{otherwise}}.\\end{cases}}}\r\n\r\n\r\n==== Parametric ReLU ====\r\nParametric ReLUs (PReLUs) take this idea further by making the coefficient of leakage into a parameter that is learned along with the other neural-network parameters.\r\nf(x)={xif x>0,a\u22c5xotherwise.f\u2032(x)={1if x>0,aotherwise.{\\displaystyle f(x)={\\begin{cases}x&{\\text{if }}x>0,\\\\a\\cdot x&{\\text{otherwise}}.\\end{cases}}\\qquad \\qquad \\qquad f'(x)={\\begin{cases}1&{\\text{if }}x>0,\\\\a&{\\text{otherwise}}.\\end{cases}}}Note that for a \u2264 1, this is equivalent to\r\n\r\nf(x)=max(x,ax){\\displaystyle f(x)=\\max(x,ax)}and thus has a relation to \"maxout\" networks.\r\n\r\n\r\n=== Other non-linear variants ===\r\n\r\n\r\n==== Gaussian-error linear unit (GELU) ====\r\nGELU is a smooth approximation to the rectifier:\r\n\r\nf(x)=x\u22c5\u03a6(x),{\\displaystyle f(x)=x\\cdot \\Phi (x),}f\u2032(x)=x\u22c5\u03a6\u2032(x)+\u03a6(x),{\\displaystyle f'(x)=x\\cdot \\Phi '(x)+\\Phi (x),}where \u03a6(x)=P(X\u2a7dx){\\displaystyle \\Phi (x)=P(X\\leqslant x)} is the cumulative distribution function of the standard normal distribution.\r\nThis activation function is illustrated in the figure at the start of this article. It has a \"bump\" to the left of x < 0 and serves as the default activation for models such as BERT.\r\n\r\n\r\n==== SiLU ====\r\n\r\nThe SiLU (sigmoid linear unit) or swish function is another smooth approximation, first coined in the GELU paper:\r\nf(x)=x\u22c5sigmoid\u2061(x),{\\displaystyle f(x)=x\\cdot \\operatorname {sigmoid} (x),}f\u2032(x)=x\u22c5sigmoid\u2032\u2061(x)+sigmoid\u2061(x),{\\displaystyle f'(x)=x\\cdot \\operatorname {sigmoid} '(x)+\\operatorname {sigmoid} (x),}where sigmoid\u2061(x){\\displaystyle \\operatorname {sigmoid} (x)} is the sigmoid function.\r\n\r\n\r\n==== Softplus ====\r\nA smooth approximation to the rectifier is the analytic function\r\n\r\nf(x)=ln\u2061(1+ex),f\u2032(x)=ex1+ex=11+e\u2212x,{\\displaystyle f(x)=\\ln(1+e^{x}),\\qquad \\qquad f'(x)={\\frac {e^{x}}{1+e^{x}}}={\\frac {1}{1+e^{-x}}},}which is called the softplus or SmoothReLU function. For large negative x{\\displaystyle x} it is roughly ln\u20611{\\displaystyle \\ln 1}, so just above 0, while for large positive x{\\displaystyle x} it is roughly ln\u2061(ex){\\displaystyle \\ln(e^{x})}, so just above x{\\displaystyle x}. \r\nThis function can be approximated as:\r\n\r\nln\u2061(1+ex)\u2248{ln\u20612,x=0,x1\u2212e\u2212x/ln\u20612,x\u22600{\\displaystyle \\ln \\left(1+e^{x}\\right)\\approx {\\begin{cases}\\ln 2,&x=0,\\\\[6pt]{\\frac {x}{1-e^{-x/\\ln 2}}},&x\\neq 0\\end{cases}}}By making the change of variables x=yln\u2061(2){\\displaystyle x=y\\ln(2)}, this is equivalent to\r\n\r\nlog2\u2061(1+2y)\u2248{1,y=0,y1\u2212e\u2212y,y\u22600.", "start_char_idx": 3394, "end_char_idx": 6044, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "004d0974-94af-4f09-b309-f480e2ac7873": {"__data__": {"id_": "004d0974-94af-4f09-b309-f480e2ac7873", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Rectifier (neural networks).txt", "file_name": "Rectifier (neural networks).txt", "file_type": "text/plain", "file_size": 9659, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9ccd033a-d520-40e3-9826-a461456a44c3", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Rectifier (neural networks).txt", "file_name": "Rectifier (neural networks).txt", "file_type": "text/plain", "file_size": 9659, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "ed9303c22ea9b950b95f3de52b9a752e0b4cea7e7aeed756c7338f1b3af8dd9b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c614bf6e-137a-467b-808c-ddca9eb19fee", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Rectifier (neural networks).txt", "file_name": "Rectifier (neural networks).txt", "file_type": "text/plain", "file_size": 9659, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "e7aca5cde7bb5d112b7faeb0f25f333a4ce8b86d29c26c0de7e03a9dae171d5e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9b57f5cc-b285-4415-8954-2021cf0a79e7", "node_type": "1", "metadata": {}, "hash": "f56d2d6e8b58f8e5ad0448de757f342a7b3c413eac7ddfc58576eb3090b63c5a", "class_name": "RelatedNodeInfo"}}, "text": "This function can be approximated as:\r\n\r\nln\u2061(1+ex)\u2248{ln\u20612,x=0,x1\u2212e\u2212x/ln\u20612,x\u22600{\\displaystyle \\ln \\left(1+e^{x}\\right)\\approx {\\begin{cases}\\ln 2,&x=0,\\\\[6pt]{\\frac {x}{1-e^{-x/\\ln 2}}},&x\\neq 0\\end{cases}}}By making the change of variables x=yln\u2061(2){\\displaystyle x=y\\ln(2)}, this is equivalent to\r\n\r\nlog2\u2061(1+2y)\u2248{1,y=0,y1\u2212e\u2212y,y\u22600.{\\displaystyle \\log _{2}(1+2^{y})\\approx {\\begin{cases}1,&y=0,\\\\[6pt]{\\frac {y}{1-e^{-y}}},&y\\neq 0.\\end{cases}}}A sharpness parameter k{\\displaystyle k} may be included:\r\n\r\nf(x)=ln\u2061(1+ekx)k,f\u2032(x)=ekx1+ekx=11+e\u2212kx.{\\displaystyle f(x)={\\frac {\\ln(1+e^{kx})}{k}},\\qquad \\qquad f'(x)={\\frac {e^{kx}}{1+e^{kx}}}={\\frac {1}{1+e^{-kx}}}.}The derivative of softplus is the logistic function.\r\nThe logistic sigmoid function is a smooth approximation of the derivative of the rectifier, the Heaviside step function.\r\nThe multivariable generalization of single-variable softplus is the LogSumExp with the first argument set to zero:\r\n\r\nLSE0+\u2061(x1,\u2026,xn):=LSE\u2061(0,x1,\u2026,xn)=ln\u2061(1+ex1+\u22ef+exn).{\\displaystyle \\operatorname {LSE_{0}} ^{+}(x_{1},\\dots ,x_{n}):=\\operatorname {LSE} (0,x_{1},\\dots ,x_{n})=\\ln(1+e^{x_{1}}+\\cdots +e^{x_{n}}).}The LogSumExp function is\r\n\r\nLSE\u2061(x1,\u2026,xn)=ln\u2061(ex1+\u22ef+exn),{\\displaystyle \\operatorname {LSE} (x_{1},\\dots ,x_{n})=\\ln(e^{x_{1}}+\\cdots +e^{x_{n}}),}and its gradient is the softmax; the softmax with the first argument set to zero is the multivariable generalization of the logistic function. Both LogSumExp and softmax are used in machine learning.\r\n\r\n\r\n==== ELU ====\r\nExponential linear units try to make the mean activations closer to zero, which speeds up learning. It has been shown that ELUs can obtain higher classification accuracy than ReLUs.\r\nf(x)={xif x>0,a(ex\u22121)otherwise.f\u2032(x)={1if x>0,a\u22c5exotherwise.{\\displaystyle f(x)={\\begin{cases}x&{\\text{if }}x>0,\\\\a\\left(e^{x}-1\\right)&{\\text{otherwise}}.\\end{cases}}\\qquad \\qquad f'(x)={\\begin{cases}1&{\\text{if }}x>0,\\\\a\\cdot e^{x}&{\\text{otherwise}}.\\end{cases}}}In these formulas, a{\\displaystyle a} is a hyper-parameter to be tuned with the constraint a\u22650{\\displaystyle a\\geq 0}.\r\nThe ELU can be viewed as a smoothed version of a shifted ReLU (SReLU), which has the form f(x)=max(\u2212a,x){\\displaystyle f(x)=\\max(-a,x)}, given the same interpretation of a{\\displaystyle a}.\r\n\r\n\r\n==== Mish ====\r\nThe mish function can also be used as a smooth approximation of the rectifier.", "start_char_idx": 5715, "end_char_idx": 8090, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9b57f5cc-b285-4415-8954-2021cf0a79e7": {"__data__": {"id_": "9b57f5cc-b285-4415-8954-2021cf0a79e7", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Rectifier (neural networks).txt", "file_name": "Rectifier (neural networks).txt", "file_type": "text/plain", "file_size": 9659, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9ccd033a-d520-40e3-9826-a461456a44c3", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Rectifier (neural networks).txt", "file_name": "Rectifier (neural networks).txt", "file_type": "text/plain", "file_size": 9659, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "ed9303c22ea9b950b95f3de52b9a752e0b4cea7e7aeed756c7338f1b3af8dd9b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "004d0974-94af-4f09-b309-f480e2ac7873", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Rectifier (neural networks).txt", "file_name": "Rectifier (neural networks).txt", "file_type": "text/plain", "file_size": 9659, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "fba50796b7394f1c8ff7ab02d21b894f07ba21ccc96731c825f90121b5dac6e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4e62bd62-dd3b-4a27-a900-bf8014da78ec", "node_type": "1", "metadata": {}, "hash": "b95a8b6b01fd5ba0953cc07eec699bd9d36c20303f6b6dd455aadc291612470e", "class_name": "RelatedNodeInfo"}}, "text": "The ELU can be viewed as a smoothed version of a shifted ReLU (SReLU), which has the form f(x)=max(\u2212a,x){\\displaystyle f(x)=\\max(-a,x)}, given the same interpretation of a{\\displaystyle a}.\r\n\r\n\r\n==== Mish ====\r\nThe mish function can also be used as a smooth approximation of the rectifier. It is defined as \r\n\r\nf(x)=xtanh\u2061(softplus\u2061(x)),{\\displaystyle f(x)=x\\tanh {\\big (}\\operatorname {softplus} (x){\\big )},}where tanh\u2061(x){\\displaystyle \\tanh(x)} is the hyperbolic tangent, and softplus\u2061(x){\\displaystyle \\operatorname {softplus} (x)} is the softplus function.\r\nMish is non-monotonic and self-gated. It was inspired by Swish, itself a variant of ReLU.\r\n\r\n\r\n==== Squareplus ====\r\nSquareplus is the function\r\n\r\nsquareplusb\u2061(x)=x+x2+b2{\\displaystyle \\operatorname {squareplus} _{b}(x)={\\frac {x+{\\sqrt {x^{2}+b}}}{2}}}where b\u22650{\\displaystyle b\\geq 0} is a hyperparameter that determines the \"size\" of the curved region near x=0{\\displaystyle x=0}. (For example, letting b=0{\\displaystyle b=0} yields ReLU, and letting b=4{\\displaystyle b=4} yields the metallic mean function.)\r\nSquareplus shares many properties with softplus: It is monotonic, strictly positive, approaches 0 as x\u2192\u2212\u221e{\\displaystyle x\\to -\\infty }, approaches the identity as x\u2192+\u221e{\\displaystyle x\\to +\\infty }, and is C\u221e{\\displaystyle C^{\\infty }} smooth. However, squareplus can be computed using only algebraic functions, making it well-suited for settings where computational resources or instruction sets are limited. Additionally, squareplus requires no special consideration to ensure numerical stability when x{\\displaystyle x} is large.\r\n\r\n\r\n== See also ==\r\nSoftmax function\r\nSigmoid function\r\nTobit model\r\nLayer (deep learning)\r\n\r\n\r\n== References ==", "start_char_idx": 7801, "end_char_idx": 9523, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4e62bd62-dd3b-4a27-a900-bf8014da78ec": {"__data__": {"id_": "4e62bd62-dd3b-4a27-a900-bf8014da78ec", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Recurrent neural network.txt", "file_name": "Recurrent neural network.txt", "file_type": "text/plain", "file_size": 28077, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb6471bf-2893-4828-b5b5-0d44aa11201d", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Recurrent neural network.txt", "file_name": "Recurrent neural network.txt", "file_type": "text/plain", "file_size": 28077, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "eec251e99ecb878c757de3abb5b6e769ec64078fe7d70e72c33607a29a5d67da", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9b57f5cc-b285-4415-8954-2021cf0a79e7", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Rectifier (neural networks).txt", "file_name": "Rectifier (neural networks).txt", "file_type": "text/plain", "file_size": 9659, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "202ba5dcc2e429b0042655c175aca268006393c6f0ca55ec9eadf5a576503a50", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8282590e-446e-49e4-8706-8631ef149490", "node_type": "1", "metadata": {}, "hash": "1a5ec5db4a30f09585f6403bf8f6281200bcba695b19119b77157fe6b63ba8a7", "class_name": "RelatedNodeInfo"}}, "text": "A recurrent neural network (RNN) is one of the two broad types of artificial neural network, characterized by direction of the flow of information between its layers. In contrast to the uni-directional feedforward neural network, it is a bi-directional artificial neural network, meaning that it allows the output from some nodes to affect subsequent input to the same nodes. Their ability to use internal state (memory) to process arbitrary sequences of inputs makes them applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition. The term \"recurrent neural network\" is used to refer to the class of networks with an infinite impulse response, whereas \"convolutional neural network\" refers to the class of finite impulse response. Both classes of networks exhibit temporal dynamic behavior. A finite impulse recurrent network is a directed acyclic graph that can be unrolled and replaced with a strictly feedforward neural network, while an infinite impulse recurrent network is a directed cyclic graph that can not be unrolled.\r\nAdditional stored states and the storage under direct control by the network can be added to both infinite-impulse and finite-impulse networks. Another network or graph can also replace the storage if that incorporates time delays or has feedback loops. Such controlled states are referred to as gated states or gated memory and are part of long short-term memory networks (LSTMs) and gated recurrent units. This is also called Feedforward Neural Network (FNN). Recurrent neural networks are theoretically Turing complete and can run arbitrary programs to process arbitrary sequences of inputs.\r\n\r\n\r\n== History ==\r\nThe Ising model (1925) by Wilhelm Lenz and Ernst Ising\r\nwas the first RNN architecture that did not learn. Shun'ichi Amari made it adaptive in 1972. This was also called the Hopfield network (1982). See also David Rumelhart's work in 1986.  In 1993, a neural history compressor system solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time.\r\n\r\n\r\n=== LSTM ===\r\nLong short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1997 and set accuracy records in multiple applications domains.Around 2007, LSTM started to revolutionize speech recognition, outperforming traditional models in certain speech applications. In 2009, a Connectionist Temporal Classification (CTC)-trained LSTM network was the first RNN to win pattern recognition contests when it won several competitions in connected handwriting recognition. In 2014, the Chinese company Baidu used CTC-trained RNNs to break the 2S09 Switchboard Hub5'00 speech recognition dataset benchmark without using any traditional speech processing methods.LSTM also improved large-vocabulary speech recognition and text-to-speech synthesis and was used in Google Android. In 2015, Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM.LSTM broke records for improved machine translation, Language Modeling and Multilingual Language Processing. LSTM combined with convolutional neural networks (CNNs) improved automatic image captioning.\r\n\r\n\r\n== Architectures ==\r\n\r\nRNNs come in many variants.\r\n\r\n\r\n=== Fully recurrent ===\r\nFully recurrent neural networks (FRNN) connect the outputs of all neurons to the inputs of all neurons.  This is the most general neural network topology because all other topologies can be represented by setting some connection weights to zero to simulate the lack of connections between those neurons. The illustration to the right may be misleading to many because practical neural network topologies are frequently organized in \"layers\" and the drawing gives that appearance. However, what appears to be layers are, in fact, different steps in time of the same fully recurrent neural network. The left-most item in the illustration shows the recurrent connections as the arc labeled 'v'.  It is \"unfolded\" in time to produce the appearance of layers.\r\n\r\n\r\n=== Elman networks and Jordan networks ===\r\nAn Elman network is a three-layer network (arranged horizontally as x, y, and z in the illustration) with the addition of a set of context units (u in the illustration). The middle (hidden) layer is connected to these context units fixed with a weight of one. At each time step, the input is fed forward and a learning rule is applied. The fixed back-connections save a copy of the previous values of the hidden units in the context units (since they propagate over the connections before the learning rule is applied). Thus the network can maintain a sort of state, allowing it to perform such tasks as sequence-prediction that are beyond the power of a standard multilayer perceptron.", "start_char_idx": 0, "end_char_idx": 4779, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8282590e-446e-49e4-8706-8631ef149490": {"__data__": {"id_": "8282590e-446e-49e4-8706-8631ef149490", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Recurrent neural network.txt", "file_name": "Recurrent neural network.txt", "file_type": "text/plain", "file_size": 28077, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb6471bf-2893-4828-b5b5-0d44aa11201d", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Recurrent neural network.txt", "file_name": "Recurrent neural network.txt", "file_type": "text/plain", "file_size": 28077, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "eec251e99ecb878c757de3abb5b6e769ec64078fe7d70e72c33607a29a5d67da", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4e62bd62-dd3b-4a27-a900-bf8014da78ec", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Recurrent neural network.txt", "file_name": "Recurrent neural network.txt", "file_type": "text/plain", "file_size": 28077, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "4d0ab24ed4c8f725c91a2e3168b981125034c85d32693e4f43ec42786af4aeb6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f0c6dbf3-3671-4fdc-a183-c4827b7226f4", "node_type": "1", "metadata": {}, "hash": "0fe6fb4238e75d4626fe543d3a54bb9f1f2476b01803c4c5dcb9914c7746f8a9", "class_name": "RelatedNodeInfo"}}, "text": "The left-most item in the illustration shows the recurrent connections as the arc labeled 'v'.  It is \"unfolded\" in time to produce the appearance of layers.\r\n\r\n\r\n=== Elman networks and Jordan networks ===\r\nAn Elman network is a three-layer network (arranged horizontally as x, y, and z in the illustration) with the addition of a set of context units (u in the illustration). The middle (hidden) layer is connected to these context units fixed with a weight of one. At each time step, the input is fed forward and a learning rule is applied. The fixed back-connections save a copy of the previous values of the hidden units in the context units (since they propagate over the connections before the learning rule is applied). Thus the network can maintain a sort of state, allowing it to perform such tasks as sequence-prediction that are beyond the power of a standard multilayer perceptron.\r\nJordan networks are similar to Elman networks. The context units are fed from the output layer instead of the hidden layer. The context units in a Jordan network are also called the state layer. They have a recurrent connection to themselves.Elman and Jordan networks are also known as \"Simple recurrent networks\" (SRN).\r\n\r\nElman network\r\nht=\u03c3h(Whxt+Uhht\u22121+bh)yt=\u03c3y(Wyht+by){\\displaystyle {\\begin{aligned}h_{t}&=\\sigma _{h}(W_{h}x_{t}+U_{h}h_{t-1}+b_{h})\\\\y_{t}&=\\sigma _{y}(W_{y}h_{t}+b_{y})\\end{aligned}}}\r\nJordan network\r\nht=\u03c3h(Whxt+Uhyt\u22121+bh)yt=\u03c3y(Wyht+by){\\displaystyle {\\begin{aligned}h_{t}&=\\sigma _{h}(W_{h}x_{t}+U_{h}y_{t-1}+b_{h})\\\\y_{t}&=\\sigma _{y}(W_{y}h_{t}+b_{y})\\end{aligned}}}Variables and functions\r\n\r\nxt{\\displaystyle x_{t}}: input vector\r\nht{\\displaystyle h_{t}}: hidden layer vector\r\nyt{\\displaystyle y_{t}}: output vector\r\nW{\\displaystyle W}, U{\\displaystyle U} and b{\\displaystyle b}: parameter matrices and vector\r\n\u03c3h{\\displaystyle \\sigma _{h}} and \u03c3y{\\displaystyle \\sigma _{y}}: Activation functions\r\n\r\n\r\n=== Hopfield ===\r\n\r\nThe Hopfield network is an RNN in which all connections across layers are equally sized. It requires stationary inputs and is thus not a general RNN, as it does not process sequences of patterns. However, it guarantees that it will converge. If the connections are trained using Hebbian learning, then the Hopfield network can perform as robust content-addressable memory, resistant to connection alteration.\r\n\r\n\r\n==== Bidirectional associative memory ====\r\n\r\nIntroduced by Bart Kosko, a bidirectional associative memory (BAM) network is a variant of a Hopfield network that stores associative data as a vector. The bi-directionality comes from passing information through a matrix and its transpose. Typically, bipolar encoding is preferred to binary encoding of the associative pairs. Recently, stochastic BAM models using Markov stepping were optimized for increased network stability and relevance to real-world applications.A BAM network has two layers, either of which can be driven as an input to recall an association and produce an output on the other layer.\r\n\r\n\r\n=== Echo state ===\r\n\r\nEcho state networks (ESN) have a sparsely connected random hidden layer. The weights of output neurons are the only part of the network that can change (be trained). ESNs are good at reproducing certain time series. A variant for spiking neurons is known as a liquid state machine.\r\n\r\n\r\n=== Independently RNN (IndRNN) ===\r\nThe independently recurrent neural network (IndRNN) addresses the gradient vanishing and exploding problems in the traditional fully connected RNN. Each neuron in one layer only receives its own past state as context information (instead of full connectivity to all other neurons in this layer) and thus neurons are independent of each other's history. The gradient backpropagation can be regulated to avoid gradient vanishing and exploding in order to keep long or short-term memory. The cross-neuron information is explored in the next layers. IndRNN can be robustly trained with non-saturated nonlinear functions such as ReLU. Deep networks can be trained using skip connections.", "start_char_idx": 3886, "end_char_idx": 7931, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f0c6dbf3-3671-4fdc-a183-c4827b7226f4": {"__data__": {"id_": "f0c6dbf3-3671-4fdc-a183-c4827b7226f4", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Recurrent neural network.txt", "file_name": "Recurrent neural network.txt", "file_type": "text/plain", "file_size": 28077, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb6471bf-2893-4828-b5b5-0d44aa11201d", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Recurrent neural network.txt", "file_name": "Recurrent neural network.txt", "file_type": "text/plain", "file_size": 28077, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "eec251e99ecb878c757de3abb5b6e769ec64078fe7d70e72c33607a29a5d67da", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8282590e-446e-49e4-8706-8631ef149490", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Recurrent neural network.txt", "file_name": "Recurrent neural network.txt", "file_type": "text/plain", "file_size": 28077, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "685f541f2e5b02b670ca854db89c7cfa3f2e016704971008d8fecb6cfcc0b365", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4cbcecb1-2b04-4f2d-88ea-9f23f18f33c2", "node_type": "1", "metadata": {}, "hash": "6f67855487f384c3ca35ea744507d1088ff9775513d80f527b2c6ce8dd2b656d", "class_name": "RelatedNodeInfo"}}, "text": "The weights of output neurons are the only part of the network that can change (be trained). ESNs are good at reproducing certain time series. A variant for spiking neurons is known as a liquid state machine.\r\n\r\n\r\n=== Independently RNN (IndRNN) ===\r\nThe independently recurrent neural network (IndRNN) addresses the gradient vanishing and exploding problems in the traditional fully connected RNN. Each neuron in one layer only receives its own past state as context information (instead of full connectivity to all other neurons in this layer) and thus neurons are independent of each other's history. The gradient backpropagation can be regulated to avoid gradient vanishing and exploding in order to keep long or short-term memory. The cross-neuron information is explored in the next layers. IndRNN can be robustly trained with non-saturated nonlinear functions such as ReLU. Deep networks can be trained using skip connections.\r\n\r\n\r\n=== Recursive ===\r\n\r\nA recursive neural network is created by applying the same set of weights recursively over a differentiable graph-like structure by traversing the structure in topological order. Such networks are typically also trained by the reverse mode of automatic differentiation. They can process distributed representations of structure, such as logical terms. A special case of recursive neural networks is the RNN whose structure corresponds to a linear chain. Recursive neural networks have been applied to natural language processing. The Recursive Neural Tensor Network uses a tensor-based composition function for all nodes in the tree.\r\n\r\n\r\n=== Neural history compressor ===\r\nThe neural history compressor is an unsupervised stack of RNNs. At the input level, it learns to predict its next input from the previous inputs. Only unpredictable inputs of some RNN in the hierarchy become inputs to the next higher level RNN, which therefore recomputes its internal state only rarely. Each higher level RNN thus studies a compressed representation of the information in the RNN below. This is done such that the input sequence can be precisely reconstructed from the representation at the highest level.\r\nThe system effectively minimizes the description length or the negative logarithm of the probability of the data. Given a lot of learnable predictability in the incoming data sequence, the highest level RNN can use supervised learning to easily classify even deep sequences with long intervals between important events.\r\nIt is possible to distill the RNN hierarchy into two RNNs: the \"conscious\" chunker (higher level) and the \"subconscious\" automatizer (lower level). Once the chunker has learned to predict and compress inputs that are unpredictable by the automatizer, then the automatizer can be forced in the next learning phase to predict or imitate through additional units the hidden units of the more slowly changing chunker. This makes it easy for the automatizer to learn appropriate, rarely changing memories across long intervals. In turn, this helps the automatizer to make many of its once unpredictable inputs predictable, such that the chunker can focus on the remaining unpredictable events.A generative model partially overcame the vanishing gradient problem of automatic differentiation or backpropagation in neural networks in 1992. In 1993, such a system solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time.\r\n\r\n\r\n=== Second order RNNs ===\r\nSecond-order RNNs use higher order weights wijk{\\displaystyle w{}_{ijk}} instead of the standard wij{\\displaystyle w{}_{ij}} weights, and states can be a product. This allows a direct mapping to a finite-state machine both in training, stability, and representation. Long short-term memory is an example of this but has no such formal mappings or proof of stability.\r\n\r\n\r\n=== Long short-term memory ===\r\n\r\nLong short-term memory (LSTM) is a deep learning system that avoids the vanishing gradient problem. LSTM is normally augmented by recurrent gates called \"forget gates\". LSTM prevents backpropagated errors from vanishing or exploding. Instead, errors can flow backward through unlimited numbers of virtual layers unfolded in space. That is, LSTM can learn tasks that require memories of events that happened thousands or even millions of discrete time steps earlier. Problem-specific LSTM-like topologies can be evolved. LSTM works even given long delays between significant events and can handle signals that mix low and high-frequency components.\r\nMany applications use stacks of LSTM RNNs and train them by connectionist temporal classification (CTC) to find an RNN weight matrix that maximizes the probability of the label sequences in a training set, given the corresponding input sequences. CTC achieves both alignment and recognition.", "start_char_idx": 6999, "end_char_idx": 11821, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4cbcecb1-2b04-4f2d-88ea-9f23f18f33c2": {"__data__": {"id_": "4cbcecb1-2b04-4f2d-88ea-9f23f18f33c2", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Recurrent neural network.txt", "file_name": "Recurrent neural network.txt", "file_type": "text/plain", "file_size": 28077, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb6471bf-2893-4828-b5b5-0d44aa11201d", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Recurrent neural network.txt", "file_name": "Recurrent neural network.txt", "file_type": "text/plain", "file_size": 28077, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "eec251e99ecb878c757de3abb5b6e769ec64078fe7d70e72c33607a29a5d67da", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f0c6dbf3-3671-4fdc-a183-c4827b7226f4", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Recurrent neural network.txt", "file_name": "Recurrent neural network.txt", "file_type": "text/plain", "file_size": 28077, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "cdb55be836926c7145e7c9a6157291687c9a535c315762a64ca4fbc8aa2e28e8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b557405d-f6e5-4aee-b6dd-02fc5f42c078", "node_type": "1", "metadata": {}, "hash": "00ef552c0bd3b1504d0f3b708ae6de627fb249cf994db755d7167a187f3f3690", "class_name": "RelatedNodeInfo"}}, "text": "=== Long short-term memory ===\r\n\r\nLong short-term memory (LSTM) is a deep learning system that avoids the vanishing gradient problem. LSTM is normally augmented by recurrent gates called \"forget gates\". LSTM prevents backpropagated errors from vanishing or exploding. Instead, errors can flow backward through unlimited numbers of virtual layers unfolded in space. That is, LSTM can learn tasks that require memories of events that happened thousands or even millions of discrete time steps earlier. Problem-specific LSTM-like topologies can be evolved. LSTM works even given long delays between significant events and can handle signals that mix low and high-frequency components.\r\nMany applications use stacks of LSTM RNNs and train them by connectionist temporal classification (CTC) to find an RNN weight matrix that maximizes the probability of the label sequences in a training set, given the corresponding input sequences. CTC achieves both alignment and recognition.\r\nLSTM can learn to recognize context-sensitive languages unlike previous models based on hidden Markov models (HMM) and similar concepts.\r\n\r\n\r\n=== Gated recurrent unit ===\r\n\r\nGated recurrent units (GRUs) are a gating mechanism in recurrent neural networks introduced in 2014. They are used in the full form and several simplified variants. Their performance on polyphonic music modeling and speech signal modeling was found to be similar to that of long short-term memory. They have fewer parameters than LSTM, as they lack an output gate.\r\n\r\n\r\n=== Bi-directional ===\r\n\r\nBi-directional RNNs use a finite sequence to predict or label each element of the sequence based on the element's past and future contexts. This is done by concatenating the outputs of two RNNs, one processing the sequence from left to right, and the other one from right to left. The combined outputs are the predictions of the teacher-given target signals. This technique has been proven to be especially useful when combined with LSTM RNNs.\r\n\r\n\r\n=== Continuous-time ===\r\nA continuous-time recurrent neural network (CTRNN) uses a system of ordinary differential equations to model the effects on a neuron of the incoming inputs.\r\nFor a neuron i{\\displaystyle i} in the network with activation yi{\\displaystyle y_{i}}, the rate of change of activation is given by:\r\n\r\n\u03c4iy\u02d9i=\u2212yi+\u2211j=1nwji\u03c3(yj\u2212\u0398j)+Ii(t){\\displaystyle \\tau _{i}{\\dot {y}}_{i}=-y_{i}+\\sum _{j=1}^{n}w_{ji}\\sigma (y_{j}-\\Theta _{j})+I_{i}(t)}Where:\r\n\r\n\u03c4i{\\displaystyle \\tau _{i}} : Time constant of postsynaptic node\r\nyi{\\displaystyle y_{i}} : Activation of postsynaptic node\r\ny\u02d9i{\\displaystyle {\\dot {y}}_{i}} : Rate of change of activation of postsynaptic node\r\nwji{\\displaystyle w{}_{ji}} : Weight of connection from pre to postsynaptic node\r\n\u03c3(x){\\displaystyle \\sigma (x)} : Sigmoid of x e.g. \u03c3(x)=1/(1+e\u2212x){\\displaystyle \\sigma (x)=1/(1+e^{-x})}.\r\nyj{\\displaystyle y_{j}} : Activation of presynaptic node\r\n\u0398j{\\displaystyle \\Theta _{j}} : Bias of presynaptic node\r\nIi(t){\\displaystyle I_{i}(t)} : Input (if any) to nodeCTRNNs have been applied to evolutionary robotics where they have been used to address vision, co-operation, and minimal cognitive behaviour.Note that, by the Shannon sampling theorem, discrete-time recurrent neural networks can be viewed as continuous-time recurrent neural networks where the differential equations have transformed into equivalent difference equations. This transformation can be thought of as occurring after the post-synaptic node activation functions yi(t){\\displaystyle y_{i}(t)} have been low-pass filtered but prior to sampling.\r\n\r\n\r\n=== Hierarchical recurrent neural network ===\r\nHierarchical recurrent neural networks (HRNN) connect their neurons in various ways to decompose hierarchical behavior into useful subprograms. Such hierarchical structures of cognition are present in theories of memory presented by philosopher Henri Bergson, whose philosophical views have inspired hierarchical models.Hierarchical recurrent neural networks are useful in forecasting, helping to predict disaggregated inflation components of the consumer price index (CPI). The HRNN model leverages information from higher levels in the CPI hierarchy to enhance lower-level predictions. Evaluation of a substantial dataset from the US CPI-U index demonstrates the superior performance of the HRNN model compared to various established inflation prediction methods.", "start_char_idx": 10847, "end_char_idx": 15249, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b557405d-f6e5-4aee-b6dd-02fc5f42c078": {"__data__": {"id_": "b557405d-f6e5-4aee-b6dd-02fc5f42c078", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Recurrent neural network.txt", "file_name": "Recurrent neural network.txt", "file_type": "text/plain", "file_size": 28077, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb6471bf-2893-4828-b5b5-0d44aa11201d", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Recurrent neural network.txt", "file_name": "Recurrent neural network.txt", "file_type": "text/plain", "file_size": 28077, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "eec251e99ecb878c757de3abb5b6e769ec64078fe7d70e72c33607a29a5d67da", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4cbcecb1-2b04-4f2d-88ea-9f23f18f33c2", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Recurrent neural network.txt", "file_name": "Recurrent neural network.txt", "file_type": "text/plain", "file_size": 28077, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "9f131e8c44cff4a9e02cdf077d39e67a73e2d363984b36174faba4fc49fec02b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "512d8208-d923-4637-bfaf-ab61c1643f53", "node_type": "1", "metadata": {}, "hash": "6b6f4e92f9b03b2227efc44fbc1522541e5ebefb55f63106643263b2e69910a9", "class_name": "RelatedNodeInfo"}}, "text": "This transformation can be thought of as occurring after the post-synaptic node activation functions yi(t){\\displaystyle y_{i}(t)} have been low-pass filtered but prior to sampling.\r\n\r\n\r\n=== Hierarchical recurrent neural network ===\r\nHierarchical recurrent neural networks (HRNN) connect their neurons in various ways to decompose hierarchical behavior into useful subprograms. Such hierarchical structures of cognition are present in theories of memory presented by philosopher Henri Bergson, whose philosophical views have inspired hierarchical models.Hierarchical recurrent neural networks are useful in forecasting, helping to predict disaggregated inflation components of the consumer price index (CPI). The HRNN model leverages information from higher levels in the CPI hierarchy to enhance lower-level predictions. Evaluation of a substantial dataset from the US CPI-U index demonstrates the superior performance of the HRNN model compared to various established inflation prediction methods.\r\n\r\n\r\n=== Recurrent multilayer perceptron network ===\r\nGenerally, a recurrent multilayer perceptron network (RMLP network) consists of cascaded subnetworks, each containing multiple layers of nodes. Each subnetwork is feed-forward except for the last layer, which can have feedback connections. Each of these subnets is connected only by feed-forward connections.\r\n\r\n\r\n=== Multiple timescales model ===\r\nA multiple timescales recurrent neural network (MTRNN) is a neural-based computational model that can simulate the functional hierarchy of the brain through self-organization depending on the spatial connection between neurons and on distinct types of neuron activities, each with distinct time properties. With such varied neuronal activities, continuous sequences of any set of behaviors are segmented into reusable primitives, which in turn are flexibly integrated into diverse sequential behaviors. The biological approval of such a type of hierarchy was discussed in the memory-prediction theory of brain function by Hawkins in his book On Intelligence. Such a hierarchy also agrees with theories of memory posited by philosopher Henri Bergson, which have been incorporated into an MTRNN model.\r\n\r\n\r\n=== Neural Turing machines ===\r\n\r\nNeural Turing machines (NTMs) are a method of extending recurrent neural networks by coupling them to external memory resources which they can interact with by attentional processes. The combined system is analogous to a Turing machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent.\r\n\r\n\r\n=== Differentiable neural computer ===\r\n\r\nDifferentiable neural computers (DNCs) are an extension of Neural Turing machines, allowing for the usage of fuzzy amounts of each memory address and a record of chronology.\r\n\r\n\r\n=== Neural network pushdown automata ===\r\nNeural network pushdown automata (NNPDA) are similar to NTMs, but tapes are replaced by analog stacks that are differentiable and trained. In this way, they are similar in complexity to recognizers of context free grammars (CFGs).\r\n\r\n\r\n=== Memristive networks ===\r\nGreg Snider of HP Labs describes a system of cortical computing with memristive nanodevices. The memristors (memory resistors) are implemented by thin film materials in which the resistance is electrically tuned via the transport of ions or oxygen vacancies within the film. DARPA's SyNAPSE project has funded IBM Research and HP Labs, in collaboration with the Boston University Department of Cognitive and Neural Systems (CNS), to develop neuromorphic architectures that may be based on memristive systems.\r\nMemristive networks are a particular type of physical neural network that have very similar properties to (Little-)Hopfield networks, as they have continuous dynamics, a limited memory capacity and natural relaxation via the minimization of a function which is asymptotic to the Ising model. In this sense, the dynamics of a memristive circuit have the advantage compared to a Resistor-Capacitor network to have a more interesting non-linear behavior. From this point of view, engineering analog memristive networks account for a peculiar type of neuromorphic engineering in which the device behavior depends on the circuit wiring or topology.\r\nThe evolution of these networks can be studied analytically using variations of the Caravelli\u2013Traversa\u2013Di Ventra equation.\r\n\r\n\r\n== Pseudocode ==\r\nGiven a time series x of length sequence_length.\r\nIn the recurrent neural network, there is a loop that processes all entries of the time series x through the layers neural_network one after another. These have as return value in each time step i both the prediction y_pred[i] and an updated hidden state hidden, which has the length hidden_size. As a result, after the loop, the collection of all predictions y_pred is returned.\r\nThe following pseudocode (based on the programming language Python) illustrates the functionality of a recurrent neural network.", "start_char_idx": 14250, "end_char_idx": 19244, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "512d8208-d923-4637-bfaf-ab61c1643f53": {"__data__": {"id_": "512d8208-d923-4637-bfaf-ab61c1643f53", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Recurrent neural network.txt", "file_name": "Recurrent neural network.txt", "file_type": "text/plain", "file_size": 28077, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb6471bf-2893-4828-b5b5-0d44aa11201d", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Recurrent neural network.txt", "file_name": "Recurrent neural network.txt", "file_type": "text/plain", "file_size": 28077, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "eec251e99ecb878c757de3abb5b6e769ec64078fe7d70e72c33607a29a5d67da", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b557405d-f6e5-4aee-b6dd-02fc5f42c078", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Recurrent neural network.txt", "file_name": "Recurrent neural network.txt", "file_type": "text/plain", "file_size": 28077, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "1115957c311fd792311c33bdc48943510966485a22c403f13ea1a8e52379e4e9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04ed0a87-ab76-4389-ae6f-656b9aa11037", "node_type": "1", "metadata": {}, "hash": "bfb75aba4dd0074e95f35b77d5f780cc059a6dfe8fe2cc7fc24483ca1a72e016", "class_name": "RelatedNodeInfo"}}, "text": "From this point of view, engineering analog memristive networks account for a peculiar type of neuromorphic engineering in which the device behavior depends on the circuit wiring or topology.\r\nThe evolution of these networks can be studied analytically using variations of the Caravelli\u2013Traversa\u2013Di Ventra equation.\r\n\r\n\r\n== Pseudocode ==\r\nGiven a time series x of length sequence_length.\r\nIn the recurrent neural network, there is a loop that processes all entries of the time series x through the layers neural_network one after another. These have as return value in each time step i both the prediction y_pred[i] and an updated hidden state hidden, which has the length hidden_size. As a result, after the loop, the collection of all predictions y_pred is returned.\r\nThe following pseudocode (based on the programming language Python) illustrates the functionality of a recurrent neural network.\r\n\r\nModern libraries provide runtime-optimized implementations of the above functionality or allow to speed up the slow loop by just-in-time compilation.\r\n\r\n\r\n== Training ==\r\n\r\n\r\n=== Gradient descent ===\r\n\r\nGradient descent is a first-order iterative optimization algorithm for finding the minimum of a function. In neural networks, it can be used to minimize the error term by changing each weight in proportion to the derivative of the error with respect to that weight, provided the non-linear activation functions are differentiable. Various methods for doing so were developed in the 1980s and early 1990s by Werbos, Williams, Robinson, Schmidhuber, Hochreiter, Pearlmutter and others.\r\nThe standard method is called \"backpropagation through time\" or BPTT, and is a generalization of back-propagation for feed-forward networks. Like that method, it is an instance of automatic differentiation in the reverse accumulation mode of Pontryagin's minimum principle. A more computationally expensive online variant is called \"Real-Time Recurrent Learning\" or RTRL, which is an instance of automatic differentiation in the forward accumulation mode with stacked tangent vectors. Unlike BPTT, this algorithm is local in time but not local in space.\r\nIn this context, local in space means that a unit's weight vector can be updated using only information stored in the connected units and the unit itself such that update complexity of a single unit is linear in the dimensionality of the weight vector. Local in time means that the updates take place continually (on-line) and depend only on the most recent time step rather than on multiple time steps within a given time horizon as in BPTT. Biological neural networks appear to be local with respect to both time and space.For recursively computing the partial derivatives, RTRL has a time-complexity of O(number of hidden x number of weights) per time step for computing the Jacobian matrices, while BPTT only takes O(number of weights) per time step, at the cost of storing all forward activations within the given time horizon. An online hybrid between BPTT and RTRL with intermediate complexity exists, along with variants for continuous time.A major problem with gradient descent for standard RNN architectures is that error gradients vanish exponentially quickly with the size of the time lag between important events. LSTM combined with a BPTT/RTRL hybrid learning method attempts to overcome these problems. This problem is also solved in the independently recurrent neural network (IndRNN) by reducing the context of a neuron to its own past state and the cross-neuron information can then be explored in the following layers. Memories of different ranges including long-term memory can be learned without the gradient vanishing and exploding problem.\r\nThe on-line algorithm called causal recursive backpropagation (CRBP), implements and combines BPTT and RTRL paradigms for locally recurrent networks. It works with the most general locally recurrent networks. The CRBP algorithm can minimize the global error term. This fact improves the stability of the algorithm, providing a unifying view of gradient calculation techniques for recurrent networks with local feedback.\r\nOne approach to gradient information computation in RNNs with arbitrary architectures is based on signal-flow graphs diagrammatic derivation. It uses the BPTT batch algorithm, based on Lee's theorem for network sensitivity calculations. It was proposed by Wan and Beaufays, while its fast online version was proposed by Campolucci, Uncini and Piazza.\r\n\r\n\r\n=== Global optimization methods ===\r\nTraining the weights in a neural network can be modeled as a non-linear global optimization problem. A target function can be formed to evaluate the fitness or error of a particular weight vector as follows: First, the weights in the network are set according to the weight vector. Next, the network is evaluated against the training sequence.", "start_char_idx": 18346, "end_char_idx": 23212, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "04ed0a87-ab76-4389-ae6f-656b9aa11037": {"__data__": {"id_": "04ed0a87-ab76-4389-ae6f-656b9aa11037", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Recurrent neural network.txt", "file_name": "Recurrent neural network.txt", "file_type": "text/plain", "file_size": 28077, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb6471bf-2893-4828-b5b5-0d44aa11201d", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Recurrent neural network.txt", "file_name": "Recurrent neural network.txt", "file_type": "text/plain", "file_size": 28077, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "eec251e99ecb878c757de3abb5b6e769ec64078fe7d70e72c33607a29a5d67da", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "512d8208-d923-4637-bfaf-ab61c1643f53", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Recurrent neural network.txt", "file_name": "Recurrent neural network.txt", "file_type": "text/plain", "file_size": 28077, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "3db8dca494cb8aa53d7fd3af817abb9e4cb78c0a45a7cb43c49edeef51360ce2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "662b93c3-d8df-42e9-b02c-bb6d2ec34f3b", "node_type": "1", "metadata": {}, "hash": "030436bb3b5bd0aab039c0ac7e830d91e5f50f646beee051699e78b655867a9a", "class_name": "RelatedNodeInfo"}}, "text": "It works with the most general locally recurrent networks. The CRBP algorithm can minimize the global error term. This fact improves the stability of the algorithm, providing a unifying view of gradient calculation techniques for recurrent networks with local feedback.\r\nOne approach to gradient information computation in RNNs with arbitrary architectures is based on signal-flow graphs diagrammatic derivation. It uses the BPTT batch algorithm, based on Lee's theorem for network sensitivity calculations. It was proposed by Wan and Beaufays, while its fast online version was proposed by Campolucci, Uncini and Piazza.\r\n\r\n\r\n=== Global optimization methods ===\r\nTraining the weights in a neural network can be modeled as a non-linear global optimization problem. A target function can be formed to evaluate the fitness or error of a particular weight vector as follows: First, the weights in the network are set according to the weight vector. Next, the network is evaluated against the training sequence. Typically, the sum-squared difference between the predictions and the target values specified in the training sequence is used to represent the error of the current weight vector. Arbitrary global optimization techniques may then be used to minimize this target function.\r\nThe most common global optimization method for training RNNs is genetic algorithms, especially in unstructured networks.Initially, the genetic algorithm is encoded with the neural network weights in a predefined manner where one gene in the chromosome represents one weight link. The whole network is represented as a single chromosome. The fitness function is evaluated as follows:\r\n\r\nEach weight encoded in the chromosome is assigned to the respective weight link of the network.\r\nThe training set is presented to the network which propagates the input signals forward.\r\nThe mean-squared error is returned to the fitness function.\r\nThis function drives the genetic selection process.Many chromosomes make up the population; therefore, many different neural networks are evolved until a stopping criterion is satisfied. A common stopping scheme is: \r\n\r\nWhen the neural network has learned a certain percentage of the training data or\r\nWhen the minimum value of the mean-squared-error is satisfied or\r\nWhen the maximum number of training generations has been reached.The fitness function evaluates the stopping criterion as it receives the mean-squared error reciprocal from each network during training. Therefore, the goal of the genetic algorithm is to maximize the fitness function, reducing the mean-squared error.\r\nOther global (and/or evolutionary) optimization techniques may be used to seek a good set of weights, such as simulated annealing or particle swarm optimization.\r\n\r\n\r\n== Related fields and models ==\r\nRNNs may behave chaotically. In such cases, dynamical systems theory may be used for analysis.\r\nThey are in fact recursive neural networks with a particular structure: that of a linear chain. Whereas recursive neural networks operate on any hierarchical structure, combining child representations into parent representations, recurrent neural networks operate on the linear progression of time, combining the previous time step and a hidden representation into the representation for the current time step.\r\nIn particular, RNNs can appear as nonlinear versions of finite impulse response and infinite impulse response filters and also as a nonlinear autoregressive exogenous model (NARX).The effect of memory-based learning for the recognition of sequences can also be implemented by a more biological-based model which uses the silencing mechanism exhibited in neurons with a relatively high frequency spiking activity.\r\n\r\n\r\n== Libraries ==\r\nApache Singa\r\nCaffe: Created by the Berkeley Vision and Learning Center (BVLC). It supports both CPU and GPU. Developed in C++, and has Python and MATLAB wrappers.\r\nChainer: Fully in Python, production support for CPU, GPU, distributed training.\r\nDeeplearning4j: Deep learning in Java and Scala on multi-GPU-enabled Spark.\r\nFlux: includes interfaces for RNNs, including GRUs and LSTMs, written in Julia.\r\nKeras: High-level API, providing a wrapper to many other deep learning libraries.\r\nMicrosoft Cognitive Toolkit\r\nMXNet: an open-source deep learning framework used to train and deploy deep neural networks.\r\nPyTorch: Tensors and Dynamic neural networks in Python with GPU acceleration.\r\nTensorFlow: Apache 2.0-licensed Theano-like library with support for CPU, GPU and Google's proprietary TPU, mobile\r\nTheano: A deep-learning library for Python with an API largely compatible with the NumPy library.\r\nTorch: A scientific computing framework with support for machine learning algorithms, written in C and Lua.", "start_char_idx": 22205, "end_char_idx": 26962, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "662b93c3-d8df-42e9-b02c-bb6d2ec34f3b": {"__data__": {"id_": "662b93c3-d8df-42e9-b02c-bb6d2ec34f3b", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Recurrent neural network.txt", "file_name": "Recurrent neural network.txt", "file_type": "text/plain", "file_size": 28077, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb6471bf-2893-4828-b5b5-0d44aa11201d", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Recurrent neural network.txt", "file_name": "Recurrent neural network.txt", "file_type": "text/plain", "file_size": 28077, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "eec251e99ecb878c757de3abb5b6e769ec64078fe7d70e72c33607a29a5d67da", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04ed0a87-ab76-4389-ae6f-656b9aa11037", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Recurrent neural network.txt", "file_name": "Recurrent neural network.txt", "file_type": "text/plain", "file_size": 28077, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "14a708e9a8ea895131f47c0bc3c74defefc0e2fb45b1c0e12e18b5a5f15504fe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f5b9277-3671-4c3e-a7e8-569a1a6c9080", "node_type": "1", "metadata": {}, "hash": "c91599e8c400d7090f3d564d631af2963a099ee5b6c1b8c83d506894e665b242", "class_name": "RelatedNodeInfo"}}, "text": "Developed in C++, and has Python and MATLAB wrappers.\r\nChainer: Fully in Python, production support for CPU, GPU, distributed training.\r\nDeeplearning4j: Deep learning in Java and Scala on multi-GPU-enabled Spark.\r\nFlux: includes interfaces for RNNs, including GRUs and LSTMs, written in Julia.\r\nKeras: High-level API, providing a wrapper to many other deep learning libraries.\r\nMicrosoft Cognitive Toolkit\r\nMXNet: an open-source deep learning framework used to train and deploy deep neural networks.\r\nPyTorch: Tensors and Dynamic neural networks in Python with GPU acceleration.\r\nTensorFlow: Apache 2.0-licensed Theano-like library with support for CPU, GPU and Google's proprietary TPU, mobile\r\nTheano: A deep-learning library for Python with an API largely compatible with the NumPy library.\r\nTorch: A scientific computing framework with support for machine learning algorithms, written in C and Lua.\r\n\r\n\r\n== Applications ==\r\nApplications of recurrent neural networks include:\r\n\r\nMachine translation\r\nRobot control\r\nTime series prediction\r\nSpeech recognition\r\nSpeech synthesis\r\nBrain\u2013computer interfaces\r\nTime series anomaly detection\r\nText-to-Video model\r\nRhythm learning\r\nMusic composition\r\nGrammar learning\r\nHandwriting recognition\r\nHuman action recognition\r\nProtein homology detection\r\nPredicting subcellular localization of proteins\r\nSeveral prediction tasks in the area of business process management\r\nPrediction in medical care pathways\r\nPredictions of fusion plasma disruptions in reactors (Fusion Recurrent Neural Network (FRNN) code) \r\n\r\n\r\n== References ==\r\n\r\n\r\n== Further reading ==\r\nMandic, Danilo P.; Chambers, Jonathon A. (2001). Recurrent Neural Networks for Prediction: Learning Algorithms, Architectures and Stability. Wiley. ISBN 978-0-471-49517-8.\r\n\r\n\r\n== External links ==\r\nRecurrent Neural Networks with over 60 RNN papers by J\u00fcrgen Schmidhuber's group at Dalle Molle Institute for Artificial Intelligence Research\r\nElman Neural Network implementation for WEKA", "start_char_idx": 26060, "end_char_idx": 28043, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9f5b9277-3671-4c3e-a7e8-569a1a6c9080": {"__data__": {"id_": "9f5b9277-3671-4c3e-a7e8-569a1a6c9080", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Residual neural network.txt", "file_name": "Residual neural network.txt", "file_type": "text/plain", "file_size": 13238, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "72e5032e-1334-4ccc-8191-84c5c54f0619", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Residual neural network.txt", "file_name": "Residual neural network.txt", "file_type": "text/plain", "file_size": 13238, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "d2b0c26c183cf943f2761114c363ab3a620056196826ab86518bb2ecaecf31f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "662b93c3-d8df-42e9-b02c-bb6d2ec34f3b", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Recurrent neural network.txt", "file_name": "Recurrent neural network.txt", "file_type": "text/plain", "file_size": 28077, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "42c8da06dabbea3e6cb34aaf039ec362d1080a571591dd95426efa51bf26bce9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0be24465-312b-4be7-8115-00e27a818048", "node_type": "1", "metadata": {}, "hash": "6de445109254a14616be863eef1042603db7b8b00ee15b4279b986ab4a7ae6d0", "class_name": "RelatedNodeInfo"}}, "text": "A residual neural network (also referred to as a residual network or ResNet) is a deep learning model in which the weight layers learn residual functions with reference to the layer inputs. It behaves like a highway network whose gates are opened through strongly positive bias weights. This enables deep learning models with tens or hundreds of layers to train easily and approach better accuracy when going deeper. The identity skip connections, often referred to as \"residual connections\", are also used in the 1997 LSTM networks, Transformer models (e.g., BERT, GPT models such as ChatGPT), the AlphaGo Zero system, the AlphaStar system, and the AlphaFold system.\r\nResidual networks were developed by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, who won the 2015 ImageNet competition.\r\n\r\n\r\n== Formulation ==\r\n\r\n\r\n=== Background ===\r\nThe AlexNet model developed in 2012 for ImageNet was an eight-layer convolutional neural network.\r\nThe neural networks developed in 2014 by the Visual Geometry Group (VGG) at the University of Oxford approached a depth of 19 layers by stacking 3-by-3 convolutional layers.\r\nHowever, stacking more layers led to a steep reduction in training accuracy, which is referred to as the \"degradation\" problem.A deeper network should not produce a higher training loss than its shallower counterpart, if this deeper network can be constructed by its shallower counterpart stacked with extra layers. If the extra layers can be set as identity mappings, the deeper network would represent the same function as the shallower counterpart. It is hypothesized that the optimizer is not able to approach identity mappings for the parameterized layers.\r\n\r\n\r\n=== Residual learning ===\r\nIn a multi-layer neural network model, consider a subnetwork with a certain number (e.g., 2 or 3) of stacked layers. Denote the underlying function performed by this subnetwork as H(x){\\textstyle H(x)}, where x{\\textstyle x} is the input to this subnetwork.\r\nThe idea of \"Residual Learning\" re-parameterizes this subnetwork and lets the parameter layers represent a residual function F(x):=H(x)\u2212x{\\textstyle F(x):=H(x)-x}.\r\nThe output y{\\textstyle y} of this subnetwork is represented as:\r\n\r\ny=F(x)+x{\\displaystyle {\\begin{aligned}y&=F(x)+x\\end{aligned}}}This is also the principle of the 1997 LSTM cell computing yt+1=F(xt)+xt{\\textstyle y_{t+1}=F(x_{t})+x_{t}}, which becomes y=F(x)+x{\\textstyle y=F(x)+x} during backpropagation through time.\r\nThe function F(x){\\textstyle F(x)} is often represented by matrix multiplication interlaced with activation functions and normalization operations (e.g., Batch Normalization or Layer Normalization).\r\nThis subnetwork is referred to as a \"Residual Block\". A deep residual network is constructed by stacking a series of residual blocks.\r\nThe operation of \"+ x{\\textstyle +\\ x}\" in \"y=F(x)+x{\\textstyle y=F(x)+x}\" is approached by a skip connection that performs identity mapping and connects the input of a residual block with its output. This connection is often referred to as a \"Residual Connection\" in later work.\r\n\r\n\r\n=== Signal propagation ===\r\nThe introduction of identity mappings facilitates signal propagation in both forward and backward paths.", "start_char_idx": 0, "end_char_idx": 3213, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0be24465-312b-4be7-8115-00e27a818048": {"__data__": {"id_": "0be24465-312b-4be7-8115-00e27a818048", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Residual neural network.txt", "file_name": "Residual neural network.txt", "file_type": "text/plain", "file_size": 13238, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "72e5032e-1334-4ccc-8191-84c5c54f0619", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Residual neural network.txt", "file_name": "Residual neural network.txt", "file_type": "text/plain", "file_size": 13238, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "d2b0c26c183cf943f2761114c363ab3a620056196826ab86518bb2ecaecf31f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9f5b9277-3671-4c3e-a7e8-569a1a6c9080", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Residual neural network.txt", "file_name": "Residual neural network.txt", "file_type": "text/plain", "file_size": 13238, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "1321739e1d4f6ecb2751f07fcdfada9a2d2603e682507ec4011bc8ab3286bfc2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c44331cb-8338-4a74-b20a-543ff0e64dff", "node_type": "1", "metadata": {}, "hash": "1c426025117eacde0d6bd7956124e6933def15cceea1bf3de8ee614456c9a76e", "class_name": "RelatedNodeInfo"}}, "text": "The function F(x){\\textstyle F(x)} is often represented by matrix multiplication interlaced with activation functions and normalization operations (e.g., Batch Normalization or Layer Normalization).\r\nThis subnetwork is referred to as a \"Residual Block\". A deep residual network is constructed by stacking a series of residual blocks.\r\nThe operation of \"+ x{\\textstyle +\\ x}\" in \"y=F(x)+x{\\textstyle y=F(x)+x}\" is approached by a skip connection that performs identity mapping and connects the input of a residual block with its output. This connection is often referred to as a \"Residual Connection\" in later work.\r\n\r\n\r\n=== Signal propagation ===\r\nThe introduction of identity mappings facilitates signal propagation in both forward and backward paths.\r\n\r\n\r\n==== Forward propagation ====\r\nIf the output of the \u2113{\\textstyle \\ell }-th residual block is the input to the (\u2113+1){\\textstyle (\\ell +1)}-th residual block (i.e., assuming no activation function between blocks), we have:\r\nx\u2113+1=F(x\u2113)+x\u2113{\\displaystyle {\\begin{aligned}x_{\\ell +1}&=F(x_{\\ell })+x_{\\ell }\\end{aligned}}}Applying this formulation recursively, e.g., x\u2113+2=F(x\u2113+1)+x\u2113+1=F(x\u2113+1)+F(x\u2113)+x\u2113{\\displaystyle {\\begin{aligned}x_{\\ell +2}=F(x_{\\ell +1})+x_{\\ell +1}=F(x_{\\ell +1})+F(x_{\\ell })+x_{\\ell }\\end{aligned}}}, we have:\r\n\r\nxL=x\u2113+\u2211i=lL\u22121F(xi){\\displaystyle {\\begin{aligned}x_{L}&=x_{\\ell }+\\sum _{i=l}^{L-1}F(x_{i})\\\\\\end{aligned}}}where L{\\textstyle L} is the index of any later residual block (e.g., the last block) and \u2113{\\textstyle \\ell } is the index of any earlier block. This formulation suggests that there is always a signal that is directly sent from a shallower block \u2113{\\textstyle \\ell } to a deeper block L{\\textstyle L}.\r\n\r\n\r\n==== Backward propagation ====\r\nThe Residual Learning formulation provides the added benefit of addressing the vanishing gradient problem to some extent. However, it is crucial to acknowledge that the vanishing gradient issue is not the root cause of the degradation problem, as it has already been tackled through the use of normalization layers. Taking the derivative w.r.t. x\u2113{\\textstyle x_{\\ell }} according to the above forward propagation, we have:\r\n\u2202E\u2202x\u2113=\u2202E\u2202xL\u2202xL\u2202x\u2113=\u2202E\u2202xL(1+\u2202\u2202x\u2113\u2211i=lL\u22121F(xi))=\u2202E\u2202xL+\u2202E\u2202xL\u2202\u2202x\u2113\u2211i=lL\u22121F(xi){\\displaystyle {\\begin{aligned}{\\frac {\\partial {\\mathcal {E}}}{\\partial x_{\\ell }}}&={\\frac {\\partial {\\mathcal {E}}}{\\partial x_{L}}}{\\frac {\\partial x_{L}}{\\partial x_{\\ell }}}\\\\&={\\frac {\\partial {\\mathcal {E}}}{\\partial x_{L}}}\\left(1+{\\frac {\\partial }{\\partial x_{\\ell }}}\\sum _{i=l}^{L-1}F(x_{i})\\right)\\\\&={\\frac {\\partial {\\mathcal {E}}}{\\partial x_{L}}}+{\\frac {\\partial {\\mathcal {E}}}{\\partial x_{L}}}{\\frac {\\partial }{\\partial x_{\\ell }}}\\sum _{i=l}^{L-1}F(x_{i})\\\\\\end{aligned}}}Here E{\\textstyle {\\mathcal {E}}} is the loss function to be minimized.\r\nThis formulation suggests that the gradient computation of a shallower layer \r\n\u2202E\u2202x\u2113{\\textstyle {\\frac {\\partial {\\mathcal {E}}}{\\partial x_{\\ell }}}}\r\nalways has a term \u2202E\u2202xL{\\textstyle {\\frac {\\partial {\\mathcal {E}}}{\\partial x_{L}}}} that is directly added.", "start_char_idx": 2461, "end_char_idx": 5520, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c44331cb-8338-4a74-b20a-543ff0e64dff": {"__data__": {"id_": "c44331cb-8338-4a74-b20a-543ff0e64dff", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Residual neural network.txt", "file_name": "Residual neural network.txt", "file_type": "text/plain", "file_size": 13238, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "72e5032e-1334-4ccc-8191-84c5c54f0619", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Residual neural network.txt", "file_name": "Residual neural network.txt", "file_type": "text/plain", "file_size": 13238, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "d2b0c26c183cf943f2761114c363ab3a620056196826ab86518bb2ecaecf31f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0be24465-312b-4be7-8115-00e27a818048", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Residual neural network.txt", "file_name": "Residual neural network.txt", "file_type": "text/plain", "file_size": 13238, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "3eccb506a2dde058750d94ca44a1246ae361e36b6f3359efc25dd65518be6cc4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "92a43d90-7087-4bab-9b7a-2a5997a6ac5c", "node_type": "1", "metadata": {}, "hash": "df5a2ffc619294e48da0a59acf34537ceef7754eb0c5a60fb6b3d044b3193ece", "class_name": "RelatedNodeInfo"}}, "text": "This formulation suggests that the gradient computation of a shallower layer \r\n\u2202E\u2202x\u2113{\\textstyle {\\frac {\\partial {\\mathcal {E}}}{\\partial x_{\\ell }}}}\r\nalways has a term \u2202E\u2202xL{\\textstyle {\\frac {\\partial {\\mathcal {E}}}{\\partial x_{L}}}} that is directly added. Even if the gradients of the F(xi){\\textstyle F(x_{i})} terms are small, the total gradient \u2202E\u2202x\u2113{\\textstyle {\\frac {\\partial {\\mathcal {E}}}{\\partial x_{\\ell }}}} is not vanishing thanks to the added term \u2202E\u2202xL{\\textstyle {\\frac {\\partial {\\mathcal {E}}}{\\partial x_{L}}}}.\r\n\r\n\r\n== Variants of residual blocks ==\r\n\r\n\r\n=== Basic block ===\r\nA Basic Block is the simplest building block studied in the original ResNet. This block consists of two sequential 3x3 convolutional layers and a residual connection. The input and output dimensions of both layers are equal.\r\n\r\n\r\n=== Bottleneck block ===\r\nA Bottleneck Block consists of three sequential convolutional layers and a residual connection. The first layer in this block is a 1x1 convolution for dimension reduction, e.g., to 1/4 of the input dimension; the second layer performs a 3x3 convolution; the last layer is another 1x1 convolution for dimension restoration. The models of ResNet-50, ResNet-101, and ResNet-152 in  are all based on Bottleneck Blocks.\r\n\r\n\r\n=== Pre-activation block ===\r\nThe Pre-activation Residual Block applies the activation functions (e.g., non-linearity and normalization) before applying the residual function F{\\textstyle F}. Formally, the computation of a Pre-activation Residual Block can be written as:\r\n\r\nx\u2113+1=F(\u03d5(x\u2113))+x\u2113{\\displaystyle {\\begin{aligned}x_{\\ell +1}&=F(\\phi (x_{\\ell }))+x_{\\ell }\\end{aligned}}}where \u03d5{\\textstyle \\phi } can be any non-linearity activation (e.g., ReLU) or normalization (e.g., LayerNorm) operation. This design reduces the number of non-identity mappings between Residual Blocks. This design was used to train models with 200 to over 1000 layers.Since GPT-2, the Transformer Blocks have been dominantly implemented as Pre-activation Blocks. This is often referred to as \"pre-normalization\" in the literature of Transformer models.\r\n\r\n\r\n=== Transformer block ===\r\nA Transformer Block is a stack of two Residual Blocks. Each Residual Block has a Residual Connection.\r\nThe first Residual Block is a Multi-Head Attention Block, which performs (self-)attention computation followed by a linear projection.\r\nThe second Residual Block is a feed-forward Multi-Layer Perceptron (MLP) Block. This block is analogous to an \"inverse\" bottleneck block: it has a linear projection layer (which is equivalent to a 1x1 convolution in the context of Convolutional Neural Networks) that increases the dimension, and another linear projection that reduces the dimension.\r\nA Transformer Block has a depth of 4 layers (linear projections).\r\nThe GPT-3 model has 96 Transformer Blocks (in the literature of Transformers, a Transformer Block is often referred to as a \"Transformer Layer\"). This model has a depth of about 400 projection layers, including 96x4 layers in Transformer Blocks and a few extra layers for input embedding and output prediction.\r\nVery deep Transformer models cannot be successfully trained without Residual Connections.\r\n\r\n\r\n== Related Work ==\r\nIn 1961, Frank Rosenblatt described a three-layer multilayer perceptron (MLP) model with skip connections. The model was referred to as a \"cross-coupled system\", and the skip connections were forms of cross-coupled connections.\r\nIn two books published in 1994\r\n\r\nand 1996, \"skip-layer\" connections were presented in feed-forward MLP models: \"The general definition [of MLP] allows more than one hidden layer, and it also allows 'skip-layer' connections from input to output\" (p261 in, p144 in ), \"... which allows the non-linear units to perturb a linear functional form\" (p262 in ). This description suggests that the non-linear MLP performs like a residual function (perturbation) added to a linear function.\r\nSepp Hochreiter analyzed the vanishing gradient problem in 1991 and attributed to it the reason why deep learning did not work well.", "start_char_idx": 5259, "end_char_idx": 9329, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "92a43d90-7087-4bab-9b7a-2a5997a6ac5c": {"__data__": {"id_": "92a43d90-7087-4bab-9b7a-2a5997a6ac5c", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Residual neural network.txt", "file_name": "Residual neural network.txt", "file_type": "text/plain", "file_size": 13238, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "72e5032e-1334-4ccc-8191-84c5c54f0619", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Residual neural network.txt", "file_name": "Residual neural network.txt", "file_type": "text/plain", "file_size": 13238, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "d2b0c26c183cf943f2761114c363ab3a620056196826ab86518bb2ecaecf31f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c44331cb-8338-4a74-b20a-543ff0e64dff", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Residual neural network.txt", "file_name": "Residual neural network.txt", "file_type": "text/plain", "file_size": 13238, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "6243f9baf7937b1b6aaef7aff0402156f1f18f8fad85ef2daf6e28076f276a9f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c4f71283-9bbd-43f2-a96f-175fb7cafc7e", "node_type": "1", "metadata": {}, "hash": "0746601536faec7694cc99c3af645c071ecec12a0ac774673d61f5bafadc3e64", "class_name": "RelatedNodeInfo"}}, "text": "== Related Work ==\r\nIn 1961, Frank Rosenblatt described a three-layer multilayer perceptron (MLP) model with skip connections. The model was referred to as a \"cross-coupled system\", and the skip connections were forms of cross-coupled connections.\r\nIn two books published in 1994\r\n\r\nand 1996, \"skip-layer\" connections were presented in feed-forward MLP models: \"The general definition [of MLP] allows more than one hidden layer, and it also allows 'skip-layer' connections from input to output\" (p261 in, p144 in ), \"... which allows the non-linear units to perturb a linear functional form\" (p262 in ). This description suggests that the non-linear MLP performs like a residual function (perturbation) added to a linear function.\r\nSepp Hochreiter analyzed the vanishing gradient problem in 1991 and attributed to it the reason why deep learning did not work well.\r\nTo overcome this problem, long short-term memory (LSTM) recurrent neural networks\r\nhad skip connections or residual connections with a weight of 1.0 in every LSTM cell (called the constant error carrousel) to compute yt+1=F(xt)+xt{\\textstyle y_{t+1}=F(x_{t})+x_{t}}. During backpropagation through time, this becomes the above-mentioned residual formula y=F(x)+x{\\textstyle y=F(x)+x} for feedforward neural networks. This enables training very deep recurrent neural networks with a very long time span t. A later LSTM version published in 2000 modulates the identity LSTM connections by so-called forget gates such that their weights are not fixed to 1.0 but can be learned. In experiments, the forget gates were initialized with positive bias weights, thus being opened, addressing the vanishing gradient problem.\r\nThe highway network of May 2015 \r\napplies these principles to feedforward neural networks.\r\nIt was reported to be \"the first very deep feedforward network with hundreds of layers\". \r\nIt is like an LSTM with forget gates unfolded in time, while the later Residual Nets have no equivalent of forget gates and are like the unfolded original LSTM.\r\nIf the skip connections in Highway Networks are \"without gates\", or if their gates are kept open (activation 1.0) through strong positive bias weights, they become the identity skip connections in Residual Networks.\r\nThe original Highway Network paper not only introduced the basic principle for very deep feedforward networks, but also included experimental results with 20, 50, and 100 layers networks, and mentioned ongoing experiments with up to 900 layers.\r\nNetworks with 50 or 100 layers had lower training error than their plain network counterparts, but no lower training error than their 20 layers counterpart (on the MNIST dataset, Figure 1 in ). No improvement on test accuracy was reported with networks deeper than 19 layers (on the CIFAR-10 dataset; Table 1 in ). The ResNet paper, however, provided strong experimental evidence of the benefits of going deeper than 20 layers. It argued that the identity mapping without modulation is crucial and mentioned that modulation in the skip connection can still lead to vanishing signals in forward and backward propagation (Section 3 in ). This is also  why the forget gates of the 2000 LSTM were initially opened through positive bias weights: as long as the gates are open, it behaves like the 1997 LSTM. Similarly, a Highway Net whose gates are opened through strongly positive bias weights behaves like a ResNet.\r\nThe skip connections used in modern neural networks (e.g., Transformers) are dominantly identity mappings.\r\nDenseNets in 2016\r\n were designed as deep neural networks that attempt to connect each layer to every other layer. DenseNets approached this goal by using identity mappings as skip connections. Unlike ResNets, DenseNets merge the layer output with skip connections by concatenation, not addition.\r\nNeural networks with Stochastic Depth\r\n were made possible given the Residual Network architectures. This training procedure randomly drops a subset of layers and lets the signal propagate through the identity skip connection. Also known as \"DropPath\", this is an effective regularization method for training large and deep models, such as the Vision Transformer (ViT).\r\n\r\n\r\n== Biological relation ==\r\nThe original Residual Network paper made no claim on being inspired by biological systems. But research later on has related Residual Networks to biologically-plausible algorithms.\r\nA study published in Science in 2023\r\n disclosed the complete connectome of an insect brain (of a fruit fly larva). This study discovered \"multilayer shortcuts\" that resemble the skip connections in artificial neural networks, including ResNets.", "start_char_idx": 8465, "end_char_idx": 13104, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c4f71283-9bbd-43f2-a96f-175fb7cafc7e": {"__data__": {"id_": "c4f71283-9bbd-43f2-a96f-175fb7cafc7e", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Residual neural network.txt", "file_name": "Residual neural network.txt", "file_type": "text/plain", "file_size": 13238, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "72e5032e-1334-4ccc-8191-84c5c54f0619", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Residual neural network.txt", "file_name": "Residual neural network.txt", "file_type": "text/plain", "file_size": 13238, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "d2b0c26c183cf943f2761114c363ab3a620056196826ab86518bb2ecaecf31f5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "92a43d90-7087-4bab-9b7a-2a5997a6ac5c", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Residual neural network.txt", "file_name": "Residual neural network.txt", "file_type": "text/plain", "file_size": 13238, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "a3c33d8cbfaf25608de188c173104245d4eb9c0b43065b80abf5ca539a04bf2e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "57bc95d5-41c4-4203-bc03-970260c56dd1", "node_type": "1", "metadata": {}, "hash": "0018e6d520da3d7d7d47bbc82e1642a779944b875c6d5c2af6020a654ee7d828", "class_name": "RelatedNodeInfo"}}, "text": "DenseNets approached this goal by using identity mappings as skip connections. Unlike ResNets, DenseNets merge the layer output with skip connections by concatenation, not addition.\r\nNeural networks with Stochastic Depth\r\n were made possible given the Residual Network architectures. This training procedure randomly drops a subset of layers and lets the signal propagate through the identity skip connection. Also known as \"DropPath\", this is an effective regularization method for training large and deep models, such as the Vision Transformer (ViT).\r\n\r\n\r\n== Biological relation ==\r\nThe original Residual Network paper made no claim on being inspired by biological systems. But research later on has related Residual Networks to biologically-plausible algorithms.\r\nA study published in Science in 2023\r\n disclosed the complete connectome of an insect brain (of a fruit fly larva). This study discovered \"multilayer shortcuts\" that resemble the skip connections in artificial neural networks, including ResNets.\r\n\r\n\r\n== References ==", "start_char_idx": 12092, "end_char_idx": 13126, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "57bc95d5-41c4-4203-bc03-970260c56dd1": {"__data__": {"id_": "57bc95d5-41c4-4203-bc03-970260c56dd1", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Siamese neural network.txt", "file_name": "Siamese neural network.txt", "file_type": "text/plain", "file_size": 7467, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cf3ccf7b-1635-46d8-939e-567bb626882a", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Siamese neural network.txt", "file_name": "Siamese neural network.txt", "file_type": "text/plain", "file_size": 7467, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "3273124fc155f541000afbb8d7cf00c92bb60b9720c7af7b04080c64a2755252", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c4f71283-9bbd-43f2-a96f-175fb7cafc7e", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Residual neural network.txt", "file_name": "Residual neural network.txt", "file_type": "text/plain", "file_size": 13238, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "a0b01bb876eddf5010146acb4465fb4477af07e2d3499c4041f81c06df3ae869", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4a76c575-6013-4ca4-9283-7a04c1f75083", "node_type": "1", "metadata": {}, "hash": "ddfee3f93dc0ca59264e23349422281b69f516bcd62eab0d2d82fb6ff010f637", "class_name": "RelatedNodeInfo"}}, "text": "A Siamese neural network (sometimes called a twin neural network) is an artificial neural network that uses the same weights while working in tandem on two different input vectors to compute comparable output vectors. Often one of the output vectors is precomputed, thus forming a baseline against which the other output vector is compared. This is similar to comparing fingerprints but can be described more technically as a distance function for locality-sensitive hashing.It is possible to build an architecture that is functionally similar to a siamese network but implements a slightly different function. This is typically used for comparing similar instances in different type sets.Uses of similarity measures where a twin network might be used are such things as recognizing handwritten checks, automatic detection of faces in camera images, and matching queries with indexed documents. The perhaps most well-known application of twin networks are face recognition, where known images of people are precomputed and compared to an image from a turnstile or similar. It is not obvious at first, but there are two slightly different problems. One is recognizing a person among a large number of other persons, that is the facial recognition problem. DeepFace is an example of such a system. In its most extreme form this is recognizing a single person at a train station or airport. The other is face verification, that is to verify whether the photo in a pass is the same as the person claiming he or she is the same person. The twin network might be the same, but the implementation can be quite different.\r\n\r\n\r\n== Learning ==\r\nLearning in twin networks can be done with triplet loss or contrastive loss. For learning by triplet loss a baseline vector (anchor image) is compared against a positive vector (truthy image) and a negative vector (falsy image). The negative vector will force learning in the network, while the positive vector will act like a regularizer. For learning by contrastive loss there must be a weight decay to regularize the weights, or some similar operation like a normalization.\r\nA distance metric for a loss function may have the following properties\r\nNon-negativity: \u03b4(x,y)\u22650{\\displaystyle \\delta (x,y)\\geq 0}\r\nIdentity of Non-discernibles: \u03b4(x,y)=0\u27fax=y{\\displaystyle \\delta (x,y)=0\\iff x=y}\r\nCommutativity: \u03b4(x,y)=\u03b4(y,x){\\displaystyle \\delta (x,y)=\\delta (y,x)}\r\nTriangle inequality: \u03b4(x,z)\u2264\u03b4(x,y)+\u03b4(y,z){\\displaystyle \\delta (x,z)\\leq \\delta (x,y)+\\delta (y,z)}In particular, the triplet loss algorithm is often defined with squared Euclidean (which unlike Euclidean, does not have triangle inequality) distance at its core.\r\n\r\n\r\n=== Predefined metrics, Euclidean distance metric ===\r\nThe common learning goal is to minimize a distance metric for similar objects and maximize for distinct ones.", "start_char_idx": 0, "end_char_idx": 2831, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4a76c575-6013-4ca4-9283-7a04c1f75083": {"__data__": {"id_": "4a76c575-6013-4ca4-9283-7a04c1f75083", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Siamese neural network.txt", "file_name": "Siamese neural network.txt", "file_type": "text/plain", "file_size": 7467, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cf3ccf7b-1635-46d8-939e-567bb626882a", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Siamese neural network.txt", "file_name": "Siamese neural network.txt", "file_type": "text/plain", "file_size": 7467, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "3273124fc155f541000afbb8d7cf00c92bb60b9720c7af7b04080c64a2755252", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "57bc95d5-41c4-4203-bc03-970260c56dd1", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Siamese neural network.txt", "file_name": "Siamese neural network.txt", "file_type": "text/plain", "file_size": 7467, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "bc9f62d9a4f87e182b8387523e6ba3fb8e6bef51e5d50f5d6dc8dc1fa8a6cb24", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71d825cc-39b1-41a0-aec4-6c0816f596c6", "node_type": "1", "metadata": {}, "hash": "bcea2286a77d265e4b4adb60fb5febbd67353192852031d6a3b33c0b8c9bc848", "class_name": "RelatedNodeInfo"}}, "text": "A distance metric for a loss function may have the following properties\r\nNon-negativity: \u03b4(x,y)\u22650{\\displaystyle \\delta (x,y)\\geq 0}\r\nIdentity of Non-discernibles: \u03b4(x,y)=0\u27fax=y{\\displaystyle \\delta (x,y)=0\\iff x=y}\r\nCommutativity: \u03b4(x,y)=\u03b4(y,x){\\displaystyle \\delta (x,y)=\\delta (y,x)}\r\nTriangle inequality: \u03b4(x,z)\u2264\u03b4(x,y)+\u03b4(y,z){\\displaystyle \\delta (x,z)\\leq \\delta (x,y)+\\delta (y,z)}In particular, the triplet loss algorithm is often defined with squared Euclidean (which unlike Euclidean, does not have triangle inequality) distance at its core.\r\n\r\n\r\n=== Predefined metrics, Euclidean distance metric ===\r\nThe common learning goal is to minimize a distance metric for similar objects and maximize for distinct ones. This gives a loss function like\r\n\r\n\u03b4(x(i),x(j))={min \u2016f\u2061(x(i))\u2212f\u2061(x(j))\u2016,i=jmax \u2016f\u2061(x(i))\u2212f\u2061(x(j))\u2016,i\u2260j{\\displaystyle {\\begin{aligned}\\delta (x^{(i)},x^{(j)})={\\begin{cases}\\min \\ \\|\\operatorname {f} \\left(x^{(i)}\\right)-\\operatorname {f} \\left(x^{(j)}\\right)\\|\\,,i=j\\\\\\max \\ \\|\\operatorname {f} \\left(x^{(i)}\\right)-\\operatorname {f} \\left(x^{(j)}\\right)\\|\\,,i\\neq j\\end{cases}}\\end{aligned}}}\r\ni,j{\\displaystyle i,j} are indexes into a set of vectors\r\nf\u2061(\u22c5){\\displaystyle \\operatorname {f} (\\cdot )} function implemented by the twin networkThe most common distance metric used is Euclidean distance, in case of which the loss function can be rewritten in matrix form as\r\n\r\n\u03b4\u2061(x(i),x(j))\u2248(x(i)\u2212x(j))T(x(i)\u2212x(j)){\\displaystyle \\operatorname {\\delta } (\\mathbf {x} ^{(i)},\\mathbf {x} ^{(j)})\\approx (\\mathbf {x} ^{(i)}-\\mathbf {x} ^{(j)})^{T}(\\mathbf {x} ^{(i)}-\\mathbf {x} ^{(j)})}\r\n\r\n\r\n=== Learned metrics, nonlinear distance metric ===\r\nA more general case is where the output vector from the twin network is passed through additional network layers implementing non-linear distance metrics.", "start_char_idx": 2113, "end_char_idx": 3925, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "71d825cc-39b1-41a0-aec4-6c0816f596c6": {"__data__": {"id_": "71d825cc-39b1-41a0-aec4-6c0816f596c6", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Siamese neural network.txt", "file_name": "Siamese neural network.txt", "file_type": "text/plain", "file_size": 7467, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cf3ccf7b-1635-46d8-939e-567bb626882a", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Siamese neural network.txt", "file_name": "Siamese neural network.txt", "file_type": "text/plain", "file_size": 7467, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "3273124fc155f541000afbb8d7cf00c92bb60b9720c7af7b04080c64a2755252", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4a76c575-6013-4ca4-9283-7a04c1f75083", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Siamese neural network.txt", "file_name": "Siamese neural network.txt", "file_type": "text/plain", "file_size": 7467, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "d5374b29c9581b36062537b1d6914631aa2aaf175153d9fa6013ce917f3e064e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8ee44e45-414e-4229-87fb-d1c997d79310", "node_type": "1", "metadata": {}, "hash": "cc72d7c2c18df15c0020bbfa4befa9d33a998086b1f43c0811ab77eb280d6ae0", "class_name": "RelatedNodeInfo"}}, "text": "ifi=jthen\u03b4\u2061[f\u2061(x(i)),f\u2061(x(j))]is smallotherwise\u03b4\u2061[f\u2061(x(i)),f\u2061(x(j))]is large{\\displaystyle {\\begin{aligned}{\\text{if}}\\,i=j\\,{\\text{then}}&\\,\\operatorname {\\delta } \\left[\\operatorname {f} \\left(x^{(i)}\\right),\\,\\operatorname {f} \\left(x^{(j)}\\right)\\right]\\,{\\text{is small}}\\\\{\\text{otherwise}}&\\,\\operatorname {\\delta } \\left[\\operatorname {f} \\left(x^{(i)}\\right),\\,\\operatorname {f} \\left(x^{(j)}\\right)\\right]\\,{\\text{is large}}\\end{aligned}}}\r\ni,j{\\displaystyle i,j} are indexes into a set of vectors\r\nf\u2061(\u22c5){\\displaystyle \\operatorname {f} (\\cdot )}function implemented by the twin network\r\n\u03b4\u2061(\u22c5){\\displaystyle \\operatorname {\\delta } (\\cdot )}function implemented by the network joining outputs from the twin networkOn a matrix form the previous is often approximated as a Mahalanobis distance for a linear space as\r\n\u03b4\u2061(x(i),x(j))\u2248(x(i)\u2212x(j))TM(x(i)\u2212x(j)){\\displaystyle \\operatorname {\\delta } (\\mathbf {x} ^{(i)},\\mathbf {x} ^{(j)})\\approx (\\mathbf {x} ^{(i)}-\\mathbf {x} ^{(j)})^{T}\\mathbf {M} (\\mathbf {x} ^{(i)}-\\mathbf {x} ^{(j)})}This can be further subdivided in at least Unsupervised learning and Supervised learning.\r\n\r\n\r\n=== Learned metrics, half-twin networks ===\r\nThis form also allows the twin network to be more of a half-twin, implementing a slightly different functions\r\n\r\nifi=jthen\u03b4\u2061[f\u2061(x(i)),g\u2061(x(j))]is smallotherwise\u03b4\u2061[f\u2061(x(i)),g\u2061(x(j))]is large{\\displaystyle {\\begin{aligned}{\\text{if}}\\,i=j\\,{\\text{then}}&\\,\\operatorname {\\delta } \\left[\\operatorname {f} \\left(x^{(i)}\\right),\\,\\operatorname {g} \\left(x^{(j)}\\right)\\right]\\,{\\text{is small}}\\\\{\\text{otherwise}}&\\,\\operatorname {\\delta } \\left[\\operatorname {f} \\left(x^{(i)}\\right),\\,\\operatorname {g} \\left(x^{(j)}\\right)\\right]\\,{\\text{is large}}\\end{aligned}}}\r\ni,j{\\displaystyle i,j} are indexes into a set of vectors\r\nf\u2061(\u22c5),g\u2061(\u22c5){\\displaystyle \\operatorname {f} (\\cdot ),\\operatorname {g} (\\cdot )}function implemented by the half-twin network\r\n\u03b4\u2061(\u22c5){\\displaystyle \\operatorname {\\delta } (\\cdot )}function implemented by the network joining outputs from the twin network\r\n\r\n\r\n== Twin networks for object tracking ==\r\nTwin networks have been used in object tracking because of its unique two tandem inputs and similarity measurement. In object tracking, one input of the twin network is user pre-selected exemplar image, the other input is a larger search image, which twin network's job is to locate exemplar inside of search image. By measuring the similarity between exemplar and each part of the search image, a map of similarity score can be given by the twin network. Furthermore, using a Fully Convolutional Network, the process of computing each sector's similarity score can be replaced with only one cross correlation layer.After being first introduced in 2016, Twin fully convolutional network has been used in many High-performance Real-time Object Tracking Neural Networks.  Like CFnet, StructSiam, SiamFC-tri, DSiam, SA-Siam, SiamRPN, DaSiamRPN, Cascaded SiamRPN, SiamMask, SiamRPN++, Deeper and Wider SiamRPN.", "start_char_idx": 3929, "end_char_idx": 6941, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8ee44e45-414e-4229-87fb-d1c997d79310": {"__data__": {"id_": "8ee44e45-414e-4229-87fb-d1c997d79310", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Siamese neural network.txt", "file_name": "Siamese neural network.txt", "file_type": "text/plain", "file_size": 7467, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cf3ccf7b-1635-46d8-939e-567bb626882a", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Siamese neural network.txt", "file_name": "Siamese neural network.txt", "file_type": "text/plain", "file_size": 7467, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "3273124fc155f541000afbb8d7cf00c92bb60b9720c7af7b04080c64a2755252", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "71d825cc-39b1-41a0-aec4-6c0816f596c6", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Siamese neural network.txt", "file_name": "Siamese neural network.txt", "file_type": "text/plain", "file_size": 7467, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "7bd1cf37f0aef45c5a8c2a460f9746a480c60783e2536cc2024cb234dbbee582", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "27f9a601-e371-4aa2-a75a-e97560d660ad", "node_type": "1", "metadata": {}, "hash": "9f30fa8eaf81ed03da0e50e02400edb8dd21ebab751fa1fe0af2209b44ed9b81", "class_name": "RelatedNodeInfo"}}, "text": "In object tracking, one input of the twin network is user pre-selected exemplar image, the other input is a larger search image, which twin network's job is to locate exemplar inside of search image. By measuring the similarity between exemplar and each part of the search image, a map of similarity score can be given by the twin network. Furthermore, using a Fully Convolutional Network, the process of computing each sector's similarity score can be replaced with only one cross correlation layer.After being first introduced in 2016, Twin fully convolutional network has been used in many High-performance Real-time Object Tracking Neural Networks.  Like CFnet, StructSiam, SiamFC-tri, DSiam, SA-Siam, SiamRPN, DaSiamRPN, Cascaded SiamRPN, SiamMask, SiamRPN++, Deeper and Wider SiamRPN.\r\n\r\n\r\n== See also ==\r\nArtificial neural network\r\nTriplet loss\r\n\r\n\r\n== Further reading ==\r\nChicco, Davide (2020), \"Siamese neural networks: an overview\", Artificial Neural Networks, Methods in Molecular Biology, vol. 2190 (3rd ed.), New York City, New York, USA: Springer Protocols, Humana Press, pp. 73\u201394, doi:10.1007/978-1-0716-0826-5_3, ISBN 978-1-0716-0826-5, PMID 32804361, S2CID 221144012\r\n\r\n\r\n== References ==", "start_char_idx": 6151, "end_char_idx": 7357, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "27f9a601-e371-4aa2-a75a-e97560d660ad": {"__data__": {"id_": "27f9a601-e371-4aa2-a75a-e97560d660ad", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Spiking neural network.txt", "file_name": "Spiking neural network.txt", "file_type": "text/plain", "file_size": 14746, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "445f09fa-ef83-4d20-bc51-e3c582e9f4ab", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Spiking neural network.txt", "file_name": "Spiking neural network.txt", "file_type": "text/plain", "file_size": 14746, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "184ef86967fab077506da0b3d31444dc594fa477a8104708f0409f1039a4e404", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8ee44e45-414e-4229-87fb-d1c997d79310", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Siamese neural network.txt", "file_name": "Siamese neural network.txt", "file_type": "text/plain", "file_size": 7467, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "21ad60127cb1d1f70dcad294832405f0a416c8cefdbdac7ec3112e783c5af72c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d0d06dc-33fe-4b18-b329-372fbb4e4e8e", "node_type": "1", "metadata": {}, "hash": "f528db93d3db418883d3c2d6257094992e4920d868f216ddc5e587b258dbce1f", "class_name": "RelatedNodeInfo"}}, "text": "Spiking neural networks (SNNs) are artificial neural networks (ANN) that more closely mimic natural neural networks. In addition to neuronal and synaptic state, SNNs incorporate the concept of time into their operating model. The idea is that neurons in the SNN do not transmit information at each propagation cycle (as it happens with typical multi-layer perceptron networks), but rather transmit information only when a membrane potential\u2014an intrinsic quality of the neuron related to its membrane electrical charge\u2014reaches a specific value, called the threshold. When the membrane potential reaches the threshold, the neuron fires, and generates a signal that travels to other neurons which, in turn, increase or decrease their potentials in response to this signal. A neuron model  that fires at the moment of threshold crossing is also called a spiking neuron model.Although it was previously believed that the brain encoded information through spike rates, which can be considered as the analogue variable output of a traditional ANN, research in the field of neurobiology has indicated that high speed processing cannot solely be performed through a rate based scheme. For example humans can perform an image recognition task at rate requiring no more than 10ms of processing time per neuron through the successive layers (going from the retina to the temporal lobe). This time window is too short for a rate based encoding. The precise spike timings in a small set of spiking neurons also has a higher information coding capacity compared with a rate based approach.The most prominent spiking neuron model is the leaky integrate-and-fire model. In the integrate-and-fire model, the momentary activation level (modeled as a differential equation) is normally considered to be the neuron's state, with incoming spikes pushing this value higher or lower, until the state eventually either decays or\u2014if the firing threshold is reached\u2014the neuron fires. After firing, the state variable is reset to a lower value.\r\nVarious decoding methods exist for interpreting the outgoing spike train as a real-value number, relying on either the frequency of spikes (rate-code), the time-to-first-spike after stimulation, or the interval between spikes.\r\n\r\n\r\n== History ==\r\nMany multi-layer artificial neural networks are fully connected, receiving input from every neuron in the previous layer and signalling every neuron in the subsequent layer. Although these networks have achieved breakthroughs in many fields, they are biologically inaccurate and do not mimic the operation mechanism of neurons in the brain of a living thing.\r\n\r\nThe biologically inspired Hodgkin\u2013Huxley model of a spiking neuron was proposed  in 1952. This model describes how action potentials are initiated and propagated. Communication between neurons, which requires the exchange of chemical neurotransmitters in the synaptic gap, is described in various models, such as the integrate-and-fire model, FitzHugh\u2013Nagumo model (1961\u20131962), and Hindmarsh\u2013Rose model (1984). The leaky integrate-and-fire model (or a derivative) is commonly used as it is easier to compute than the Hodgkin\u2013Huxley model.\r\n\r\n\r\n== Underpinnings ==\r\nInformation in the brain is represented as action potentials (neuron spikes), which may be grouped into spike trains or even coordinated waves of brain activity. A fundamental question of neuroscience is to determine whether neurons communicate by a rate or temporal code. Temporal coding suggests that a single spiking neuron can replace hundreds of hidden units on a sigmoidal neural net.An SNN computes in the continuous rather than the discrete domain. The idea is that neurons may not test for activation in every iteration of propagation (as is the case in a typical multilayer perceptron network), but only when their membrane potentials reach a certain value. When a neuron is activated, it produces a signal that is passed to connected neurons, raising or lowering their membrane potential.\r\nIn a spiking neural network, a neuron's current state is defined as its membrane potential (possibly modeled as a differential equation). An input pulse causes the membrane potential to rise for a period of time and then gradually decline. Encoding schemes have been constructed to interpret these pulse sequences as a number, taking into account both pulse frequency and pulse interval. A neural network model based on pulse generation time can be established. Using the exact time of pulse occurrence, a neural network can employ more information and offer better computing properties.\r\nThe SNN approach produces a continuous output instead of the binary output of traditional artificial neural networks (ANNs). Pulse trains are not easily interpretable, hence the need for encoding schemes as above. However, a pulse train representation may be more suited for processing spatiotemporal data (or continual real-world sensory data classification). SNNs consider space by connecting neurons only to nearby neurons so that they process input blocks separately (similar to CNN using filters).", "start_char_idx": 0, "end_char_idx": 5084, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1d0d06dc-33fe-4b18-b329-372fbb4e4e8e": {"__data__": {"id_": "1d0d06dc-33fe-4b18-b329-372fbb4e4e8e", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Spiking neural network.txt", "file_name": "Spiking neural network.txt", "file_type": "text/plain", "file_size": 14746, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "445f09fa-ef83-4d20-bc51-e3c582e9f4ab", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Spiking neural network.txt", "file_name": "Spiking neural network.txt", "file_type": "text/plain", "file_size": 14746, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "184ef86967fab077506da0b3d31444dc594fa477a8104708f0409f1039a4e404", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "27f9a601-e371-4aa2-a75a-e97560d660ad", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Spiking neural network.txt", "file_name": "Spiking neural network.txt", "file_type": "text/plain", "file_size": 14746, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "21ad0659c377e3ce4f29b7d38ce67caee1892cd4b8ec661a95b6380cbbe5c470", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "645252aa-2919-42d3-bb00-8c100099c178", "node_type": "1", "metadata": {}, "hash": "5a489b58f2ed2f363a0825c66601bddc8c6b6709a3fb51c6bfabb344903755f4", "class_name": "RelatedNodeInfo"}}, "text": "An input pulse causes the membrane potential to rise for a period of time and then gradually decline. Encoding schemes have been constructed to interpret these pulse sequences as a number, taking into account both pulse frequency and pulse interval. A neural network model based on pulse generation time can be established. Using the exact time of pulse occurrence, a neural network can employ more information and offer better computing properties.\r\nThe SNN approach produces a continuous output instead of the binary output of traditional artificial neural networks (ANNs). Pulse trains are not easily interpretable, hence the need for encoding schemes as above. However, a pulse train representation may be more suited for processing spatiotemporal data (or continual real-world sensory data classification). SNNs consider space by connecting neurons only to nearby neurons so that they process input blocks separately (similar to CNN using filters). They consider time by encoding information as pulse trains so as not to lose information in a binary encoding. This avoids the additional complexity of a recurrent neural network (RNN). It turns out that impulse neurons are more powerful computational units than traditional artificial neurons.SNNs are theoretically more powerful than so called \"second-generation networks\" defined in as \"[ANNs] based on computational units that apply activation function with a continuous set of possible output values to a weighted sum (or polynomial) of the inputs; however, SNN training issues and hardware requirements limit their use. Although unsupervised biologically inspired learning methods are available such as Hebbian learning and STDP, no effective supervised training method is suitable for SNNs that can provide better performance than second-generation networks. Spike-based activation of SNNs is not differentiable thus making it hard to develop gradient descent based training methods to perform error backpropagation.\r\nSNNs have much larger computational costs for simulating realistic neural models than traditional ANNs.Pulse-coupled neural networks (PCNN) are often confused with SNNs. A PCNN can be seen as a kind of SNN.\r\nCurrently there are a few challenges when using SNNs that researchers are actively working on. The first challenge concerns the nondifferentiability of the spiking nonlinearity. The expressions for both the forward- and backward-learning methods contain the derivative of the neural activation function which is non-differentiable because neuron's output is either 1 when it spikes, and 0 otherwise. This all-or-nothing behavior of the binary spiking nonlinearity stops gradients from \u201cflowing\u201d and makes LIF neurons unsuitable for gradient-based optimization. The second challenge concerns the implementation of the optimization algorithm itself. Standard BP can be expensive in terms of computation, memory, and communication and may be poorly suited to the constraints dictated by the hardware that implements it (e.g., a computer, brain, or neuromorphic device). Regarding the first challenge there are several approached in order to overcome it. A few of them are:\r\n\r\nresorting to entirely biologically inspired local learning rules for the hidden units\r\ntranslating conventionally trained \u201crate-based\u201d NNs to SNNs\r\nsmoothing the network model to be continuously differentiable\r\ndefining an SG (Surogate Gradient) as a continuous relaxation of the real gradientsIn the development of SNNs, incorporating additional neuron dynamics like Spike Frequency Adaptation (SFA) into neuron models marks a notable advance, enhancing both efficiency and computational power. These neurons stand in between biological complexity and compuational complexity. Originating from biological insights, SFA offers significant computational benefits by reducing power usage through efficient coding, especially in cases of repetitive or intense stimuli. This adaptation improves signal clarity against background noise and introduces an elementary short-term memory at the neuron level, which in turn, refines the accuracy and efficiency of information processing. Recently, This phenomenon is achieved mostly achieved using Compartmental neuron models.\r\nThe simpler versions are of neuron models with adaptive thresholds, indirect way of achieving SFA, equips SNNs with improved learning capabilities, even with constrained synaptic plasticity, and elevates computational efficiency. This feature lessens the demand on network layers by decreasing the need for spike processing, thus cutting down on computational load and memory access time\u2014essential aspects of neural computation.\r\nMoreover, SNNs utilizing neurons capable of SFA achieve levels of accuracy that rival those of conventional artificial neural networks, including those based on long short-term memory models, while also requiring fewer neurons for comparable computational tasks. This efficiency not only streamlines the computational workflow but also conserves space and energy, offering a pragmatic step forward in the practical application of SNNs for complex computing tasks, all while maintaining a commitment to technical integrity.", "start_char_idx": 4131, "end_char_idx": 9310, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "645252aa-2919-42d3-bb00-8c100099c178": {"__data__": {"id_": "645252aa-2919-42d3-bb00-8c100099c178", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Spiking neural network.txt", "file_name": "Spiking neural network.txt", "file_type": "text/plain", "file_size": 14746, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "445f09fa-ef83-4d20-bc51-e3c582e9f4ab", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Spiking neural network.txt", "file_name": "Spiking neural network.txt", "file_type": "text/plain", "file_size": 14746, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "184ef86967fab077506da0b3d31444dc594fa477a8104708f0409f1039a4e404", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d0d06dc-33fe-4b18-b329-372fbb4e4e8e", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Spiking neural network.txt", "file_name": "Spiking neural network.txt", "file_type": "text/plain", "file_size": 14746, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "45c8cbbebaae10d686314c60620d64fe1686da742c8616720d0ac9218d51b38a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "749b128b-e945-46d9-b5ec-90948f4af010", "node_type": "1", "metadata": {}, "hash": "ec68df9d55cb981d39300632a5acc634fe91030ec2a4e520ab10c0b9c5d69921", "class_name": "RelatedNodeInfo"}}, "text": "Recently, This phenomenon is achieved mostly achieved using Compartmental neuron models.\r\nThe simpler versions are of neuron models with adaptive thresholds, indirect way of achieving SFA, equips SNNs with improved learning capabilities, even with constrained synaptic plasticity, and elevates computational efficiency. This feature lessens the demand on network layers by decreasing the need for spike processing, thus cutting down on computational load and memory access time\u2014essential aspects of neural computation.\r\nMoreover, SNNs utilizing neurons capable of SFA achieve levels of accuracy that rival those of conventional artificial neural networks, including those based on long short-term memory models, while also requiring fewer neurons for comparable computational tasks. This efficiency not only streamlines the computational workflow but also conserves space and energy, offering a pragmatic step forward in the practical application of SNNs for complex computing tasks, all while maintaining a commitment to technical integrity.\r\n\r\n\r\n== Applications ==\r\nSNNs can in principle apply to the same applications as traditional ANNs. In addition, SNNs can model the central nervous system of biological organisms, such as an insect seeking food without prior knowledge of the environment. Due to their relative realism, they can be used to study the operation of biological neural circuits. Starting with a hypothesis about the topology of a biological neuronal circuit and its function, recordings of this circuit can be compared to the output of the corresponding SNN, evaluating the plausibility of the hypothesis. However, there is a lack of effective training mechanisms for SNNs, which can be inhibitory for some applications, including computer vision tasks.\r\nAs of 2019 SNNs lag behind ANNs in terms of accuracy, but the gap is decreasing, and has vanished on some tasks.When using SNNs for image based data we need to convert static images into binary spike trains coding. Types of encodings:\r\nTemporal coding generates one spike per neuron in which spike latency is inversely proportional to the pixel intensity.\r\nRate coding converts pixel intensity into a spike train where the number of spikes is proportional to the pixel intensity.\r\nDirect coding uses a trainable layer to generate float value for each time-step. We have a learnable layer which converts each pixel at certain time step in float number and then threshold is used on the generated floating numbers to see if they will be 1 or 0.\r\nPhase coding encodes temporal information into spike patterns based on a global oscillator.\r\nBurst coding transmits the burst of spikes in a small-time duration, increasing the reliability of synaptic communication between neurons.\r\n\r\n\r\n== Software ==\r\nA diverse range of application software can simulate SNNs. This software can be classified according to its uses:\r\n\r\n\r\n=== SNN simulation ===\r\n These simulate complex neural models with a high level of detail and accuracy. Large networks usually require lengthy processing. Candidates include:Brian \u2013 developed by Romain Brette and Dan Goodman at the \u00c9cole Normale Sup\u00e9rieure;\r\nGENESIS (the GEneral NEural SImulation System) \u2013 developed in James Bower's laboratory at Caltech;\r\nNEST \u2013 developed by the NEST Initiative;\r\nNEURON \u2013 mainly developed by Michael Hines, John W. Moore and Ted Carnevale in Yale University and Duke University;\r\nRAVSim (Runtime Tool)  \u2013 mainly developed by Sanaullah in Bielefeld University of Applied Sciences and Arts;\r\n\r\n\r\n== Hardware ==\r\nFuture neuromorphic architectures will comprise billions of such nanosynapses, which require a clear understanding of the physical mechanisms responsible for plasticity. Experimental systems based on ferroelectric tunnel junctions have been used to show that STDP can be harnessed from heterogeneous polarization switching. Through combined scanning probe imaging, electrical transport and atomic-scale molecular dynamics, conductance variations can be modelled by nucleation-dominated reversal of domains. Simulations show that arrays of ferroelectric nanosynapses can autonomously learn to recognize patterns in a predictable way, opening the path towards unsupervised learning.\r\nAkida is a completely digital event-based neural processing device with 1.2 million artificial neurons and 10 billion artificial synapses developed by BrainChip. Utilizing event-based possessing, it analyzes essential inputs at specific points. Results are stored in the on-chip memory units.\r\nNeurogrid is a board that can simulate spiking neural networks directly in hardware. (Stanford University)\r\nSpiNNaker (Spiking Neural Network Architecture) uses ARM processors as the building blocks of a massively parallel computing platform based on a six-layer thalamocortical model. (University of Manchester) The SpiNNaker system is based on numerical models running in real time on custom digital multicore chips using the ARM architecture.", "start_char_idx": 8268, "end_char_idx": 13228, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "749b128b-e945-46d9-b5ec-90948f4af010": {"__data__": {"id_": "749b128b-e945-46d9-b5ec-90948f4af010", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Spiking neural network.txt", "file_name": "Spiking neural network.txt", "file_type": "text/plain", "file_size": 14746, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "445f09fa-ef83-4d20-bc51-e3c582e9f4ab", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Spiking neural network.txt", "file_name": "Spiking neural network.txt", "file_type": "text/plain", "file_size": 14746, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "184ef86967fab077506da0b3d31444dc594fa477a8104708f0409f1039a4e404", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "645252aa-2919-42d3-bb00-8c100099c178", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Spiking neural network.txt", "file_name": "Spiking neural network.txt", "file_type": "text/plain", "file_size": 14746, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "bd79a9dd1c6ce700f9a4e909a02eb08d8364e79e3233005486a5a4a8fd4fc61d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3784e59e-859b-4284-8301-a78cd765e3ed", "node_type": "1", "metadata": {}, "hash": "e7e256cd7211f7f8121914f205d5f9ea31de009d1a4d00ebe97d168f4423163f", "class_name": "RelatedNodeInfo"}}, "text": "Simulations show that arrays of ferroelectric nanosynapses can autonomously learn to recognize patterns in a predictable way, opening the path towards unsupervised learning.\r\nAkida is a completely digital event-based neural processing device with 1.2 million artificial neurons and 10 billion artificial synapses developed by BrainChip. Utilizing event-based possessing, it analyzes essential inputs at specific points. Results are stored in the on-chip memory units.\r\nNeurogrid is a board that can simulate spiking neural networks directly in hardware. (Stanford University)\r\nSpiNNaker (Spiking Neural Network Architecture) uses ARM processors as the building blocks of a massively parallel computing platform based on a six-layer thalamocortical model. (University of Manchester) The SpiNNaker system is based on numerical models running in real time on custom digital multicore chips using the ARM architecture. It provides custom digital chips, each with eighteen cores and a shared local 128 Mbyte RAM, with a total of over 1,000,000 cores. A single chip can simulate 16,000 neurons with eight million plastic synapses running in real time.\r\nTrueNorth is a processor that contains 5.4 billion transistors that consumes only 70 milliwatts; most processors in personal computers contain about 1.4 billion transistors and require 35 watts or more. IBM refers to the design principle behind TrueNorth as neuromorphic computing. Its primary purpose is pattern recognition. While critics say the chip isn't powerful enough, its supporters point out that this is only the first generation, and the capabilities of improved iterations will become clear. (IBM)\r\n\r\n\r\n== Benchmarks ==\r\nClassification capabilities of spiking networks trained according to unsupervised learning methods have been tested on the common benchmark datasets, such as, Iris, Wisconsin Breast Cancer or Statlog Landsat dataset. Various approaches to information encoding and network design have been used. For example, a 2-layer feedforward network for data clustering and classification. Based on the idea proposed in Hopfield (1995) the authors implemented models of local receptive fields combining the properties of radial basis functions (RBF) and spiking neurons to convert input signals (classified data) having a floating-point representation into a spiking representation.\r\n\r\n\r\n== See also ==\r\n\r\n\r\n== References ==", "start_char_idx": 12314, "end_char_idx": 14706, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3784e59e-859b-4284-8301-a78cd765e3ed": {"__data__": {"id_": "3784e59e-859b-4284-8301-a78cd765e3ed", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Types of artificial neural networks.txt", "file_name": "Types of artificial neural networks.txt", "file_type": "text/plain", "file_size": 43865, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bb0d5604-c0f9-4a94-b06f-5075d5cf5210", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Types of artificial neural networks.txt", "file_name": "Types of artificial neural networks.txt", "file_type": "text/plain", "file_size": 43865, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "d277accaa570b90c3c6dfbfddc510a116fa13b6f836e4a224859f2f8403fea65", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "749b128b-e945-46d9-b5ec-90948f4af010", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Spiking neural network.txt", "file_name": "Spiking neural network.txt", "file_type": "text/plain", "file_size": 14746, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "05d151a0e10c6725d030bb8de7c8c0236aaa9002d870c0861ec1d44fc74a8e25", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "82d03e73-f80c-4e33-87f2-70583aac83dc", "node_type": "1", "metadata": {}, "hash": "ef0bc6c5df352e265b9f5153dda2c1b18ded5e6e23fe614842fa0c5601f646d3", "class_name": "RelatedNodeInfo"}}, "text": "There are many types of artificial neural networks (ANN).\r\nArtificial neural networks are computational models inspired by biological neural networks, and are used to approximate functions that are generally unknown. Particularly, they are inspired by the behaviour of neurons and the electrical signals they convey between input (such as from the eyes or nerve endings in the hand), processing, and output from the brain (such as reacting to light, touch, or heat). The way neurons semantically communicate is an area of ongoing research. Most artificial neural networks bear only some resemblance to their more complex biological counterparts, but are very effective at their intended tasks (e.g. classification or segmentation).\r\nSome artificial neural networks are adaptive systems and are used for example to model populations and environments, which constantly change.\r\nNeural networks can be hardware- (neurons are represented by physical components) or software-based (computer models), and can use a variety of topologies and learning algorithms.\r\n\r\n\r\n== Feedforward ==\r\n\r\nThe feedforward neural network was the first and simplest type. In this network the information moves only from the input layer directly through any hidden layers to the output layer without cycles/loops. Feedforward networks can be constructed with various types of units, such as binary McCulloch\u2013Pitts neurons, the simplest of which is the perceptron. Continuous neurons, frequently with sigmoidal activation, are used in the context of backpropagation.\r\n\r\n\r\n=== Group method of data handling ===\r\n\r\nThe Group Method of Data Handling (GMDH) features fully automatic structural and parametric model optimization. The node activation functions are Kolmogorov\u2013Gabor polynomials that permit additions and multiplications. It uses a deep multilayer perceptron with eight layers. It is a supervised learning network that grows layer by layer, where each layer is trained by regression analysis. Useless items are detected using a validation set, and pruned through regularization. The size and depth of the resulting network depends on the task.\r\n\r\n\r\n=== Autoencoder ===\r\n\r\nAn autoencoder, autoassociator or Diabolo network:\u200a19\u200a is similar to the multilayer perceptron (MLP) \u2013 with an input layer, an output layer and one or more hidden layers connecting them. However, the output layer has the same number of units as the input layer. Its purpose is to reconstruct its own inputs (instead of emitting a target value). Therefore, autoencoders are unsupervised learning models. An autoencoder is used for unsupervised learning of efficient codings, typically for the purpose of dimensionality reduction and for learning generative models of data.\r\n\r\n\r\n=== Probabilistic ===\r\n\r\nA probabilistic neural network (PNN) is a four-layer feedforward neural network. The layers are Input, hidden pattern/summation, and output. In the PNN algorithm, the parent probability distribution function (PDF) of each class is approximated by a Parzen window and a non-parametric function. Then, using PDF of each class, the class probability of a new input is estimated and Bayes\u2019 rule is employed to allocate it to the class with the highest posterior probability. It was derived from the Bayesian network and a statistical algorithm called Kernel Fisher discriminant analysis. It is used for classification and pattern recognition.\r\n\r\n\r\n=== Time delay ===\r\n\r\nA time delay neural network (TDNN)  is a feedforward architecture for sequential data that recognizes features independent of sequence position. In order to achieve time-shift invariance, delays are added to the input so that multiple data points (points in time) are analyzed together.\r\nIt usually forms part of a larger pattern recognition system. It has been implemented using a perceptron network whose connection weights were trained with back propagation (supervised learning).\r\n\r\n\r\n=== Convolutional ===\r\n\r\nA convolutional neural network (CNN, or ConvNet or shift invariant or space invariant) is a class of deep network, composed of one or more convolutional layers with fully connected layers (matching those in typical ANNs) on top. It uses tied weights and pooling layers. In particular, max-pooling. It is often structured via Fukushima's convolutional architecture. They are variations of multilayer perceptrons that use minimal preprocessing. This architecture allows CNNs to take advantage of the 2D structure of input data.\r\nIts unit connectivity pattern is inspired by the organization of the visual cortex. Units respond to stimuli in a restricted region of space known as the receptive field.  Receptive fields partially overlap, over-covering the entire visual field.  Unit response can be approximated mathematically by a convolution operation.CNNs are suitable for processing visual and other two-dimensional data.", "start_char_idx": 0, "end_char_idx": 4857, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "82d03e73-f80c-4e33-87f2-70583aac83dc": {"__data__": {"id_": "82d03e73-f80c-4e33-87f2-70583aac83dc", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Types of artificial neural networks.txt", "file_name": "Types of artificial neural networks.txt", "file_type": "text/plain", "file_size": 43865, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bb0d5604-c0f9-4a94-b06f-5075d5cf5210", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Types of artificial neural networks.txt", "file_name": "Types of artificial neural networks.txt", "file_type": "text/plain", "file_size": 43865, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "d277accaa570b90c3c6dfbfddc510a116fa13b6f836e4a224859f2f8403fea65", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3784e59e-859b-4284-8301-a78cd765e3ed", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Types of artificial neural networks.txt", "file_name": "Types of artificial neural networks.txt", "file_type": "text/plain", "file_size": 43865, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "1e194acb28afa7ebb23858fa7d47732b3b7c5e2c47b0a717e33d653037b9d420", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ff6d664b-1345-42ac-b869-def8f9de77d5", "node_type": "1", "metadata": {}, "hash": "b090d8baab9a1c97cd43cea68b20cc589b08eefac1b7ad4e83a4975fe0748bac", "class_name": "RelatedNodeInfo"}}, "text": "=== Convolutional ===\r\n\r\nA convolutional neural network (CNN, or ConvNet or shift invariant or space invariant) is a class of deep network, composed of one or more convolutional layers with fully connected layers (matching those in typical ANNs) on top. It uses tied weights and pooling layers. In particular, max-pooling. It is often structured via Fukushima's convolutional architecture. They are variations of multilayer perceptrons that use minimal preprocessing. This architecture allows CNNs to take advantage of the 2D structure of input data.\r\nIts unit connectivity pattern is inspired by the organization of the visual cortex. Units respond to stimuli in a restricted region of space known as the receptive field.  Receptive fields partially overlap, over-covering the entire visual field.  Unit response can be approximated mathematically by a convolution operation.CNNs are suitable for processing visual and other two-dimensional data. They have shown superior results in both image and speech applications. They can be trained with standard backpropagation. CNNs are easier to train than other regular, deep, feed-forward neural networks and have many fewer parameters to estimate.Capsule Neural Networks (CapsNet) add structures called capsules to a CNN and reuse output from several capsules to form more stable (with respect to various perturbations) representations.Examples of applications in computer vision include DeepDream and robot navigation. They have wide applications in image and video recognition, recommender systems and natural language processing.\r\n\r\n\r\n=== Deep stacking network ===\r\nA deep stacking network (DSN) (deep convex network) is based on a hierarchy of blocks of simplified neural network modules. It was introduced in 2011 by Deng and Yu. It formulates the learning as a convex optimization problem with a closed-form solution, emphasizing the mechanism's similarity to stacked generalization. Each DSN block is a simple module that is easy to train by itself in a supervised fashion without backpropagation for the entire blocks.Each block consists of a simplified multi-layer perceptron (MLP) with a single hidden layer. The hidden layer h has logistic sigmoidal units, and the output layer has linear units. Connections between these layers are represented by weight matrix U; input-to-hidden-layer connections have weight matrix W. Target vectors t form the columns of matrix T, and the input data vectors x form the columns of matrix X. The matrix of hidden units is H=\u03c3(WTX){\\displaystyle {\\boldsymbol {H}}=\\sigma ({\\boldsymbol {W}}^{T}{\\boldsymbol {X}})}. Modules are trained in order, so lower-layer weights W are known at each stage. The function performs the element-wise logistic sigmoid operation. Each block estimates the same final label class y, and its estimate is concatenated with original input X to form the expanded input for the next block. Thus, the input to the first block contains the original data only, while downstream blocks' input adds the output of preceding blocks. Then learning the upper-layer weight matrix U given other weights in the network can be formulated as a convex optimization problem:\r\n\r\nminUTf=\u2016UTH\u2212T\u2016F2,{\\displaystyle \\min _{U^{T}}f=\\|{\\boldsymbol {U}}^{T}{\\boldsymbol {H}}-{\\boldsymbol {T}}\\|_{F}^{2},}which has a closed-form solution.Unlike other deep architectures, such as DBNs, the goal is not to discover the transformed feature representation. The structure of the hierarchy of this kind of architecture makes parallel learning straightforward, as a batch-mode optimization problem. In purely discriminative tasks, DSNs outperform conventional DBNs.\r\n\r\n\r\n==== Tensor deep stacking networks ====\r\nThis architecture is a DSN extension. It offers two important improvements: it uses higher-order information from covariance statistics, and it transforms the non-convex problem of a lower-layer to a convex sub-problem of an upper-layer. TDSNs use covariance statistics in a bilinear mapping from each of two distinct sets of hidden units in the same layer to predictions, via a third-order tensor.\r\nWhile parallelization and scalability are not considered seriously in conventional DNNs, all learning for DSNs and TDSNs is done in batch mode, to allow parallelization. Parallelization allows scaling the design to larger (deeper) architectures and data sets.\r\nThe basic architecture is suitable for diverse tasks such as classification and regression.\r\n\r\n\r\n== Regulatory feedback ==\r\nRegulatory feedback networks started as a model to explain brain phenomena found during recognition including network-wide bursting and difficulty with similarity found universally in sensory recognition.", "start_char_idx": 3910, "end_char_idx": 8594, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ff6d664b-1345-42ac-b869-def8f9de77d5": {"__data__": {"id_": "ff6d664b-1345-42ac-b869-def8f9de77d5", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Types of artificial neural networks.txt", "file_name": "Types of artificial neural networks.txt", "file_type": "text/plain", "file_size": 43865, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bb0d5604-c0f9-4a94-b06f-5075d5cf5210", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Types of artificial neural networks.txt", "file_name": "Types of artificial neural networks.txt", "file_type": "text/plain", "file_size": 43865, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "d277accaa570b90c3c6dfbfddc510a116fa13b6f836e4a224859f2f8403fea65", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "82d03e73-f80c-4e33-87f2-70583aac83dc", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Types of artificial neural networks.txt", "file_name": "Types of artificial neural networks.txt", "file_type": "text/plain", "file_size": 43865, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "a29a6358a59abbbf1ff0154e5c682210f1a69da524bb147cb5e5bc03fc90eab0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2544d06a-6fcc-4bd0-a8fc-622e6cc88eea", "node_type": "1", "metadata": {}, "hash": "e563e04cfa23cbcf4f037ac7ea847bdb8d89ffe8f07eefa4fb15f35d8f50b905", "class_name": "RelatedNodeInfo"}}, "text": "==== Tensor deep stacking networks ====\r\nThis architecture is a DSN extension. It offers two important improvements: it uses higher-order information from covariance statistics, and it transforms the non-convex problem of a lower-layer to a convex sub-problem of an upper-layer. TDSNs use covariance statistics in a bilinear mapping from each of two distinct sets of hidden units in the same layer to predictions, via a third-order tensor.\r\nWhile parallelization and scalability are not considered seriously in conventional DNNs, all learning for DSNs and TDSNs is done in batch mode, to allow parallelization. Parallelization allows scaling the design to larger (deeper) architectures and data sets.\r\nThe basic architecture is suitable for diverse tasks such as classification and regression.\r\n\r\n\r\n== Regulatory feedback ==\r\nRegulatory feedback networks started as a model to explain brain phenomena found during recognition including network-wide bursting and difficulty with similarity found universally in sensory recognition. A mechanism to perform optimization during recognition is created using inhibitory feedback connections back to the same inputs that activate them.  This reduces requirements during learning and allows learning and updating to be easier while still being able to perform complex recognition.\r\nA regulatory feedback network makes inferences using negative feedback.   The feedback is used to find the optimal activation of units. It is most similar to a non-parametric method but is different from K-nearest neighbor in that it mathematically emulates feedforward networks.\r\n\r\n\r\n== Radial basis function (RBF) ==\r\nRadial basis functions are functions that have a distance criterion with respect to a center. Radial basis functions have been applied as a replacement for the sigmoidal hidden layer transfer characteristic in multi-layer perceptrons. RBF networks have two layers: In the first, input is mapped onto each RBF in the 'hidden' layer. The RBF chosen is usually a Gaussian. In regression problems the output layer is a linear combination of hidden layer values representing mean predicted output. The interpretation of this output layer value is the same as a regression model in statistics. In classification problems the output layer is typically a sigmoid function of a linear combination of hidden layer values, representing a posterior probability. Performance in both cases is often improved by shrinkage techniques, known as ridge regression in classical statistics. This corresponds to a prior belief in small parameter values (and therefore smooth output functions) in a Bayesian framework.\r\nRBF networks have the advantage of avoiding local minima in the same way as multi-layer perceptrons. This is because the only parameters that are adjusted in the learning process are the linear mapping from hidden layer to output layer. Linearity ensures that the error surface is quadratic and therefore has a single easily found minimum. In regression problems this can be found in one matrix operation. In classification problems the fixed non-linearity introduced by the sigmoid output function is most efficiently dealt with using iteratively re-weighted least squares.\r\nRBF networks have the disadvantage of requiring good coverage of the input space by radial basis functions. RBF centres are determined with reference to the distribution of the input data, but without reference to the prediction task. As a result, representational resources may be wasted on areas of the input space that are irrelevant to the task. A common solution is to associate each data point with its own centre, although this can expand the linear system to be solved in the final layer and requires shrinkage techniques to avoid overfitting.\r\nAssociating each input datum with an RBF leads naturally to kernel methods such as support vector machines (SVM) and Gaussian processes (the RBF is the kernel function). All three approaches use a non-linear kernel function to project the input data into a space where the learning problem can be solved using a linear model. Like Gaussian processes, and unlike SVMs, RBF networks are typically trained in a maximum likelihood framework by maximizing the probability (minimizing the error). SVMs avoid overfitting by maximizing instead a margin. SVMs outperform RBF networks in most classification applications. In regression applications they can be competitive when the dimensionality of the input space is relatively small.\r\n\r\n\r\n=== How RBF networks work ===\r\nRBF neural networks are conceptually similar to K-Nearest Neighbor (k-NN) models. The basic idea is that similar inputs produce similar outputs.\r\nAssume that each case in a training set has two predictor variables, x and y, and the target variable has two categories, positive and negative. Given a new case with predictor values x=6, y=5.1, how is the target variable computed?\r\nThe nearest neighbor classification performed for this example depends on how many neighboring points are considered.", "start_char_idx": 7564, "end_char_idx": 12594, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2544d06a-6fcc-4bd0-a8fc-622e6cc88eea": {"__data__": {"id_": "2544d06a-6fcc-4bd0-a8fc-622e6cc88eea", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Types of artificial neural networks.txt", "file_name": "Types of artificial neural networks.txt", "file_type": "text/plain", "file_size": 43865, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bb0d5604-c0f9-4a94-b06f-5075d5cf5210", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Types of artificial neural networks.txt", "file_name": "Types of artificial neural networks.txt", "file_type": "text/plain", "file_size": 43865, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "d277accaa570b90c3c6dfbfddc510a116fa13b6f836e4a224859f2f8403fea65", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ff6d664b-1345-42ac-b869-def8f9de77d5", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Types of artificial neural networks.txt", "file_name": "Types of artificial neural networks.txt", "file_type": "text/plain", "file_size": 43865, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "fb45087e5f40509fd10bf098e9b65487e4f08920f71f1d7f25702d60a1f26630", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6ca97326-c3f6-4865-bdfb-66199e0a01d8", "node_type": "1", "metadata": {}, "hash": "f9a86736ddf11e6e905ed1e0069fc9853d36be02f17c6d44a08f63f75d2da91e", "class_name": "RelatedNodeInfo"}}, "text": "Like Gaussian processes, and unlike SVMs, RBF networks are typically trained in a maximum likelihood framework by maximizing the probability (minimizing the error). SVMs avoid overfitting by maximizing instead a margin. SVMs outperform RBF networks in most classification applications. In regression applications they can be competitive when the dimensionality of the input space is relatively small.\r\n\r\n\r\n=== How RBF networks work ===\r\nRBF neural networks are conceptually similar to K-Nearest Neighbor (k-NN) models. The basic idea is that similar inputs produce similar outputs.\r\nAssume that each case in a training set has two predictor variables, x and y, and the target variable has two categories, positive and negative. Given a new case with predictor values x=6, y=5.1, how is the target variable computed?\r\nThe nearest neighbor classification performed for this example depends on how many neighboring points are considered. If 1-NN is used and the closest point is negative, then the new point should be classified as negative. Alternatively, if 9-NN classification is used and the closest 9 points are considered, then the effect of the surrounding 8 positive points may outweigh the closest 9-th (negative) point.\r\nAn RBF network positions neurons in the space described by the predictor variables (x,y in this example). This space has as many dimensions as predictor variables. The Euclidean distance is computed from the new point to the center of each neuron, and a radial basis function (RBF, also called a kernel function) is applied to the distance to compute the weight (influence) for each neuron. The radial basis function is so named because the radius distance is the argument to the function.\r\n\r\nWeight = RBF(distance)\r\n\r\n\r\n==== Radial Basis Function ====\r\nThe value for the new point is found by summing the output values of the RBF functions multiplied by weights computed for each neuron.\r\nThe radial basis function for a neuron has a center and a radius (also called a spread). The radius may be different for each neuron, and, in RBF networks generated by DTREG, the radius may be different in each dimension.\r\nWith larger spread, neurons at a distance from a point have a greater influence.\r\n\r\n\r\n==== Architecture ====\r\nRBF networks have three layers:\r\n\r\nInput layer: One neuron appears in the input layer for each predictor variable. In the case of categorical variables, N-1 neurons are used where N is the number of categories. The input neurons standardizes the value ranges by subtracting the median and dividing by the interquartile range. The input neurons then feed the values to each of the neurons in the hidden layer.\r\nHidden layer: This layer has a variable number of neurons (determined by the training process). Each neuron consists of a radial basis function centered on a point with as many dimensions as predictor variables. The spread (radius) of the RBF function may be different for each dimension. The centers and spreads are determined by training. When presented with the x vector of input values from the input layer, a hidden neuron computes the Euclidean distance of the test case from the neuron's center point and then applies the RBF kernel function to this distance using the spread values. The resulting value is passed to the summation layer.\r\nSummation layer: The value coming out of a neuron in the hidden layer is multiplied by a weight associated with the neuron and adds to the weighted values of other neurons. This sum becomes the output. For classification problems, one output is produced (with a separate set of weights and summation unit) for each target category. The value output for a category is the probability that the case being evaluated has that category.\r\n\r\n\r\n==== Training ====\r\nThe following parameters are determined by the training process:\r\n\r\nThe number of neurons in the hidden layer\r\nThe coordinates of the center of each hidden-layer RBF function\r\nThe radius (spread) of each RBF function in each dimension\r\nThe weights applied to the RBF function outputs as they pass to the summation layerVarious methods have been used to train RBF networks. One approach first uses K-means clustering to find cluster centers which are then used as the centers for the RBF functions. However, K-means clustering is computationally intensive and it often does not generate the optimal number of centers. Another approach is to use a random subset of the training points as the centers.\r\nDTREG uses a training algorithm that uses an evolutionary approach to determine the optimal center points and spreads for each neuron. It determines when to stop adding neurons to the network by monitoring the estimated leave-one-out (LOO) error and terminating when the LOO error begins to increase because of overfitting.", "start_char_idx": 11660, "end_char_idx": 16447, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ca97326-c3f6-4865-bdfb-66199e0a01d8": {"__data__": {"id_": "6ca97326-c3f6-4865-bdfb-66199e0a01d8", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Types of artificial neural networks.txt", "file_name": "Types of artificial neural networks.txt", "file_type": "text/plain", "file_size": 43865, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bb0d5604-c0f9-4a94-b06f-5075d5cf5210", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Types of artificial neural networks.txt", "file_name": "Types of artificial neural networks.txt", "file_type": "text/plain", "file_size": 43865, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "d277accaa570b90c3c6dfbfddc510a116fa13b6f836e4a224859f2f8403fea65", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2544d06a-6fcc-4bd0-a8fc-622e6cc88eea", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Types of artificial neural networks.txt", "file_name": "Types of artificial neural networks.txt", "file_type": "text/plain", "file_size": 43865, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "24e578c85ab601116d783d1ad1328fb5936820d7e49bae6958df2f7ce52999ff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "466a7fe3-71fe-4eaf-ada7-d94251b68b7e", "node_type": "1", "metadata": {}, "hash": "585d3a508747f2ec313b376fab8d83f24aefaa56359e98b6be3006c75db6758f", "class_name": "RelatedNodeInfo"}}, "text": "One approach first uses K-means clustering to find cluster centers which are then used as the centers for the RBF functions. However, K-means clustering is computationally intensive and it often does not generate the optimal number of centers. Another approach is to use a random subset of the training points as the centers.\r\nDTREG uses a training algorithm that uses an evolutionary approach to determine the optimal center points and spreads for each neuron. It determines when to stop adding neurons to the network by monitoring the estimated leave-one-out (LOO) error and terminating when the LOO error begins to increase because of overfitting.\r\nThe computation of the optimal weights between the neurons in the hidden layer and the summation layer is done using ridge regression. An iterative procedure computes the optimal regularization Lambda parameter that minimizes the generalized cross-validation (GCV) error.\r\n\r\n\r\n=== General regression neural network ===\r\n\r\nA GRNN is an associative memory neural network that is similar to the probabilistic neural network but it is used for regression and approximation rather than classification.\r\n\r\n\r\n== Deep belief network ==\r\n\r\nA deep belief network (DBN) is a probabilistic, generative model made up of multiple hidden layers. It can be considered a composition of simple learning modules.A DBN can be used to generatively pre-train a deep neural network (DNN) by using the learned DBN weights as the initial DNN weights. Various discriminative algorithms can then tune these weights. This is particularly helpful when training data are limited, because poorly initialized weights can significantly hinder learning. These pre-trained weights end up in a region of the weight space that is closer to the optimal weights than random choices. This allows for both improved modeling and faster ultimate convergence.\r\n\r\n\r\n== Recurrent neural network ==\r\n\r\nRecurrent neural networks (RNN) propagate data forward, but also backwards, from later processing stages to earlier stages. RNN can be used as general sequence processors.\r\n\r\n\r\n=== Fully recurrent ===\r\nThis architecture was developed in the 1980s. Its network creates a directed connection between every pair of units. Each has a time-varying, real-valued (more than just zero or one) activation (output). Each connection has a modifiable real-valued weight. Some of the nodes are called labeled nodes, some output nodes, the rest hidden nodes.\r\nFor supervised learning in discrete time settings, training sequences of real-valued input vectors become sequences of activations of the input nodes, one input vector at a time. At each time step, each non-input unit computes its current activation as a nonlinear function of the weighted sum of the activations of all units from which it receives connections. The system can explicitly activate (independent of incoming signals) some output units at certain time steps. For example, if the input sequence is a speech signal corresponding to a spoken digit, the final target output at the end of the sequence may be a label classifying the digit. For each sequence, its error is the sum of the deviations of all activations computed by the network from the corresponding target signals. For a training set of numerous sequences, the total error is the sum of the errors of all individual sequences.\r\nTo minimize total error, gradient descent can be used to change each weight in proportion to its derivative with respect to the error, provided the non-linear activation functions are differentiable. The standard method is called \"backpropagation through time\" or BPTT, a generalization of back-propagation for feedforward networks. A more computationally expensive online variant is called \"Real-Time Recurrent Learning\" or RTRL. Unlike BPTT this algorithm is local in time but not local in space. An online hybrid between BPTT and RTRL with intermediate complexity exists, with variants for continuous time. A major problem with gradient descent for standard RNN architectures is that error gradients vanish exponentially quickly with the size of the time lag between important events. The Long short-term memory architecture overcomes these problems.In reinforcement learning settings, no teacher provides target signals. Instead a fitness function or reward function or utility function is occasionally used to evaluate performance, which influences its input stream through output units connected to actuators that affect the environment. Variants of evolutionary computation are often used to optimize the weight matrix.\r\n\r\n\r\n==== Hopfield ====\r\n\r\nThe Hopfield network (like similar attractor-based networks) is of historic interest although it is not a general RNN, as it is not designed to process sequences of patterns. Instead it requires stationary inputs. It is an RNN in which all connections are symmetric. It guarantees that it will converge. If the connections are trained using Hebbian learning the Hopfield network can perform as robust content-addressable memory, resistant to connection alteration.", "start_char_idx": 15797, "end_char_idx": 20869, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "466a7fe3-71fe-4eaf-ada7-d94251b68b7e": {"__data__": {"id_": "466a7fe3-71fe-4eaf-ada7-d94251b68b7e", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Types of artificial neural networks.txt", "file_name": "Types of artificial neural networks.txt", "file_type": "text/plain", "file_size": 43865, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bb0d5604-c0f9-4a94-b06f-5075d5cf5210", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Types of artificial neural networks.txt", "file_name": "Types of artificial neural networks.txt", "file_type": "text/plain", "file_size": 43865, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "d277accaa570b90c3c6dfbfddc510a116fa13b6f836e4a224859f2f8403fea65", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ca97326-c3f6-4865-bdfb-66199e0a01d8", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Types of artificial neural networks.txt", "file_name": "Types of artificial neural networks.txt", "file_type": "text/plain", "file_size": 43865, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "4e1a3c31aace5153d2d1f3a7d691a8dba91040501f4152e3c955ac7d283f2a6c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1803e097-fef3-4177-bf1a-ff8603f52552", "node_type": "1", "metadata": {}, "hash": "cde3baa48ae5e6cc0fb72a16bc1e00b5c3a6d4c146b16c0f915c9435ba7c2f92", "class_name": "RelatedNodeInfo"}}, "text": "The Long short-term memory architecture overcomes these problems.In reinforcement learning settings, no teacher provides target signals. Instead a fitness function or reward function or utility function is occasionally used to evaluate performance, which influences its input stream through output units connected to actuators that affect the environment. Variants of evolutionary computation are often used to optimize the weight matrix.\r\n\r\n\r\n==== Hopfield ====\r\n\r\nThe Hopfield network (like similar attractor-based networks) is of historic interest although it is not a general RNN, as it is not designed to process sequences of patterns. Instead it requires stationary inputs. It is an RNN in which all connections are symmetric. It guarantees that it will converge. If the connections are trained using Hebbian learning the Hopfield network can perform as robust content-addressable memory, resistant to connection alteration.\r\n\r\n\r\n==== Boltzmann machine ====\r\n\r\nThe Boltzmann machine can be thought of as a noisy Hopfield network. It is one of the first neural networks to demonstrate learning of latent variables (hidden units). Boltzmann machine learning was at first slow to simulate, but the contrastive divergence algorithm speeds up training for Boltzmann machines and Products of Experts.\r\n\r\n\r\n==== Self-organizing map ====\r\n\r\nThe self-organizing map (SOM) uses unsupervised learning. A set of neurons learn to map points in an input space to coordinates in an output space. The input space can have different dimensions and topology from the output space, and SOM attempts to preserve these.\r\n\r\n\r\n==== Learning vector quantization ====\r\n\r\nLearning vector quantization (LVQ) can be interpreted as a neural network architecture. Prototypical representatives of the classes parameterize, together with an appropriate distance measure, in a distance-based classification scheme.\r\n\r\n\r\n=== Simple recurrent ===\r\nSimple recurrent networks have three layers, with the addition of a set of \"context units\" in the input layer. These units connect from the hidden layer or the output layer with a fixed weight of one. At each time step, the input is propagated in a standard feedforward fashion, and then a backpropagation-like learning rule is applied (not performing gradient descent). The fixed back connections leave a copy of the previous values of the hidden units in the context units (since they propagate over the connections before the learning rule is applied).\r\n\r\n\r\n=== Reservoir computing ===\r\n\r\nReservoir computing is a computation framework that may be viewed as an extension of neural networks. Typically an input signal is fed into a fixed (random) dynamical system called a reservoir whose dynamics map the input to a higher dimension. A readout mechanism is trained to map the reservoir to the desired output. Training is performed only at the readout stage. Liquid-state machines are a type of reservoir computing.\r\n\r\n\r\n==== Echo state ====\r\n\r\nThe echo state network (ESN) employs a sparsely connected random hidden layer. The weights of output neurons are the only part of the network that are trained. ESN are good at reproducing certain time series.\r\n\r\n\r\n=== Long short-term memory ===\r\n\r\nThe long short-term memory (LSTM) avoids the vanishing gradient problem. It works even when with long delays between inputs and can handle signals that mix low and high frequency components. LSTM RNN outperformed other RNN and other sequence learning methods such as HMM in applications such as language learning and connected handwriting recognition.\r\n\r\n\r\n=== Bi-directional ===\r\n\r\nBi-directional RNN, or BRNN, use a finite sequence to predict or label each element of a sequence based on both the past and future context of the element. This is done by adding the outputs of two RNNs: one processing the sequence from left to right, the other one from right to left. The combined outputs are the predictions of the teacher-given target signals. This technique proved to be especially useful when combined with LSTM.\r\n\r\n\r\n=== Hierarchical ===\r\n\r\nHierarchical RNN connects elements in various ways to decompose hierarchical behavior into useful subprograms.\r\n\r\n\r\n=== Stochastic ===\r\n\r\nA district from conventional neural networks, stochastic artificial neural network used as an approximation to \r\nrandom functions.\r\n\r\n\r\n=== Genetic Scale ===\r\nA RNN (often a LSTM) where a series is decomposed into a number of scales where every scale informs the primary length between two consecutive points. A first order scale consists of a normal RNN, a second order consists of all points separated by two indices and so on. The Nth order RNN connects the first and last node. The outputs from all the various scales are treated as a Committee of Machines and the associated scores are used genetically for the next iteration.\r\n\r\n\r\n== Modular ==\r\n\r\nBiological studies have shown that the human brain operates as a collection of small networks.", "start_char_idx": 19939, "end_char_idx": 24886, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1803e097-fef3-4177-bf1a-ff8603f52552": {"__data__": {"id_": "1803e097-fef3-4177-bf1a-ff8603f52552", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Types of artificial neural networks.txt", "file_name": "Types of artificial neural networks.txt", "file_type": "text/plain", "file_size": 43865, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bb0d5604-c0f9-4a94-b06f-5075d5cf5210", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Types of artificial neural networks.txt", "file_name": "Types of artificial neural networks.txt", "file_type": "text/plain", "file_size": 43865, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "d277accaa570b90c3c6dfbfddc510a116fa13b6f836e4a224859f2f8403fea65", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "466a7fe3-71fe-4eaf-ada7-d94251b68b7e", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Types of artificial neural networks.txt", "file_name": "Types of artificial neural networks.txt", "file_type": "text/plain", "file_size": 43865, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "cfc4768884b555222fb66e51c3242b453138fe5b89934576d1c14b510ca7a7a8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "10aadcfd-ade9-4d1a-b2c3-231617c52a1e", "node_type": "1", "metadata": {}, "hash": "ecb52d7af169a9c9fc832d8c350cf1f14cc28e6ced1fd6524ea8f9efb2c1c198", "class_name": "RelatedNodeInfo"}}, "text": "This technique proved to be especially useful when combined with LSTM.\r\n\r\n\r\n=== Hierarchical ===\r\n\r\nHierarchical RNN connects elements in various ways to decompose hierarchical behavior into useful subprograms.\r\n\r\n\r\n=== Stochastic ===\r\n\r\nA district from conventional neural networks, stochastic artificial neural network used as an approximation to \r\nrandom functions.\r\n\r\n\r\n=== Genetic Scale ===\r\nA RNN (often a LSTM) where a series is decomposed into a number of scales where every scale informs the primary length between two consecutive points. A first order scale consists of a normal RNN, a second order consists of all points separated by two indices and so on. The Nth order RNN connects the first and last node. The outputs from all the various scales are treated as a Committee of Machines and the associated scores are used genetically for the next iteration.\r\n\r\n\r\n== Modular ==\r\n\r\nBiological studies have shown that the human brain operates as a collection of small networks. This realization gave birth to the concept of modular neural networks, in which several small networks cooperate or compete to solve problems.\r\n\r\n\r\n=== Committee of machines ===\r\n\r\nA committee of machines (CoM) is a collection of different neural networks that together \"vote\" on a given example. This generally gives a much better result than individual networks. Because neural networks suffer from local minima, starting with the same architecture and training but using randomly different initial weights often gives vastly different results. A CoM tends to stabilize the result.\r\nThe CoM is similar to the general machine learning bagging method, except that the necessary variety of machines in the committee is obtained by training from different starting weights rather than training on different randomly selected subsets of the training data.\r\n\r\n\r\n=== Associative ===\r\nThe associative neural network (ASNN) is an extension of committee of machines that combines multiple feedforward neural networks and the k-nearest neighbor technique. It uses the correlation between ensemble responses as a measure of distance amid the analyzed cases for the kNN. This corrects the Bias of the neural network ensemble. An associative neural network has a memory that can coincide with the training set. If new data become available, the network instantly improves its predictive ability and provides data approximation (self-learns) without retraining. Another important feature of ASNN is the possibility to interpret neural network results by analysis of correlations between data cases in the space of models.\r\n\r\n\r\n== Physical ==\r\n\r\nA physical neural network includes electrically adjustable resistance material to simulate artificial synapses. Examples include the ADALINE memristor-based neural network. An optical neural network is a physical implementation of an artificial neural network with optical components.\r\n\r\n\r\n== Dynamic ==\r\nDynamic neural networks address nonlinear multivariate behaviour and include (learning of) time-dependent behaviour, such as transient phenomena and delay effects. Techniques to estimate a system process from observed data fall under the general category of system identification.\r\n\r\n\r\n=== Cascading ===\r\nCascade correlation is an architecture and supervised learning algorithm. Instead of just adjusting the weights in a network of fixed topology, Cascade-Correlation begins with a minimal network, then automatically trains and adds new hidden units one by one, creating a multi-layer structure. Once a new hidden unit has been added to the network, its input-side weights are frozen. This unit then becomes a permanent feature-detector in the network, available for producing outputs or for creating other, more complex feature detectors. The Cascade-Correlation architecture has several advantages: It learns quickly, determines its own size and topology, retains the structures it has built even if the training set changes and requires no backpropagation.\r\n\r\n\r\n=== Neuro-fuzzy ===\r\n\r\nA neuro-fuzzy network is a fuzzy inference system in the body of an artificial neural network. Depending on the FIS type, several layers simulate the processes involved in a fuzzy inference-like fuzzification, inference, aggregation and defuzzification. Embedding an FIS in a general structure of an ANN has the benefit of using available ANN training methods to find the parameters of a fuzzy system.\r\n\r\n\r\n=== Compositional pattern-producing ===\r\n\r\nCompositional pattern-producing networks (CPPNs) are a variation of artificial neural networks which differ in their set of activation functions and how they are applied. While typical artificial neural networks often contain only sigmoid functions (and sometimes Gaussian functions), CPPNs can include both types of functions and many others. Furthermore, unlike typical artificial neural networks, CPPNs are applied across the entire space of possible inputs so that they can represent a complete image. Since they are compositions of functions, CPPNs in effect encode images at infinite resolution and can be sampled for a particular display at whatever resolution is optimal.\r\n\r\n\r\n== Memory networks ==\r\nMemory networks incorporate long-term memory.", "start_char_idx": 23900, "end_char_idx": 29117, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "10aadcfd-ade9-4d1a-b2c3-231617c52a1e": {"__data__": {"id_": "10aadcfd-ade9-4d1a-b2c3-231617c52a1e", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Types of artificial neural networks.txt", "file_name": "Types of artificial neural networks.txt", "file_type": "text/plain", "file_size": 43865, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bb0d5604-c0f9-4a94-b06f-5075d5cf5210", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Types of artificial neural networks.txt", "file_name": "Types of artificial neural networks.txt", "file_type": "text/plain", "file_size": 43865, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "d277accaa570b90c3c6dfbfddc510a116fa13b6f836e4a224859f2f8403fea65", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1803e097-fef3-4177-bf1a-ff8603f52552", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Types of artificial neural networks.txt", "file_name": "Types of artificial neural networks.txt", "file_type": "text/plain", "file_size": 43865, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "056a765fd3437e82d3425d6a4e128f28922cfb316a7d0ed0d92fb76cdda71b35", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "615d4c2d-63c3-463c-a518-959f4bbe309d", "node_type": "1", "metadata": {}, "hash": "f0b571db30b2a7882e320eb193bbba7188dd387e1f311ba4fe8d1ee8d510ccfc", "class_name": "RelatedNodeInfo"}}, "text": "Embedding an FIS in a general structure of an ANN has the benefit of using available ANN training methods to find the parameters of a fuzzy system.\r\n\r\n\r\n=== Compositional pattern-producing ===\r\n\r\nCompositional pattern-producing networks (CPPNs) are a variation of artificial neural networks which differ in their set of activation functions and how they are applied. While typical artificial neural networks often contain only sigmoid functions (and sometimes Gaussian functions), CPPNs can include both types of functions and many others. Furthermore, unlike typical artificial neural networks, CPPNs are applied across the entire space of possible inputs so that they can represent a complete image. Since they are compositions of functions, CPPNs in effect encode images at infinite resolution and can be sampled for a particular display at whatever resolution is optimal.\r\n\r\n\r\n== Memory networks ==\r\nMemory networks incorporate long-term memory. The long-term memory can be read and written to, with the goal of using it for prediction. These models have been applied in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base and the output is a textual response.In sparse distributed memory or hierarchical temporal memory, the patterns encoded by neural networks are used as addresses for content-addressable memory, with \"neurons\" essentially serving as address encoders and decoders. However, the early controllers of such memories were not differentiable.\r\n\r\n\r\n=== One-shot associative memory ===\r\nThis type of network can add new patterns without re-training. It is done by creating a specific memory structure, which assigns each new pattern to an orthogonal plane using adjacently connected hierarchical arrays. The network offers real-time pattern recognition and high scalability; this requires parallel processing and is thus best suited for platforms such as wireless sensor networks, grid computing, and GPGPUs.\r\n\r\n\r\n=== Hierarchical temporal memory ===\r\n\r\nHierarchical temporal memory (HTM) models some of the structural and algorithmic properties of the neocortex. HTM is a biomimetic model based on memory-prediction theory. HTM is a method for discovering and inferring the high-level causes of observed input patterns and sequences, thus building an increasingly complex model of the world.\r\nHTM combines existing ideas to mimic the neocortex with a simple design that provides many capabilities. HTM combines and extends approaches used in Bayesian networks, spatial and temporal clustering algorithms, while using a tree-shaped hierarchy of nodes that is common in neural networks.\r\n\r\n\r\n=== Holographic associative memory ===\r\n\r\nHolographic Associative Memory (HAM) is an analog, correlation-based, associative, stimulus-response system. Information is mapped onto the phase orientation of complex numbers. The memory is effective for associative memory tasks, generalization and pattern recognition with changeable attention. Dynamic search localization is central to biological memory. In visual perception, humans focus on specific objects in a pattern. Humans can change focus from object to object without learning. HAM can mimic this ability by creating explicit representations for focus. It uses a bi-modal representation of pattern and a hologram-like complex spherical weight state-space. HAMs are useful for optical realization because the underlying hyper-spherical computations can be implemented with optical computation.\r\n\r\n\r\n=== LSTM-related differentiable memory structures ===\r\nApart from long short-term memory (LSTM), other approaches also added differentiable memory to recurrent functions. For example:\r\n\r\nDifferentiable push and pop actions for alternative memory networks called neural stack machines\r\nMemory networks where the control network's external differentiable storage is in the fast weights of another network\r\nLSTM forget gates\r\nSelf-referential RNNs with special output units for addressing and rapidly manipulating the RNN's own weights in differentiable fashion (internal storage)\r\nLearning to transduce with unbounded memory\r\n\r\n\r\n=== Neural Turing machines ===\r\n\r\nNeural Turing machines (NTM) couple LSTM networks to external memory resources, with which they can interact by attentional processes. The combined system is analogous to a Turing machine but is differentiable end-to-end, allowing it to be efficiently trained by gradient descent. Preliminary results demonstrate that neural Turing machines can infer simple algorithms such as copying, sorting and associative recall from input and output examples.\r\nDifferentiable neural computers (DNC) are an NTM extension. They out-performed Neural turing machines, long short-term memory systems and memory networks on sequence-processing tasks.\r\n\r\n\r\n=== Semantic hashing ===\r\nApproaches that represent previous experiences directly and use a similar experience to form a local model are often called nearest neighbour or k-nearest neighbors methods. Deep learning is useful in semantic hashing where a deep graphical model the word-count vectors obtained from a large set of documents. Documents are mapped to memory addresses in such a way that semantically similar documents are located at nearby addresses.", "start_char_idx": 28168, "end_char_idx": 33451, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "615d4c2d-63c3-463c-a518-959f4bbe309d": {"__data__": {"id_": "615d4c2d-63c3-463c-a518-959f4bbe309d", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Types of artificial neural networks.txt", "file_name": "Types of artificial neural networks.txt", "file_type": "text/plain", "file_size": 43865, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bb0d5604-c0f9-4a94-b06f-5075d5cf5210", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Types of artificial neural networks.txt", "file_name": "Types of artificial neural networks.txt", "file_type": "text/plain", "file_size": 43865, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "d277accaa570b90c3c6dfbfddc510a116fa13b6f836e4a224859f2f8403fea65", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "10aadcfd-ade9-4d1a-b2c3-231617c52a1e", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Types of artificial neural networks.txt", "file_name": "Types of artificial neural networks.txt", "file_type": "text/plain", "file_size": 43865, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "06910648e5beb9666718503784ab219e0914dfa464fa1b7a9edf286855d8daae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f7d76aa1-75b0-49c5-a884-53a0ad72efef", "node_type": "1", "metadata": {}, "hash": "04c7d9a4a9de34d1d8830fbba5053da94ec9dcafc0851aa327db8b61c062889f", "class_name": "RelatedNodeInfo"}}, "text": "The combined system is analogous to a Turing machine but is differentiable end-to-end, allowing it to be efficiently trained by gradient descent. Preliminary results demonstrate that neural Turing machines can infer simple algorithms such as copying, sorting and associative recall from input and output examples.\r\nDifferentiable neural computers (DNC) are an NTM extension. They out-performed Neural turing machines, long short-term memory systems and memory networks on sequence-processing tasks.\r\n\r\n\r\n=== Semantic hashing ===\r\nApproaches that represent previous experiences directly and use a similar experience to form a local model are often called nearest neighbour or k-nearest neighbors methods. Deep learning is useful in semantic hashing where a deep graphical model the word-count vectors obtained from a large set of documents. Documents are mapped to memory addresses in such a way that semantically similar documents are located at nearby addresses. Documents similar to a query document can then be found by accessing all the addresses that differ by only a few bits from the address of the query document. Unlike sparse distributed memory that operates on 1000-bit addresses, semantic hashing works on 32 or 64-bit addresses found in a conventional computer architecture.\r\n\r\n\r\n=== Pointer networks ===\r\nDeep neural networks can be potentially improved by deepening and parameter reduction, while maintaining trainability. While training extremely deep (e.g., 1 million layers) neural networks might not be practical, CPU-like architectures such as pointer networks and neural random-access machines overcome this limitation by using external random-access memory and other components that typically belong to a computer architecture such as registers, ALU and pointers. Such systems operate on probability distribution vectors stored in memory cells and registers. Thus, the model is fully differentiable and trains end-to-end. The key characteristic of these models is that their depth, the size of their short-term memory, and the number of parameters can be altered independently.\r\n\r\n\r\n== Hybrids ==\r\n\r\n\r\n=== Encoder\u2013decoder networks ===\r\nEncoder\u2013decoder frameworks are based on neural networks that map highly structured input to highly structured output. The approach arose in the context of machine translation, where the input and output are written sentences in two natural languages. In that work, an LSTM RNN or CNN was used as an encoder to summarize a source sentence, and the summary was decoded using a conditional RNN language model to produce the translation. These systems share building blocks: gated RNNs and CNNs and trained attention mechanisms.\r\n\r\n\r\n== Other types ==\r\n\r\n\r\n=== Instantaneously trained ===\r\nInstantaneously trained neural networks (ITNN) were inspired by the phenomenon of short-term learning that seems to occur instantaneously. In these networks the weights of the hidden and the output layers are mapped directly from the training vector data. Ordinarily, they work on binary data, but versions for continuous data that require small additional processing exist.\r\n\r\n\r\n=== Spiking ===\r\nSpiking neural networks (SNN) explicitly consider the timing of inputs. The network input and output are usually represented as a series of spikes (delta function or more complex shapes). SNN can process information in the time domain (signals that vary over time). They are often implemented as recurrent networks. SNN are also a form of pulse computer.Spiking neural networks with axonal conduction delays exhibit polychronization, and hence could have a very large memory capacity.SNN and the temporal correlations of neural assemblies in such networks\u2014have been used to model figure/ground separation and region linking in the visual system.\r\n\r\n\r\n=== Spatial ===\r\n\r\nSpatial neural networks (SNNs) constitute a supercategory of tailored neural networks (NNs) for representing and predicting geographic phenomena. They generally improve both the statistical accuracy and reliability of the a-spatial/classic NNs whenever they handle  geo-spatial datasets, and also of the other spatial (statistical) models (e.g. spatial regression models) whenever the geo-spatial datasets' variables depict non-linear relations. Examples of SNNs are the OSFA spatial neural networks, SVANNs and GWNNs.\r\n\r\n\r\n=== Neocognitron ===\r\nThe neocognitron is a hierarchical, multilayered network that was modeled after the visual cortex. It uses multiple types of units, (originally two, called simple and complex cells), as a cascading model for use in pattern recognition tasks. Local features are extracted by S-cells whose deformation is tolerated by C-cells. Local features in the input are integrated gradually and classified at higher layers. Among the various kinds of neocognitron are systems that can detect multiple patterns in the same input by using back propagation to achieve selective attention. It has been used for pattern recognition tasks and inspired convolutional neural networks.", "start_char_idx": 32488, "end_char_idx": 37508, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f7d76aa1-75b0-49c5-a884-53a0ad72efef": {"__data__": {"id_": "f7d76aa1-75b0-49c5-a884-53a0ad72efef", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Types of artificial neural networks.txt", "file_name": "Types of artificial neural networks.txt", "file_type": "text/plain", "file_size": 43865, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bb0d5604-c0f9-4a94-b06f-5075d5cf5210", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Types of artificial neural networks.txt", "file_name": "Types of artificial neural networks.txt", "file_type": "text/plain", "file_size": 43865, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "d277accaa570b90c3c6dfbfddc510a116fa13b6f836e4a224859f2f8403fea65", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "615d4c2d-63c3-463c-a518-959f4bbe309d", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Types of artificial neural networks.txt", "file_name": "Types of artificial neural networks.txt", "file_type": "text/plain", "file_size": 43865, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "b6e4f5827c823ff59e446bd00646d7988146ea78e014be63f8af61ecca5965bc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c2bed30-dcd6-4e54-a08b-cb12aa1a1829", "node_type": "1", "metadata": {}, "hash": "82c7b1ad89d5207722bb0258ae47a11c6607ec4482a1ac83fc0c807dbf085441", "class_name": "RelatedNodeInfo"}}, "text": "spatial regression models) whenever the geo-spatial datasets' variables depict non-linear relations. Examples of SNNs are the OSFA spatial neural networks, SVANNs and GWNNs.\r\n\r\n\r\n=== Neocognitron ===\r\nThe neocognitron is a hierarchical, multilayered network that was modeled after the visual cortex. It uses multiple types of units, (originally two, called simple and complex cells), as a cascading model for use in pattern recognition tasks. Local features are extracted by S-cells whose deformation is tolerated by C-cells. Local features in the input are integrated gradually and classified at higher layers. Among the various kinds of neocognitron are systems that can detect multiple patterns in the same input by using back propagation to achieve selective attention. It has been used for pattern recognition tasks and inspired convolutional neural networks.\r\n\r\n\r\n=== Compound hierarchical-deep models ===\r\nCompound hierarchical-deep models compose deep networks with non-parametric Bayesian models. Features can be learned using deep architectures such as DBNs, deep Boltzmann machines (DBM), deep auto encoders, convolutional variants, ssRBMs, deep coding networks, DBNs with sparse feature learning, RNNs, conditional DBNs, denoising autoencoders. This provides a better representation, allowing faster learning and more accurate classification with high-dimensional data. However, these architectures are poor at learning novel classes with few examples, because all network units are involved in representing the input (a distributed representation) and must be adjusted together (high degree of freedom). Limiting the degree of freedom reduces the number of parameters to learn, facilitating learning of new classes from few examples. Hierarchical Bayesian (HB) models allow learning from few examples, for example for computer vision, statistics and cognitive science.\r\nCompound HD architectures aim to integrate characteristics of both HB and deep networks. The compound HDP-DBM architecture is a hierarchical Dirichlet process (HDP) as a hierarchical model, incorporating DBM architecture. It is a full generative model, generalized from abstract concepts flowing through the model layers, which is able to synthesize new examples in novel classes that look \"reasonably\" natural. All the levels are learned jointly by maximizing a joint log-probability score.In a DBM with three hidden layers, the probability of a visible input ''\u03bd'' is:\r\n\r\np(\u03bd,\u03c8)=1Z\u2211hexp\u2061(\u2211ijWij(1)\u03bdihj1+\u2211j\u2113Wj\u2113(2)hj1h\u21132+\u2211\u2113mW\u2113m(3)h\u21132hm3),{\\displaystyle p({\\boldsymbol {\\nu }},\\psi )={\\frac {1}{Z}}\\sum _{h}\\exp \\left(\\sum _{ij}W_{ij}^{(1)}\\nu _{i}h_{j}^{1}+\\sum _{j\\ell }W_{j\\ell }^{(2)}h_{j}^{1}h_{\\ell }^{2}+\\sum _{\\ell m}W_{\\ell m}^{(3)}h_{\\ell }^{2}h_{m}^{3}\\right),}where h={h(1),h(2),h(3)}{\\displaystyle {\\boldsymbol {h}}=\\{{\\boldsymbol {h}}^{(1)},{\\boldsymbol {h}}^{(2)},{\\boldsymbol {h}}^{(3)}\\}} is the set of hidden units, and \u03c8={W(1),W(2),W(3)}{\\displaystyle \\psi =\\{{\\boldsymbol {W}}^{(1)},{\\boldsymbol {W}}^{(2)},{\\boldsymbol {W}}^{(3)}\\}} are the model parameters, representing visible-hidden and hidden-hidden symmetric interaction terms.\r\nA learned DBM model is an undirected model that defines the joint distribution P(\u03bd,h1,h2,h3){\\displaystyle P(\\nu ,h^{1},h^{2},h^{3})}. One way to express what has been learned is the conditional model P(\u03bd,h1,h2\u2223h3){\\displaystyle P(\\nu ,h^{1},h^{2}\\mid h^{3})} and a prior term P(h3){\\displaystyle P(h^{3})}.", "start_char_idx": 36644, "end_char_idx": 40089, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c2bed30-dcd6-4e54-a08b-cb12aa1a1829": {"__data__": {"id_": "2c2bed30-dcd6-4e54-a08b-cb12aa1a1829", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Types of artificial neural networks.txt", "file_name": "Types of artificial neural networks.txt", "file_type": "text/plain", "file_size": 43865, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bb0d5604-c0f9-4a94-b06f-5075d5cf5210", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Types of artificial neural networks.txt", "file_name": "Types of artificial neural networks.txt", "file_type": "text/plain", "file_size": 43865, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "d277accaa570b90c3c6dfbfddc510a116fa13b6f836e4a224859f2f8403fea65", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f7d76aa1-75b0-49c5-a884-53a0ad72efef", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Types of artificial neural networks.txt", "file_name": "Types of artificial neural networks.txt", "file_type": "text/plain", "file_size": 43865, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "0fc6e29c7b9b1fc6e12914c2d43474abd02e7d0eaa5be2d3bdf39c2f55c2aeb2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "248e1579-94b9-4c0a-94df-803f58a29a0b", "node_type": "1", "metadata": {}, "hash": "6826354204f34b35c1a4349c6dd38d07f693df3edb2c629404b2892a3ecde4d8", "class_name": "RelatedNodeInfo"}}, "text": "A learned DBM model is an undirected model that defines the joint distribution P(\u03bd,h1,h2,h3){\\displaystyle P(\\nu ,h^{1},h^{2},h^{3})}. One way to express what has been learned is the conditional model P(\u03bd,h1,h2\u2223h3){\\displaystyle P(\\nu ,h^{1},h^{2}\\mid h^{3})} and a prior term P(h3){\\displaystyle P(h^{3})}.\r\nHere P(\u03bd,h1,h2\u2223h3){\\displaystyle P(\\nu ,h^{1},h^{2}\\mid h^{3})} represents a conditional DBM model, which can be viewed as a two-layer DBM but with bias terms given by the states of h3{\\displaystyle h^{3}}:\r\n\r\nP(\u03bd,h1,h2\u2223h3)=1Z(\u03c8,h3)exp\u2061(\u2211ijWij(1)\u03bdihj1+\u2211j\u2113Wj\u2113(2)hj1h\u21132+\u2211\u2113mW\u2113m(3)h\u21132hm3).{\\displaystyle P(\\nu ,h^{1},h^{2}\\mid h^{3})={\\frac {1}{Z(\\psi ,h^{3})}}\\exp \\left(\\sum _{ij}W_{ij}^{(1)}\\nu _{i}h_{j}^{1}+\\sum _{j\\ell }W_{j\\ell }^{(2)}h_{j}^{1}h_{\\ell }^{2}+\\sum _{\\ell m}W_{\\ell m}^{(3)}h_{\\ell }^{2}h_{m}^{3}\\right).}\r\n\r\n\r\n=== Deep predictive coding networks ===\r\nA deep predictive coding network (DPCN) is a predictive coding scheme that uses top-down information to empirically adjust the priors needed for a bottom-up inference procedure by means of a deep, locally connected, generative model. This works by extracting sparse features from time-varying observations using a linear dynamical model. Then, a pooling strategy is used to learn invariant feature representations. These units compose to form a deep architecture and are trained by greedy layer-wise unsupervised learning. The layers constitute a kind of Markov chain such that the states at any layer depend only on the preceding and succeeding layers.\r\nDPCNs predict the representation of the layer, by using a top-down approach using the information in upper layer and temporal dependencies from previous states.DPCNs can be extended to form a convolutional network.\r\n\r\n\r\n=== Multilayer kernel machine ===\r\nMultilayer kernel machines (MKM) are a way of learning highly nonlinear functions by iterative application of weakly nonlinear kernels. They use kernel principal component analysis (KPCA), as a method for the unsupervised greedy layer-wise pre-training step of deep learning.Layer \u2113+1{\\displaystyle \\ell +1} learns the representation of the previous layer \u2113{\\displaystyle \\ell }, extracting the nl{\\displaystyle n_{l}} principal component (PC) of the projection layer l{\\displaystyle l} output in the feature domain induced by the kernel. To reduce the dimensionaliity of the updated representation in each layer, a supervised strategy selects the best informative features among features extracted by KPCA. The process is:\r\n\r\nrank the n\u2113{\\displaystyle n_{\\ell }} features according to their mutual information with the class labels;\r\nfor different values of K and m\u2113\u2208{1,\u2026,n\u2113}{\\displaystyle m_{\\ell }\\in \\{1,\\ldots ,n_{\\ell }\\}}, compute the classification error rate of a K-nearest neighbor (K-NN) classifier using only the ml{\\displaystyle m_{l}} most informative features on a validation set;\r\nthe value of m\u2113{\\displaystyle m_{\\ell }} with which the classifier has reached the lowest error rate determines the number of features to retain.Some drawbacks accompany the KPCA method for MKMs.\r\nA more straightforward way to use kernel machines for deep learning was developed for spoken language understanding. The main idea is to use a kernel machine to approximate a shallow neural net with an infinite number of hidden units, then use a deep stacking network to splice the output of the kernel machine and the raw input in building the next, higher level of the kernel machine. The number of levels in the deep convex network is a hyper-parameter of the overall system, to be determined by cross validation.", "start_char_idx": 39782, "end_char_idx": 43380, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "248e1579-94b9-4c0a-94df-803f58a29a0b": {"__data__": {"id_": "248e1579-94b9-4c0a-94df-803f58a29a0b", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Types of artificial neural networks.txt", "file_name": "Types of artificial neural networks.txt", "file_type": "text/plain", "file_size": 43865, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bb0d5604-c0f9-4a94-b06f-5075d5cf5210", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Types of artificial neural networks.txt", "file_name": "Types of artificial neural networks.txt", "file_type": "text/plain", "file_size": 43865, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "d277accaa570b90c3c6dfbfddc510a116fa13b6f836e4a224859f2f8403fea65", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2c2bed30-dcd6-4e54-a08b-cb12aa1a1829", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Types of artificial neural networks.txt", "file_name": "Types of artificial neural networks.txt", "file_type": "text/plain", "file_size": 43865, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "ebb043920732222409481efde7aecb863deab849255f41e59feae49b21a40c55", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "075a5ef4-0673-48e0-b49e-966164696450", "node_type": "1", "metadata": {}, "hash": "a30666d14317c975f0f02e84222d542119e32dc0c2ef1d7b0f8ae6f407b51c30", "class_name": "RelatedNodeInfo"}}, "text": "A more straightforward way to use kernel machines for deep learning was developed for spoken language understanding. The main idea is to use a kernel machine to approximate a shallow neural net with an infinite number of hidden units, then use a deep stacking network to splice the output of the kernel machine and the raw input in building the next, higher level of the kernel machine. The number of levels in the deep convex network is a hyper-parameter of the overall system, to be determined by cross validation.\r\n\r\n\r\n== See also ==\r\n\r\n\r\n== References ==\r\n\r\n\r\n== Bibliography ==\r\nFukushima, Kunihiko (1987). \"A hierarchical neural network model for selective attention\". In Eckmiller, R.; Von der Malsburg, C. (eds.). Neural computers. Springer-Verlag. pp. 81\u201390.\r\nFukushima, Kunihiko (2007). \"Neocognitron\". Scholarpedia. 2 (1): 1717. Bibcode:2007SchpJ...2.1717F. doi:10.4249/scholarpedia.1717.", "start_char_idx": 42864, "end_char_idx": 43763, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "075a5ef4-0673-48e0-b49e-966164696450": {"__data__": {"id_": "075a5ef4-0673-48e0-b49e-966164696450", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Unified Modeling Language.txt", "file_name": "Unified Modeling Language.txt", "file_type": "text/plain", "file_size": 13214, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "37f790c9-29da-4d45-bab5-71dc6795e41e", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Unified Modeling Language.txt", "file_name": "Unified Modeling Language.txt", "file_type": "text/plain", "file_size": 13214, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "1bbb4ba053b19b112181ae724233ae3673e97b226b2896a3461e039b1f1bd9ba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "248e1579-94b9-4c0a-94df-803f58a29a0b", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Types of artificial neural networks.txt", "file_name": "Types of artificial neural networks.txt", "file_type": "text/plain", "file_size": 43865, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "ecc706f3a9c16a55f52bcd5f58583bebc057cf631e947502fb7836ea8699b7e6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "743b900b-b021-45db-8b95-a57987c864b8", "node_type": "1", "metadata": {}, "hash": "d2e0a010d64194eec39f1b0d1eb1eb2b1ddc14d900bae7ace57325f963ecaafd", "class_name": "RelatedNodeInfo"}}, "text": "The unified modeling language (UML) is a general-purpose visual modeling language that is intended to provide a standard way to visualize the design of a system.UML provides a standard notation for many types of diagrams which can be roughly divided into three main groups: behavior diagrams, interaction diagrams, and structure diagrams. \r\nThe creation of UML was originally motivated by the desire to standardize the disparate notational systems and approaches to software design. It was developed at Rational Software in 1994\u20131995, with further development led by them through 1996.In 1997, UML was adopted as a standard by the Object Management Group (OMG), and has been managed by this organization ever since. In 2005, UML was also published by the International Organization for Standardization (ISO) and the International Electrotechnical Commission (IEC) as the ISO/IEC 19501 standard. Since then the standard has been periodically revised to cover the latest revision of UML.In software engineering, most practitioners do not use UML, but instead produce informal hand drawn diagrams; these diagrams, however, often include elements from UML.:\u200a536\u200a\r\n\r\n\r\n== History ==\r\n\r\n\r\n=== Before UML 1.0 ===\r\nUML has been evolved since the second half of the 1990s and has its roots in the object-oriented programming methods developed in the late 1980s and early 1990s. The timeline (see image) shows the highlights of the history of object-oriented modeling methods and notation.\r\nIt is originally based on the notations of the Booch method, the object-modeling technique (OMT) and object-oriented software engineering (OOSE), which it has integrated into a single language.Rational Software Corporation hired James Rumbaugh from General Electric in 1994 and after that the company became the source for two of the most popular object-oriented modeling approaches of the day: Rumbaugh's object-modeling technique (OMT) and Grady Booch's method. They were soon assisted in their efforts by Ivar Jacobson, the creator of the object-oriented software engineering (OOSE) method, who joined them at Rational in 1995.\r\n\r\n\r\n=== UML 1.x ===\r\nUnder the technical leadership of those three (Rumbaugh, Jacobson and Booch), a consortium called the UML Partners was organized in 1996 to complete the Unified Modeling Language (UML) specification, and propose it to the Object Management Group (OMG) for standardization. The partnership also contained additional interested parties (for example HP, DEC, IBM and Microsoft). The UML Partners' UML 1.0 draft was proposed to the OMG in January 1997 by the consortium. During the same month the UML Partners formed a group, designed to define the exact meaning of language constructs, chaired by Cris Kobryn and administered by Ed Eykholt, to finalize the specification and integrate it with other standardization efforts. The result of this work, UML 1.1, was submitted to the OMG in August 1997 and adopted by the OMG in November 1997.After the first release a task force was formed to improve the language, which released several minor revisions, 1.3, 1.4, and 1.5.The standards it produced (as well as the original standard) have been noted as being ambiguous and inconsistent.\r\n\r\n\r\n==== Cardinality notation ====\r\nAs with database Chen, Bachman, and ISO ER diagrams, class models are specified to use \"look-across\" cardinalities, even though several authors (Merise, Elmasri & Navathe amongst others) prefer same-side or \"look-here\" for roles and both minimum and maximum cardinalities. Recent researchers (Feinerer, Dullea et al.) have shown that the \"look-across\" technique used by UML and ER diagrams is less effective and less coherent when applied to n-ary relationships of order strictly greater than 2.\r\nFeinerer says: \"Problems arise if we operate under the look-across semantics as used for UML associations. Hartmann investigates this situation and shows how and why different transformations fail.\", and: \"As we will see on the next few pages, the look-across interpretation introduces several difficulties which prevent the extension of simple mechanisms from binary to n-ary associations.\"", "start_char_idx": 0, "end_char_idx": 4139, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "743b900b-b021-45db-8b95-a57987c864b8": {"__data__": {"id_": "743b900b-b021-45db-8b95-a57987c864b8", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Unified Modeling Language.txt", "file_name": "Unified Modeling Language.txt", "file_type": "text/plain", "file_size": 13214, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "37f790c9-29da-4d45-bab5-71dc6795e41e", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Unified Modeling Language.txt", "file_name": "Unified Modeling Language.txt", "file_type": "text/plain", "file_size": 13214, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "1bbb4ba053b19b112181ae724233ae3673e97b226b2896a3461e039b1f1bd9ba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "075a5ef4-0673-48e0-b49e-966164696450", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Unified Modeling Language.txt", "file_name": "Unified Modeling Language.txt", "file_type": "text/plain", "file_size": 13214, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "ce61a1f089e8ca1b3f554b13854eccc081438ad9aad2005a7a5071e7622488e3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b15aee2-e3ac-4b4f-9e0f-36ecabdce1bb", "node_type": "1", "metadata": {}, "hash": "4184d6f86b87c46dfca808600101474b0f4c552501fb89964c2f2f8ad33138b9", "class_name": "RelatedNodeInfo"}}, "text": "Recent researchers (Feinerer, Dullea et al.) have shown that the \"look-across\" technique used by UML and ER diagrams is less effective and less coherent when applied to n-ary relationships of order strictly greater than 2.\r\nFeinerer says: \"Problems arise if we operate under the look-across semantics as used for UML associations. Hartmann investigates this situation and shows how and why different transformations fail.\", and: \"As we will see on the next few pages, the look-across interpretation introduces several difficulties which prevent the extension of simple mechanisms from binary to n-ary associations.\"\r\n\r\n\r\n=== UML 2 ===\r\nUML 2.0 major revision replaced version 1.5 in 2005, which was developed with an enlarged consortium to improve the language further to reflect new experience on usage of its features.Although UML 2.1 was never released as a formal specification, versions 2.1.1 and 2.1.2 appeared in 2007, followed by UML 2.2 in February 2009. UML 2.3 was formally released in May 2010. UML 2.4.1 was formally released in August 2011. UML 2.5 was released in October 2012 as an \"In progress\" version and was officially released in June 2015. Formal version 2.5.1 was adopted in December 2017.There are four parts to the UML 2.x specification:\r\n\r\nThe Superstructure that defines the notation and semantics for diagrams and their model elements\r\nThe Infrastructure that defines the core metamodel on which the Superstructure is based\r\nThe Object Constraint Language (OCL) for defining rules for model elements\r\nThe UML Diagram Interchange that defines how UML 2 diagram layouts are exchangedUntil UML 2.4.1, the latest versions of these standards were:\r\nUML Superstructure version 2.4.1\r\nUML Infrastructure version 2.4.1\r\nOCL version 2.3.1\r\nUML Diagram Interchange version 1.0.Since version 2.5, the UML Specification has been simplified (without Superstructure and Infrastructure), and the latest versions of these standards are now:\r\nUML Specification 2.5.1\r\nOCL version 2.4It continues to be updated and improved by the revision task force, who resolve any issues with the language.\r\n\r\n\r\n== Design ==\r\nUML offers a way to visualize a system's architectural blueprints in a diagram, including elements such as:\r\nany activities (jobs);\r\nindividual components of the system;\r\nand how they can interact with other software components;\r\nhow the system will run;\r\nhow entities interact with others (components and interfaces);\r\nexternal user interface.Although originally intended for object-oriented design documentation, UML has been extended to a larger set of design documentation (as listed above), and been found useful in many contexts.\r\n\r\n\r\n=== Software development methods ===\r\nUML is not a development method by itself; however, it was designed to be compatible with the leading object-oriented software development methods of its time, for example OMT, Booch method, Objectory and especially RUP that it was originally intended to be used with when work began at Rational Software.\r\n\r\n\r\n=== Modeling ===\r\nIt is important to distinguish between the UML model and the set of diagrams of a system. A diagram is a partial graphic representation of a system's model. The set of diagrams need not completely cover the model and deleting a diagram does not change the model. The model may also contain documentation that drives the model elements and diagrams (such as written use cases).\r\nUML diagrams represent two different views of a system model:\r\nStatic (or structural) view: emphasizes the static structure of the system using objects, attributes, operations and relationships. It includes class diagrams and composite structure diagrams.\r\nDynamic (or behavioral) view: emphasizes the dynamic behavior of the system by showing collaborations among objects and changes to the internal states of objects. This view includes sequence diagrams, activity diagrams and state machine diagrams.UML models can be exchanged among UML tools by using the XML Metadata Interchange (XMI) format.\r\nIn UML, one of the key tools for behavior modeling is the use-case model, caused by OOSE. Use cases are a way of specifying required usages of a system. Typically, they are used to capture the requirements of a system, that is, what a system is supposed to do.\r\n\r\n\r\n== Diagrams ==\r\nUML 2 has many types of diagrams, which are divided into two categories.", "start_char_idx": 3524, "end_char_idx": 7887, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b15aee2-e3ac-4b4f-9e0f-36ecabdce1bb": {"__data__": {"id_": "4b15aee2-e3ac-4b4f-9e0f-36ecabdce1bb", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Unified Modeling Language.txt", "file_name": "Unified Modeling Language.txt", "file_type": "text/plain", "file_size": 13214, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "37f790c9-29da-4d45-bab5-71dc6795e41e", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Unified Modeling Language.txt", "file_name": "Unified Modeling Language.txt", "file_type": "text/plain", "file_size": 13214, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "1bbb4ba053b19b112181ae724233ae3673e97b226b2896a3461e039b1f1bd9ba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "743b900b-b021-45db-8b95-a57987c864b8", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Unified Modeling Language.txt", "file_name": "Unified Modeling Language.txt", "file_type": "text/plain", "file_size": 13214, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "2014cdd903b5e294409a96e5549bc35942e0d188e3f1dab51d65c285ea85ed61", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d8fecf9a-aa70-4943-941f-cf7567fae1bd", "node_type": "1", "metadata": {}, "hash": "de9c58561d35bf559b5e0ff863e61365a2c241874f870a1ce8d22dff6a53c782", "class_name": "RelatedNodeInfo"}}, "text": "UML diagrams represent two different views of a system model:\r\nStatic (or structural) view: emphasizes the static structure of the system using objects, attributes, operations and relationships. It includes class diagrams and composite structure diagrams.\r\nDynamic (or behavioral) view: emphasizes the dynamic behavior of the system by showing collaborations among objects and changes to the internal states of objects. This view includes sequence diagrams, activity diagrams and state machine diagrams.UML models can be exchanged among UML tools by using the XML Metadata Interchange (XMI) format.\r\nIn UML, one of the key tools for behavior modeling is the use-case model, caused by OOSE. Use cases are a way of specifying required usages of a system. Typically, they are used to capture the requirements of a system, that is, what a system is supposed to do.\r\n\r\n\r\n== Diagrams ==\r\nUML 2 has many types of diagrams, which are divided into two categories. Some types represent structural information, and the rest represent general types of behavior, including a few that represent different aspects of interactions. These diagrams can be categorized hierarchically as shown in the following class diagram:\r\nThese diagrams may all contain comments or notes explaining usage, constraint, or intent.\r\n\r\n\r\n=== Structure diagrams ===\r\nStructure diagrams represent the static aspects of the system. It emphasizes the things that must be present in the system being modeled. Since structure diagrams represent the structure, they are used extensively in documenting the software architecture of software systems. For example, the component diagram describes how a software system is split up into components and shows the dependencies among these components.\r\n\r\n\t\t\t\r\n\t\t\t\r\n\t\t\r\n\t\t\r\n\t\t\t\r\n\t\t\t\r\n\t\t\r\n\r\n\r\n=== Behavior diagrams ===\r\nBehavior diagrams represent the dynamic aspect of the system. It emphasizes what must happen in the system being modeled. Since behavior diagrams illustrate the behavior of a system, they are used extensively to describe the functionality of software systems. As an example, the activity diagram describes the business and operational step-by-step activities of the components in a system.\r\n\r\n\t\t\t\r\n\t\t\t\r\n\t\t\r\n\t\t\r\n\t\t\t\r\n\t\t\t\r\n\t\t\r\n\r\n\r\n==== Interaction diagrams ====\r\nInteraction diagrams, a subset of behavior diagrams, emphasize the flow of control and data among the things in the system being modeled. For example, the sequence diagram shows how objects communicate with each other regarding a sequence of messages.\r\n\r\n\t\t\t\r\n\t\t\t\r\n\t\t\r\n\t\t\r\n\t\t\t\r\n\t\t\t\r\n\t\t\r\n\r\n\r\n== Metamodeling ==\r\n\r\nThe Object Management Group (OMG) has developed a metamodeling architecture to define the UML, called the Meta-Object Facility. MOF is designed as a four-layered architecture, as shown in the image at right. It provides a meta-meta model at the top, called the M3 layer. This M3-model is the language used by Meta-Object Facility to build metamodels, called M2-models.\r\nThe most prominent example of a Layer 2 Meta-Object Facility model is the UML metamodel, which describes the UML itself. These M2-models describe elements of the M1-layer, and thus M1-models. These would be, for example, models written in UML. The last layer is the M0-layer or data layer. It is used to describe runtime instances of the system.The meta-model can be extended using a mechanism called stereotyping. This has been criticized as being insufficient/untenable by Brian Henderson-Sellers and Cesar Gonzalez-Perez in \"Uses and Abuses of the Stereotype Mechanism in UML 1.x and 2.0\".\r\n\r\n\r\n== Adoption ==\r\nBack in 2013 UML has been marketed by OMG for many contexts, but aimed primarily at software development with limited success.It has been treated, at times, as a design silver bullet, which leads to problems. UML misuse includes overuse (designing every part of the system with it, which is unnecessary) and assuming that novices can design with it.It is considered a large language, with many constructs. Some people (including Jacobson) feel that UML's size hinders learning (and therefore using) it.MS Visual Studio dropped support for UML in 2016 due to lack of usage.According to Google Trends UML has been on steady decline since 2004.\r\n\r\n\r\n== See also ==\r\nApplications of UML\r\nBusiness Process Model and Notation (BPMN)\r\nC4 model\r\nDepartment of Defense Architecture Framework\r\nDOT (graph description language)\r\nList of Unified Modeling Language tools\r\nMODAF\r\nModel-based testing\r\nModel-driven engineering\r\nObject-oriented role analysis and modeling\r\nProcess Specification Language\r\nSystems Modeling Language (SysML)\r\n\r\n\r\n== References ==\r\n\r\n\r\n== Further reading ==\r\nAmbler, Scott William (2004). The Object Primer: Agile Model Driven Development with UML 2. Cambridge University Press.", "start_char_idx": 6933, "end_char_idx": 11701, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d8fecf9a-aa70-4943-941f-cf7567fae1bd": {"__data__": {"id_": "d8fecf9a-aa70-4943-941f-cf7567fae1bd", "embedding": null, "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Unified Modeling Language.txt", "file_name": "Unified Modeling Language.txt", "file_type": "text/plain", "file_size": 13214, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "37f790c9-29da-4d45-bab5-71dc6795e41e", "node_type": "4", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Unified Modeling Language.txt", "file_name": "Unified Modeling Language.txt", "file_type": "text/plain", "file_size": 13214, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "1bbb4ba053b19b112181ae724233ae3673e97b226b2896a3461e039b1f1bd9ba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b15aee2-e3ac-4b4f-9e0f-36ecabdce1bb", "node_type": "1", "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Unified Modeling Language.txt", "file_name": "Unified Modeling Language.txt", "file_type": "text/plain", "file_size": 13214, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}, "hash": "004b6429444449653ae4c08286694d84cdb93c787babdd44b0657bc922c5676b", "class_name": "RelatedNodeInfo"}}, "text": "Some people (including Jacobson) feel that UML's size hinders learning (and therefore using) it.MS Visual Studio dropped support for UML in 2016 due to lack of usage.According to Google Trends UML has been on steady decline since 2004.\r\n\r\n\r\n== See also ==\r\nApplications of UML\r\nBusiness Process Model and Notation (BPMN)\r\nC4 model\r\nDepartment of Defense Architecture Framework\r\nDOT (graph description language)\r\nList of Unified Modeling Language tools\r\nMODAF\r\nModel-based testing\r\nModel-driven engineering\r\nObject-oriented role analysis and modeling\r\nProcess Specification Language\r\nSystems Modeling Language (SysML)\r\n\r\n\r\n== References ==\r\n\r\n\r\n== Further reading ==\r\nAmbler, Scott William (2004). The Object Primer: Agile Model Driven Development with UML 2. Cambridge University Press. ISBN 0-521-54018-6. Archived from the original on 31 January 2010. Retrieved 29 April 2006.\r\nChonoles, Michael Jesse; James A. Schardt (2003). UML 2 for Dummies. Wiley Publishing. ISBN 0-7645-2614-6.\r\nFowler, Martin (2004). UML Distilled: A Brief Guide to the Standard Object Modeling Language (3rd ed.). Addison-Wesley. ISBN 0-321-19368-7.\r\nJacobson, Ivar; Grady Booch; James Rumbaugh (1998). The Unified Software Development Process. Addison Wesley Longman. ISBN 0-201-57169-2.\r\nMartin, Robert Cecil (2003). UML for Java Programmers. Prentice Hall. ISBN 0-13-142848-9.\r\nNoran, Ovidiu S. \"Business Modelling: UML vs. IDEF\" (PDF). Retrieved 14 November 2022.\r\nHorst Kargl. \"Interactive UML Metamodel with additional Examples\".\r\nPenker, Magnus; Hans-Erik Eriksson (2000). Business Modeling with UML. John Wiley & Sons. ISBN 0-471-29551-5.\r\nDouglass, Bruce Powel. \"Bruce Douglass: Real-Time Agile Systems and Software Development\" (web). Retrieved 1 January 2019.\r\nDouglass, Bruce (2014). Real-Time UML Workshop 2nd Edition. Newnes. ISBN 978-0-471-29551-8.\r\nDouglass, Bruce (2004). Real-Time UML 3rd Edition. Newnes. ISBN 978-0321160768.\r\nDouglass, Bruce (2002). Real-Time Design Patterns. Addison-Wesley Professional. ISBN 978-0201699562.\r\nDouglass, Bruce (2009). Real-Time Agility. Addison-Wesley Professional. ISBN 978-0321545497.\r\nDouglass, Bruce (2010). Design Patterns for Embedded Systems in C. Newnes. ISBN 978-1856177078.\r\n\r\n\r\n== External links ==\r\n\r\nOfficial website \r\nCurrent Version Specification", "start_char_idx": 10915, "end_char_idx": 13208, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"5b373bb3-a9ab-4c60-a7e3-7173dc51ff40": {"node_ids": ["f6603d07-509e-4575-89c4-bccf2206fc47", "63dab1e2-42a3-4eb3-bb6c-57f3319ccb11", "ab4ab5a1-8ef3-4081-8845-a0329d1f315e", "c89e3b13-2c6d-42c4-8843-132a050766a5"], "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\AI accelerator.txt", "file_name": "AI accelerator.txt", "file_type": "text/plain", "file_size": 15186, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}}, "2bb55be0-1379-4a71-99be-3263be74fa83": {"node_ids": ["69e369bd-aace-44c5-8f38-46c7513b1343", "018fb6c0-3406-4546-bac9-d817682f16f3", "cd292843-4843-4185-925f-64d9380d9246"], "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\BERT (language model).txt", "file_name": "BERT (language model).txt", "file_type": "text/plain", "file_size": 9285, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}}, "9e6898af-c176-436b-8a3e-151be88edbcf": {"node_ids": ["bb4bc2b4-7bdb-4111-b4d6-78c30911d723"], "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\BLOOM (language model).txt", "file_name": "BLOOM (language model).txt", "file_type": "text/plain", "file_size": 1267, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}}, "d5eaea50-ecd2-4c57-85d7-276f893e80e9": {"node_ids": ["a21b3823-2625-44e8-8214-131b22a77683"], "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Chinchilla (language model).txt", "file_name": "Chinchilla (language model).txt", "file_type": "text/plain", "file_size": 2231, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}}, "ddfbb310-f268-43fc-b55c-2576fb322b7c": {"node_ids": ["9e6d6aaf-f4e4-43b6-93de-c206c324a10d"], "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Claude (language model).txt", "file_name": "Claude (language model).txt", "file_type": "text/plain", "file_size": 4305, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}}, "65fe6622-9e19-4c31-912b-a1676a217719": {"node_ids": ["bbfb1614-1d95-474b-8672-b6909c2f58f6", "22f2b7f2-67e1-4a70-8d7a-f4a3932826d9", "a573ad77-b1e0-48e9-94fc-d40d5abdd3e7", "4b340d61-1a2a-48da-aab9-f429ed28aa5a", "0f387ccc-464e-46c4-9703-dadf47ad89b6", "724b2112-4bba-4b29-9023-e6ef6755e5ce", "9b961931-c0b6-4ff5-99db-198244be0ee5", "ab5cbcc7-f164-476c-80ed-9e0f2fcc8c12", "ce9ea363-3406-43bf-94ee-ae5f55ff43b9", "6b6f2954-0cb0-4cd8-a915-92b11769eb07", "f4112708-7739-4316-9865-8f212a7cc78b", "b6835997-6770-420c-9493-970c71189cd4", "75104176-b291-48ea-a2b8-8140b770c225", "8967e8c2-4793-4e11-821c-7bb603de0868", "c6a646ee-0c0e-40ca-b662-ec9fc6602de4", "ef8b8a5d-4ad7-423f-9a30-b254427348b5"], "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Convolutional neural network.txt", "file_name": "Convolutional neural network.txt", "file_type": "text/plain", "file_size": 59026, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}}, "ffb0bcfe-5aba-4c18-9723-b82a7d2219a0": {"node_ids": ["a7c7a032-a671-47d6-8cf3-442865fedbfe", "01017277-fbf6-4e56-8c0d-cc072f2d4159", "b65258e9-3101-4364-b643-8c44a4cee41b", "d7db71c4-6a6f-411b-aabd-b91fb6e7af8a", "23e48601-3899-4045-933d-605792de6c3a", "a38fabc6-41d4-4f3a-a7d5-e8523f309e87", "17ebeed7-747f-4cb5-acd6-b58b73906628", "fe3234a9-3da2-41f7-8957-78c1899ca95f", "6339df8d-aef9-4b85-8948-b89a05bf2611", "fc4ab0ea-dfb5-40c0-b7aa-02100c580b72", "5fa7d90f-7654-41d9-980c-4725812841e7", "aea60cca-465e-42c2-9fce-ffeaec09a8a8", "b5d6518e-adad-4314-8ab0-908733ab7041", "d640173d-29fb-4b73-bd12-b8a306a1fc88", "e9df0acf-8aa1-4692-9705-936f8f312670"], "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Deep learning.txt", "file_name": "Deep learning.txt", "file_type": "text/plain", "file_size": 57786, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}}, "bde9526a-8edb-41c5-b464-118aec879b48": {"node_ids": ["e3461faa-e9b6-4d6a-8e7b-b9b6b70ac760", "1b28a924-ee18-45d9-88d6-02c825d938b6"], "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Dilution (neural networks).txt", "file_name": "Dilution (neural networks).txt", "file_type": "text/plain", "file_size": 5621, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}}, "bceab45b-1acb-44b4-8433-ff78ba533ae9": {"node_ids": ["da1a514a-3dfd-485e-9c48-1c2a3b3ba794", "e0e51bb9-edad-41f5-adf9-bf87c2553d41", "f7efb63a-737a-4718-bdcf-239637fd47ca"], "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Feedforward neural network.txt", "file_name": "Feedforward neural network.txt", "file_type": "text/plain", "file_size": 9793, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}}, "142e908e-5e0d-4a4f-b83b-48b2efd56aab": {"node_ids": ["2317dda9-0b15-4b00-b3e1-2408c656da63", "01835aca-d678-457b-beea-cb21d4b03af1", "91a97cc5-6178-48ba-b244-f3cbd0af1b40"], "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Gemini (language model).txt", "file_name": "Gemini (language model).txt", "file_type": "text/plain", "file_size": 10191, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}}, "1a1d4c52-08d8-4f72-a6e8-e70208ed030e": {"node_ids": ["8c57d7c4-866f-4231-90a7-588b76aef41c", "0dd2c726-cf32-42d1-b131-dd77071d95f2", "6fe21eeb-0445-446f-84c2-5f96339864fe", "6d5aa1f4-3297-4551-a9bc-bf8027bc90aa", "c66c9279-119e-4954-8e9e-a7393aa10742", "b8de6e8a-2d14-47b7-88c1-ef2b3b33fcb7", "db7d8046-1f87-46b3-8af9-a52581308b68", "c5d9510f-9b4e-4aac-a106-be3d6cfc6158", "dcc1f2fc-613c-4f2a-988a-835f97b475c2", "38e3d038-36b1-48ed-b79b-21271ac6517e", "da474919-403b-488e-8482-f2b19bba8b33", "7655f3a0-21e5-466e-a93a-fb5ed4791b0c", "704b76fb-1c25-4c2b-9a52-9b54acddd768", "03bbdcab-f1ff-43e5-9ae6-70d056230b19", "cf76431f-351a-4322-b7bb-c47aeee0a607"], "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative adversarial network.txt", "file_name": "Generative adversarial network.txt", "file_type": "text/plain", "file_size": 48149, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}}, "2a159bcb-23ad-4890-bd3e-8c468e4d113d": {"node_ids": ["33e7eb09-0bd0-4c61-b41c-5b97383623fd", "7fa895fa-fe1d-426b-b8b6-2816ad3e0fbd", "9341a911-be4f-49e3-abfc-6c0ea4b86498", "5a310d23-81e9-4909-a0be-c9ef7a85d856"], "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Generative pre-trained transformer.txt", "file_name": "Generative pre-trained transformer.txt", "file_type": "text/plain", "file_size": 14085, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}}, "45a7af36-d76d-4c97-bd71-02a965a1a62c": {"node_ids": ["7f67bb38-bd16-4aa1-98c1-8ad6b1a71e93", "36b241ff-a8d1-4b1c-b5e3-9ed98e29f2e3", "2f4c12dd-969f-426e-aab7-bc08d64d9783", "e6c6979e-ff99-4310-8fb4-1cf2250ee34e", "c0b7bd4d-34d3-492d-8a74-5d0df135efc8", "a8232541-5d5d-42e0-bc08-316615718517"], "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Graph neural network.txt", "file_name": "Graph neural network.txt", "file_type": "text/plain", "file_size": 16502, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}}, "08db6b24-b93e-40bb-8bc4-355238604f5a": {"node_ids": ["241c4b6f-3158-40f5-bca3-709d04e41d88", "a33ef89f-f9f5-4f21-9691-ebed7e5d9084", "e40b6fbf-d71f-4dff-a596-ca62afb71b14", "6bb52853-a030-41ce-96a1-9dcf8439288a", "d12ee3f5-f57e-4046-ba66-5e25f2f4d9fd", "104b72a0-ad9a-4f2d-9e0e-3566926aacac"], "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\History of artificial neural networks.txt", "file_name": "History of artificial neural networks.txt", "file_type": "text/plain", "file_size": 19960, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}}, "b6d044df-fa62-42ce-9a08-dd7e535d47c4": {"node_ids": ["a1303869-5250-4660-8a0a-49e048b24471", "a9f464e4-cc53-43d3-a403-cdee2b520204", "851ec5d2-4952-4ecf-afee-ef5cdd8b545f", "bcfbcfda-25ca-4286-96af-9a6162b32ce6", "bd2c7395-c9fe-4151-a0d3-4a4cec6c808d", "5a14c2a1-c786-460a-9629-e197a849150a", "c5e8606d-dd66-42c7-808a-3a794a97ca8d", "d636e607-8626-4a08-9b76-534b6aeb32a6", "4d20d041-0bde-4d51-9c99-9037511755eb", "6cec6a3d-cb13-42f3-b385-a7d463bc9e39", "9df1a7c6-4710-4278-8eeb-10e170eb926b"], "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Language model.txt", "file_name": "Language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}}, "ebe409b7-516e-4dac-aa8f-60c282a209da": {"node_ids": ["66b3d6cf-184f-4521-8fee-73874b0cf634", "81833af5-dcc8-4f6d-9988-898c5fe855c3", "3ddd7211-4b50-4aef-9499-faf9dce24e85", "5584ae86-cdfe-4802-8a58-00239a12b1f7", "38772f86-2ba7-40df-89bc-16dce75ac534", "394e7e43-4f21-4085-a0a9-edaa587483ba", "1723ce2c-f8ca-4a28-9b01-3eec082bc36b", "3e94c9b0-6114-4cbb-b8c2-32e2633554eb", "28691699-6a7d-4170-b0da-896bfa259b87", "ee0a8472-4293-4470-b8a8-3c18b284f187", "f2ad616c-04ed-49ce-870e-ee6bb412d1df"], "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Large language model.txt", "file_name": "Large language model.txt", "file_type": "text/plain", "file_size": 38605, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}}, "7a67ba02-86ba-4e06-8582-0b6f20c739d5": {"node_ids": ["db3db074-0ef5-46c2-8333-0a520128fbf0", "a3b114e1-a5b5-481a-9091-81ce30b3a493", "ba4cf269-9d2b-4372-b49c-f6a1bf18f3c5", "fcb67fcf-cc23-49e5-bd56-fb23f3638f96", "e8a043dd-2885-4e30-80a7-9e7f6fe67635"], "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Modeling language.txt", "file_name": "Modeling language.txt", "file_type": "text/plain", "file_size": 18458, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}}, "95d73cd8-87d1-4e32-b31f-76e61e9440df": {"node_ids": ["b2c46eb6-a135-4273-9157-d669aefaa747", "ac7184a3-b11d-40d9-a40c-4c9c2069d5b3"], "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (biology).txt", "file_name": "Neural network (biology).txt", "file_type": "text/plain", "file_size": 6708, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}}, "3ac1d604-830e-41b8-a9a8-de008f222fb1": {"node_ids": ["38dadf53-fb63-4ecf-b70e-fa6cd6abe080", "204bf980-74c0-4079-8bbd-66a596330306", "56bca366-e230-4313-824e-6b5cb9f81af9", "16a72916-0807-4278-bfa9-3e59b9366f18", "a891be65-ecb9-48af-8146-47a6138a2463", "2dafca1b-2e26-44ff-b71c-3c0e026a6474", "c9c0b731-45a8-492f-a739-af16b9ec9cd2", "0fb55cac-6853-4bbf-8997-306d32a1948b", "15674a21-cfe4-45f6-a35b-48b16f779273", "0527f943-aba2-4020-ad19-a015f8e1c209", "901af7fb-6f17-4137-b50c-c6f3d5af6b66", "2b0142e5-043e-4e63-8f04-8de04859dc14", "f8b7c665-e589-48ad-9671-d189464ab198", "78d6f382-b599-4b32-b2b5-6e048e806702"], "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network (machine learning).txt", "file_name": "Neural network (machine learning).txt", "file_type": "text/plain", "file_size": 55046, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}}, "b3dd0d56-5656-4ec4-898f-0583c55fee61": {"node_ids": ["b6d7ca1a-2865-4317-a75c-a33a6b7be282"], "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Neural network.txt", "file_name": "Neural network.txt", "file_type": "text/plain", "file_size": 3884, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}}, "8723d808-8df2-4c09-85af-69e73ce0845d": {"node_ids": ["1ee57893-890b-4a65-aa27-1a9a53a7e5d3", "db403fbb-faf5-44eb-9d16-d1cb01910355"], "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Optical neural network.txt", "file_name": "Optical neural network.txt", "file_type": "text/plain", "file_size": 6069, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}}, "ce81106e-199e-40e5-9c27-b066438275c8": {"node_ids": ["d22e0f0c-429b-4e59-9af6-8240becc88ef", "ea59c639-1d74-4547-91a2-9b4560711e16", "3a47ed73-33e0-445f-97dc-e79b93ca9ec5", "11ea7199-5210-433c-b512-b2f97d6632c1"], "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Physics-informed neural networks.txt", "file_name": "Physics-informed neural networks.txt", "file_type": "text/plain", "file_size": 15160, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}}, "b5ae85da-3fb2-408e-b8a5-2dfb8d152245": {"node_ids": ["a867abeb-9fb5-4e4d-8a2e-46f692632347", "66988b57-ecea-479b-a63c-453d78d96ebc", "8586402a-2911-4dd4-8056-0ca56828b8eb"], "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Quantum neural network.txt", "file_name": "Quantum neural network.txt", "file_type": "text/plain", "file_size": 9833, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}}, "9ccd033a-d520-40e3-9826-a461456a44c3": {"node_ids": ["3d5d556a-2b5a-4d92-81de-de9613dcaf14", "c614bf6e-137a-467b-808c-ddca9eb19fee", "004d0974-94af-4f09-b309-f480e2ac7873", "9b57f5cc-b285-4415-8954-2021cf0a79e7"], "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Rectifier (neural networks).txt", "file_name": "Rectifier (neural networks).txt", "file_type": "text/plain", "file_size": 9659, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}}, "cb6471bf-2893-4828-b5b5-0d44aa11201d": {"node_ids": ["4e62bd62-dd3b-4a27-a900-bf8014da78ec", "8282590e-446e-49e4-8706-8631ef149490", "f0c6dbf3-3671-4fdc-a183-c4827b7226f4", "4cbcecb1-2b04-4f2d-88ea-9f23f18f33c2", "b557405d-f6e5-4aee-b6dd-02fc5f42c078", "512d8208-d923-4637-bfaf-ab61c1643f53", "04ed0a87-ab76-4389-ae6f-656b9aa11037", "662b93c3-d8df-42e9-b02c-bb6d2ec34f3b"], "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Recurrent neural network.txt", "file_name": "Recurrent neural network.txt", "file_type": "text/plain", "file_size": 28077, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}}, "72e5032e-1334-4ccc-8191-84c5c54f0619": {"node_ids": ["9f5b9277-3671-4c3e-a7e8-569a1a6c9080", "0be24465-312b-4be7-8115-00e27a818048", "c44331cb-8338-4a74-b20a-543ff0e64dff", "92a43d90-7087-4bab-9b7a-2a5997a6ac5c", "c4f71283-9bbd-43f2-a96f-175fb7cafc7e"], "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Residual neural network.txt", "file_name": "Residual neural network.txt", "file_type": "text/plain", "file_size": 13238, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}}, "cf3ccf7b-1635-46d8-939e-567bb626882a": {"node_ids": ["57bc95d5-41c4-4203-bc03-970260c56dd1", "4a76c575-6013-4ca4-9283-7a04c1f75083", "71d825cc-39b1-41a0-aec4-6c0816f596c6", "8ee44e45-414e-4229-87fb-d1c997d79310"], "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Siamese neural network.txt", "file_name": "Siamese neural network.txt", "file_type": "text/plain", "file_size": 7467, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}}, "445f09fa-ef83-4d20-bc51-e3c582e9f4ab": {"node_ids": ["27f9a601-e371-4aa2-a75a-e97560d660ad", "1d0d06dc-33fe-4b18-b329-372fbb4e4e8e", "645252aa-2919-42d3-bb00-8c100099c178", "749b128b-e945-46d9-b5ec-90948f4af010"], "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Spiking neural network.txt", "file_name": "Spiking neural network.txt", "file_type": "text/plain", "file_size": 14746, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}}, "bb0d5604-c0f9-4a94-b06f-5075d5cf5210": {"node_ids": ["3784e59e-859b-4284-8301-a78cd765e3ed", "82d03e73-f80c-4e33-87f2-70583aac83dc", "ff6d664b-1345-42ac-b869-def8f9de77d5", "2544d06a-6fcc-4bd0-a8fc-622e6cc88eea", "6ca97326-c3f6-4865-bdfb-66199e0a01d8", "466a7fe3-71fe-4eaf-ada7-d94251b68b7e", "1803e097-fef3-4177-bf1a-ff8603f52552", "10aadcfd-ade9-4d1a-b2c3-231617c52a1e", "615d4c2d-63c3-463c-a518-959f4bbe309d", "f7d76aa1-75b0-49c5-a884-53a0ad72efef", "2c2bed30-dcd6-4e54-a08b-cb12aa1a1829", "248e1579-94b9-4c0a-94df-803f58a29a0b"], "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Types of artificial neural networks.txt", "file_name": "Types of artificial neural networks.txt", "file_type": "text/plain", "file_size": 43865, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}}, "37f790c9-29da-4d45-bab5-71dc6795e41e": {"node_ids": ["075a5ef4-0673-48e0-b49e-966164696450", "743b900b-b021-45db-8b95-a57987c864b8", "4b15aee2-e3ac-4b4f-9e0f-36ecabdce1bb", "d8fecf9a-aa70-4943-941f-cf7567fae1bd"], "metadata": {"file_path": "C:\\Users\\MaxHu\\CLASS DOCS\\NLP\\FINAL PROJECT\\comparative-indexing-analysis\\data\\docs\\Wikipedia\\Unified Modeling Language.txt", "file_name": "Unified Modeling Language.txt", "file_type": "text/plain", "file_size": 13214, "creation_date": "2024-03-25", "last_modified_date": "2024-03-25"}}}}