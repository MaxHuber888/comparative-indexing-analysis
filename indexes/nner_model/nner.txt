An
AI
accelerator,
deep
learning
processor,
or
neural
processing
unit
(NPU)
is
a
class
of
specialized
hardware
accelerator
or
computer
system
designed
to
accelerate
artificial
intelligence
and
machine
learning
applications,
including
artificial
neural
networks
and
machine
vision.
Typical
applications
include
algorithms
for
robotics,
Internet
of
Things,
and
other
data-intensive
or
sensor-driven
tasks.
They
are
often
manycore
designs
and
generally
focus
on
low-precision
arithmetic,
novel
dataflow
architectures
or
in-memory
computing
capability.
As
of
2024,
a
typical
AI
integrated
circuit
chip
contains
tens
of
billions
of
MOSFET
transistors.AI
accelerators
are
used
in
mobile
devices,
such
as
neural
processing
units
(NPUs)
in
Apple
iPhones
or
Huawei
cellphones,
and
personal
computers
such
as
Apple
silicon
Macs,
to
cloud
computing
servers
such
as
tensor
processing
units
(TPU)
in
the
Google
Cloud
Platform.
A
number
of
vendor-specific
terms
exist
for
devices
in
this
category,
and
it
is
an
emerging
technology
without
a
dominant
design.
Graphics
processing
units
designed
by
companies
such
as
Nvidia
and
AMD
often
include
AI-specific
hardware,
and
are
commonly
used
as
AI
accelerators,
both
for
training
and
inference.
==
History
==
Computer
systems
have
frequently
complemented
the
CPU
with
special-purpose
accelerators
for
specialized
tasks,
known
as
coprocessors.
Notable
application-specific
hardware
units
include
video
cards
for
graphics,
sound
cards,
graphics
processing
units
and
digital
signal
processors.
As
deep
learning
and
artificial
intelligence
workloads
rose
in
prominence
in
the
2010s,
specialized
hardware
units
were
developed
or
adapted
from
existing
products
to
accelerate
these
tasks.
===
Early
attempts
===
First
attempts
like
Intel's
ETANN
80170NX
incorporated
analog
circuits
to
compute
neural
functions.Later
all-digital
chips
like
the
Nestor/Intel
Ni1000
followed.
As
early
as
1993,
digital
signal
processors
were
used
as
neural
network
accelerators
to
accelerate
optical
character
recognition
software.By
1988,
Wei
Zhang
et
al.
had
discussed
fast
optical
implementations
of
convolutional
neural
networks
for
alphabet
recognition.In
the
1990s,
there
were
also
attempts
to
create
parallel
high-throughput
systems
for
workstations
aimed
at
various
applications,
including
neural
network
simulations.This
presentation
covers
a
past
attempt
at
neural
net
accelerators,
notes
the
similarity
to
the
modern
SLI
GPGPU
processor
setup,
and
argues
that
general
purpose
vector
accelerators
are
the
way
forward
(in
relation
to
RISC-V
hwacha
project.
Argues
that
NN's
are
just
dense
and
sparse
matrices,
one
of
several
recurring
algorithms)FPGA-based
accelerators
were
also
first
explored
in
the
1990s
for
both
inference
and
training.In
2014,
Chen
et
al.
proposed
DianNao
(Chinese
for
"electric
brain"),
to
accelerate
deep
neural
networks
especially.
DianNao
provides
the
452
Gop/s
peak
performance
(of
key
operations
in
deep
neural
networks)
only
in
a
small
footprint
of
3.02
mm2
and
485
mW.
Later,
the
successors
(DaDianNao,
ShiDianNao,
PuDianNao)
are
proposed
by
the
same
group,
forming
the
DianNao
FamilySmartphones
began
incorporating
AI
accelerators
starting
with
the
Qualcomm
Snapdragon
820
in
2015.
===
Heterogeneous
computing
===
Heterogeneous
computing
incorporates
many
specialized
processors
in
a
single
system,
or
a
single
chip,
each
optimized
for
a
specific
type
of
task.
Architectures
such
as
the
Cell
microprocessor
have
features
significantly
overlapping
with
AI
accelerators
including:
support
for
packed
low
precision
arithmetic,
dataflow
architecture,
and
prioritizing
throughput
over
latency.
The
Cell
microprocessor
has
been
applied
to
a
number
of
tasks
including
AI.In
the
2000s,
CPUs
also
gained
increasingly
wide
SIMD
units,
driven
by
video
and
gaming
workloads;
as
well
as
support
for
packed
low-precision
data
types.
Due
to
the
increasing
performance
of
CPUs,
they
are
also
used
for
running
AI
workloads.
CPUs
are
superior
for
DNNs
with
small
or
medium-scale
parallelism,
for
sparse
DNNs
and
in
low-batch-size
scenarios.
===
Use
of
GPU
===
Graphics
processing
units
or
GPUs
are
specialized
hardware
for
the
manipulation
of
images
and
calculation
of
local
image
properties.
The
mathematical
basis
of
neural
networks
and
image
manipulation
are
similar,
embarrassingly
parallel
tasks
involving
matrices,
leading
GPUs
to
become
increasingly
used
for
machine
learning
tasks.In
2012,
Alex
Krizhevsky
adopted
two
GPUs
to
train
a
deep
learning
network,
i.e.,
AlexNet,
which
won
the
champion
of
the
ISLVRC-2012
competition.
During
the
2010's,
GPU
manufacturers
such
as
Nvidia
added
deep
learning
related
features
in
both
hardware
(e.g.,
INT8
operators)
and
software
(e.g.,
cuDNN
Library).
GPUs
continue
to
be
used
in
large-scale
AI
applications.
For
example,
Summit,
a
supercomputer
from
IBM
for
Oak
Ridge
National
Laboratory,
contains
27,648
Nvidia
Tesla
V100
cards,
which
can
be
used
to
accelerate
deep
learning
algorithms.
Over
the
2010's
GPUs
continued
to
evolve
in
a
direction
to
facilitate
deep
learning,
both
for
training
and
inference
in
devices
such
as
self-driving
cars.
GPU
developers
such
as
Nvidia
NVLink
are
developing
additional
connective
capability
for
the
kind
of
dataflow
workloads
AI
benefits
from.
As
GPUs
have
been
increasingly
applied
to
AI
acceleration,
GPU
manufacturers
have
incorporated
neural
network-specific
hardware
to
further
accelerate
these
tasks.
Tensor
cores
are
intended
to
speed
up
the
training
of
neural
networks.
===
Use
of
FPGAs
===
Deep
learning
frameworks
are
still
evolving,
making
it
hard
to
design
custom
hardware.
Reconfigurable
devices
such
as
field-programmable
gate
arrays
(FPGA)
make
it
easier
to
evolve
hardware,
frameworks,
and
software
alongside
each
other.Microsoft
has
used
FPGA
chips
to
accelerate
inference
for
real-time
deep
learning
services.
===
Emergence
of
dedicated
AI
accelerator
ASICs
===
While
GPUs
and
FPGAs
perform
far
better
than
CPUs
for
AI-related
tasks,
a
factor
of
up
to
10
in
efficiency
may
be
gained
with
a
more
specific
design,
via
an
application-specific
integrated
circuit
(ASIC).
These
accelerators
employ
strategies
such
as
optimized
memory
use
and
the
use
of
lower
precision
arithmetic
to
accelerate
calculation
and
increase
throughput
of
computation.
Some
low-precision
floating-point
formats
used
for
AI
acceleration
are
half-precision
and
the
bfloat16
floating-point
format.
Companies
such
as
Google,
Qualcomm,
Amazon,
Apple,
Facebook,
AMD
and
Samsung
are
all
designing
their
own
AI
ASICs.
Cerebras
Systems
has
built
a
dedicated
AI
accelerator
based
on
the
largest
processor
in
the
industry,
the
second-generation
Wafer
Scale
Engine
(WSE-2),
to
support
deep
learning
workloads.
==
Ongoing
research
==
===
In-memory
computing
architectures
===
In
June
2017,
IBM
researchers
announced
an
architecture
in
contrast
to
the
Von
Neumann
architecture
based
on
in-memory
computing
and
phase-change
memory
arrays
applied
to
temporal
correlation
detection,
intending
to
generalize
the
approach
to
heterogeneous
computing
and
massively
parallel
systems.
In
October
2018,
IBM
researchers
announced
an
architecture
based
on
in-memory
processing
and
modeled
on
the
human
brain's
synaptic
network
to
accelerate
deep
neural
networks.
The
system
is
based
on
phase-change
memory
arrays.
===
In-memory
computing
with
analog
resistive
memories
===
In
2019,
researchers
from
Politecnico
di
Milano
found
a
way
to
solve
systems
of
linear
equations
in
a
few
tens
of
nanoseconds
via
a
single
operation.
Their
algorithm
is
based
on
in-memory
computing
with
analog
resistive
memories
which
performs
with
high
efficiencies
of
time
and
energy,
via
conducting
matrix–vector
multiplication
in
one
step
using
Ohm's
law
and
Kirchhoff's
law.
The
researchers
showed
that
a
feedback
circuit
with
cross-point
resistive
memories
can
solve
algebraic
problems
such
as
systems
of
linear
equations,
matrix
eigenvectors,
and
differential
equations
in
just
one
step.
Such
an
approach
improves
computational
times
drastically
in
comparison
with
digital
algorithms.
===
Atomically
thin
semiconductors
===
In
2020,
Marega
et
al.
published
experiments
with
a
large-area
active
channel
material
for
developing
logic-in-memory
devices
and
circuits
based
on
floating-gate
field-effect
transistors
(FGFETs).
Such
atomically
thin
semiconductors
are
considered
promising
for
energy-efficient
machine
learning
applications,
where
the
same
basic
device
structure
is
used
for
both
logic
operations
and
data
storage.
The
authors
used
two-dimensional
materials
such
as
semiconducting
molybdenum
disulphide
to
precisely
tune
FGFETs
as
building
blocks
in
which
logic
operations
can
be
performed
with
the
memory
elements.
===
Integrated
photonic
tensor
core
===
In
1988,
Wei
Zhang
et
al.
discussed
fast
optical
implementations
of
convolutional
neural
networks
for
alphabet
recognition.
In
2021,
J.
Feldmann
et
al.
proposed
an
integrated
photonic
hardware
accelerator
for
parallel
convolutional
processing.
The
authors
identify
two
key
advantages
of
integrated
photonics
over
its
electronic
counterparts:
(1)
massively
parallel
data
transfer
through
wavelength
division
multiplexing
in
conjunction
with
frequency
combs,
and
(2)
extremely
high
data
modulation
speeds.
Their
system
can
execute
trillions
of
multiply-accumulate
operations
per
second,
indicating
the
potential
of
integrated
photonics
in
data-heavy
AI
applications.
Optical
processors
that
can
also
perform
backpropagation
for
artificial
neural
networks
have
been
experimentally
developed.
==
Nomenclature
==
As
of
2016,
the
field
is
still
in
flux
and
vendors
are
pushing
their
own
marketing
term
for
what
amounts
to
an
"AI
accelerator",
in
the
hope
that
their
designs
and
APIs
will
become
the
dominant
design.
There
is
no
consensus
on
the
boundary
between
these
devices,
nor
the
exact
form
they
will
take;
however
several
examples
clearly
aim
to
fill
this
new
space,
with
a
fair
amount
of
overlap
in
capabilities.
In
the
past
when
consumer
graphics
accelerators
emerged,
the
industry
eventually
adopted
Nvidia's
self-assigned
term,
"the
GPU",
as
the
collective
noun
for
"graphics
accelerators",
which
had
taken
many
forms
before
settling
on
an
overall
pipeline
implementing
a
model
presented
by
Direct3D.
All
models
of
Intel
Meteor
Lake
processors
have
a
Versatile
Processor
Unit
(VPU)
built-in
for
accelerating
inference
for
computer
vision
and
deep
learning.
==
Deep
Learning
Processors
(DLP)
==
Inspired
from
the
pioneer
work
of
DianNao
Family,
many
DLPs
are
proposed
in
both
academia
and
industry
with
design
optimized
to
leverage
the
features
of
deep
neural
networks
for
high
efficiency.
Only
at
ISCA
2016,
three
sessions,
15%
(!)
of
the
accepted
papers,
are
all
architecture
designs
about
deep
learning.
Such
efforts
include
Eyeriss
(MIT),
EIE
(Stanford),
Minerva
(Harvard),
Stripes
(University
of
Toronto)
in
academia,
TPU
(Google),
and
MLU
(Cambricon)
in
industry.
We
listed
several
representative
works
in
Table
1.
===
Digital
DLPs
===
The
major
components
of
DLPs
architecture
usually
include
a
computation
component,
the
on-chip
memory
hierarchy,
and
the
control
logic
that
manages
the
data
communication
and
computing
flows.
Regarding
the
computation
component,
as
most
operations
in
deep
learning
can
be
aggregated
into
vector
operations,
the
most
common
ways
for
building
computation
components
in
digital
DLPs
are
the
MAC-based
(multiplier-accumulation)
organization,
either
with
vector
MACs
or
scalar
MACs.
Rather
than
SIMD
or
SIMT
in
general
processing
devices,
deep
learning
domain-specific
parallelism
is
better
explored
on
these
MAC-based
organizations.
Regarding
the
memory
hierarchy,
as
deep
learning
algorithms
require
high
bandwidth
to
provide
the
computation
component
with
sufficient
data,
DLPs
usually
employ
a
relatively
larger
size
(tens
of
kilobytes
or
several
megabytes)
on-chip
buffer
but
with
dedicated
on-chip
data
reuse
strategy
and
data
exchange
strategy
to
alleviate
the
burden
for
memory
bandwidth.
For
example,
DianNao,
16
16-in
vector
MAC,
requires
16
×
16
×
2
=
512
16-bit
data,
i.e.,
almost
1024GB/s
bandwidth
requirements
between
computation
components
and
buffers.
With
on-chip
reuse,
such
bandwidth
requirements
are
reduced
drastically.
Instead
of
the
widely
used
cache
in
general
processing
devices,
DLPs
always
use
scratchpad
memory
as
it
could
provide
higher
data
reuse
opportunities
by
leveraging
the
relatively
regular
data
access
pattern
in
deep
learning
algorithms.
Regarding
the
control
logic,
as
the
deep
learning
algorithms
keep
evolving
at
a
dramatic
speed,
DLPs
start
to
leverage
dedicated
ISA
(instruction
set
architecture)
to
support
the
deep
learning
domain
flexibly.
At
first,
DianNao
used
a
VLIW-style
instruction
set
where
each
instruction
could
finish
a
layer
in
a
DNN.
Cambricon
introduces
the
first
deep
learning
domain-specific
ISA,
which
could
support
more
than
ten
different
deep
learning
algorithms.
TPU
also
reveals
five
key
instructions
from
the
CISC-style
ISA.
===
Hybrid
DLPs
===
Hybrid
DLPs
emerge
for
DNN
inference
and
training
acceleration
because
of
their
high
efficiency.
Processing-in-memory
(PIM)
architectures
are
one
most
important
type
of
hybrid
DLP.
The
key
design
concept
of
PIM
is
to
bridge
the
gap
between
computing
and
memory,
with
the
following
manners:
1)
Moving
computation
components
into
memory
cells,
controllers,
or
memory
chips
to
alleviate
the
memory
wall
issue.
Such
architectures
significantly
shorten
data
paths
and
leverage
much
higher
internal
bandwidth,
hence
resulting
in
attractive
performance
improvement.
2)
Build
high
efficient
DNN
engines
by
adopting
computational
devices.
In
2013,
HP
Lab
demonstrated
the
astonishing
capability
of
adopting
ReRAM
crossbar
structure
for
computing.
Inspiring
by
this
work,
tremendous
work
are
proposed
to
explore
the
new
architecture
and
system
design
based
on
ReRAM,
phase
change
memory,
etc.
==
Benchmarks
==
Benchmarks
such
as
MLPerf
and
others
may
be
used
to
evaluate
the
performance
of
AI
accelerators.
Table
2
lists
several
typical
benchmarks
for
AI
accelerators.
==
Potential
applications
==
Agricultural
robots,
for
example,
herbicide-free
weed
control.
Autonomous
vehicles:
Nvidia
has
targeted
their
Drive
PX-series
boards
at
this
application.
Computer-aided
diagnosis
Industrial
robots,
increasing
the
range
of
tasks
that
can
be
automated,
by
adding
adaptability
to
variable
situations.
Machine
translation
Military
robots
Natural
language
processing
Search
engines,
increasing
the
energy
efficiency
of
data
centers
and
the
ability
to
use
increasingly
advanced
queries.
Unmanned
aerial
vehicles,
e.g.
navigation
systems,
e.g.
the
Movidius
Myriad
2
has
been
demonstrated
successfully
guiding
autonomous
drones.
Voice
user
interface,
e.g.
in
mobile
phones,
a
target
for
Qualcomm
Zeroth.
==
See
also
==
Cognitive
computer
Neuromorphic
engineering
Optical
neural
network
Physical
neural
network
Cerebras
Systems
==
References
==
==
External
links
==
Nvidia
Puts
The
Accelerator
To
The
Metal
With
Pascal.htm,
The
Next
Platform
Eyeriss
Project,
MIT
https://alphaics.ai/
Bidirectional
Encoder
Representations
from
Transformers
(BERT)
is
a
language
model
based
on
the
transformer
architecture,
notable
for
its
dramatic
improvement
over
previous
state
of
the
art
models.
It
was
introduced
in
October
2018
by
researchers
at
Google.
A
2020
literature
survey
concluded
that
"in
a
little
over
a
year,
BERT
has
become
a
ubiquitous
baseline
in
Natural
Language
Processing
(NLP)
experiments
counting
over
150
research
publications
analyzing
and
improving
the
model."BERT
was
originally
implemented
in
the
English
language
at
two
model
sizes:
(1)
BERTBASE:
12
encoders
with
12
bidirectional
self-attention
heads
totaling
110
million
parameters,
and
(2)
BERTLARGE:
24
encoders
with
16
bidirectional
self-attention
heads
totaling
340
million
parameters.
Both
models
were
pre-trained
on
the
Toronto
BookCorpus
(800M
words)
and
English
Wikipedia
(2,500M
words).
==
Design
==
BERT
is
an
"encoder-only"
transformer
architecture.
On
a
high
level,
BERT
consists
of
three
modules:
embedding.
This
module
converts
an
array
of
one-hot
encoded
tokens
into
an
array
of
vectors
representing
the
tokens.
a
stack
of
encoders.
These
encoders
are
the
Transformer
encoders.
They
perform
transformations
over
the
array
of
representation
vectors.
un-embedding.
This
module
converts
the
final
representation
vectors
into
one-hot
encoded
tokens
again.The
un-embedding
module
is
necessary
for
pretraining,
but
it
is
often
unnecessary
for
downstream
tasks.
Instead,
one
would
take
the
representation
vectors
output
at
the
end
of
the
stack
of
encoders,
and
use
those
as
a
vector
representation
of
the
text
input,
and
train
a
smaller
model
on
top
of
that.
BERT
uses
WordPiece
to
convert
each
English
word
into
an
integer
code.
Its
vocabulary
has
size
30,000.
Any
token
not
appearing
in
its
vocabulary
is
replaced
by
[UNK]
for
"unknown".
===
Pretraining
===
BERT
was
pre-trained
simultaneously
on
two
tasks:language
modeling:
15%
of
tokens
were
selected
for
prediction,
and
the
training
objective
was
to
predict
the
selected
token
given
its
context.
The
selected
token
is
replaced
with
a
[MASK]
token
with
probability
80%,
replaced
with
a
random
word
token
with
probability
10%,
not
replaced
with
probability
10%.For
example,
the
sentence
"my
dog
is
cute"
may
have
the
4-th
token
selected
for
prediction.
The
model
would
have
input
text
"my
dog
is
[MASK]"
with
probability
80%,
"my
dog
is
happy"
with
probability
10%,
"my
dog
is
cute"
with
probability
10%.After
processing
the
input
text,
the
model's
4-th
output
vector
is
passed
to
a
separate
neural
network,
which
outputs
a
probability
distribution
over
its
30,000-large
vocabulary.
next
sentence
prediction:
Given
two
spans
of
text,
the
model
predicts
if
these
two
spans
appeared
sequentially
in
the
training
corpus,
outputting
either
[IsNext]
or
[NotNext].
The
first
span
starts
with
a
special
token
[CLS]
(for
"classify").
The
two
spans
are
separated
by
a
special
token
[SEP]
(for
"separate").
After
processing
the
two
spans,
the
1-st
output
vector
(the
vector
coding
for
[CLS])
is
passed
to
a
separate
neural
network
for
the
binary
classification
into
[IsNext]
and
[NotNext].
For
example,
given
"[CLS]
my
dog
is
cute
[SEP]
he
likes
playing"
the
model
should
output
token
[IsNext].
Given
"[CLS]
my
dog
is
cute
[SEP]
how
do
magnets
work"
the
model
should
output
token
[NotNext].As
a
result
of
this
training
process,
BERT
learns
latent
representations
of
words
and
sentences
in
context.
After
pre-training,
BERT
can
be
fine-tuned
with
fewer
resources
on
smaller
datasets
to
optimize
its
performance
on
specific
tasks
such
as
NLP
tasks
(language
inference,
text
classification)
and
sequence-to-sequence
based
language
generation
tasks
(question-answering,
conversational
response
generation).
The
pre-training
stage
is
significantly
more
computationally
expensive
than
fine-tuning.
===
Architecture
details
===
This
section
describes
BERTBASE.
The
other
one,
BERTLARGE,
is
similar,
just
larger.
The
lowest
layer
is
the
embedding
layer,
which
contains
three
components:
word_embeddings,
position_embeddings,
token_type_embeddings.
word_embeddings
takes
in
a
one-hot
vector
of
the
input
token.
The
one-hot
vector
input
has
dimension
30,000,
because
BERT
has
a
vocabulary
size
that
large.
position_embeddings
performs
absolute
position
embedding.
It
is
like
word_embeddings,
but
on
a
vocabulary
consisting
of
just
the
time-stamps
0
to
511,
since
BERT
has
a
context
window
of
512.
token_type_embeddings
is
like
word_embeddings,
but
on
a
vocabulary
consisting
of
just
0
and
1.
The
only
type-1
tokens
are
those
that
appear
after
the
[SEP].
All
other
tokens
are
type-0.The
three
outputs
are
added,
then
pushed
through
a
LayerNorm
(layer
normalization),
obtaining
an
array
of
representation
vectors,
each
having
768
dimensions.
After
this,
the
representation
vectors
move
through
12
Transformer
encoders,
then
they
are
un-embedded
by
an
affine-Add
&
LayerNorm-linear.
==
Performance
==
When
BERT
was
published,
it
achieved
state-of-the-art
performance
on
a
number
of
natural
language
understanding
tasks:
GLUE
(General
Language
Understanding
Evaluation)
task
set
(consisting
of
9
tasks)
SQuAD
(Stanford
Question
Answering
Dataset)
v1.1
and
v2.0
SWAG
(Situations
With
Adversarial
Generations)
==
Analysis
==
The
reasons
for
BERT's
state-of-the-art
performance
on
these
natural
language
understanding
tasks
are
not
yet
well
understood.
Current
research
has
focused
on
investigating
the
relationship
behind
BERT's
output
as
a
result
of
carefully
chosen
input
sequences,
analysis
of
internal
vector
representations
through
probing
classifiers,
and
the
relationships
represented
by
attention
weights.
The
high
performance
of
the
BERT
model
could
also
be
attributed
to
the
fact
that
it
is
bidirectionally
trained.
This
means
that
BERT,
based
on
the
Transformer
model
architecture,
applies
its
self-attention
mechanism
to
learn
information
from
a
text
from
the
left
and
right
side
during
training,
and
consequently
gains
a
deep
understanding
of
the
context.
For
example,
the
word
fine
can
have
two
different
meanings
depending
on
the
context
(I
feel
fine
today,
She
has
fine
blond
hair).
BERT
considers
the
words
surrounding
the
target
word
fine
from
the
left
and
right
side.
However
it
comes
at
a
cost:
due
to
encoder-only
architecture
lacking
a
decoder,
BERT
can't
be
prompted
and
can't
generate
text,
while
bidirectional
models
in
general
do
not
work
effectively
without
the
right
side,
thus
being
difficult
to
prompt,
with
even
short
text
generation
requiring
sophisticated
computationally
expensive
techniques.In
contrast
to
deep
learning
neural
networks
which
require
very
large
amounts
of
data,
BERT
has
already
been
pre-trained
which
means
that
it
has
learnt
the
representations
of
the
words
and
sentences
as
well
as
the
underlying
semantic
relations
that
they
are
connected
with.
BERT
can
then
be
fine-tuned
on
smaller
datasets
for
specific
tasks
such
as
sentiment
classification.
The
pre-trained
models
are
chosen
according
to
the
content
of
the
given
dataset
one
uses
but
also
the
goal
of
the
task.
For
example,
if
the
task
is
a
sentiment
classification
task
on
financial
data,
a
pre-trained
model
for
the
analysis
of
sentiment
of
financial
text
should
be
chosen.
The
weights
of
the
original
pre-trained
models
were
released
on
GitHub.
==
History
==
BERT
was
originally
published
by
Google
researchers
Jacob
Devlin,
Ming-Wei
Chang,
Kenton
Lee,
and
Kristina
Toutanova.
The
design
has
its
origins
from
pre-training
contextual
representations,
including
semi-supervised
sequence
learning,
generative
pre-training,
ELMo,
and
ULMFit.
Unlike
previous
models,
BERT
is
a
deeply
bidirectional,
unsupervised
language
representation,
pre-trained
using
only
a
plain
text
corpus.
Context-free
models
such
as
word2vec
or
GloVe
generate
a
single
word
embedding
representation
for
each
word
in
the
vocabulary,
whereas
BERT
takes
into
account
the
context
for
each
occurrence
of
a
given
word.
For
instance,
whereas
the
vector
for
"running"
will
have
the
same
word2vec
vector
representation
for
both
of
its
occurrences
in
the
sentences
"He
is
running
a
company"
and
"He
is
running
a
marathon",
BERT
will
provide
a
contextualized
embedding
that
will
be
different
according
to
the
sentence.On
October
25,
2019,
Google
announced
that
they
had
started
applying
BERT
models
for
English
language
search
queries
within
the
US.
On
December
9,
2019,
it
was
reported
that
BERT
had
been
adopted
by
Google
Search
for
over
70
languages.
In
October
2020,
almost
every
single
English-based
query
was
processed
by
a
BERT
model.A
later
paper
proposes
RoBERTa,
which
preserves
BERT's
architecture,
but
improves
its
training,
changing
key
hyperparameters,
removing
the
next-sentence
prediction
task,
and
using
much
larger
mini-batch
sizes.
==
Recognition
==
The
research
paper
describing
BERT
won
the
Best
Long
Paper
Award
at
the
2019
Annual
Conference
of
the
North
American
Chapter
of
the
Association
for
Computational
Linguistics
(NAACL).
==
References
==
==
Further
reading
==
Rogers,
Anna;
Kovaleva,
Olga;
Rumshisky,
Anna
(2020).
"A
Primer
in
BERTology:
What
we
know
about
how
BERT
works".
arXiv:2002.12327
[cs.CL].
==
External
links
==
Official
GitHub
repository
BERT
on
Devopedia
BigScience
Large
Open-science
Open-access
Multilingual
Language
Model
(BLOOM)
is
a
176-billion-parameter
transformer-based
autoregressive
large
language
model
(LLM).
The
model,
as
well
as
the
code
base
and
the
data
used
to
train
it,
are
distributed
under
free
licences.
BLOOM
was
trained
on
approximately
366
billion
(1.6TB)
tokens
from
March
to
July
2022.BLOOM
is
the
main
outcome
of
the
BigScience
collaborative
initiative,
a
one-year-long
research
workshop
that
took
place
between
May
2021
and
May
2022.
BigScience
was
led
by
HuggingFace
and
involved
several
hundreds
of
researchers
and
engineers
from
France
and
abroad
representing
both
the
academia
and
the
private
sector.
BigScience
was
supported
by
a
large-scale
public
compute
grant
on
the
French
public
supercomputer
Jean
Zay,
managed
by
GENCI
and
IDRIS
(CNRS),
on
which
it
was
trained.
BLOOM's
training
corpus,
named
ROOTS,
combines
data
extracted
from
the
then-latest
version
of
the
web-based
OSCAR
corpus
(38%
of
ROOTS)
and
newly
collected
data
extracted
from
a
manually
selected
and
documented
list
of
language
data
sources.
It
encompasses
46
natural
languages
(in
amounts
ranging
from
30%
of
the
whole
dataset
for
English
to
0.00002%
for
Chi
Tumbuka)
and
13
programming
languages.
==
References
==
Chinchilla
is
a
family
of
large
language
models
developed
by
the
research
team
at
DeepMind,
presented
in
March
2022.
It
is
named
"chinchilla"
because
it
is
a
further
development
over
a
previous
model
family
named
Gopher.
Both
model
families
were
trained
in
order
to
investigate
the
scaling
laws
of
large
language
models.It
claimed
to
outperform
GPT-3.
It
considerably
simplifies
downstream
utilization
because
it
requires
much
less
computer
power
for
inference
and
fine-tuning.
Based
on
the
training
of
previously
employed
language
models,
it
has
been
determined
that
if
one
doubles
the
model
size,
one
must
also
have
twice
the
number
of
training
tokens.
This
hypothesis
has
been
used
to
train
Chinchilla
by
DeepMind.
Similar
to
Gopher
in
terms
of
cost,
Chinchilla
has
70B
parameters
and
four
times
as
much
data.Chinchilla
has
an
average
accuracy
of
67.5%
on
the
MMLU
benchmark
(Measuring
Massive
Multitask
Language
Understanding),
which
is
7%
higher
than
Gopher's
performance.
Chinchilla
was
still
in
the
testing
phase
as
of
January
12,
2023.Chinchilla
contributes
to
developing
an
effective
training
paradigm
for
large
autoregressive
language
models
with
limited
compute
resources.
The
Chinchilla
team
recommends
that
the
number
of
training
tokens
is
twice
for
every
model
size
doubling,
meaning
that
using
larger,
higher-quality
training
datasets
can
lead
to
better
results
on
downstream
tasks.
==
Architecture
==
Both
the
Gopher
family
and
Chinchilla
family
are
families
of
transformer
models.
In
particular,
they
are
essentially
the
same
as
GPT-2,
with
different
sizes
and
minor
modifications.
Gopher
family
uses
RMSNorm
instead
of
LayerNorm;
relative
positional
encoding
rather
than
absolute
positional
encoding.
The
Chinchilla
family
is
the
same
as
the
Gopher
family,
but
trained
with
AdamW
instead
of
Adam
optimizer.
The
Gopher
family
contains
six
models
of
increasing
size,
from
44
million
parameters
to
280
billion
parameters.
They
refer
to
the
largest
one
as
"Gopher"
by
default.
Similar
naming
conventions
apply
for
the
Chinchilla
family.
Table
1
of
shows
the
entire
Gopher
family:
Table
4
of
compares
the
70-billion-parameter
Chinchilla
with
Gopher
280B.
==
See
also
==
LaMDA
==
References
==
Claude
is
a
family
of
large
language
models
developed
by
Anthropic.
The
first
model
was
released
in
March
2023.
Claude
3,
released
in
March
2024,
can
also
analyze
images.
==
Training
==
Claude
models
are
generative
pre-trained
transformers.
They
have
been
pre-trained
to
predict
the
next
word
in
large
amounts
of
text.
Claude
models
have
then
been
fine-tuned
with
Constitutional
AI
and
Reinforcement
Learning
From
Human
Feedback,
with
the
aim
of
making
them
less
likely
to
give
harmful
responses,
while
still
being
helpful
to
the
user.
===
Constitutional
AI
===
Constitutional
AI
is
an
approach
developed
by
Anthropic
for
training
AI
systems,
particularly
language
models
like
Claude,
to
be
harmless
and
helpful
without
relying
on
extensive
human
feedback.
The
method,
detailed
in
the
paper
"Constitutional
AI:
Harmlessness
from
AI
Feedback"
involves
two
phases:
supervised
learning
and
reinforcement
learning.
In
the
supervised
learning
phase,
the
model
generates
responses
to
prompts,
self-critiques
these
responses
based
on
a
set
of
guiding
principles
(a
"constitution"),
and
then
revises
the
responses.
The
reinforcement
learning
phase
involves
training
the
model
with
AI-generated
feedback,
where
the
AI
evaluates
responses
according
to
the
constitutional
principles.
This
approach
enables
the
training
of
AI
assistants
that
are
both
helpful
and
harmless,
and
that
can
explain
their
objections
to
harmful
requests,
enhancing
transparency
and
reducing
reliance
on
human
supervision.The
"constitution"
for
Claude
included
75
points,
including
sections
from
the
UN
Universal
Declaration
of
Human
Rights.
==
Models
==
===
Claude
===
Claude
was
the
initial
version
of
Anthropic's
language
model
released
in
March
2023,
Claude
demonstrated
proficiency
in
various
tasks
but
had
certain
limitations
in
coding,
math,
and
reasoning
capabilities.
Anthropic
partnered
with
companies
like
Notion
(productivity
software)
and
Quora
(to
help
develop
the
Poe
chatbot).
===
Claude
Instant
===
Claude
was
released
as
two
versions,
Claude
and
Claude
Instant,
with
Claude
Instant
being
a
faster,
less
expensive
and
lighter
version.
Claude
Instant
has
a
input
context
length
of
100,000
tokens
(which
corresponds
to
around
75,000
words).
===
Claude
2
===
Claude
2
was
the
next
major
iteration
of
Claude,
which
was
released
in
July
11
2023
and
available
to
the
general
public,
whereas
the
Claude
1
was
only
available
to
selected
users
approved
by
Anthropic.Claude
2
expanded
its
context
window
from
9,000
tokens
to
100,000
tokens.
Features
included
ability
to
upload
PDFs
and
other
documents
that
enables
Claude
to
read,
summarise
and
assist
with
tasks.
====
Claude
2.1
====
Claude
2.1
doubled
the
number
of
tokens
that
the
chatbot
could
handle,
increasing
it
to
a
window
of
200,000
tokens,
which
equals
around
500
pages
of
written
material.Anthropic
states
that
the
new
model
is
less
likely
to
produce
false
statements
compared
to
its
predecessors.
===
Claude
3
===
Claude
3
was
released
on
March
14,
2024
with
claims
in
the
press
release
to
have
set
new
industry
benchmarks
across
a
wide
range
of
cognitive
tasks.
The
Claude
3
family
includes
three
state-of-the-art
models
in
ascending
order
of
capability:
Haiku,
Sonnet,
and
Opus.
The
default
version
of
Claude
3
Opus
has
a
context
window
of
200,000
tokens,
but
this
is
being
expanded
to
1
million
for
specific
use
cases.Claude
3
has
seemed
to
perform
meta-cognitive
reasoning,
including
the
ability
to
realize
it
is
being
artificially
tested
during
needle
in
a
haystack
evaluations.
==
Access
==
Limited-use
access
is
free
of
charge,
but
requires
both
an
e-mail
address
and
a
cellphone
number.
==
Criticism
==
Claude
2
has
faced
criticism
for
its
stringent
ethical
alignment
that
may
reduce
usability
and
performance.
Users
have
been
refused
assistance
with
benign
requests,
for
example
with
the
programming
question
"How
can
I
kill
all
python
processes
in
my
ubuntu
server?"
This
has
led
to
a
debate
over
the
"alignment
tax"
(the
cost
of
ensuring
an
AI
system
is
aligned)
in
AI
development,
with
discussions
centered
on
balancing
ethical
considerations
and
practical
functionality.
Critics
argue
for
user
autonomy
and
effectiveness,
while
proponents
stress
the
importance
of
ethical
AI.
==
References
==
Convolutional
neural
network
(CNN)
is
a
regularized
type
of
feed-forward
neural
network
that
learns
feature
engineering
by
itself
via
filters
(or
kernel)
optimization.
Vanishing
gradients
and
exploding
gradients,
seen
during
backpropagation
in
earlier
neural
networks,
are
prevented
by
using
regularized
weights
over
fewer
connections.
For
example,
for
each
neuron
in
the
fully-connected
layer
10,000
weights
would
be
required
for
processing
an
image
sized
100
×
100
pixels.
However,
applying
cascaded
convolution
(or
cross-correlation)
kernels,
only
25
neurons
are
required
to
process
5x5-sized
tiles.
Higher-layer
features
are
extracted
from
wider
context
windows,
compared
to
lower-layer
features.
They
have
applications
in:
image
and
video
recognition,
recommender
systems,
image
classification,
image
segmentation,
medical
image
analysis,
natural
language
processing,
brain–computer
interfaces,
and
financial
time
series.CNNs
are
also
known
as
Shift
Invariant
or
Space
Invariant
Artificial
Neural
Networks
(SIANN),
based
on
the
shared-weight
architecture
of
the
convolution
kernels
or
filters
that
slide
along
input
features
and
provide
translation-equivariant
responses
known
as
feature
maps.
Counter-intuitively,
most
convolutional
neural
networks
are
not
invariant
to
translation,
due
to
the
downsampling
operation
they
apply
to
the
input.Feed-forward
neural
networks
are
usually
fully
connected
networks,
that
is,
each
neuron
in
one
layer
is
connected
to
all
neurons
in
the
next
layer.
The
"full
connectivity"
of
these
networks
make
them
prone
to
overfitting
data.
Typical
ways
of
regularization,
or
preventing
overfitting,
include:
penalizing
parameters
during
training
(such
as
weight
decay)
or
trimming
connectivity
(skipped
connections,
dropout,
etc.)
Robust
datasets
also
increases
the
probability
that
CNNs
will
learn
the
generalized
principles
that
characterize
a
given
dataset
rather
than
the
biases
of
a
poorly-populated
set.Convolutional
networks
were
inspired
by
biological
processes
in
that
the
connectivity
pattern
between
neurons
resembles
the
organization
of
the
animal
visual
cortex.
Individual
cortical
neurons
respond
to
stimuli
only
in
a
restricted
region
of
the
visual
field
known
as
the
receptive
field.
The
receptive
fields
of
different
neurons
partially
overlap
such
that
they
cover
the
entire
visual
field.
CNNs
use
relatively
little
pre-processing
compared
to
other
image
classification
algorithms.
This
means
that
the
network
learns
to
optimize
the
filters
(or
kernels)
through
automated
learning,
whereas
in
traditional
algorithms
these
filters
are
hand-engineered.
This
independence
from
prior
knowledge
and
human
intervention
in
feature
extraction
is
a
major
advantage.
==
Architecture
==
A
convolutional
neural
network
consists
of
an
input
layer,
hidden
layers
and
an
output
layer.
In
a
convolutional
neural
network,
the
hidden
layers
include
one
or
more
layers
that
perform
convolutions.
Typically
this
includes
a
layer
that
performs
a
dot
product
of
the
convolution
kernel
with
the
layer's
input
matrix.
This
product
is
usually
the
Frobenius
inner
product,
and
its
activation
function
is
commonly
ReLU.
As
the
convolution
kernel
slides
along
the
input
matrix
for
the
layer,
the
convolution
operation
generates
a
feature
map,
which
in
turn
contributes
to
the
input
of
the
next
layer.
This
is
followed
by
other
layers
such
as
pooling
layers,
fully
connected
layers,
and
normalization
layers.
Here
it
should
be
noted
how
close
a
convolutional
neural
network
is
to
a
matched
filter.
===
Convolutional
layers
===
In
a
CNN,
the
input
is
a
tensor
with
shape:
(number
of
inputs)
×
(input
height)
×
(input
width)
×
(input
channels)
After
passing
through
a
convolutional
layer,
the
image
becomes
abstracted
to
a
feature
map,
also
called
an
activation
map,
with
shape:
(number
of
inputs)
×
(feature
map
height)
×
(feature
map
width)
×
(feature
map
channels).
Convolutional
layers
convolve
the
input
and
pass
its
result
to
the
next
layer.
This
is
similar
to
the
response
of
a
neuron
in
the
visual
cortex
to
a
specific
stimulus.
Each
convolutional
neuron
processes
data
only
for
its
receptive
field.
Although
fully
connected
feedforward
neural
networks
can
be
used
to
learn
features
and
classify
data,
this
architecture
is
generally
impractical
for
larger
inputs
(e.g.,
high-resolution
images),
which
would
require
massive
numbers
of
neurons
because
each
pixel
is
a
relevant
input
feature.
A
fully
connected
layer
for
an
image
of
size
100
×
100
has
10,000
weights
for
each
neuron
in
the
second
layer.
Convolution
reduces
the
number
of
free
parameters,
allowing
the
network
to
be
deeper.
For
example,
using
a
5
×
5
tiling
region,
each
with
the
same
shared
weights,
requires
only
25
neurons.
Using
regularized
weights
over
fewer
parameters
avoids
the
vanishing
gradients
and
exploding
gradients
problems
seen
during
backpropagation
in
earlier
neural
networks.To
speed
processing,
standard
convolutional
layers
can
be
replaced
by
depthwise
separable
convolutional
layers,
which
are
based
on
a
depthwise
convolution
followed
by
a
pointwise
convolution.
The
depthwise
convolution
is
a
spatial
convolution
applied
independently
over
each
channel
of
the
input
tensor,
while
the
pointwise
convolution
is
a
standard
convolution
restricted
to
the
use
of
1×1{\displaystyle
1\times
1}
kernels.
===
Pooling
layers
===
Convolutional
networks
may
include
local
and/or
global
pooling
layers
along
with
traditional
convolutional
layers.
Pooling
layers
reduce
the
dimensions
of
data
by
combining
the
outputs
of
neuron
clusters
at
one
layer
into
a
single
neuron
in
the
next
layer.
Local
pooling
combines
small
clusters,
tiling
sizes
such
as
2
×
2
are
commonly
used.
Global
pooling
acts
on
all
the
neurons
of
the
feature
map.
There
are
two
common
types
of
pooling
in
popular
use:
max
and
average.
Max
pooling
uses
the
maximum
value
of
each
local
cluster
of
neurons
in
the
feature
map,
while
average
pooling
takes
the
average
value.
===
Fully
connected
layers
===
Fully
connected
layers
connect
every
neuron
in
one
layer
to
every
neuron
in
another
layer.
It
is
the
same
as
a
traditional
multilayer
perceptron
neural
network
(MLP).
The
flattened
matrix
goes
through
a
fully
connected
layer
to
classify
the
images.
===
Receptive
field
===
In
neural
networks,
each
neuron
receives
input
from
some
number
of
locations
in
the
previous
layer.
In
a
convolutional
layer,
each
neuron
receives
input
from
only
a
restricted
area
of
the
previous
layer
called
the
neuron's
receptive
field.
Typically
the
area
is
a
square
(e.g.
5
by
5
neurons).
Whereas,
in
a
fully
connected
layer,
the
receptive
field
is
the
entire
previous
layer.
Thus,
in
each
convolutional
layer,
each
neuron
takes
input
from
a
larger
area
in
the
input
than
previous
layers.
This
is
due
to
applying
the
convolution
over
and
over,
which
takes
the
value
of
a
pixel
into
account,
as
well
as
its
surrounding
pixels.
When
using
dilated
layers,
the
number
of
pixels
in
the
receptive
field
remains
constant,
but
the
field
is
more
sparsely
populated
as
its
dimensions
grow
when
combining
the
effect
of
several
layers.
To
manipulate
the
receptive
field
size
as
desired,
there
are
some
alternatives
to
the
standard
convolutional
layer.
For
example,
atrous
or
dilated
convolution
expands
the
receptive
field
size
without
increasing
the
number
of
parameters
by
interleaving
visible
and
blind
regions.
Moreover,
a
single
dilated
convolutional
layer
can
comprise
filters
with
multiple
dilation
ratios,
thus
having
a
variable
receptive
field
size.
===
Weights
===
Each
neuron
in
a
neural
network
computes
an
output
value
by
applying
a
specific
function
to
the
input
values
received
from
the
receptive
field
in
the
previous
layer.
The
function
that
is
applied
to
the
input
values
is
determined
by
a
vector
of
weights
and
a
bias
(typically
real
numbers).
Learning
consists
of
iteratively
adjusting
these
biases
and
weights.
The
vectors
of
weights
and
biases
are
called
filters
and
represent
particular
features
of
the
input
(e.g.,
a
particular
shape).
A
distinguishing
feature
of
CNNs
is
that
many
neurons
can
share
the
same
filter.
This
reduces
the
memory
footprint
because
a
single
bias
and
a
single
vector
of
weights
are
used
across
all
receptive
fields
that
share
that
filter,
as
opposed
to
each
receptive
field
having
its
own
bias
and
vector
weighting.
==
History
==
CNN
are
often
compared
to
the
way
the
brain
achieves
vision
processing
in
living
organisms.
===
Receptive
fields
in
the
visual
cortex
===
Work
by
Hubel
and
Wiesel
in
the
1950s
and
1960s
showed
that
cat
visual
cortices
contain
neurons
that
individually
respond
to
small
regions
of
the
visual
field.
Provided
the
eyes
are
not
moving,
the
region
of
visual
space
within
which
visual
stimuli
affect
the
firing
of
a
single
neuron
is
known
as
its
receptive
field.
Neighboring
cells
have
similar
and
overlapping
receptive
fields.
Receptive
field
size
and
location
varies
systematically
across
the
cortex
to
form
a
complete
map
of
visual
space.
The
cortex
in
each
hemisphere
represents
the
contralateral
visual
field.Their
1968
paper
identified
two
basic
visual
cell
types
in
the
brain:
simple
cells,
whose
output
is
maximized
by
straight
edges
having
particular
orientations
within
their
receptive
field
complex
cells,
which
have
larger
receptive
fields,
whose
output
is
insensitive
to
the
exact
position
of
the
edges
in
the
field.Hubel
and
Wiesel
also
proposed
a
cascading
model
of
these
two
types
of
cells
for
use
in
pattern
recognition
tasks.
===
Neocognitron,
origin
of
the
CNN
architecture
===
The
"neocognitron"
was
introduced
by
Kunihiko
Fukushima
in
1980.
It
was
inspired
by
the
above-mentioned
work
of
Hubel
and
Wiesel.
The
neocognitron
introduced
the
two
basic
types
of
layers
in
CNNs:
A
convolutional
layer
which
contains
units
whose
receptive
fields
cover
a
patch
of
the
previous
layer.
The
weight
vector
(the
set
of
adaptive
parameters)
of
such
a
unit
is
often
called
a
filter.
Units
can
share
filters.
Downsampling
layers
which
contain
units
whose
receptive
fields
cover
patches
of
previous
convolutional
layers.
Such
a
unit
typically
computes
the
average
of
the
activations
of
the
units
in
its
patch.
This
downsampling
helps
to
correctly
classify
objects
in
visual
scenes
even
when
the
objects
are
shifted.In
1969,
Kunihiko
Fukushima
also
introduced
the
ReLU
(rectified
linear
unit)
activation
function.
The
rectifier
has
become
the
most
popular
activation
function
for
CNNs
and
deep
neural
networks
in
general.In
a
variant
of
the
neocognitron
called
the
cresceptron,
instead
of
using
Fukushima's
spatial
averaging,
J.
Weng
et
al.
in
1993
introduced
a
method
called
max-pooling
where
a
downsampling
unit
computes
the
maximum
of
the
activations
of
the
units
in
its
patch.
Max-pooling
is
often
used
in
modern
CNNs.Several
supervised
and
unsupervised
learning
algorithms
have
been
proposed
over
the
decades
to
train
the
weights
of
a
neocognitron.
Today,
however,
the
CNN
architecture
is
usually
trained
through
backpropagation.
The
neocognitron
is
the
first
CNN
which
requires
units
located
at
multiple
network
positions
to
have
shared
weights.
Convolutional
neural
networks
were
presented
at
the
Neural
Information
Processing
Workshop
in
1987,
automatically
analyzing
time-varying
signals
by
replacing
learned
multiplication
with
convolution
in
time,
and
demonstrated
for
speech
recognition.
===
Time
delay
neural
networks
===
The
time
delay
neural
network
(TDNN)
was
introduced
in
1987
by
Alex
Waibel
et
al.
for
phoneme
recognition
and
was
one
of
the
first
convolutional
networks,
as
it
achieved
shift-invariance.
A
TDNN
is
a
1-D
convolutional
neural
net
where
the
convolution
is
performed
along
the
time
axis
of
the
data.
It
is
the
first
CNN
utilizing
weight
sharing
in
combination
with
a
training
by
gradient
descent,
using
backpropagation.
Thus,
while
also
using
a
pyramidal
structure
as
in
the
neocognitron,
it
performed
a
global
optimization
of
the
weights
instead
of
a
local
one..
TDNNs
are
convolutional
networks
that
share
weights
along
the
temporal
dimension.
They
allow
speech
signals
to
be
processed
time-invariantly.
In
1990
Hampshire
and
Waibel
introduced
a
variant
that
performs
a
two-dimensional
convolution.
Since
these
TDNNs
operated
on
spectrograms,
the
resulting
phoneme
recognition
system
was
invariant
to
both
time
and
frequency
shifts.
This
inspired
translation
invariance
in
image
processing
with
CNNs.
The
tiling
of
neuron
outputs
can
cover
timed
stages.TDNNs
now
achieve
the
best
performance
in
far-distance
speech
recognition.
====
Max
pooling
====
In
1990
Yamaguchi
et
al.
introduced
the
concept
of
max
pooling,
a
fixed
filtering
operation
that
calculates
and
propagates
the
maximum
value
of
a
given
region.
They
did
so
by
combining
TDNNs
with
max
pooling
to
realize
a
speaker-independent
isolated
word
recognition
system.
In
their
system
they
used
several
TDNNs
per
word,
one
for
each
syllable.
The
results
of
each
TDNN
over
the
input
signal
were
combined
using
max
pooling
and
the
outputs
of
the
pooling
layers
were
then
passed
on
to
networks
performing
the
actual
word
classification.
===
Image
recognition
with
CNNs
trained
by
gradient
descent
===
Denker
et
al.
(1989)
designed
a
2-D
CNN
system
to
recognize
hand-written
ZIP
Code
numbers.
However,
the
lack
of
an
efficient
training
method
to
determine
the
kernel
coefficients
of
the
involved
convolutions
meant
that
all
the
coefficients
had
to
be
laboriously
hand-designed.Following
the
advances
in
the
training
of
1-D
CNNs
by
Waibel
et
al.
(1987),
Yann
LeCun
et
al.
(1989)
used
back-propagation
to
learn
the
convolution
kernel
coefficients
directly
from
images
of
hand-written
numbers.
Learning
was
thus
fully
automatic,
performed
better
than
manual
coefficient
design,
and
was
suited
to
a
broader
range
of
image
recognition
problems
and
image
types.
Wei
Zhang
et
al.
(1988)
used
back-propagation
to
train
the
convolution
kernels
of
a
CNN
for
alphabets
recognition.
The
model
was
called
Shift-Invariant
Artificial
Neural
Network
(SIANN)
before
the
name
CNN
was
coined
later
in
the
early
1990s.
Wei
Zhang
et
al.
also
applied
the
same
CNN
without
the
last
fully
connected
layer
for
medical
image
object
segmentation
(1991)
and
breast
cancer
detection
in
mammograms
(1994).This
approach
became
a
foundation
of
modern
computer
vision.
====
LeNet-5
====
LeNet-5,
a
pioneering
7-level
convolutional
network
by
LeCun
et
al.
in
1995,
that
classifies
digits,
was
applied
by
several
banks
to
recognize
hand-written
numbers
on
checks
(British
English:
cheques)
digitized
in
32x32
pixel
images.
The
ability
to
process
higher-resolution
images
requires
larger
and
more
layers
of
convolutional
neural
networks,
so
this
technique
is
constrained
by
the
availability
of
computing
resources.
===
Shift-invariant
neural
network
===
A
shift-invariant
neural
network
was
proposed
by
Wei
Zhang
et
al.
for
image
character
recognition
in
1988.
It
is
a
modified
Neocognitron
by
keeping
only
the
convolutional
interconnections
between
the
image
feature
layers
and
the
last
fully
connected
layer.
The
model
was
trained
with
back-propagation.
The
training
algorithm
were
further
improved
in
1991
to
improve
its
generalization
ability.
The
model
architecture
was
modified
by
removing
the
last
fully
connected
layer
and
applied
for
medical
image
segmentation
(1991)
and
automatic
detection
of
breast
cancer
in
mammograms
(1994).A
different
convolution-based
design
was
proposed
in
1988
for
application
to
decomposition
of
one-dimensional
electromyography
convolved
signals
via
de-convolution.
This
design
was
modified
in
1989
to
other
de-convolution-based
designs.
===
Neural
abstraction
pyramid
===
The
feed-forward
architecture
of
convolutional
neural
networks
was
extended
in
the
neural
abstraction
pyramid
by
lateral
and
feedback
connections.
The
resulting
recurrent
convolutional
network
allows
for
the
flexible
incorporation
of
contextual
information
to
iteratively
resolve
local
ambiguities.
In
contrast
to
previous
models,
image-like
outputs
at
the
highest
resolution
were
generated,
e.g.,
for
semantic
segmentation,
image
reconstruction,
and
object
localization
tasks.
===
GPU
implementations
===
Although
CNNs
were
invented
in
the
1980s,
their
breakthrough
in
the
2000s
required
fast
implementations
on
graphics
processing
units
(GPUs).
In
2004,
it
was
shown
by
K.
S.
Oh
and
K.
Jung
that
standard
neural
networks
can
be
greatly
accelerated
on
GPUs.
Their
implementation
was
20
times
faster
than
an
equivalent
implementation
on
CPU.
In
2005,
another
paper
also
emphasised
the
value
of
GPGPU
for
machine
learning.The
first
GPU-implementation
of
a
CNN
was
described
in
2006
by
K.
Chellapilla
et
al.
Their
implementation
was
4
times
faster
than
an
equivalent
implementation
on
CPU.
Subsequent
work
also
used
GPUs,
initially
for
other
types
of
neural
networks
(different
from
CNNs),
especially
unsupervised
neural
networks.In
2010,
Dan
Ciresan
et
al.
at
IDSIA
showed
that
even
deep
standard
neural
networks
with
many
layers
can
be
quickly
trained
on
GPU
by
supervised
learning
through
the
old
method
known
as
backpropagation.
Their
network
outperformed
previous
machine
learning
methods
on
the
MNIST
handwritten
digits
benchmark.
In
2011,
they
extended
this
GPU
approach
to
CNNs,
achieving
an
acceleration
factor
of
60,
with
impressive
results.
In
2011,
they
used
such
CNNs
on
GPU
to
win
an
image
recognition
contest
where
they
achieved
superhuman
performance
for
the
first
time.
Between
May
15,
2011,
and
September
30,
2012,
their
CNNs
won
no
less
than
four
image
competitions.
In
2012,
they
also
significantly
improved
on
the
best
performance
in
the
literature
for
multiple
image
databases,
including
the
MNIST
database,
the
NORB
database,
the
HWDB1.0
dataset
(Chinese
characters)
and
the
CIFAR10
dataset
(dataset
of
60000
32x32
labeled
RGB
images).Subsequently,
a
similar
GPU-based
CNN
by
Alex
Krizhevsky
et
al.
won
the
ImageNet
Large
Scale
Visual
Recognition
Challenge
2012.
A
very
deep
CNN
with
over
100
layers
by
Microsoft
won
the
ImageNet
2015
contest.
===
Intel
Xeon
Phi
implementations
===
Compared
to
the
training
of
CNNs
using
GPUs,
not
much
attention
was
given
to
the
Intel
Xeon
Phi
coprocessor.
A
notable
development
is
a
parallelization
method
for
training
convolutional
neural
networks
on
the
Intel
Xeon
Phi,
named
Controlled
Hogwild
with
Arbitrary
Order
of
Synchronization
(CHAOS).
CHAOS
exploits
both
the
thread-
and
SIMD-level
parallelism
that
is
available
on
the
Intel
Xeon
Phi.
==
Distinguishing
features
==
In
the
past,
traditional
multilayer
perceptron
(MLP)
models
were
used
for
image
recognition.
However,
the
full
connectivity
between
nodes
caused
the
curse
of
dimensionality,
and
was
computationally
intractable
with
higher-resolution
images.
A
1000×1000-pixel
image
with
RGB
color
channels
has
3
million
weights
per
fully-connected
neuron,
which
is
too
high
to
feasibly
process
efficiently
at
scale.
For
example,
in
CIFAR-10,
images
are
only
of
size
32×32×3
(32
wide,
32
high,
3
color
channels),
so
a
single
fully
connected
neuron
in
the
first
hidden
layer
of
a
regular
neural
network
would
have
32*32*3
=
3,072
weights.
A
200×200
image,
however,
would
lead
to
neurons
that
have
200*200*3
=
120,000
weights.
Also,
such
network
architecture
does
not
take
into
account
the
spatial
structure
of
data,
treating
input
pixels
which
are
far
apart
in
the
same
way
as
pixels
that
are
close
together.
This
ignores
locality
of
reference
in
data
with
a
grid-topology
(such
as
images),
both
computationally
and
semantically.
Thus,
full
connectivity
of
neurons
is
wasteful
for
purposes
such
as
image
recognition
that
are
dominated
by
spatially
local
input
patterns.
Convolutional
neural
networks
are
variants
of
multilayer
perceptrons,
designed
to
emulate
the
behavior
of
a
visual
cortex.
These
models
mitigate
the
challenges
posed
by
the
MLP
architecture
by
exploiting
the
strong
spatially
local
correlation
present
in
natural
images.
As
opposed
to
MLPs,
CNNs
have
the
following
distinguishing
features:
3D
volumes
of
neurons.
The
layers
of
a
CNN
have
neurons
arranged
in
3
dimensions:
width,
height
and
depth.
Where
each
neuron
inside
a
convolutional
layer
is
connected
to
only
a
small
region
of
the
layer
before
it,
called
a
receptive
field.
Distinct
types
of
layers,
both
locally
and
completely
connected,
are
stacked
to
form
a
CNN
architecture.
Local
connectivity:
following
the
concept
of
receptive
fields,
CNNs
exploit
spatial
locality
by
enforcing
a
local
connectivity
pattern
between
neurons
of
adjacent
layers.
The
architecture
thus
ensures
that
the
learned
"filters"
produce
the
strongest
response
to
a
spatially
local
input
pattern.
Stacking
many
such
layers
leads
to
nonlinear
filters
that
become
increasingly
global
(i.e.
responsive
to
a
larger
region
of
pixel
space)
so
that
the
network
first
creates
representations
of
small
parts
of
the
input,
then
from
them
assembles
representations
of
larger
areas.
Shared
weights:
In
CNNs,
each
filter
is
replicated
across
the
entire
visual
field.
These
replicated
units
share
the
same
parameterization
(weight
vector
and
bias)
and
form
a
feature
map.
This
means
that
all
the
neurons
in
a
given
convolutional
layer
respond
to
the
same
feature
within
their
specific
response
field.
Replicating
units
in
this
way
allows
for
the
resulting
activation
map
to
be
equivariant
under
shifts
of
the
locations
of
input
features
in
the
visual
field,
i.e.
they
grant
translational
equivariance
-
given
that
the
layer
has
a
stride
of
one.
Pooling:
In
a
CNN's
pooling
layers,
feature
maps
are
divided
into
rectangular
sub-regions,
and
the
features
in
each
rectangle
are
independently
down-sampled
to
a
single
value,
commonly
by
taking
their
average
or
maximum
value.
In
addition
to
reducing
the
sizes
of
feature
maps,
the
pooling
operation
grants
a
degree
of
local
translational
invariance
to
the
features
contained
therein,
allowing
the
CNN
to
be
more
robust
to
variations
in
their
positions.Together,
these
properties
allow
CNNs
to
achieve
better
generalization
on
vision
problems.
Weight
sharing
dramatically
reduces
the
number
of
free
parameters
learned,
thus
lowering
the
memory
requirements
for
running
the
network
and
allowing
the
training
of
larger,
more
powerful
networks.
==
Building
blocks
==
A
CNN
architecture
is
formed
by
a
stack
of
distinct
layers
that
transform
the
input
volume
into
an
output
volume
(e.g.
holding
the
class
scores)
through
a
differentiable
function.
A
few
distinct
types
of
layers
are
commonly
used.
These
are
further
discussed
below.
===
Convolutional
layer
===
The
convolutional
layer
is
the
core
building
block
of
a
CNN.
The
layer's
parameters
consist
of
a
set
of
learnable
filters
(or
kernels),
which
have
a
small
receptive
field,
but
extend
through
the
full
depth
of
the
input
volume.
During
the
forward
pass,
each
filter
is
convolved
across
the
width
and
height
of
the
input
volume,
computing
the
dot
product
between
the
filter
entries
and
the
input,
producing
a
2-dimensional
activation
map
of
that
filter.
As
a
result,
the
network
learns
filters
that
activate
when
it
detects
some
specific
type
of
feature
at
some
spatial
position
in
the
input.Stacking
the
activation
maps
for
all
filters
along
the
depth
dimension
forms
the
full
output
volume
of
the
convolution
layer.
Every
entry
in
the
output
volume
can
thus
also
be
interpreted
as
an
output
of
a
neuron
that
looks
at
a
small
region
in
the
input.
Each
entry
in
an
activation
map
use
the
same
set
of
parameters
that
define
the
filter.
Self-supervised
learning
has
been
adapted
for
use
in
convolutional
layers
by
using
sparse
patches
with
a
high-mask
ratio
and
a
global
response
normalization
layer.
====
Local
connectivity
====
When
dealing
with
high-dimensional
inputs
such
as
images,
it
is
impractical
to
connect
neurons
to
all
neurons
in
the
previous
volume
because
such
a
network
architecture
does
not
take
the
spatial
structure
of
the
data
into
account.
Convolutional
networks
exploit
spatially
local
correlation
by
enforcing
a
sparse
local
connectivity
pattern
between
neurons
of
adjacent
layers:
each
neuron
is
connected
to
only
a
small
region
of
the
input
volume.
The
extent
of
this
connectivity
is
a
hyperparameter
called
the
receptive
field
of
the
neuron.
The
connections
are
local
in
space
(along
width
and
height),
but
always
extend
along
the
entire
depth
of
the
input
volume.
Such
an
architecture
ensures
that
the
learned
(British
English:
learnt)
filters
produce
the
strongest
response
to
a
spatially
local
input
pattern.
====
Spatial
arrangement
====
Three
hyperparameters
control
the
size
of
the
output
volume
of
the
convolutional
layer:
the
depth,
stride,
and
padding
size:
The
depth
of
the
output
volume
controls
the
number
of
neurons
in
a
layer
that
connect
to
the
same
region
of
the
input
volume.
These
neurons
learn
to
activate
for
different
features
in
the
input.
For
example,
if
the
first
convolutional
layer
takes
the
raw
image
as
input,
then
different
neurons
along
the
depth
dimension
may
activate
in
the
presence
of
various
oriented
edges,
or
blobs
of
color.
Stride
controls
how
depth
columns
around
the
width
and
height
are
allocated.
If
the
stride
is
1,
then
we
move
the
filters
one
pixel
at
a
time.
This
leads
to
heavily
overlapping
receptive
fields
between
the
columns,
and
to
large
output
volumes.
For
any
integer
S>0,{\textstyle
S>0,}
a
stride
S
means
that
the
filter
is
translated
S
units
at
a
time
per
output.
In
practice,
S≥3{\textstyle
S\geq
3}
is
rare.
A
greater
stride
means
smaller
overlap
of
receptive
fields
and
smaller
spatial
dimensions
of
the
output
volume.
Sometimes,
it
is
convenient
to
pad
the
input
with
zeros
(or
other
values,
such
as
the
average
of
the
region)
on
the
border
of
the
input
volume.
The
size
of
this
padding
is
a
third
hyperparameter.
Padding
provides
control
of
the
output
volume's
spatial
size.
In
particular,
sometimes
it
is
desirable
to
exactly
preserve
the
spatial
size
of
the
input
volume,
this
is
commonly
referred
to
as
"same"
padding.The
spatial
size
of
the
output
volume
is
a
function
of
the
input
volume
size
W{\displaystyle
W},
the
kernel
field
size
K{\displaystyle
K}
of
the
convolutional
layer
neurons,
the
stride
S{\displaystyle
S},
and
the
amount
of
zero
padding
P{\displaystyle
P}
on
the
border.
The
number
of
neurons
that
"fit"
in
a
given
volume
is
then:
If
this
number
is
not
an
integer,
then
the
strides
are
incorrect
and
the
neurons
cannot
be
tiled
to
fit
across
the
input
volume
in
a
symmetric
way.
In
general,
setting
zero
padding
to
be
P=(K−1)/2{\textstyle
P=(K-1)/2}
when
the
stride
is
S=1{\displaystyle
S=1}
ensures
that
the
input
volume
and
output
volume
will
have
the
same
size
spatially.
However,
it
is
not
always
completely
necessary
to
use
all
of
the
neurons
of
the
previous
layer.
For
example,
a
neural
network
designer
may
decide
to
use
just
a
portion
of
padding.
====
Parameter
sharing
====
A
parameter
sharing
scheme
is
used
in
convolutional
layers
to
control
the
number
of
free
parameters.
It
relies
on
the
assumption
that
if
a
patch
feature
is
useful
to
compute
at
some
spatial
position,
then
it
should
also
be
useful
to
compute
at
other
positions.
Denoting
a
single
2-dimensional
slice
of
depth
as
a
depth
slice,
the
neurons
in
each
depth
slice
are
constrained
to
use
the
same
weights
and
bias.
Since
all
neurons
in
a
single
depth
slice
share
the
same
parameters,
the
forward
pass
in
each
depth
slice
of
the
convolutional
layer
can
be
computed
as
a
convolution
of
the
neuron's
weights
with
the
input
volume.
Therefore,
it
is
common
to
refer
to
the
sets
of
weights
as
a
filter
(or
a
kernel),
which
is
convolved
with
the
input.
The
result
of
this
convolution
is
an
activation
map,
and
the
set
of
activation
maps
for
each
different
filter
are
stacked
together
along
the
depth
dimension
to
produce
the
output
volume.
Parameter
sharing
contributes
to
the
translation
invariance
of
the
CNN
architecture.Sometimes,
the
parameter
sharing
assumption
may
not
make
sense.
This
is
especially
the
case
when
the
input
images
to
a
CNN
have
some
specific
centered
structure;
for
which
we
expect
completely
different
features
to
be
learned
on
different
spatial
locations.
One
practical
example
is
when
the
inputs
are
faces
that
have
been
centered
in
the
image:
we
might
expect
different
eye-specific
or
hair-specific
features
to
be
learned
in
different
parts
of
the
image.
In
that
case
it
is
common
to
relax
the
parameter
sharing
scheme,
and
instead
simply
call
the
layer
a
"locally
connected
layer".
===
Pooling
layer
===
Another
important
concept
of
CNNs
is
pooling,
which
is
a
form
of
non-linear
down-sampling.
There
are
several
non-linear
functions
to
implement
pooling,
where
max
pooling
is
the
most
common.
It
partitions
the
input
image
into
a
set
of
rectangles
and,
for
each
such
sub-region,
outputs
the
maximum.
Intuitively,
the
exact
location
of
a
feature
is
less
important
than
its
rough
location
relative
to
other
features.
This
is
the
idea
behind
the
use
of
pooling
in
convolutional
neural
networks.
The
pooling
layer
serves
to
progressively
reduce
the
spatial
size
of
the
representation,
to
reduce
the
number
of
parameters,
memory
footprint
and
amount
of
computation
in
the
network,
and
hence
to
also
control
overfitting.
This
is
known
as
down-sampling.
It
is
common
to
periodically
insert
a
pooling
layer
between
successive
convolutional
layers
(each
one
typically
followed
by
an
activation
function,
such
as
a
ReLU
layer)
in
a
CNN
architecture.:
460–461
While
pooling
layers
contribute
to
local
translation
invariance,
they
do
not
provide
global
translation
invariance
in
a
CNN,
unless
a
form
of
global
pooling
is
used.
The
pooling
layer
commonly
operates
independently
on
every
depth,
or
slice,
of
the
input
and
resizes
it
spatially.
A
very
common
form
of
max
pooling
is
a
layer
with
filters
of
size
2×2,
applied
with
a
stride
of
2,
which
subsamples
every
depth
slice
in
the
input
by
2
along
both
width
and
height,
discarding
75%
of
the
activations:
In
this
case,
every
max
operation
is
over
4
numbers.
The
depth
dimension
remains
unchanged
(this
is
true
for
other
forms
of
pooling
as
well).
In
addition
to
max
pooling,
pooling
units
can
use
other
functions,
such
as
average
pooling
or
ℓ2-norm
pooling.
Average
pooling
was
often
used
historically
but
has
recently
fallen
out
of
favor
compared
to
max
pooling,
which
generally
performs
better
in
practice.Due
to
the
effects
of
fast
spatial
reduction
of
the
size
of
the
representation,
there
is
a
recent
trend
towards
using
smaller
filters
or
discarding
pooling
layers
altogether.
"Region
of
Interest"
pooling
(also
known
as
RoI
pooling)
is
a
variant
of
max
pooling,
in
which
output
size
is
fixed
and
input
rectangle
is
a
parameter.Pooling
is
a
downsampling
method
and
an
important
component
of
convolutional
neural
networks
for
object
detection
based
on
the
Fast
R-CNN
architecture.
===
Channel
Max
Pooling
===
A
CMP
operation
layer
conducts
the
MP
operation
along
the
channel
side
among
the
corresponding
positions
of
the
consecutive
feature
maps
for
the
purpose
of
redundant
information
elimination.
The
CMP
makes
the
significant
features
gather
together
within
fewer
channels,
which
is
important
for
fine-grained
image
classification
that
needs
more
discriminating
features.
Meanwhile,
another
advantage
of
the
CMP
operation
is
to
make
the
channel
number
of
feature
maps
smaller
before
it
connects
to
the
first
fully
connected
(FC)
layer.
Similar
to
the
MP
operation,
we
denote
the
input
feature
maps
and
output
feature
maps
of
a
CMP
layer
as
F
∈
R(C×M×N)
and
C
∈
R(c×M×N),
respectively,
where
C
and
c
are
the
channel
numbers
of
the
input
and
output
feature
maps,
M
and
N
are
the
widths
and
the
height
of
the
feature
maps,
respectively.
Note
that
the
CMP
operation
only
changes
the
channel
number
of
the
feature
maps.
The
width
and
the
height
of
the
feature
maps
are
not
changed,
which
is
different
from
the
MP
operation.
===
ReLU
layer
===
ReLU
is
the
abbreviation
of
rectified
linear
unit
introduced
by
Kunihiko
Fukushima
in
1969.
ReLU
applies
the
non-saturating
activation
function
f(x)=max(0,x){\textstyle
f(x)=\max(0,x)}.
It
effectively
removes
negative
values
from
an
activation
map
by
setting
them
to
zero.
It
introduces
nonlinearity
to
the
decision
function
and
in
the
overall
network
without
affecting
the
receptive
fields
of
the
convolution
layers.
In
2011,
Xavier
Glorot,
Antoine
Bordes
and
Yoshua
Bengio
found
that
ReLU
enables
better
training
of
deeper
networks,
compared
to
widely
used
activation
functions
prior
to
2011.
Other
functions
can
also
be
used
to
increase
nonlinearity,
for
example
the
saturating
hyperbolic
tangent
f(x)=tanh⁡(x){\displaystyle
f(x)=\tanh(x)},
f(x)=|tanh⁡(x)|{\displaystyle
f(x)=|\tanh(x)|},
and
the
sigmoid
function
σ(x)=(1+e−x)−1{\textstyle
\sigma
(x)=(1+e^{-x})^{-1}}.
ReLU
is
often
preferred
to
other
functions
because
it
trains
the
neural
network
several
times
faster
without
a
significant
penalty
to
generalization
accuracy.
===
Fully
connected
layer
===
After
several
convolutional
and
max
pooling
layers,
the
final
classification
is
done
via
fully
connected
layers.
Neurons
in
a
fully
connected
layer
have
connections
to
all
activations
in
the
previous
layer,
as
seen
in
regular
(non-convolutional)
artificial
neural
networks.
Their
activations
can
thus
be
computed
as
an
affine
transformation,
with
matrix
multiplication
followed
by
a
bias
offset
(vector
addition
of
a
learned
or
fixed
bias
term).
===
Loss
layer
===
The
"loss
layer",
or
"loss
function",
specifies
how
training
penalizes
the
deviation
between
the
predicted
output
of
the
network,
and
the
true
data
labels
(during
supervised
learning).
Various
loss
functions
can
be
used,
depending
on
the
specific
task.
The
Softmax
loss
function
is
used
for
predicting
a
single
class
of
K
mutually
exclusive
classes.
Sigmoid
cross-entropy
loss
is
used
for
predicting
K
independent
probability
values
in
[0,1]{\displaystyle
[0,1]}.
Euclidean
loss
is
used
for
regressing
to
real-valued
labels
(−∞,∞){\displaystyle
(-\infty
,\infty
)}.
==
Hyperparameters
==
Hyperparameters
are
various
settings
that
are
used
to
control
the
learning
process.
CNNs
use
more
hyperparameters
than
a
standard
multilayer
perceptron
(MLP).
===
Kernel
size
===
The
kernel
is
the
number
of
pixels
processed
together.
It
is
typically
expressed
as
the
kernel's
dimensions,
e.g.,
2x2,
or
3x3.
===
Padding
===
Padding
is
the
addition
of
(typically)
0-valued
pixels
on
the
borders
of
an
image.
This
is
done
so
that
the
border
pixels
are
not
undervalued
(lost)
from
the
output
because
they
would
ordinarily
participate
in
only
a
single
receptive
field
instance.
The
padding
applied
is
typically
one
less
than
the
corresponding
kernel
dimension.
For
example,
a
convolutional
layer
using
3x3
kernels
would
receive
a
2-pixel
pad,
that
is
1
pixel
on
each
side
of
the
image.
===
Stride
===
The
stride
is
the
number
of
pixels
that
the
analysis
window
moves
on
each
iteration.
A
stride
of
2
means
that
each
kernel
is
offset
by
2
pixels
from
its
predecessor.
===
Number
of
filters
===
Since
feature
map
size
decreases
with
depth,
layers
near
the
input
layer
tend
to
have
fewer
filters
while
higher
layers
can
have
more.
To
equalize
computation
at
each
layer,
the
product
of
feature
values
va
with
pixel
position
is
kept
roughly
constant
across
layers.
Preserving
more
information
about
the
input
would
require
keeping
the
total
number
of
activations
(number
of
feature
maps
times
number
of
pixel
positions)
non-decreasing
from
one
layer
to
the
next.
The
number
of
feature
maps
directly
controls
the
capacity
and
depends
on
the
number
of
available
examples
and
task
complexity.
===
Filter
size
===
Common
filter
sizes
found
in
the
literature
vary
greatly,
and
are
usually
chosen
based
on
the
data
set.
The
challenge
is
to
find
the
right
level
of
granularity
so
as
to
create
abstractions
at
the
proper
scale,
given
a
particular
data
set,
and
without
overfitting.
===
Pooling
type
and
size
===
Max
pooling
is
typically
used,
often
with
a
2x2
dimension.
This
implies
that
the
input
is
drastically
downsampled,
reducing
processing
cost.
Greater
pooling
reduces
the
dimension
of
the
signal,
and
may
result
in
unacceptable
information
loss.
Often,
non-overlapping
pooling
windows
perform
best.
===
Dilation
===
Dilation
involves
ignoring
pixels
within
a
kernel.
This
reduces
processing/memory
potentially
without
significant
signal
loss.
A
dilation
of
2
on
a
3x3
kernel
expands
the
kernel
to
5x5,
while
still
processing
9
(evenly
spaced)
pixels.
Accordingly,
dilation
of
4
expands
the
kernel
to
7x7.
==
Translation
equivariance
and
aliasing
==
It
is
commonly
assumed
that
CNNs
are
invariant
to
shifts
of
the
input.
Convolution
or
pooling
layers
within
a
CNN
that
do
not
have
a
stride
greater
than
one
are
indeed
equivariant
to
translations
of
the
input.
However,
layers
with
a
stride
greater
than
one
ignore
the
Nyquist-Shannon
sampling
theorem
and
might
lead
to
aliasing
of
the
input
signal
While,
in
principle,
CNNs
are
capable
of
implementing
anti-aliasing
filters,
it
has
been
observed
that
this
does
not
happen
in
practice
and
yield
models
that
are
not
equivariant
to
translations.
Furthermore,
if
a
CNN
makes
use
of
fully
connected
layers,
translation
equivariance
does
not
imply
translation
invariance,
as
the
fully
connected
layers
are
not
invariant
to
shifts
of
the
input.
One
solution
for
complete
translation
invariance
is
avoiding
any
down-sampling
throughout
the
network
and
applying
global
average
pooling
at
the
last
layer.
Additionally,
several
other
partial
solutions
have
been
proposed,
such
as
anti-aliasing
before
downsampling
operations,
spatial
transformer
networks,
data
augmentation,
subsampling
combined
with
pooling,
and
capsule
neural
networks.
==
Evaluation
==
The
accuracy
of
the
final
model
is
based
on
a
sub-part
of
the
dataset
set
apart
at
the
start,
often
called
a
test-set.
Other
times
methods
such
as
k-fold
cross-validation
are
applied.
Other
strategies
include
using
conformal
prediction.
==
Regularization
methods
==
Regularization
is
a
process
of
introducing
additional
information
to
solve
an
ill-posed
problem
or
to
prevent
overfitting.
CNNs
use
various
types
of
regularization.
===
Empirical
===
====
Dropout
====
Because
a
fully
connected
layer
occupies
most
of
the
parameters,
it
is
prone
to
overfitting.
One
method
to
reduce
overfitting
is
dropout,
introduced
in
2014.
At
each
training
stage,
individual
nodes
are
either
"dropped
out"
of
the
net
(ignored)
with
probability
1−p{\displaystyle
1-p}
or
kept
with
probability
p{\displaystyle
p},
so
that
a
reduced
network
is
left;
incoming
and
outgoing
edges
to
a
dropped-out
node
are
also
removed.
Only
the
reduced
network
is
trained
on
the
data
in
that
stage.
The
removed
nodes
are
then
reinserted
into
the
network
with
their
original
weights.
In
the
training
stages,
p{\displaystyle
p}
is
usually
0.5;
for
input
nodes,
it
is
typically
much
higher
because
information
is
directly
lost
when
input
nodes
are
ignored.
At
testing
time
after
training
has
finished,
we
would
ideally
like
to
find
a
sample
average
of
all
possible
2n{\displaystyle
2^{n}}
dropped-out
networks;
unfortunately
this
is
unfeasible
for
large
values
of
n{\displaystyle
n}.
However,
we
can
find
an
approximation
by
using
the
full
network
with
each
node's
output
weighted
by
a
factor
of
p{\displaystyle
p},
so
the
expected
value
of
the
output
of
any
node
is
the
same
as
in
the
training
stages.
This
is
the
biggest
contribution
of
the
dropout
method:
although
it
effectively
generates
2n{\displaystyle
2^{n}}
neural
nets,
and
as
such
allows
for
model
combination,
at
test
time
only
a
single
network
needs
to
be
tested.
By
avoiding
training
all
nodes
on
all
training
data,
dropout
decreases
overfitting.
The
method
also
significantly
improves
training
speed.
This
makes
the
model
combination
practical,
even
for
deep
neural
networks.
The
technique
seems
to
reduce
node
interactions,
leading
them
to
learn
more
robust
features
that
better
generalize
to
new
data.
====
DropConnect
====
DropConnect
is
the
generalization
of
dropout
in
which
each
connection,
rather
than
each
output
unit,
can
be
dropped
with
probability
1−p{\displaystyle
1-p}.
Each
unit
thus
receives
input
from
a
random
subset
of
units
in
the
previous
layer.DropConnect
is
similar
to
dropout
as
it
introduces
dynamic
sparsity
within
the
model,
but
differs
in
that
the
sparsity
is
on
the
weights,
rather
than
the
output
vectors
of
a
layer.
In
other
words,
the
fully
connected
layer
with
DropConnect
becomes
a
sparsely
connected
layer
in
which
the
connections
are
chosen
at
random
during
the
training
stage.
====
Stochastic
pooling
====
A
major
drawback
to
Dropout
is
that
it
does
not
have
the
same
benefits
for
convolutional
layers,
where
the
neurons
are
not
fully
connected.
Even
before
Dropout,
in
2013
a
technique
called
stochastic
pooling,
the
conventional
deterministic
pooling
operations
were
replaced
with
a
stochastic
procedure,
where
the
activation
within
each
pooling
region
is
picked
randomly
according
to
a
multinomial
distribution,
given
by
the
activities
within
the
pooling
region.
This
approach
is
free
of
hyperparameters
and
can
be
combined
with
other
regularization
approaches,
such
as
dropout
and
data
augmentation.
An
alternate
view
of
stochastic
pooling
is
that
it
is
equivalent
to
standard
max
pooling
but
with
many
copies
of
an
input
image,
each
having
small
local
deformations.
This
is
similar
to
explicit
elastic
deformations
of
the
input
images,
which
delivers
excellent
performance
on
the
MNIST
data
set.
Using
stochastic
pooling
in
a
multilayer
model
gives
an
exponential
number
of
deformations
since
the
selections
in
higher
layers
are
independent
of
those
below.
====
Artificial
data
====
Because
the
degree
of
model
overfitting
is
determined
by
both
its
power
and
the
amount
of
training
it
receives,
providing
a
convolutional
network
with
more
training
examples
can
reduce
overfitting.
Because
there
is
often
not
enough
available
data
to
train,
especially
considering
that
some
part
should
be
spared
for
later
testing,
two
approaches
are
to
either
generate
new
data
from
scratch
(if
possible)
or
perturb
existing
data
to
create
new
ones.
The
latter
one
is
used
since
mid-1990s.
For
example,
input
images
can
be
cropped,
rotated,
or
rescaled
to
create
new
examples
with
the
same
labels
as
the
original
training
set.
===
Explicit
===
====
Early
stopping
====
One
of
the
simplest
methods
to
prevent
overfitting
of
a
network
is
to
simply
stop
the
training
before
overfitting
has
had
a
chance
to
occur.
It
comes
with
the
disadvantage
that
the
learning
process
is
halted.
====
Number
of
parameters
====
Another
simple
way
to
prevent
overfitting
is
to
limit
the
number
of
parameters,
typically
by
limiting
the
number
of
hidden
units
in
each
layer
or
limiting
network
depth.
For
convolutional
networks,
the
filter
size
also
affects
the
number
of
parameters.
Limiting
the
number
of
parameters
restricts
the
predictive
power
of
the
network
directly,
reducing
the
complexity
of
the
function
that
it
can
perform
on
the
data,
and
thus
limits
the
amount
of
overfitting.
This
is
equivalent
to
a
"zero
norm".
====
Weight
decay
====
A
simple
form
of
added
regularizer
is
weight
decay,
which
simply
adds
an
additional
error,
proportional
to
the
sum
of
weights
(L1
norm)
or
squared
magnitude
(L2
norm)
of
the
weight
vector,
to
the
error
at
each
node.
The
level
of
acceptable
model
complexity
can
be
reduced
by
increasing
the
proportionality
constant('alpha'
hyperparameter),
thus
increasing
the
penalty
for
large
weight
vectors.
L2
regularization
is
the
most
common
form
of
regularization.
It
can
be
implemented
by
penalizing
the
squared
magnitude
of
all
parameters
directly
in
the
objective.
The
L2
regularization
has
the
intuitive
interpretation
of
heavily
penalizing
peaky
weight
vectors
and
preferring
diffuse
weight
vectors.
Due
to
multiplicative
interactions
between
weights
and
inputs
this
has
the
useful
property
of
encouraging
the
network
to
use
all
of
its
inputs
a
little
rather
than
some
of
its
inputs
a
lot.
L1
regularization
is
also
common.
It
makes
the
weight
vectors
sparse
during
optimization.
In
other
words,
neurons
with
L1
regularization
end
up
using
only
a
sparse
subset
of
their
most
important
inputs
and
become
nearly
invariant
to
the
noisy
inputs.
L1
with
L2
regularization
can
be
combined;
this
is
called
elastic
net
regularization.
====
Max
norm
constraints
====
Another
form
of
regularization
is
to
enforce
an
absolute
upper
bound
on
the
magnitude
of
the
weight
vector
for
every
neuron
and
use
projected
gradient
descent
to
enforce
the
constraint.
In
practice,
this
corresponds
to
performing
the
parameter
update
as
normal,
and
then
enforcing
the
constraint
by
clamping
the
weight
vector
w→{\displaystyle
{\vec
{w}}}
of
every
neuron
to
satisfy
‖w→‖2<c{\displaystyle
\|{\vec
{w}}\|_{2}<c}.
Typical
values
of
c{\displaystyle
c}
are
order
of
3–4.
Some
papers
report
improvements
when
using
this
form
of
regularization.
==
Hierarchical
coordinate
frames
==
Pooling
loses
the
precise
spatial
relationships
between
high-level
parts
(such
as
nose
and
mouth
in
a
face
image).
These
relationships
are
needed
for
identity
recognition.
Overlapping
the
pools
so
that
each
feature
occurs
in
multiple
pools,
helps
retain
the
information.
Translation
alone
cannot
extrapolate
the
understanding
of
geometric
relationships
to
a
radically
new
viewpoint,
such
as
a
different
orientation
or
scale.
On
the
other
hand,
people
are
very
good
at
extrapolating;
after
seeing
a
new
shape
once
they
can
recognize
it
from
a
different
viewpoint.An
earlier
common
way
to
deal
with
this
problem
is
to
train
the
network
on
transformed
data
in
different
orientations,
scales,
lighting,
etc.
so
that
the
network
can
cope
with
these
variations.
This
is
computationally
intensive
for
large
data-sets.
The
alternative
is
to
use
a
hierarchy
of
coordinate
frames
and
use
a
group
of
neurons
to
represent
a
conjunction
of
the
shape
of
the
feature
and
its
pose
relative
to
the
retina.
The
pose
relative
to
the
retina
is
the
relationship
between
the
coordinate
frame
of
the
retina
and
the
intrinsic
features'
coordinate
frame.Thus,
one
way
to
represent
something
is
to
embed
the
coordinate
frame
within
it.
This
allows
large
features
to
be
recognized
by
using
the
consistency
of
the
poses
of
their
parts
(e.g.
nose
and
mouth
poses
make
a
consistent
prediction
of
the
pose
of
the
whole
face).
This
approach
ensures
that
the
higher-level
entity
(e.g.
face)
is
present
when
the
lower-level
(e.g.
nose
and
mouth)
agree
on
its
prediction
of
the
pose.
The
vectors
of
neuronal
activity
that
represent
pose
("pose
vectors")
allow
spatial
transformations
modeled
as
linear
operations
that
make
it
easier
for
the
network
to
learn
the
hierarchy
of
visual
entities
and
generalize
across
viewpoints.
This
is
similar
to
the
way
the
human
visual
system
imposes
coordinate
frames
in
order
to
represent
shapes.
==
Applications
==
===
Image
recognition
===
CNNs
are
often
used
in
image
recognition
systems.
In
2012,
an
error
rate
of
0.23%
on
the
MNIST
database
was
reported.
Another
paper
on
using
CNN
for
image
classification
reported
that
the
learning
process
was
"surprisingly
fast";
in
the
same
paper,
the
best
published
results
as
of
2011
were
achieved
in
the
MNIST
database
and
the
NORB
database.
Subsequently,
a
similar
CNN
called
AlexNet
won
the
ImageNet
Large
Scale
Visual
Recognition
Challenge
2012.
When
applied
to
facial
recognition,
CNNs
achieved
a
large
decrease
in
error
rate.
Another
paper
reported
a
97.6%
recognition
rate
on
"5,600
still
images
of
more
than
10
subjects".
CNNs
were
used
to
assess
video
quality
in
an
objective
way
after
manual
training;
the
resulting
system
had
a
very
low
root
mean
square
error.The
ImageNet
Large
Scale
Visual
Recognition
Challenge
is
a
benchmark
in
object
classification
and
detection,
with
millions
of
images
and
hundreds
of
object
classes.
In
the
ILSVRC
2014,
a
large-scale
visual
recognition
challenge,
almost
every
highly
ranked
team
used
CNN
as
their
basic
framework.
The
winner
GoogLeNet
(the
foundation
of
DeepDream)
increased
the
mean
average
precision
of
object
detection
to
0.439329,
and
reduced
classification
error
to
0.06656,
the
best
result
to
date.
Its
network
applied
more
than
30
layers.
That
performance
of
convolutional
neural
networks
on
the
ImageNet
tests
was
close
to
that
of
humans.
The
best
algorithms
still
struggle
with
objects
that
are
small
or
thin,
such
as
a
small
ant
on
a
stem
of
a
flower
or
a
person
holding
a
quill
in
their
hand.
They
also
have
trouble
with
images
that
have
been
distorted
with
filters,
an
increasingly
common
phenomenon
with
modern
digital
cameras.
By
contrast,
those
kinds
of
images
rarely
trouble
humans.
Humans,
however,
tend
to
have
trouble
with
other
issues.
For
example,
they
are
not
good
at
classifying
objects
into
fine-grained
categories
such
as
the
particular
breed
of
dog
or
species
of
bird,
whereas
convolutional
neural
networks
handle
this.In
2015,
a
many-layered
CNN
demonstrated
the
ability
to
spot
faces
from
a
wide
range
of
angles,
including
upside
down,
even
when
partially
occluded,
with
competitive
performance.
The
network
was
trained
on
a
database
of
200,000
images
that
included
faces
at
various
angles
and
orientations
and
a
further
20
million
images
without
faces.
They
used
batches
of
128
images
over
50,000
iterations.
===
Video
analysis
===
Compared
to
image
data
domains,
there
is
relatively
little
work
on
applying
CNNs
to
video
classification.
Video
is
more
complex
than
images
since
it
has
another
(temporal)
dimension.
However,
some
extensions
of
CNNs
into
the
video
domain
have
been
explored.
One
approach
is
to
treat
space
and
time
as
equivalent
dimensions
of
the
input
and
perform
convolutions
in
both
time
and
space.
Another
way
is
to
fuse
the
features
of
two
convolutional
neural
networks,
one
for
the
spatial
and
one
for
the
temporal
stream.
Long
short-term
memory
(LSTM)
recurrent
units
are
typically
incorporated
after
the
CNN
to
account
for
inter-frame
or
inter-clip
dependencies.
Unsupervised
learning
schemes
for
training
spatio-temporal
features
have
been
introduced,
based
on
Convolutional
Gated
Restricted
Boltzmann
Machines
and
Independent
Subspace
Analysis.
It's
Application
can
be
seen
in
Text-to-Video
model.
===
Natural
language
processing
===
CNNs
have
also
been
explored
for
natural
language
processing.
CNN
models
are
effective
for
various
NLP
problems
and
achieved
excellent
results
in
semantic
parsing,
search
query
retrieval,
sentence
modeling,
classification,
prediction
and
other
traditional
NLP
tasks.
Compared
to
traditional
language
processing
methods
such
as
recurrent
neural
networks,
CNNs
can
represent
different
contextual
realities
of
language
that
do
not
rely
on
a
series-sequence
assumption,
while
RNNs
are
better
suitable
when
classical
time
series
modeling
is
required.
===
Anomaly
Detection
===
A
CNN
with
1-D
convolutions
was
used
on
time
series
in
the
frequency
domain
(spectral
residual)
by
an
unsupervised
model
to
detect
anomalies
in
the
time
domain.
===
Drug
discovery
===
CNNs
have
been
used
in
drug
discovery.
Predicting
the
interaction
between
molecules
and
biological
proteins
can
identify
potential
treatments.
In
2015,
Atomwise
introduced
AtomNet,
the
first
deep
learning
neural
network
for
structure-based
drug
design.
The
system
trains
directly
on
3-dimensional
representations
of
chemical
interactions.
Similar
to
how
image
recognition
networks
learn
to
compose
smaller,
spatially
proximate
features
into
larger,
complex
structures,
AtomNet
discovers
chemical
features,
such
as
aromaticity,
sp3
carbons,
and
hydrogen
bonding.
Subsequently,
AtomNet
was
used
to
predict
novel
candidate
biomolecules
for
multiple
disease
targets,
most
notably
treatments
for
the
Ebola
virus
and
multiple
sclerosis.
===
Checkers
game
===
CNNs
have
been
used
in
the
game
of
checkers.
From
1999
to
2001,
Fogel
and
Chellapilla
published
papers
showing
how
a
convolutional
neural
network
could
learn
to
play
checker
using
co-evolution.
The
learning
process
did
not
use
prior
human
professional
games,
but
rather
focused
on
a
minimal
set
of
information
contained
in
the
checkerboard:
the
location
and
type
of
pieces,
and
the
difference
in
number
of
pieces
between
the
two
sides.
Ultimately,
the
program
(Blondie24)
was
tested
on
165
games
against
players
and
ranked
in
the
highest
0.4%.
It
also
earned
a
win
against
the
program
Chinook
at
its
"expert"
level
of
play.
===
Go
===
CNNs
have
been
used
in
computer
Go.
In
December
2014,
Clark
and
Storkey
published
a
paper
showing
that
a
CNN
trained
by
supervised
learning
from
a
database
of
human
professional
games
could
outperform
GNU
Go
and
win
some
games
against
Monte
Carlo
tree
search
Fuego
1.1
in
a
fraction
of
the
time
it
took
Fuego
to
play.
Later
it
was
announced
that
a
large
12-layer
convolutional
neural
network
had
correctly
predicted
the
professional
move
in
55%
of
positions,
equalling
the
accuracy
of
a
6
dan
human
player.
When
the
trained
convolutional
network
was
used
directly
to
play
games
of
Go,
without
any
search,
it
beat
the
traditional
search
program
GNU
Go
in
97%
of
games,
and
matched
the
performance
of
the
Monte
Carlo
tree
search
program
Fuego
simulating
ten
thousand
playouts
(about
a
million
positions)
per
move.A
couple
of
CNNs
for
choosing
moves
to
try
("policy
network")
and
evaluating
positions
("value
network")
driving
MCTS
were
used
by
AlphaGo,
the
first
to
beat
the
best
human
player
at
the
time.
===
Time
series
forecasting
===
Recurrent
neural
networks
are
generally
considered
the
best
neural
network
architectures
for
time
series
forecasting
(and
sequence
modeling
in
general),
but
recent
studies
show
that
convolutional
networks
can
perform
comparably
or
even
better.
Dilated
convolutions
might
enable
one-dimensional
convolutional
neural
networks
to
effectively
learn
time
series
dependences.
Convolutions
can
be
implemented
more
efficiently
than
RNN-based
solutions,
and
they
do
not
suffer
from
vanishing
(or
exploding)
gradients.
Convolutional
networks
can
provide
an
improved
forecasting
performance
when
there
are
multiple
similar
time
series
to
learn
from.
CNNs
can
also
be
applied
to
further
tasks
in
time
series
analysis
(e.g.,
time
series
classification
or
quantile
forecasting).
===
Cultural
Heritage
and
3D-datasets
===
As
archaeological
findings
like
clay
tablets
with
cuneiform
writing
are
increasingly
acquired
using
3D
scanners
first
benchmark
datasets
are
becoming
available
like
HeiCuBeDa
providing
almost
2.000
normalized
2D-
and
3D-datasets
prepared
with
the
GigaMesh
Software
Framework.
So
curvature-based
measures
are
used
in
conjunction
with
Geometric
Neural
Networks
(GNNs)
e.g.
for
period
classification
of
those
clay
tablets
being
among
the
oldest
documents
of
human
history.
==
Fine-tuning
==
For
many
applications,
the
training
data
is
less
available.
Convolutional
neural
networks
usually
require
a
large
amount
of
training
data
in
order
to
avoid
overfitting.
A
common
technique
is
to
train
the
network
on
a
larger
data
set
from
a
related
domain.
Once
the
network
parameters
have
converged
an
additional
training
step
is
performed
using
the
in-domain
data
to
fine-tune
the
network
weights,
this
is
known
as
transfer
learning.
Furthermore,
this
technique
allows
convolutional
network
architectures
to
successfully
be
applied
to
problems
with
tiny
training
sets.
==
Human
interpretable
explanations
==
End-to-end
training
and
prediction
are
common
practice
in
computer
vision.
However,
human
interpretable
explanations
are
required
for
critical
systems
such
as
a
self-driving
cars.
With
recent
advances
in
visual
salience,
spatial
attention,
and
temporal
attention,
the
most
critical
spatial
regions/temporal
instants
could
be
visualized
to
justify
the
CNN
predictions.
==
Related
architectures
==
===
Deep
Q-networks
===
A
deep
Q-network
(DQN)
is
a
type
of
deep
learning
model
that
combines
a
deep
neural
network
with
Q-learning,
a
form
of
reinforcement
learning.
Unlike
earlier
reinforcement
learning
agents,
DQNs
that
utilize
CNNs
can
learn
directly
from
high-dimensional
sensory
inputs
via
reinforcement
learning.Preliminary
results
were
presented
in
2014,
with
an
accompanying
paper
in
February
2015.
The
research
described
an
application
to
Atari
2600
gaming.
Other
deep
reinforcement
learning
models
preceded
it.
===
Deep
belief
networks
===
Convolutional
deep
belief
networks
(CDBN)
have
structure
very
similar
to
convolutional
neural
networks
and
are
trained
similarly
to
deep
belief
networks.
Therefore,
they
exploit
the
2D
structure
of
images,
like
CNNs
do,
and
make
use
of
pre-training
like
deep
belief
networks.
They
provide
a
generic
structure
that
can
be
used
in
many
image
and
signal
processing
tasks.
Benchmark
results
on
standard
image
datasets
like
CIFAR
have
been
obtained
using
CDBNs.
==
Notable
libraries
==
Caffe:
A
library
for
convolutional
neural
networks.
Created
by
the
Berkeley
Vision
and
Learning
Center
(BVLC).
It
supports
both
CPU
and
GPU.
Developed
in
C++,
and
has
Python
and
MATLAB
wrappers.
Deeplearning4j:
Deep
learning
in
Java
and
Scala
on
multi-GPU-enabled
Spark.
A
general-purpose
deep
learning
library
for
the
JVM
production
stack
running
on
a
C++
scientific
computing
engine.
Allows
the
creation
of
custom
layers.
Integrates
with
Hadoop
and
Kafka.
Dlib:
A
toolkit
for
making
real
world
machine
learning
and
data
analysis
applications
in
C++.
Microsoft
Cognitive
Toolkit:
A
deep
learning
toolkit
written
by
Microsoft
with
several
unique
features
enhancing
scalability
over
multiple
nodes.
It
supports
full-fledged
interfaces
for
training
in
C++
and
Python
and
with
additional
support
for
model
inference
in
C#
and
Java.
TensorFlow:
Apache
2.0-licensed
Theano-like
library
with
support
for
CPU,
GPU,
Google's
proprietary
tensor
processing
unit
(TPU),
and
mobile
devices.
Theano:
The
reference
deep-learning
library
for
Python
with
an
API
largely
compatible
with
the
popular
NumPy
library.
Allows
user
to
write
symbolic
mathematical
expressions,
then
automatically
generates
their
derivatives,
saving
the
user
from
having
to
code
gradients
or
backpropagation.
These
symbolic
expressions
are
automatically
compiled
to
CUDA
code
for
a
fast,
on-the-GPU
implementation.
Torch:
A
scientific
computing
framework
with
wide
support
for
machine
learning
algorithms,
written
in
C
and
Lua.
==
See
also
==
Attention
(machine
learning)
Convolution
Deep
learning
Natural-language
processing
Neocognitron
Scale-invariant
feature
transform
Time
delay
neural
network
Vision
processing
unit
==
Notes
==
==
References
==
==
External
links
==
CS231n:
Convolutional
Neural
Networks
for
Visual
Recognition
—
Andrej
Karpathy's
Stanford
computer
science
course
on
CNNs
in
computer
vision
Deep
learning
is
the
subset
of
machine
learning
methods
based
on
artificial
neural
networks
(ANNs)
with
representation
learning.
The
adjective
"deep"
refers
to
the
use
of
multiple
layers
in
the
network.
Methods
used
can
be
either
supervised,
semi-supervised
or
unsupervised.Deep-learning
architectures
such
as
deep
neural
networks,
deep
belief
networks,
recurrent
neural
networks,
convolutional
neural
networks
and
transformers
have
been
applied
to
fields
including
computer
vision,
speech
recognition,
natural
language
processing,
machine
translation,
bioinformatics,
drug
design,
medical
image
analysis,
climate
science,
material
inspection
and
board
game
programs,
where
they
have
produced
results
comparable
to
and
in
some
cases
surpassing
human
expert
performance.Artificial
neural
networks
were
inspired
by
information
processing
and
distributed
communication
nodes
in
biological
systems.
ANNs
have
various
differences
from
biological
brains.
Specifically,
artificial
neural
networks
tend
to
be
static
and
symbolic,
while
the
biological
brain
of
most
living
organisms
is
dynamic
(plastic)
and
analog.
ANNs
are
generally
seen
as
low
quality
models
for
brain
function.
==
Definition
==
Deep
learning
is
a
class
of
machine
learning
algorithms
that:
199–200
uses
multiple
layers
to
progressively
extract
higher-level
features
from
the
raw
input.
For
example,
in
image
processing,
lower
layers
may
identify
edges,
while
higher
layers
may
identify
the
concepts
relevant
to
a
human
such
as
digits
or
letters
or
faces.
From
another
angle
to
view
deep
learning,
deep
learning
refers
to
"computer-simulate"
or
"automate"
human
learning
processes
from
a
source
(e.g.,
an
image
of
dogs)
to
a
learned
object
(dogs).
Therefore,
a
notion
coined
as
"deeper"
learning
or
"deepest"
learning
makes
sense.
The
deepest
learning
refers
to
the
fully
automatic
learning
from
a
source
to
a
final
learned
object.
A
deeper
learning
thus
refers
to
a
mixed
learning
process:
a
human
learning
process
from
a
source
to
a
learned
semi-object,
followed
by
a
computer
learning
process
from
the
human
learned
semi-object
to
a
final
learned
object.
==
Overview
==
Most
modern
deep
learning
models
are
based
on
multi-layered
artificial
neural
networks
such
as
convolutional
neural
networks
and
transformers,
although
they
can
also
include
propositional
formulas
or
latent
variables
organized
layer-wise
in
deep
generative
models
such
as
the
nodes
in
deep
belief
networks
and
deep
Boltzmann
machines.In
deep
learning,
each
level
learns
to
transform
its
input
data
into
a
slightly
more
abstract
and
composite
representation.
In
an
image
recognition
application,
the
raw
input
may
be
a
matrix
of
pixels;
the
first
representational
layer
may
abstract
the
pixels
and
encode
edges;
the
second
layer
may
compose
and
encode
arrangements
of
edges;
the
third
layer
may
encode
a
nose
and
eyes;
and
the
fourth
layer
may
recognize
that
the
image
contains
a
face.
Importantly,
a
deep
learning
process
can
learn
which
features
to
optimally
place
in
which
level
on
its
own.
This
does
not
eliminate
the
need
for
hand-tuning;
for
example,
varying
numbers
of
layers
and
layer
sizes
can
provide
different
degrees
of
abstraction.The
word
"deep"
in
"deep
learning"
refers
to
the
number
of
layers
through
which
the
data
is
transformed.
More
precisely,
deep
learning
systems
have
a
substantial
credit
assignment
path
(CAP)
depth.
The
CAP
is
the
chain
of
transformations
from
input
to
output.
CAPs
describe
potentially
causal
connections
between
input
and
output.
For
a
feedforward
neural
network,
the
depth
of
the
CAPs
is
that
of
the
network
and
is
the
number
of
hidden
layers
plus
one
(as
the
output
layer
is
also
parameterized).
For
recurrent
neural
networks,
in
which
a
signal
may
propagate
through
a
layer
more
than
once,
the
CAP
depth
is
potentially
unlimited.
No
universally
agreed-upon
threshold
of
depth
divides
shallow
learning
from
deep
learning,
but
most
researchers
agree
that
deep
learning
involves
CAP
depth
higher
than
2.
CAP
of
depth
2
has
been
shown
to
be
a
universal
approximator
in
the
sense
that
it
can
emulate
any
function.
Beyond
that,
more
layers
do
not
add
to
the
function
approximator
ability
of
the
network.
Deep
models
(CAP
>
2)
are
able
to
extract
better
features
than
shallow
models
and
hence,
extra
layers
help
in
learning
the
features
effectively.
Deep
learning
architectures
can
be
constructed
with
a
greedy
layer-by-layer
method.
Deep
learning
helps
to
disentangle
these
abstractions
and
pick
out
which
features
improve
performance.For
supervised
learning
tasks,
deep
learning
methods
enable
elimination
of
feature
engineering,
by
translating
the
data
into
compact
intermediate
representations
akin
to
principal
components,
and
derive
layered
structures
that
remove
redundancy
in
representation.
Deep
learning
algorithms
can
be
applied
to
unsupervised
learning
tasks.
This
is
an
important
benefit
because
unlabeled
data
are
more
abundant
than
the
labeled
data.
Examples
of
deep
structures
that
can
be
trained
in
an
unsupervised
manner
are
deep
belief
networks.Machine
learning
models
are
now
adept
at
identifying
complex
patterns
in
financial
market
data.
Due
to
the
benefits
of
artificial
intelligence,
investors
are
increasingly
utilizing
deep
learning
techniques
to
forecast
and
analyze
trends
in
stock
and
foreign
exchange
markets.
==
Interpretations
==
Deep
neural
networks
are
generally
interpreted
in
terms
of
the
universal
approximation
theorem
or
probabilistic
inference.The
classic
universal
approximation
theorem
concerns
the
capacity
of
feedforward
neural
networks
with
a
single
hidden
layer
of
finite
size
to
approximate
continuous
functions.
In
1989,
the
first
proof
was
published
by
George
Cybenko
for
sigmoid
activation
functions
and
was
generalised
to
feed-forward
multi-layer
architectures
in
1991
by
Kurt
Hornik.
Recent
work
also
showed
that
universal
approximation
also
holds
for
non-bounded
activation
functions
such
as
Kunihiko
Fukushima's
rectified
linear
unit.The
universal
approximation
theorem
for
deep
neural
networks
concerns
the
capacity
of
networks
with
bounded
width
but
the
depth
is
allowed
to
grow.
Lu
et
al.
proved
that
if
the
width
of
a
deep
neural
network
with
ReLU
activation
is
strictly
larger
than
the
input
dimension,
then
the
network
can
approximate
any
Lebesgue
integrable
function;
if
the
width
is
smaller
or
equal
to
the
input
dimension,
then
a
deep
neural
network
is
not
a
universal
approximator.
The
probabilistic
interpretation
derives
from
the
field
of
machine
learning.
It
features
inference,
as
well
as
the
optimization
concepts
of
training
and
testing,
related
to
fitting
and
generalization,
respectively.
More
specifically,
the
probabilistic
interpretation
considers
the
activation
nonlinearity
as
a
cumulative
distribution
function.
The
probabilistic
interpretation
led
to
the
introduction
of
dropout
as
regularizer
in
neural
networks.
The
probabilistic
interpretation
was
introduced
by
researchers
including
Hopfield,
Widrow
and
Narendra
and
popularized
in
surveys
such
as
the
one
by
Bishop.
==
History
==
There
are
two
types
of
artificial
neural
network
(ANN):
feedforward
neural
networks
(FNNs)
and
recurrent
neural
networks
(RNNs).
RNNs
have
cycles
in
their
connectivity
structure,
FNNs
don't.
In
the
1920s,
Wilhelm
Lenz
and
Ernst
Ising
created
and
analyzed
the
Ising
model
which
is
essentially
a
non-learning
RNN
architecture
consisting
of
neuron-like
threshold
elements.
In
1972,
Shun'ichi
Amari
made
this
architecture
adaptive.
His
learning
RNN
was
popularised
by
John
Hopfield
in
1982.
RNNs
have
become
central
for
speech
recognition
and
language
processing.
Charles
Tappert
writes
that
Frank
Rosenblatt
developed
and
explored
all
of
the
basic
ingredients
of
the
deep
learning
systems
of
today,
referring
to
Rosenblatt's
1962
book
which
introduced
multilayer
perceptron
(MLP)
with
3
layers:
an
input
layer,
a
hidden
layer
with
randomized
weights
that
did
not
learn,
and
an
output
layer.
It
also
introduced
variants,
including
a
version
with
four-layer
perceptrons
where
the
last
two
layers
have
learned
weights
(and
thus
a
proper
multilayer
perceptron).:
section
16
In
addition,
term
deep
learning
was
proposed
in
1986
by
Rina
Dechter
although
the
history
of
its
appearance
is
apparently
more
complicated.The
first
general,
working
learning
algorithm
for
supervised,
deep,
feedforward,
multilayer
perceptrons
was
published
by
Alexey
Ivakhnenko
and
Lapa
in
1967.
A
1971
paper
described
a
deep
network
with
eight
layers
trained
by
the
group
method
of
data
handling.The
first
deep
learning
multilayer
perceptron
trained
by
stochastic
gradient
descent
was
published
in
1967
by
Shun'ichi
Amari.
In
computer
experiments
conducted
by
Amari's
student
Saito,
a
five
layer
MLP
with
two
modifiable
layers
learned
internal
representations
to
classify
non-linearily
separable
pattern
classes.
In
1987
Matthew
Brand
reported
that
wide
12-layer
nonlinear
perceptrons
could
be
fully
end-to-end
trained
to
reproduce
logic
functions
of
nontrivial
circuit
depth
via
gradient
descent
on
small
batches
of
random
input/output
samples,
but
concluded
that
training
time
on
contemporary
hardware
(sub-megaflop
computers)
made
the
technique
impractical,
and
proposed
using
fixed
random
early
layers
as
an
input
hash
for
a
single
modifiable
layer.
Instead,
subsequent
developments
in
hardware
and
hyperparameter
tunings
have
made
end-to-end
stochastic
gradient
descent
the
currently
dominant
training
technique.
In
1970,
Seppo
Linnainmaa
published
the
reverse
mode
of
automatic
differentiation
of
discrete
connected
networks
of
nested
differentiable
functions.
This
became
known
as
backpropagation.
It
is
an
efficient
application
of
the
chain
rule
derived
by
Gottfried
Wilhelm
Leibniz
in
1673
to
networks
of
differentiable
nodes.
The
terminology
"back-propagating
errors"
was
actually
introduced
in
1962
by
Rosenblatt,
but
he
did
not
know
how
to
implement
this,
although
Henry
J.
Kelley
had
a
continuous
precursor
of
backpropagation
already
in
1960
in
the
context
of
control
theory.
In
1982,
Paul
Werbos
applied
backpropagation
to
MLPs
in
the
way
that
has
become
standard.
In
1985,
David
E.
Rumelhart
et
al.
published
an
experimental
analysis
of
the
technique.Deep
learning
architectures
for
convolutional
neural
networks
(CNNs)
with
convolutional
layers
and
downsampling
layers
began
with
the
Neocognitron
introduced
by
Kunihiko
Fukushima
in
1980.
In
1969,
he
also
introduced
the
ReLU
(rectified
linear
unit)
activation
function.
The
rectifier
has
become
the
most
popular
activation
function
for
CNNs
and
deep
learning
in
general.
CNNs
have
become
an
essential
tool
for
computer
vision.
The
term
Deep
Learning
was
introduced
to
the
machine
learning
community
by
Rina
Dechter
in
1986,
and
to
artificial
neural
networks
by
Igor
Aizenberg
and
colleagues
in
2000,
in
the
context
of
Boolean
threshold
neurons.In
1988,
Wei
Zhang
et
al.
applied
the
backpropagation
algorithm
to
a
convolutional
neural
network
(a
simplified
Neocognitron
with
convolutional
interconnections
between
the
image
feature
layers
and
the
last
fully
connected
layer)
for
alphabet
recognition.
They
also
proposed
an
implementation
of
the
CNN
with
an
optical
computing
system.
In
1989,
Yann
LeCun
et
al.
applied
backpropagation
to
a
CNN
with
the
purpose
of
recognizing
handwritten
ZIP
codes
on
mail.
While
the
algorithm
worked,
training
required
3
days.
Subsequently,
Wei
Zhang,
et
al.
modified
their
model
by
removing
the
last
fully
connected
layer
and
applied
it
for
medical
image
object
segmentation
in
1991
and
breast
cancer
detection
in
mammograms
in
1994.
LeNet-5
(1998),
a
7-level
CNN
by
Yann
LeCun
et
al.,
that
classifies
digits,
was
applied
by
several
banks
to
recognize
hand-written
numbers
on
checks
digitized
in
32x32
pixel
images.
In
the
1980s,
backpropagation
did
not
work
well
for
deep
learning
with
long
credit
assignment
paths.
To
overcome
this
problem,
Jürgen
Schmidhuber
(1992)
proposed
a
hierarchy
of
RNNs
pre-trained
one
level
at
a
time
by
self-supervised
learning.
It
uses
predictive
coding
to
learn
internal
representations
at
multiple
self-organizing
time
scales.
This
can
substantially
facilitate
downstream
deep
learning.
The
RNN
hierarchy
can
be
collapsed
into
a
single
RNN,
by
distilling
a
higher
level
chunker
network
into
a
lower
level
automatizer
network.
In
1993,
a
chunker
solved
a
deep
learning
task
whose
depth
exceeded
1000.In
1992,
Jürgen
Schmidhuber
also
published
an
alternative
to
RNNs
which
is
now
called
a
linear
Transformer
or
a
Transformer
with
linearized
self-attention
(save
for
a
normalization
operator).
It
learns
internal
spotlights
of
attention:
a
slow
feedforward
neural
network
learns
by
gradient
descent
to
control
the
fast
weights
of
another
neural
network
through
outer
products
of
self-generated
activation
patterns
FROM
and
TO
(which
are
now
called
key
and
value
for
self-attention).
This
fast
weight
attention
mapping
is
applied
to
a
query
pattern.
The
modern
Transformer
was
introduced
by
Ashish
Vaswani
et
al.
in
their
2017
paper
"Attention
Is
All
You
Need".
It
combines
this
with
a
softmax
operator
and
a
projection
matrix.
Transformers
have
increasingly
become
the
model
of
choice
for
natural
language
processing.
Many
modern
large
language
models
such
as
ChatGPT,
GPT-4,
and
BERT
use
it.
Transformers
are
also
increasingly
being
used
in
computer
vision.In
1991,
Jürgen
Schmidhuber
also
published
adversarial
neural
networks
that
contest
with
each
other
in
the
form
of
a
zero-sum
game,
where
one
network's
gain
is
the
other
network's
loss.
The
first
network
is
a
generative
model
that
models
a
probability
distribution
over
output
patterns.
The
second
network
learns
by
gradient
descent
to
predict
the
reactions
of
the
environment
to
these
patterns.
This
was
called
"artificial
curiosity".
In
2014,
this
principle
was
used
in
a
generative
adversarial
network
(GAN)
by
Ian
Goodfellow
et
al.
Here
the
environmental
reaction
is
1
or
0
depending
on
whether
the
first
network's
output
is
in
a
given
set.
This
can
be
used
to
create
realistic
deepfakes.
Excellent
image
quality
is
achieved
by
Nvidia's
StyleGAN
(2018)
based
on
the
Progressive
GAN
by
Tero
Karras
et
al.
Here
the
GAN
generator
is
grown
from
small
to
large
scale
in
a
pyramidal
fashion.
Sepp
Hochreiter's
diploma
thesis
(1991)
was
called
"one
of
the
most
important
documents
in
the
history
of
machine
learning"
by
his
supervisor
Schmidhuber.
It
not
only
tested
the
neural
history
compressor,
but
also
identified
and
analyzed
the
vanishing
gradient
problem.
Hochreiter
proposed
recurrent
residual
connections
to
solve
this
problem.
This
led
to
the
deep
learning
method
called
long
short-term
memory
(LSTM),
published
in
1997.
LSTM
recurrent
neural
networks
can
learn
"very
deep
learning"
tasks
with
long
credit
assignment
paths
that
require
memories
of
events
that
happened
thousands
of
discrete
time
steps
before.
The
"vanilla
LSTM"
with
forget
gate
was
introduced
in
1999
by
Felix
Gers,
Schmidhuber
and
Fred
Cummins.
LSTM
has
become
the
most
cited
neural
network
of
the
20th
century.
In
2015,
Rupesh
Kumar
Srivastava,
Klaus
Greff,
and
Schmidhuber
used
LSTM
principles
to
create
the
Highway
network,
a
feedforward
neural
network
with
hundreds
of
layers,
much
deeper
than
previous
networks.
7
months
later,
Kaiming
He,
Xiangyu
Zhang;
Shaoqing
Ren,
and
Jian
Sun
won
the
ImageNet
2015
competition
with
an
open-gated
or
gateless
Highway
network
variant
called
Residual
neural
network.
This
has
become
the
most
cited
neural
network
of
the
21st
century.In
1994,
André
de
Carvalho,
together
with
Mike
Fairhurst
and
David
Bisset,
published
experimental
results
of
a
multi-layer
boolean
neural
network,
also
known
as
a
weightless
neural
network,
composed
of
a
3-layers
self-organising
feature
extraction
neural
network
module
(SOFT)
followed
by
a
multi-layer
classification
neural
network
module
(GSN),
which
were
independently
trained.
Each
layer
in
the
feature
extraction
module
extracted
features
with
growing
complexity
regarding
the
previous
layer.In
1995,
Brendan
Frey
demonstrated
that
it
was
possible
to
train
(over
two
days)
a
network
containing
six
fully
connected
layers
and
several
hundred
hidden
units
using
the
wake-sleep
algorithm,
co-developed
with
Peter
Dayan
and
Hinton.Since
1997,
Sven
Behnke
extended
the
feed-forward
hierarchical
convolutional
approach
in
the
Neural
Abstraction
Pyramid
by
lateral
and
backward
connections
in
order
to
flexibly
incorporate
context
into
decisions
and
iteratively
resolve
local
ambiguities.
Simpler
models
that
use
task-specific
handcrafted
features
such
as
Gabor
filters
and
support
vector
machines
(SVMs)
were
a
popular
choice
in
the
1990s
and
2000s,
because
of
artificial
neural
networks'
computational
cost
and
a
lack
of
understanding
of
how
the
brain
wires
its
biological
networks.
Both
shallow
and
deep
learning
(e.g.,
recurrent
nets)
of
ANNs
for
speech
recognition
have
been
explored
for
many
years.
These
methods
never
outperformed
non-uniform
internal-handcrafting
Gaussian
mixture
model/Hidden
Markov
model
(GMM-HMM)
technology
based
on
generative
models
of
speech
trained
discriminatively.
Key
difficulties
have
been
analyzed,
including
gradient
diminishing
and
weak
temporal
correlation
structure
in
neural
predictive
models.
Additional
difficulties
were
the
lack
of
training
data
and
limited
computing
power.
Most
speech
recognition
researchers
moved
away
from
neural
nets
to
pursue
generative
modeling.
An
exception
was
at
SRI
International
in
the
late
1990s.
Funded
by
the
US
government's
NSA
and
DARPA,
SRI
studied
deep
neural
networks
(DNNs)
in
speech
and
speaker
recognition.
The
speaker
recognition
team
led
by
Larry
Heck
reported
significant
success
with
deep
neural
networks
in
speech
processing
in
the
1998
National
Institute
of
Standards
and
Technology
Speaker
Recognition
evaluation.
The
SRI
deep
neural
network
was
then
deployed
in
the
Nuance
Verifier,
representing
the
first
major
industrial
application
of
deep
learning.
The
principle
of
elevating
"raw"
features
over
hand-crafted
optimization
was
first
explored
successfully
in
the
architecture
of
deep
autoencoder
on
the
"raw"
spectrogram
or
linear
filter-bank
features
in
the
late
1990s,
showing
its
superiority
over
the
Mel-Cepstral
features
that
contain
stages
of
fixed
transformation
from
spectrograms.
The
raw
features
of
speech,
waveforms,
later
produced
excellent
larger-scale
results.Speech
recognition
was
taken
over
by
LSTM.
In
2003,
LSTM
started
to
become
competitive
with
traditional
speech
recognizers
on
certain
tasks.
In
2006,
Alex
Graves,
Santiago
Fernández,
Faustino
Gomez,
and
Schmidhuber
combined
it
with
connectionist
temporal
classification
(CTC)
in
stacks
of
LSTM
RNNs.
In
2015,
Google's
speech
recognition
reportedly
experienced
a
dramatic
performance
jump
of
49%
through
CTC-trained
LSTM,
which
they
made
available
through
Google
Voice
Search.The
impact
of
deep
learning
in
industry
began
in
the
early
2000s,
when
CNNs
already
processed
an
estimated
10%
to
20%
of
all
the
checks
written
in
the
US,
according
to
Yann
LeCun.
Industrial
applications
of
deep
learning
to
large-scale
speech
recognition
started
around
2010.
In
2006,
publications
by
Geoff
Hinton,
Ruslan
Salakhutdinov,
Osindero
and
Teh
showed
how
a
many-layered
feedforward
neural
network
could
be
effectively
pre-trained
one
layer
at
a
time,
treating
each
layer
in
turn
as
an
unsupervised
restricted
Boltzmann
machine,
then
fine-tuning
it
using
supervised
backpropagation.
The
papers
referred
to
learning
for
deep
belief
nets.
The
2009
NIPS
Workshop
on
Deep
Learning
for
Speech
Recognition
was
motivated
by
the
limitations
of
deep
generative
models
of
speech,
and
the
possibility
that
given
more
capable
hardware
and
large-scale
data
sets
that
deep
neural
nets
might
become
practical.
It
was
believed
that
pre-training
DNNs
using
generative
models
of
deep
belief
nets
(DBN)
would
overcome
the
main
difficulties
of
neural
nets.
However,
it
was
discovered
that
replacing
pre-training
with
large
amounts
of
training
data
for
straightforward
backpropagation
when
using
DNNs
with
large,
context-dependent
output
layers
produced
error
rates
dramatically
lower
than
then-state-of-the-art
Gaussian
mixture
model
(GMM)/Hidden
Markov
Model
(HMM)
and
also
than
more-advanced
generative
model-based
systems.
The
nature
of
the
recognition
errors
produced
by
the
two
types
of
systems
was
characteristically
different,
offering
technical
insights
into
how
to
integrate
deep
learning
into
the
existing
highly
efficient,
run-time
speech
decoding
system
deployed
by
all
major
speech
recognition
systems.
Analysis
around
2009–2010,
contrasting
the
GMM
(and
other
generative
speech
models)
vs.
DNN
models,
stimulated
early
industrial
investment
in
deep
learning
for
speech
recognition.
That
analysis
was
done
with
comparable
performance
(less
than
1.5%
in
error
rate)
between
discriminative
DNNs
and
generative
models.
In
2010,
researchers
extended
deep
learning
from
TIMIT
to
large
vocabulary
speech
recognition,
by
adopting
large
output
layers
of
the
DNN
based
on
context-dependent
HMM
states
constructed
by
decision
trees.Deep
learning
is
part
of
state-of-the-art
systems
in
various
disciplines,
particularly
computer
vision
and
automatic
speech
recognition
(ASR).
Results
on
commonly
used
evaluation
sets
such
as
TIMIT
(ASR)
and
MNIST
(image
classification),
as
well
as
a
range
of
large-vocabulary
speech
recognition
tasks
have
steadily
improved.
Convolutional
neural
networks
were
superseded
for
ASR
by
CTC
for
LSTM.
but
are
more
successful
in
computer
vision.
Advances
in
hardware
have
driven
renewed
interest
in
deep
learning.
In
2009,
Nvidia
was
involved
in
what
was
called
the
"big
bang"
of
deep
learning,
"as
deep-learning
neural
networks
were
trained
with
Nvidia
graphics
processing
units
(GPUs)".
That
year,
Andrew
Ng
determined
that
GPUs
could
increase
the
speed
of
deep-learning
systems
by
about
100
times.
In
particular,
GPUs
are
well-suited
for
the
matrix/vector
computations
involved
in
machine
learning.
GPUs
speed
up
training
algorithms
by
orders
of
magnitude,
reducing
running
times
from
weeks
to
days.
Further,
specialized
hardware
and
algorithm
optimizations
can
be
used
for
efficient
processing
of
deep
learning
models.
===
Deep
learning
revolution
===
In
the
late
2000s,
deep
learning
started
to
outperform
other
methods
in
machine
learning
competitions.
In
2009,
a
long
short-term
memory
trained
by
connectionist
temporal
classification
(Alex
Graves,
Santiago
Fernández,
Faustino
Gomez,
and
Jürgen
Schmidhuber,
2006)
was
the
first
RNN
to
win
pattern
recognition
contests,
winning
three
competitions
in
connected
handwriting
recognition.
Google
later
used
CTC-trained
LSTM
for
speech
recognition
on
the
smartphone.Significant
impacts
in
image
or
object
recognition
were
felt
from
2011
to
2012.
Although
CNNs
trained
by
backpropagation
had
been
around
for
decades,
and
GPU
implementations
of
NNs
for
years,
including
CNNs,
faster
implementations
of
CNNs
on
GPUs
were
needed
to
progress
on
computer
vision.
In
2011,
the
DanNet
by
Dan
Ciresan,
Ueli
Meier,
Jonathan
Masci,
Luca
Maria
Gambardella,
and
Jürgen
Schmidhuber
achieved
for
the
first
time
superhuman
performance
in
a
visual
pattern
recognition
contest,
outperforming
traditional
methods
by
a
factor
of
3.
Also
in
2011,
DanNet
won
the
ICDAR
Chinese
handwriting
contest,
and
in
May
2012,
it
won
the
ISBI
image
segmentation
contest.
Until
2011,
CNNs
did
not
play
a
major
role
at
computer
vision
conferences,
but
in
June
2012,
a
paper
by
Ciresan
et
al.
at
the
leading
conference
CVPR
showed
how
max-pooling
CNNs
on
GPU
can
dramatically
improve
many
vision
benchmark
records.
In
September
2012,
DanNet
also
won
the
ICPR
contest
on
analysis
of
large
medical
images
for
cancer
detection,
and
in
the
following
year
also
the
MICCAI
Grand
Challenge
on
the
same
topic.
In
October
2012,
the
similar
AlexNet
by
Alex
Krizhevsky,
Ilya
Sutskever,
and
Geoffrey
Hinton
won
the
large-scale
ImageNet
competition
by
a
significant
margin
over
shallow
machine
learning
methods.
The
VGG-16
network
by
Karen
Simonyan
and
Andrew
Zisserman
further
reduced
the
error
rate
and
won
the
ImageNet
2014
competition,
following
a
similar
trend
in
large-scale
speech
recognition.
Image
classification
was
then
extended
to
the
more
challenging
task
of
generating
descriptions
(captions)
for
images,
often
as
a
combination
of
CNNs
and
LSTMs.In
2012,
a
team
led
by
George
E.
Dahl
won
the
"Merck
Molecular
Activity
Challenge"
using
multi-task
deep
neural
networks
to
predict
the
biomolecular
target
of
one
drug.
In
2014,
Sepp
Hochreiter's
group
used
deep
learning
to
detect
off-target
and
toxic
effects
of
environmental
chemicals
in
nutrients,
household
products
and
drugs
and
won
the
"Tox21
Data
Challenge"
of
NIH,
FDA
and
NCATS.In
2016,
Roger
Parloff
mentioned
a
"deep
learning
revolution"
that
has
transformed
the
AI
industry.In
March
2019,
Yoshua
Bengio,
Geoffrey
Hinton
and
Yann
LeCun
were
awarded
the
Turing
Award
for
conceptual
and
engineering
breakthroughs
that
have
made
deep
neural
networks
a
critical
component
of
computing.
==
Neural
networks
==
Artificial
neural
networks
(ANNs)
or
connectionist
systems
are
computing
systems
inspired
by
the
biological
neural
networks
that
constitute
animal
brains.
Such
systems
learn
(progressively
improve
their
ability)
to
do
tasks
by
considering
examples,
generally
without
task-specific
programming.
For
example,
in
image
recognition,
they
might
learn
to
identify
images
that
contain
cats
by
analyzing
example
images
that
have
been
manually
labeled
as
"cat"
or
"no
cat"
and
using
the
analytic
results
to
identify
cats
in
other
images.
They
have
found
most
use
in
applications
difficult
to
express
with
a
traditional
computer
algorithm
using
rule-based
programming.
An
ANN
is
based
on
a
collection
of
connected
units
called
artificial
neurons,
(analogous
to
biological
neurons
in
a
biological
brain).
Each
connection
(synapse)
between
neurons
can
transmit
a
signal
to
another
neuron.
The
receiving
(postsynaptic)
neuron
can
process
the
signal(s)
and
then
signal
downstream
neurons
connected
to
it.
Neurons
may
have
state,
generally
represented
by
real
numbers,
typically
between
0
and
1.
Neurons
and
synapses
may
also
have
a
weight
that
varies
as
learning
proceeds,
which
can
increase
or
decrease
the
strength
of
the
signal
that
it
sends
downstream.
Typically,
neurons
are
organized
in
layers.
Different
layers
may
perform
different
kinds
of
transformations
on
their
inputs.
Signals
travel
from
the
first
(input),
to
the
last
(output)
layer,
possibly
after
traversing
the
layers
multiple
times.
The
original
goal
of
the
neural
network
approach
was
to
solve
problems
in
the
same
way
that
a
human
brain
would.
Over
time,
attention
focused
on
matching
specific
mental
abilities,
leading
to
deviations
from
biology
such
as
backpropagation,
or
passing
information
in
the
reverse
direction
and
adjusting
the
network
to
reflect
that
information.
Neural
networks
have
been
used
on
a
variety
of
tasks,
including
computer
vision,
speech
recognition,
machine
translation,
social
network
filtering,
playing
board
and
video
games
and
medical
diagnosis.
As
of
2017,
neural
networks
typically
have
a
few
thousand
to
a
few
million
units
and
millions
of
connections.
Despite
this
number
being
several
order
of
magnitude
less
than
the
number
of
neurons
on
a
human
brain,
these
networks
can
perform
many
tasks
at
a
level
beyond
that
of
humans
(e.g.,
recognizing
faces,
or
playing
"Go").
===
Deep
neural
networks
===
A
deep
neural
network
(DNN)
is
an
artificial
neural
network
with
multiple
layers
between
the
input
and
output
layers.
There
are
different
types
of
neural
networks
but
they
always
consist
of
the
same
components:
neurons,
synapses,
weights,
biases,
and
functions.
These
components
as
a
whole
function
in
a
way
that
mimics
functions
of
the
human
brain,
and
can
be
trained
like
any
other
ML
algorithm.For
example,
a
DNN
that
is
trained
to
recognize
dog
breeds
will
go
over
the
given
image
and
calculate
the
probability
that
the
dog
in
the
image
is
a
certain
breed.
The
user
can
review
the
results
and
select
which
probabilities
the
network
should
display
(above
a
certain
threshold,
etc.)
and
return
the
proposed
label.
Each
mathematical
manipulation
as
such
is
considered
a
layer,
and
complex
DNN
have
many
layers,
hence
the
name
"deep"
networks.
DNNs
can
model
complex
non-linear
relationships.
DNN
architectures
generate
compositional
models
where
the
object
is
expressed
as
a
layered
composition
of
primitives.
The
extra
layers
enable
composition
of
features
from
lower
layers,
potentially
modeling
complex
data
with
fewer
units
than
a
similarly
performing
shallow
network.
For
instance,
it
was
proved
that
sparse
multivariate
polynomials
are
exponentially
easier
to
approximate
with
DNNs
than
with
shallow
networks.Deep
architectures
include
many
variants
of
a
few
basic
approaches.
Each
architecture
has
found
success
in
specific
domains.
It
is
not
always
possible
to
compare
the
performance
of
multiple
architectures,
unless
they
have
been
evaluated
on
the
same
data
sets.
DNNs
are
typically
feedforward
networks
in
which
data
flows
from
the
input
layer
to
the
output
layer
without
looping
back.
At
first,
the
DNN
creates
a
map
of
virtual
neurons
and
assigns
random
numerical
values,
or
"weights",
to
connections
between
them.
The
weights
and
inputs
are
multiplied
and
return
an
output
between
0
and
1.
If
the
network
did
not
accurately
recognize
a
particular
pattern,
an
algorithm
would
adjust
the
weights.
That
way
the
algorithm
can
make
certain
parameters
more
influential,
until
it
determines
the
correct
mathematical
manipulation
to
fully
process
the
data.
Recurrent
neural
networks,
in
which
data
can
flow
in
any
direction,
are
used
for
applications
such
as
language
modeling.
Long
short-term
memory
is
particularly
effective
for
this
use.Convolutional
neural
networks
(CNNs)
are
used
in
computer
vision.
CNNs
also
have
been
applied
to
acoustic
modeling
for
automatic
speech
recognition
(ASR).
====
Challenges
====
As
with
ANNs,
many
issues
can
arise
with
naively
trained
DNNs.
Two
common
issues
are
overfitting
and
computation
time.
DNNs
are
prone
to
overfitting
because
of
the
added
layers
of
abstraction,
which
allow
them
to
model
rare
dependencies
in
the
training
data.
Regularization
methods
such
as
Ivakhnenko's
unit
pruning
or
weight
decay
(ℓ2{\displaystyle
\ell
_{2}}-regularization)
or
sparsity
(ℓ1{\displaystyle
\ell
_{1}}-regularization)
can
be
applied
during
training
to
combat
overfitting.
Alternatively
dropout
regularization
randomly
omits
units
from
the
hidden
layers
during
training.
This
helps
to
exclude
rare
dependencies.
Finally,
data
can
be
augmented
via
methods
such
as
cropping
and
rotating
such
that
smaller
training
sets
can
be
increased
in
size
to
reduce
the
chances
of
overfitting.DNNs
must
consider
many
training
parameters,
such
as
the
size
(number
of
layers
and
number
of
units
per
layer),
the
learning
rate,
and
initial
weights.
Sweeping
through
the
parameter
space
for
optimal
parameters
may
not
be
feasible
due
to
the
cost
in
time
and
computational
resources.
Various
tricks,
such
as
batching
(computing
the
gradient
on
several
training
examples
at
once
rather
than
individual
examples)
speed
up
computation.
Large
processing
capabilities
of
many-core
architectures
(such
as
GPUs
or
the
Intel
Xeon
Phi)
have
produced
significant
speedups
in
training,
because
of
the
suitability
of
such
processing
architectures
for
the
matrix
and
vector
computations.Alternatively,
engineers
may
look
for
other
types
of
neural
networks
with
more
straightforward
and
convergent
training
algorithms.
CMAC
(cerebellar
model
articulation
controller)
is
one
such
kind
of
neural
network.
It
doesn't
require
learning
rates
or
randomized
initial
weights.
The
training
process
can
be
guaranteed
to
converge
in
one
step
with
a
new
batch
of
data,
and
the
computational
complexity
of
the
training
algorithm
is
linear
with
respect
to
the
number
of
neurons
involved.
==
Hardware
==
Since
the
2010s,
advances
in
both
machine
learning
algorithms
and
computer
hardware
have
led
to
more
efficient
methods
for
training
deep
neural
networks
that
contain
many
layers
of
non-linear
hidden
units
and
a
very
large
output
layer.
By
2019,
graphic
processing
units
(GPUs),
often
with
AI-specific
enhancements,
had
displaced
CPUs
as
the
dominant
method
of
training
large-scale
commercial
cloud
AI.
OpenAI
estimated
the
hardware
computation
used
in
the
largest
deep
learning
projects
from
AlexNet
(2012)
to
AlphaZero
(2017),
and
found
a
300,000-fold
increase
in
the
amount
of
computation
required,
with
a
doubling-time
trendline
of
3.4
months.Special
electronic
circuits
called
deep
learning
processors
were
designed
to
speed
up
deep
learning
algorithms.
Deep
learning
processors
include
neural
processing
units
(NPUs)
in
Huawei
cellphones
and
cloud
computing
servers
such
as
tensor
processing
units
(TPU)
in
the
Google
Cloud
Platform.
Cerebras
Systems
has
also
built
a
dedicated
system
to
handle
large
deep
learning
models,
the
CS-2,
based
on
the
largest
processor
in
the
industry,
the
second-generation
Wafer
Scale
Engine
(WSE-2).Atomically
thin
semiconductors
are
considered
promising
for
energy-efficient
deep
learning
hardware
where
the
same
basic
device
structure
is
used
for
both
logic
operations
and
data
storage.
In
2020,
Marega
et
al.
published
experiments
with
a
large-area
active
channel
material
for
developing
logic-in-memory
devices
and
circuits
based
on
floating-gate
field-effect
transistors
(FGFETs).In
2021,
J.
Feldmann
et
al.
proposed
an
integrated
photonic
hardware
accelerator
for
parallel
convolutional
processing.
The
authors
identify
two
key
advantages
of
integrated
photonics
over
its
electronic
counterparts:
(1)
massively
parallel
data
transfer
through
wavelength
division
multiplexing
in
conjunction
with
frequency
combs,
and
(2)
extremely
high
data
modulation
speeds.
Their
system
can
execute
trillions
of
multiply-accumulate
operations
per
second,
indicating
the
potential
of
integrated
photonics
in
data-heavy
AI
applications.
==
Applications
==
===
Automatic
speech
recognition
===
Large-scale
automatic
speech
recognition
is
the
first
and
most
convincing
successful
case
of
deep
learning.
LSTM
RNNs
can
learn
"Very
Deep
Learning"
tasks
that
involve
multi-second
intervals
containing
speech
events
separated
by
thousands
of
discrete
time
steps,
where
one
time
step
corresponds
to
about
10
ms.
LSTM
with
forget
gates
is
competitive
with
traditional
speech
recognizers
on
certain
tasks.The
initial
success
in
speech
recognition
was
based
on
small-scale
recognition
tasks
based
on
TIMIT.
The
data
set
contains
630
speakers
from
eight
major
dialects
of
American
English,
where
each
speaker
reads
10
sentences.
Its
small
size
lets
many
configurations
be
tried.
More
importantly,
the
TIMIT
task
concerns
phone-sequence
recognition,
which,
unlike
word-sequence
recognition,
allows
weak
phone
bigram
language
models.
This
lets
the
strength
of
the
acoustic
modeling
aspects
of
speech
recognition
be
more
easily
analyzed.
The
error
rates
listed
below,
including
these
early
results
and
measured
as
percent
phone
error
rates
(PER),
have
been
summarized
since
1991.
The
debut
of
DNNs
for
speaker
recognition
in
the
late
1990s
and
speech
recognition
around
2009-2011
and
of
LSTM
around
2003–2007,
accelerated
progress
in
eight
major
areas:
Scale-up/out
and
accelerated
DNN
training
and
decoding
Sequence
discriminative
training
Feature
processing
by
deep
models
with
solid
understanding
of
the
underlying
mechanisms
Adaptation
of
DNNs
and
related
deep
models
Multi-task
and
transfer
learning
by
DNNs
and
related
deep
models
CNNs
and
how
to
design
them
to
best
exploit
domain
knowledge
of
speech
RNN
and
its
rich
LSTM
variants
Other
types
of
deep
models
including
tensor-based
models
and
integrated
deep
generative/discriminative
models.All
major
commercial
speech
recognition
systems
(e.g.,
Microsoft
Cortana,
Xbox,
Skype
Translator,
Amazon
Alexa,
Google
Now,
Apple
Siri,
Baidu
and
iFlyTek
voice
search,
and
a
range
of
Nuance
speech
products,
etc.)
are
based
on
deep
learning.
===
Image
recognition
===
A
common
evaluation
set
for
image
classification
is
the
MNIST
database
data
set.
MNIST
is
composed
of
handwritten
digits
and
includes
60,000
training
examples
and
10,000
test
examples.
As
with
TIMIT,
its
small
size
lets
users
test
multiple
configurations.
A
comprehensive
list
of
results
on
this
set
is
available.Deep
learning-based
image
recognition
has
become
"superhuman",
producing
more
accurate
results
than
human
contestants.
This
first
occurred
in
2011
in
recognition
of
traffic
signs,
and
in
2014,
with
recognition
of
human
faces.Deep
learning-trained
vehicles
now
interpret
360°
camera
views.
Another
example
is
Facial
Dysmorphology
Novel
Analysis
(FDNA)
used
to
analyze
cases
of
human
malformation
connected
to
a
large
database
of
genetic
syndromes.
===
Visual
art
processing
===
Closely
related
to
the
progress
that
has
been
made
in
image
recognition
is
the
increasing
application
of
deep
learning
techniques
to
various
visual
art
tasks.
DNNs
have
proven
themselves
capable,
for
example,
of
identifying
the
style
period
of
a
given
painting
Neural
Style
Transfer
–
capturing
the
style
of
a
given
artwork
and
applying
it
in
a
visually
pleasing
manner
to
an
arbitrary
photograph
or
video
generating
striking
imagery
based
on
random
visual
input
fields.
===
Natural
language
processing
===
Neural
networks
have
been
used
for
implementing
language
models
since
the
early
2000s.
LSTM
helped
to
improve
machine
translation
and
language
modeling.Other
key
techniques
in
this
field
are
negative
sampling
and
word
embedding.
Word
embedding,
such
as
word2vec,
can
be
thought
of
as
a
representational
layer
in
a
deep
learning
architecture
that
transforms
an
atomic
word
into
a
positional
representation
of
the
word
relative
to
other
words
in
the
dataset;
the
position
is
represented
as
a
point
in
a
vector
space.
Using
word
embedding
as
an
RNN
input
layer
allows
the
network
to
parse
sentences
and
phrases
using
an
effective
compositional
vector
grammar.
A
compositional
vector
grammar
can
be
thought
of
as
probabilistic
context
free
grammar
(PCFG)
implemented
by
an
RNN.
Recursive
auto-encoders
built
atop
word
embeddings
can
assess
sentence
similarity
and
detect
paraphrasing.
Deep
neural
architectures
provide
the
best
results
for
constituency
parsing,
sentiment
analysis,
information
retrieval,
spoken
language
understanding,
machine
translation,
contextual
entity
linking,
writing
style
recognition,
named-entity
recognition
(token
classification),
text
classification,
and
others.Recent
developments
generalize
word
embedding
to
sentence
embedding.
Google
Translate
(GT)
uses
a
large
end-to-end
long
short-term
memory
(LSTM)
network.
Google
Neural
Machine
Translation
(GNMT)
uses
an
example-based
machine
translation
method
in
which
the
system
"learns
from
millions
of
examples".
It
translates
"whole
sentences
at
a
time,
rather
than
pieces".
Google
Translate
supports
over
one
hundred
languages.
The
network
encodes
the
"semantics
of
the
sentence
rather
than
simply
memorizing
phrase-to-phrase
translations".
GT
uses
English
as
an
intermediate
between
most
language
pairs.
===
Drug
discovery
and
toxicology
===
A
large
percentage
of
candidate
drugs
fail
to
win
regulatory
approval.
These
failures
are
caused
by
insufficient
efficacy
(on-target
effect),
undesired
interactions
(off-target
effects),
or
unanticipated
toxic
effects.
Research
has
explored
use
of
deep
learning
to
predict
the
biomolecular
targets,
off-targets,
and
toxic
effects
of
environmental
chemicals
in
nutrients,
household
products
and
drugs.AtomNet
is
a
deep
learning
system
for
structure-based
rational
drug
design.
AtomNet
was
used
to
predict
novel
candidate
biomolecules
for
disease
targets
such
as
the
Ebola
virus
and
multiple
sclerosis.In
2017
graph
neural
networks
were
used
for
the
first
time
to
predict
various
properties
of
molecules
in
a
large
toxicology
data
set.
In
2019,
generative
neural
networks
were
used
to
produce
molecules
that
were
validated
experimentally
all
the
way
into
mice.
===
Customer
relationship
management
===
Deep
reinforcement
learning
has
been
used
to
approximate
the
value
of
possible
direct
marketing
actions,
defined
in
terms
of
RFM
variables.
The
estimated
value
function
was
shown
to
have
a
natural
interpretation
as
customer
lifetime
value.
===
Recommendation
systems
===
Recommendation
systems
have
used
deep
learning
to
extract
meaningful
features
for
a
latent
factor
model
for
content-based
music
and
journal
recommendations.
Multi-view
deep
learning
has
been
applied
for
learning
user
preferences
from
multiple
domains.
The
model
uses
a
hybrid
collaborative
and
content-based
approach
and
enhances
recommendations
in
multiple
tasks.
===
Bioinformatics
===
An
autoencoder
ANN
was
used
in
bioinformatics,
to
predict
gene
ontology
annotations
and
gene-function
relationships.In
medical
informatics,
deep
learning
was
used
to
predict
sleep
quality
based
on
data
from
wearables
and
predictions
of
health
complications
from
electronic
health
record
data.
===
Deep
Neural
Network
Estimations
===
Deep
neural
networks
can
be
used
to
estimate
the
entropy
of
a
stochastic
process
and
called
Neural
Joint
Entropy
Estimator
(NJEE).
Such
an
estimation
provides
insights
on
the
effects
of
input
random
variables
on
an
independent
random
variable.
Practically,
the
DNN
is
trained
as
a
classifier
that
maps
an
input
vector
or
matrix
X
to
an
output
probability
distribution
over
the
possible
classes
of
random
variable
Y,
given
input
X.
For
example,
in
image
classification
tasks,
the
NJEE
maps
a
vector
of
pixels'
color
values
to
probabilities
over
possible
image
classes.
In
practice,
the
probability
distribution
of
Y
is
obtained
by
a
Softmax
layer
with
number
of
nodes
that
is
equal
to
the
alphabet
size
of
Y.
NJEE
uses
continuously
differentiable
activation
functions,
such
that
the
conditions
for
the
universal
approximation
theorem
holds.
It
is
shown
that
this
method
provides
a
strongly
consistent
estimator
and
outperforms
other
methods
in
case
of
large
alphabet
sizes.
===
Medical
image
analysis
===
Deep
learning
has
been
shown
to
produce
competitive
results
in
medical
application
such
as
cancer
cell
classification,
lesion
detection,
organ
segmentation
and
image
enhancement.
Modern
deep
learning
tools
demonstrate
the
high
accuracy
of
detecting
various
diseases
and
the
helpfulness
of
their
use
by
specialists
to
improve
the
diagnosis
efficiency.
===
Mobile
advertising
===
Finding
the
appropriate
mobile
audience
for
mobile
advertising
is
always
challenging,
since
many
data
points
must
be
considered
and
analyzed
before
a
target
segment
can
be
created
and
used
in
ad
serving
by
any
ad
server.
Deep
learning
has
been
used
to
interpret
large,
many-dimensioned
advertising
datasets.
Many
data
points
are
collected
during
the
request/serve/click
internet
advertising
cycle.
This
information
can
form
the
basis
of
machine
learning
to
improve
ad
selection.
===
Image
restoration
===
Deep
learning
has
been
successfully
applied
to
inverse
problems
such
as
denoising,
super-resolution,
inpainting,
and
film
colorization.
These
applications
include
learning
methods
such
as
"Shrinkage
Fields
for
Effective
Image
Restoration"
which
trains
on
an
image
dataset,
and
Deep
Image
Prior,
which
trains
on
the
image
that
needs
restoration.
===
Financial
fraud
detection
===
Deep
learning
is
being
successfully
applied
to
financial
fraud
detection,
tax
evasion
detection,
and
anti-money
laundering.
===
Materials
science
===
In
November
2023,
researchers
at
Google
DeepMind
and
Lawrence
Berkeley
National
Laboratory
announced
that
they
had
developed
an
AI
system
known
as
GNoME.
This
system
has
contributed
to
materials
science
by
discovering
over
2
million
new
materials
within
a
relatively
short
timeframe.
GNoME
employs
deep
learning
techniques
to
efficiently
explore
potential
material
structures,
achieving
a
significant
increase
in
the
identification
of
stable
inorganic
crystal
structures.
The
system's
predictions
were
validated
through
autonomous
robotic
experiments,
demonstrating
a
noteworthy
success
rate
of
71%.
The
data
of
newly
discovered
materials
is
publicly
available
through
the
Materials
Project
database,
offering
researchers
the
opportunity
to
identify
materials
with
desired
properties
for
various
applications.
This
development
has
implications
for
the
future
of
scientific
discovery
and
the
integration
of
AI
in
material
science
research,
potentially
expediting
material
innovation
and
reducing
costs
in
product
development.
The
use
of
AI
and
deep
learning
suggests
the
possibility
of
minimizing
or
eliminating
manual
lab
experiments
and
allowing
scientists
to
focus
more
on
the
design
and
analysis
of
unique
compounds.
===
Military
===
The
United
States
Department
of
Defense
applied
deep
learning
to
train
robots
in
new
tasks
through
observation.
===
Partial
differential
equations
===
Physics
informed
neural
networks
have
been
used
to
solve
partial
differential
equations
in
both
forward
and
inverse
problems
in
a
data
driven
manner.
One
example
is
the
reconstructing
fluid
flow
governed
by
the
Navier-Stokes
equations.
Using
physics
informed
neural
networks
does
not
require
the
often
expensive
mesh
generation
that
conventional
CFD
methods
relies
on.
===
Image
reconstruction
===
Image
reconstruction
is
the
reconstruction
of
the
underlying
images
from
the
image-related
measurements.
Several
works
showed
the
better
and
superior
performance
of
the
deep
learning
methods
compared
to
analytical
methods
for
various
applications,
e.g.,
spectral
imaging
and
ultrasound
imaging.
===
Epigenetic
clock
===
An
epigenetic
clock
is
a
biochemical
test
that
can
be
used
to
measure
age.
Galkin
et
al.
used
deep
neural
networks
to
train
an
epigenetic
aging
clock
of
unprecedented
accuracy
using
>6,000
blood
samples.
The
clock
uses
information
from
1000
CpG
sites
and
predicts
people
with
certain
conditions
older
than
healthy
controls:
IBD,
frontotemporal
dementia,
ovarian
cancer,
obesity.
The
aging
clock
was
planned
to
be
released
for
public
use
in
2021
by
an
Insilico
Medicine
spinoff
company
Deep
Longevity.
==
Relation
to
human
cognitive
and
brain
development
==
Deep
learning
is
closely
related
to
a
class
of
theories
of
brain
development
(specifically,
neocortical
development)
proposed
by
cognitive
neuroscientists
in
the
early
1990s.
These
developmental
theories
were
instantiated
in
computational
models,
making
them
predecessors
of
deep
learning
systems.
These
developmental
models
share
the
property
that
various
proposed
learning
dynamics
in
the
brain
(e.g.,
a
wave
of
nerve
growth
factor)
support
the
self-organization
somewhat
analogous
to
the
neural
networks
utilized
in
deep
learning
models.
Like
the
neocortex,
neural
networks
employ
a
hierarchy
of
layered
filters
in
which
each
layer
considers
information
from
a
prior
layer
(or
the
operating
environment),
and
then
passes
its
output
(and
possibly
the
original
input),
to
other
layers.
This
process
yields
a
self-organizing
stack
of
transducers,
well-tuned
to
their
operating
environment.
A
1995
description
stated,
"...the
infant's
brain
seems
to
organize
itself
under
the
influence
of
waves
of
so-called
trophic-factors
...
different
regions
of
the
brain
become
connected
sequentially,
with
one
layer
of
tissue
maturing
before
another
and
so
on
until
the
whole
brain
is
mature".A
variety
of
approaches
have
been
used
to
investigate
the
plausibility
of
deep
learning
models
from
a
neurobiological
perspective.
On
the
one
hand,
several
variants
of
the
backpropagation
algorithm
have
been
proposed
in
order
to
increase
its
processing
realism.
Other
researchers
have
argued
that
unsupervised
forms
of
deep
learning,
such
as
those
based
on
hierarchical
generative
models
and
deep
belief
networks,
may
be
closer
to
biological
reality.
In
this
respect,
generative
neural
network
models
have
been
related
to
neurobiological
evidence
about
sampling-based
processing
in
the
cerebral
cortex.Although
a
systematic
comparison
between
the
human
brain
organization
and
the
neuronal
encoding
in
deep
networks
has
not
yet
been
established,
several
analogies
have
been
reported.
For
example,
the
computations
performed
by
deep
learning
units
could
be
similar
to
those
of
actual
neurons
and
neural
populations.
Similarly,
the
representations
developed
by
deep
learning
models
are
similar
to
those
measured
in
the
primate
visual
system
both
at
the
single-unit
and
at
the
population
levels.
==
Commercial
activity
==
Facebook's
AI
lab
performs
tasks
such
as
automatically
tagging
uploaded
pictures
with
the
names
of
the
people
in
them.Google's
DeepMind
Technologies
developed
a
system
capable
of
learning
how
to
play
Atari
video
games
using
only
pixels
as
data
input.
In
2015
they
demonstrated
their
AlphaGo
system,
which
learned
the
game
of
Go
well
enough
to
beat
a
professional
Go
player.
Google
Translate
uses
a
neural
network
to
translate
between
more
than
100
languages.
In
2017,
Covariant.ai
was
launched,
which
focuses
on
integrating
deep
learning
into
factories.As
of
2008,
researchers
at
The
University
of
Texas
at
Austin
(UT)
developed
a
machine
learning
framework
called
Training
an
Agent
Manually
via
Evaluative
Reinforcement,
or
TAMER,
which
proposed
new
methods
for
robots
or
computer
programs
to
learn
how
to
perform
tasks
by
interacting
with
a
human
instructor.
First
developed
as
TAMER,
a
new
algorithm
called
Deep
TAMER
was
later
introduced
in
2018
during
a
collaboration
between
U.S.
Army
Research
Laboratory
(ARL)
and
UT
researchers.
Deep
TAMER
used
deep
learning
to
provide
a
robot
with
the
ability
to
learn
new
tasks
through
observation.
Using
Deep
TAMER,
a
robot
learned
a
task
with
a
human
trainer,
watching
video
streams
or
observing
a
human
perform
a
task
in-person.
The
robot
later
practiced
the
task
with
the
help
of
some
coaching
from
the
trainer,
who
provided
feedback
such
as
"good
job"
and
"bad
job".
==
Criticism
and
comment
==
Deep
learning
has
attracted
both
criticism
and
comment,
in
some
cases
from
outside
the
field
of
computer
science.
===
Theory
===
A
main
criticism
concerns
the
lack
of
theory
surrounding
some
methods.
Learning
in
the
most
common
deep
architectures
is
implemented
using
well-understood
gradient
descent.
However,
the
theory
surrounding
other
algorithms,
such
as
contrastive
divergence
is
less
clear.
(e.g.,
Does
it
converge?
If
so,
how
fast?
What
is
it
approximating?)
Deep
learning
methods
are
often
looked
at
as
a
black
box,
with
most
confirmations
done
empirically,
rather
than
theoretically.Others
point
out
that
deep
learning
should
be
looked
at
as
a
step
towards
realizing
strong
AI,
not
as
an
all-encompassing
solution.
Despite
the
power
of
deep
learning
methods,
they
still
lack
much
of
the
functionality
needed
to
realize
this
goal
entirely.
Research
psychologist
Gary
Marcus
noted:
Realistically,
deep
learning
is
only
part
of
the
larger
challenge
of
building
intelligent
machines.
Such
techniques
lack
ways
of
representing
causal
relationships
(...)
have
no
obvious
ways
of
performing
logical
inferences,
and
they
are
also
still
a
long
way
from
integrating
abstract
knowledge,
such
as
information
about
what
objects
are,
what
they
are
for,
and
how
they
are
typically
used.
The
most
powerful
A.I.
systems,
like
Watson
(...)
use
techniques
like
deep
learning
as
just
one
element
in
a
very
complicated
ensemble
of
techniques,
ranging
from
the
statistical
technique
of
Bayesian
inference
to
deductive
reasoning.
In
further
reference
to
the
idea
that
artistic
sensitivity
might
be
inherent
in
relatively
low
levels
of
the
cognitive
hierarchy,
a
published
series
of
graphic
representations
of
the
internal
states
of
deep
(20-30
layers)
neural
networks
attempting
to
discern
within
essentially
random
data
the
images
on
which
they
were
trained
demonstrate
a
visual
appeal:
the
original
research
notice
received
well
over
1,000
comments,
and
was
the
subject
of
what
was
for
a
time
the
most
frequently
accessed
article
on
The
Guardian's
website.
===
Errors
===
Some
deep
learning
architectures
display
problematic
behaviors,
such
as
confidently
classifying
unrecognizable
images
as
belonging
to
a
familiar
category
of
ordinary
images
(2014)
and
misclassifying
minuscule
perturbations
of
correctly
classified
images
(2013).
Goertzel
hypothesized
that
these
behaviors
are
due
to
limitations
in
their
internal
representations
and
that
these
limitations
would
inhibit
integration
into
heterogeneous
multi-component
artificial
general
intelligence
(AGI)
architectures.
These
issues
may
possibly
be
addressed
by
deep
learning
architectures
that
internally
form
states
homologous
to
image-grammar
decompositions
of
observed
entities
and
events.
Learning
a
grammar
(visual
or
linguistic)
from
training
data
would
be
equivalent
to
restricting
the
system
to
commonsense
reasoning
that
operates
on
concepts
in
terms
of
grammatical
production
rules
and
is
a
basic
goal
of
both
human
language
acquisition
and
artificial
intelligence
(AI).
===
Cyber
threat
===
As
deep
learning
moves
from
the
lab
into
the
world,
research
and
experience
show
that
artificial
neural
networks
are
vulnerable
to
hacks
and
deception.
By
identifying
patterns
that
these
systems
use
to
function,
attackers
can
modify
inputs
to
ANNs
in
such
a
way
that
the
ANN
finds
a
match
that
human
observers
would
not
recognize.
For
example,
an
attacker
can
make
subtle
changes
to
an
image
such
that
the
ANN
finds
a
match
even
though
the
image
looks
to
a
human
nothing
like
the
search
target.
Such
manipulation
is
termed
an
"adversarial
attack".In
2016
researchers
used
one
ANN
to
doctor
images
in
trial
and
error
fashion,
identify
another's
focal
points,
and
thereby
generate
images
that
deceived
it.
The
modified
images
looked
no
different
to
human
eyes.
Another
group
showed
that
printouts
of
doctored
images
then
photographed
successfully
tricked
an
image
classification
system.
One
defense
is
reverse
image
search,
in
which
a
possible
fake
image
is
submitted
to
a
site
such
as
TinEye
that
can
then
find
other
instances
of
it.
A
refinement
is
to
search
using
only
parts
of
the
image,
to
identify
images
from
which
that
piece
may
have
been
taken.Another
group
showed
that
certain
psychedelic
spectacles
could
fool
a
facial
recognition
system
into
thinking
ordinary
people
were
celebrities,
potentially
allowing
one
person
to
impersonate
another.
In
2017
researchers
added
stickers
to
stop
signs
and
caused
an
ANN
to
misclassify
them.ANNs
can
however
be
further
trained
to
detect
attempts
at
deception,
potentially
leading
attackers
and
defenders
into
an
arms
race
similar
to
the
kind
that
already
defines
the
malware
defense
industry.
ANNs
have
been
trained
to
defeat
ANN-based
anti-malware
software
by
repeatedly
attacking
a
defense
with
malware
that
was
continually
altered
by
a
genetic
algorithm
until
it
tricked
the
anti-malware
while
retaining
its
ability
to
damage
the
target.In
2016,
another
group
demonstrated
that
certain
sounds
could
make
the
Google
Now
voice
command
system
open
a
particular
web
address,
and
hypothesized
that
this
could
"serve
as
a
stepping
stone
for
further
attacks
(e.g.,
opening
a
web
page
hosting
drive-by
malware)".In
"data
poisoning",
false
data
is
continually
smuggled
into
a
machine
learning
system's
training
set
to
prevent
it
from
achieving
mastery.
===
Data
collection
ethics
===
Most
Deep
Learning
systems
rely
on
training
and
verification
data
that
is
generated
and/or
annotated
by
humans.
It
has
been
argued
in
media
philosophy
that
not
only
low-paid
clickwork
(e.g.
on
Amazon
Mechanical
Turk)
is
regularly
deployed
for
this
purpose,
but
also
implicit
forms
of
human
microwork
that
are
often
not
recognized
as
such.
The
philosopher
Rainer
Mühlhoff
distinguishes
five
types
of
"machinic
capture"
of
human
microwork
to
generate
training
data:
(1)
gamification
(the
embedding
of
annotation
or
computation
tasks
in
the
flow
of
a
game),
(2)
"trapping
and
tracking"
(e.g.
CAPTCHAs
for
image
recognition
or
click-tracking
on
Google
search
results
pages),
(3)
exploitation
of
social
motivations
(e.g.
tagging
faces
on
Facebook
to
obtain
labeled
facial
images),
(4)
information
mining
(e.g.
by
leveraging
quantified-self
devices
such
as
activity
trackers)
and
(5)
clickwork.Mühlhoff
argues
that
in
most
commercial
end-user
applications
of
Deep
Learning
such
as
Facebook's
face
recognition
system,
the
need
for
training
data
does
not
stop
once
an
ANN
is
trained.
Rather,
there
is
a
continued
demand
for
human-generated
verification
data
to
constantly
calibrate
and
update
the
ANN.
For
this
purpose,
Facebook
introduced
the
feature
that
once
a
user
is
automatically
recognized
in
an
image,
they
receive
a
notification.
They
can
choose
whether
or
not
they
like
to
be
publicly
labeled
on
the
image,
or
tell
Facebook
that
it
is
not
them
in
the
picture.
This
user
interface
is
a
mechanism
to
generate
"a
constant
stream
of
verification
data"
to
further
train
the
network
in
real-time.
As
Mühlhoff
argues,
the
involvement
of
human
users
to
generate
training
and
verification
data
is
so
typical
for
most
commercial
end-user
applications
of
Deep
Learning
that
such
systems
may
be
referred
to
as
"human-aided
artificial
intelligence".
==
See
also
==
Applications
of
artificial
intelligence
Comparison
of
deep
learning
software
Compressed
sensing
Differentiable
programming
Echo
state
network
List
of
artificial
intelligence
projects
Liquid
state
machine
List
of
datasets
for
machine-learning
research
Reservoir
computing
Scale
space
and
deep
learning
Sparse
coding
Stochastic
parrot
==
References
==
==
Further
reading
==
Dilution
and
dropout
(also
called
DropConnect)
are
regularization
techniques
for
reducing
overfitting
in
artificial
neural
networks
by
preventing
complex
co-adaptations
on
training
data.
They
are
an
efficient
way
of
performing
model
averaging
with
neural
networks.
Dilution
refers
to
thinning
weights,
while
dropout
refers
to
randomly
"dropping
out",
or
omitting,
units
(both
hidden
and
visible)
during
the
training
process
of
a
neural
network.
Both
trigger
the
same
type
of
regularization.
==
Types
and
uses
==
Dilution
is
usually
split
in
weak
dilution
and
strong
dilution.
Weak
dilution
describes
the
process
in
which
the
finite
fraction
of
removed
connections
is
small,
and
strong
dilution
refers
to
when
this
fraction
is
large.
There
is
no
clear
distinction
on
where
the
limit
between
strong
and
weak
dilution
is,
and
often
the
distinction
is
dependent
on
the
precedent
of
a
specific
use-case
and
has
implications
for
how
to
solve
for
exact
solutions.
Sometimes
dilution
is
used
for
adding
damping
noise
to
the
inputs.
In
that
case,
weak
dilution
refers
to
adding
a
small
amount
of
damping
noise,
while
strong
dilution
refers
to
adding
a
greater
amount
of
damping
noise.
Both
can
be
rewritten
as
variants
of
weight
dilution.
These
techniques
are
also
sometimes
referred
to
as
random
pruning
of
weights,
but
this
is
usually
a
non-recurring
one-way
operation.
The
network
is
pruned,
and
then
kept
if
it
is
an
improvement
over
the
previous
model.
Dilution
and
dropout
both
refer
to
an
iterative
process.
The
pruning
of
weights
typically
does
not
imply
that
the
network
continues
learning,
while
in
dilution/dropout,
the
network
continues
to
learn
after
the
technique
is
applied.
==
Generalized
linear
network
==
Output
from
a
layer
of
linear
nodes,
in
an
artificial
neural
net
can
be
described
as
yi{\displaystyle
y_{i}}
–
output
from
node
i{\displaystyle
i}
wij{\displaystyle
w_{ij}}
–
real
weight
before
dilution,
also
called
the
Hebb
connection
strength
xj{\displaystyle
x_{j}}
–
input
from
node
j{\displaystyle
j}This
can
be
written
in
vector
notation
as
y{\displaystyle
\mathbf
{y}
}
–
output
vector
W{\displaystyle
\mathbf
{W}
}
–
weight
matrix
x{\displaystyle
\mathbf
{x}
}
–
input
vectorEquations
(1)
and
(2)
are
used
in
the
subsequent
sections.
==
Weak
dilution
==
During
weak
dilution,
the
finite
fraction
of
removed
connections
(the
weights)
is
small,
giving
rise
to
a
tiny
uncertainty.
This
edge-case
can
be
solved
exactly
with
mean
field
theory.
In
weak
dilution
the
impact
on
the
weights
can
be
described
as
wij^{\displaystyle
{\hat
{w_{ij}}}}
–
diluted
weight
wij{\displaystyle
w_{ij}}
–
real
weight
before
dilution
P(c){\displaystyle
P(c)}
–
the
probability
of
c{\displaystyle
c},
the
probability
of
keeping
a
weightThe
interpretation
of
probability
P(c){\displaystyle
P(c)}
can
also
be
changed
from
keeping
a
weight
into
pruning
a
weight.
In
vector
notation
this
can
be
written
as
where
the
function
g⁡(⋅){\displaystyle
\operatorname
{g}
(\cdot
)}
imposes
the
previous
dilution.
In
weak
dilution
only
a
small
and
fixed
fraction
of
the
weights
are
diluted.
When
the
number
of
terms
in
the
sum
goes
to
infinite
(the
weights
for
each
node)
it
is
still
infinite
(the
fraction
is
fixed),
thus
mean
field
theory
can
be
applied.
In
the
notation
from
Hertz
et
al.
this
would
be
written
as
⟨hi⟩{\displaystyle
\left\langle
h_{i}\right\rangle
}
the
mean
field
temperature
c{\displaystyle
c}
–
a
scaling
factor
for
the
temperature
from
the
probability
of
keeping
the
weight
wij{\displaystyle
w_{ij}}
–
real
weight
before
dilution,
also
called
the
Hebb
connection
strength
⟨Sj⟩{\displaystyle
\left\langle
S_{j}\right\rangle
}
–
the
mean
stable
equilibrium
statesThere
are
some
assumptions
for
this
to
hold,
which
are
not
listed
here.
==
Strong
dilution
==
When
the
dilution
is
strong,
the
finite
fraction
of
removed
connections
(the
weights)
is
large,
giving
rise
to
a
huge
uncertainty.
==
Dropout
==
Dropout
is
a
special
case
of
the
previous
weight
equation
(3),
where
the
aforementioned
equation
is
adjusted
to
remove
a
whole
row
in
the
vector
matrix,
and
not
only
random
weights
P(c){\displaystyle
P(c)}
–
the
probability
c{\displaystyle
c}
to
keep
a
row
in
the
weight
matrix
wj{\displaystyle
\mathbf
{w}
_{j}}
–
real
row
in
the
weight
matrix
before
dropout
wj^{\displaystyle
{\hat
{\mathbf
{w}
_{j}}}}
–
diluted
row
in
the
weight
matrixBecause
dropout
removes
a
whole
row
from
the
vector
matrix,
the
previous
(unlisted)
assumptions
for
weak
dilution
and
the
use
of
mean
field
theory
are
not
applicable.
The
process
by
which
the
node
is
driven
to
zero,
whether
by
setting
the
weights
to
zero,
by
“removing
the
node”,
or
by
some
other
means,
does
not
impact
the
end
result
and
does
not
create
a
new
and
unique
case.
If
the
neural
net
is
processed
by
a
high-performance
digital
array-multiplicator,
then
it
is
likely
more
effective
to
drive
the
value
to
zero
late
in
the
process
graph.
If
the
net
is
processed
by
a
constrained
processor,
perhaps
even
an
analog
neuromorph
processor,
then
it
is
likely
a
more
power-efficient
solution
is
to
drive
the
value
to
zero
early
in
the
process
graph.
==
Google's
patent
==
Although
there
have
been
examples
of
randomly
removing
connections
between
neurons
in
a
neural
network
to
improve
models,
this
technique
was
first
introduced
with
the
name
dropout
by
Geoffrey
Hinton,
et
al.
in
2012.
Google
currently
holds
the
patent
for
the
dropout
technique.
==
See
also
==
AlexNet
Convolutional
neural
network
§
Dropout
==
Notes
==
==
References
==
A
feedforward
neural
network
(FNN)
is
one
of
the
two
broad
types
of
artificial
neural
network,
characterized
by
direction
of
the
flow
of
information
between
its
layers.
Its
flow
is
uni-directional,
meaning
that
the
information
in
the
model
flows
in
only
one
direction—forward—from
the
input
nodes,
through
the
hidden
nodes
(if
any)
and
to
the
output
nodes,
without
any
cycles
or
loops,
in
contrast
to
recurrent
neural
networks,
which
have
a
bi-directional
flow.
Modern
feedforward
networks
are
trained
using
the
backpropagation
method
and
are
colloquially
referred
to
as
the
"vanilla"
neural
networks.
==
Timeline
==
In
1958,
a
layered
network
of
perceptrons,
consisting
of
an
input
layer,
a
hidden
layer
with
randomized
weights
that
did
not
learn,
and
an
output
layer
with
learning
connections,
was
introduced
already
by
Frank
Rosenblatt
in
his
book
Perceptron.
This
extreme
learning
machine
was
not
yet
a
deep
learning
network.In
1965,
the
first
deep-learning
feedforward
network,
not
yet
using
stochastic
gradient
descent,
was
published
by
Alexey
Grigorevich
Ivakhnenko
and
Valentin
Lapa,
at
the
time
called
the
Group
Method
of
Data
Handling.In
1967,
a
deep-learning
network,
using
stochastic
gradient
descent
for
the
first
time,
was
able
to
classify
non-linearily
separable
pattern
classes,
as
reported
Shun'ichi
Amari.
Amari's
student
Saito
conducted
the
computer
experiments,
using
a
five-layered
feedforward
network
with
two
learning
layers.In
1970,
modern
backpropagation
method,
an
efficient
application
of
a
chain-rule-based
supervised
learning,
was
for
the
first
time
published
by
the
Finnish
researcher
Seppo
Linnainmaa.
The
term
(i.e.
"back-propagating
errors")
itself
has
been
used
by
Rosenblatt
himself,
but
he
did
not
know
how
to
implement
it,
although
a
continuous
precursor
of
backpropagation
was
already
used
in
the
context
of
control
theory
in
1960
by
Henry
J.
Kelley.
It
is
known
also
as
a
reverse
mode
of
automatic
differentiation.In
1982,
backpropagation
was
applied
in
the
way
that
has
become
standard,
for
the
first
time
by
Paul
Werbos.In
1985,
an
experimental
analysis
of
the
technique
was
conducted
by
David
E.
Rumelhart
et
al..
Many
improvements
to
the
approach
have
been
made
in
subsequent
decades.In
1987,
using
a
stochastic
gradient
descent
within
a
(wide
12-layer
nonlinear)
feed-forward
network,
Matthew
Brand
has
trained
it
to
reproduce
logic
functions
of
nontrivial
circuit
depth,
using
small
batches
of
random
input/output
samples.
He,
however,
concluded
that
on
hardware
(sub-megaflop
computers)
available
at
the
time
it
was
impractical,
and
proposed
using
fixed
random
early
layers
as
an
input
hash
for
a
single
modifiable
layer.In
1990s,
an
(much
simpler)
alternative
to
using
neural
networks,
although
still
related
support
vector
machine
approach
was
developed
by
Vladimir
Vapnik
and
his
colleagues.
In
addition
to
performing
linear
classification,
they
were
able
to
efficiently
perform
a
non-linear
classification
using
what
is
called
the
kernel
trick,
using
high-dimensional
feature
spaces.In
2003,
interest
in
backpropagation
networks
returned
due
to
the
successes
of
deep
learning
being
applied
to
language
modelling
by
Yoshua
Bengio
with
co-authors.In
2017,
modern
transformer
architectures
were
introduced.
==
Mathematical
foundations
==
===
Activation
function
===
The
two
historically
common
activation
functions
are
both
sigmoids,
and
are
described
by
y(vi)=tanh⁡(vi)
and
y(vi)=(1+e−vi)−1{\displaystyle
y(v_{i})=\tanh(v_{i})~~{\textrm
{and}}~~y(v_{i})=(1+e^{-v_{i}})^{-1}}.The
first
is
a
hyperbolic
tangent
that
ranges
from
-1
to
1,
while
the
other
is
the
logistic
function,
which
is
similar
in
shape
but
ranges
from
0
to
1.
Here
yi{\displaystyle
y_{i}}
is
the
output
of
the
i{\displaystyle
i}th
node
(neuron)
and
vi{\displaystyle
v_{i}}
is
the
weighted
sum
of
the
input
connections.
Alternative
activation
functions
have
been
proposed,
including
the
rectifier
and
softplus
functions.
More
specialized
activation
functions
include
radial
basis
functions
(used
in
radial
basis
networks,
another
class
of
supervised
neural
network
models).
In
recent
developments
of
deep
learning
the
rectified
linear
unit
(ReLU)
is
more
frequently
used
as
one
of
the
possible
ways
to
overcome
the
numerical
problems
related
to
the
sigmoids.
===
Learning
===
Learning
occurs
by
changing
connection
weights
after
each
piece
of
data
is
processed,
based
on
the
amount
of
error
in
the
output
compared
to
the
expected
result.
This
is
an
example
of
supervised
learning,
and
is
carried
out
through
backpropagation.
We
can
represent
the
degree
of
error
in
an
output
node
j{\displaystyle
j}
in
the
n{\displaystyle
n}th
data
point
(training
example)
by
ej(n)=dj(n)−yj(n){\displaystyle
e_{j}(n)=d_{j}(n)-y_{j}(n)},
where
dj(n){\displaystyle
d_{j}(n)}
is
the
desired
target
value
for
n{\displaystyle
n}th
data
point
at
node
j{\displaystyle
j},
and
yj(n){\displaystyle
y_{j}(n)}
is
the
value
produced
at
node
j{\displaystyle
j}
when
the
n{\displaystyle
n}th
data
point
is
given
as
an
input.
The
node
weights
can
then
be
adjusted
based
on
corrections
that
minimize
the
error
in
the
entire
output
for
the
n{\displaystyle
n}th
data
point,
given
by
E(n)=12∑output
node
jej2(n){\displaystyle
{\mathcal
{E}}(n)={\frac
{1}{2}}\sum
_{{\text{output
node
}}j}e_{j}^{2}(n)}.Using
gradient
descent,
the
change
in
each
weight
wij{\displaystyle
w_{ij}}
is
Δwji(n)=−η∂E(n)∂vj(n)yi(n){\displaystyle
\Delta
w_{ji}(n)=-\eta
{\frac
{\partial
{\mathcal
{E}}(n)}{\partial
v_{j}(n)}}y_{i}(n)}where
yi(n){\displaystyle
y_{i}(n)}
is
the
output
of
the
previous
neuron
i{\displaystyle
i},
and
η{\displaystyle
\eta
}
is
the
learning
rate,
which
is
selected
to
ensure
that
the
weights
quickly
converge
to
a
response,
without
oscillations.
In
the
previous
expression,
∂E(n)∂vj(n){\displaystyle
{\frac
{\partial
{\mathcal
{E}}(n)}{\partial
v_{j}(n)}}}
denotes
the
partial
derivate
of
the
error
E(n){\displaystyle
{\mathcal
{E}}(n)}
according
to
the
weighted
sum
vj(n){\displaystyle
v_{j}(n)}
of
the
input
connections
of
neuron
i{\displaystyle
i}.
The
derivative
to
be
calculated
depends
on
the
induced
local
field
vj{\displaystyle
v_{j}},
which
itself
varies.
It
is
easy
to
prove
that
for
an
output
node
this
derivative
can
be
simplified
to
−∂E(n)∂vj(n)=ej(n)ϕ′(vj(n)){\displaystyle
-{\frac
{\partial
{\mathcal
{E}}(n)}{\partial
v_{j}(n)}}=e_{j}(n)\phi
^{\prime
}(v_{j}(n))}where
ϕ′{\displaystyle
\phi
^{\prime
}}
is
the
derivative
of
the
activation
function
described
above,
which
itself
does
not
vary.
The
analysis
is
more
difficult
for
the
change
in
weights
to
a
hidden
node,
but
it
can
be
shown
that
the
relevant
derivative
is
−∂E(n)∂vj(n)=ϕ′(vj(n))∑k−∂E(n)∂vk(n)wkj(n){\displaystyle
-{\frac
{\partial
{\mathcal
{E}}(n)}{\partial
v_{j}(n)}}=\phi
^{\prime
}(v_{j}(n))\sum
_{k}-{\frac
{\partial
{\mathcal
{E}}(n)}{\partial
v_{k}(n)}}w_{kj}(n)}.This
depends
on
the
change
in
weights
of
the
k{\displaystyle
k}th
nodes,
which
represent
the
output
layer.
So
to
change
the
hidden
layer
weights,
the
output
layer
weights
change
according
to
the
derivative
of
the
activation
function,
and
so
this
algorithm
represents
a
backpropagation
of
the
activation
function.
==
History
==
===
Linear
neural
network
===
The
simplest
kind
of
feedforward
neural
network
is
a
linear
network,
which
consists
of
a
single
layer
of
output
nodes;
the
inputs
are
fed
directly
to
the
outputs
via
a
series
of
weights.
The
sum
of
the
products
of
the
weights
and
the
inputs
is
calculated
in
each
node.
The
mean
squared
errors
between
these
calculated
outputs
and
a
given
target
values
are
minimized
by
creating
an
adjustment
to
the
weights.
This
technique
has
been
known
for
over
two
centuries
as
the
method
of
least
squares
or
linear
regression.
It
was
used
as
a
means
of
finding
a
good
rough
linear
fit
to
a
set
of
points
by
Legendre
(1805)
and
Gauss
(1795)
for
the
prediction
of
planetary
movement.
===
Perceptron
===
If
using
a
threshold,
i.e.
a
linear
activation
function,
the
resulting
linear
threshold
unit
is
called
a
perceptron.
(Often
the
term
is
used
to
denote
just
one
of
these
units.)
Multiple
parallel
linear
units
are
able
to
approximate
any
continuous
function
from
a
compact
interval
of
the
real
numbers
into
the
interval
[−1,1]
despite
the
limited
computational
power
of
single
unit
with
a
linear
threshold
function.
This
result
can
be
found
in
Peter
Auer,
Harald
Burgsteiner
and
Wolfgang
Maass
"A
learning
rule
for
very
simple
universal
approximators
consisting
of
a
single
layer
of
perceptrons".Perceptrons
can
be
trained
by
a
simple
learning
algorithm
that
is
usually
called
the
delta
rule.
It
calculates
the
errors
between
calculated
output
and
sample
output
data,
and
uses
this
to
create
an
adjustment
to
the
weights,
thus
implementing
a
form
of
gradient
descent.
===
Multilayer
perceptron
===
A
multilayer
perceptron
(MLP)
is
a
misnomer
for
a
modern
feedforward
artificial
neural
network,
consisting
of
fully
connected
neurons
with
a
nonlinear
kind
of
activation
function,
organized
in
at
least
three
layers,
notable
for
being
able
to
distinguish
data
that
is
not
linearly
separable.
It
is
a
misnomer
because
the
original
perceptron
used
a
Heaviside
step
function,
instead
of
a
nonlinear
kind
of
activation
function
(used
by
modern
networks).
==
Other
feedforward
networks
==
Examples
of
other
feedforward
networks
include
convolutional
neural
networks
and
radial
basis
function
networks,
which
use
a
different
activation
function.
==
See
also
==
Hopfield
network
Feed-forward
Backpropagation
Rprop
==
References
==
==
External
links
==
Feedforward
neural
networks
tutorial
Feedforward
Neural
Network:
Example
Feedforward
Neural
Networks:
An
Introduction
Gemini
is
a
family
of
multimodal
large
language
models
developed
by
Google
DeepMind,
serving
as
the
successor
to
LaMDA
and
PaLM
2.
Comprising
Gemini
Ultra,
Gemini
Pro,
and
Gemini
Nano,
it
was
announced
on
December
6,
2023,
positioned
as
a
competitor
to
OpenAI's
GPT-4.
It
powers
the
generative
artificial
intelligence
chatbot
of
the
same
name.
==
History
==
===
Development
===
Google
announced
Gemini,
a
large
language
model
(LLM)
developed
by
subsidiary
Google
DeepMind,
during
the
Google
I/O
keynote
on
May
10,
2023.
It
was
positioned
as
a
more
powerful
successor
to
PaLM
2,
which
was
also
unveiled
at
the
event,
with
Google
CEO
Sundar
Pichai
stating
that
Gemini
was
still
in
its
early
developmental
stages.
Unlike
other
LLMs,
Gemini
was
said
to
be
unique
in
that
it
was
not
trained
on
a
text
corpus
alone
and
was
designed
to
be
multimodal,
meaning
it
could
process
multiple
types
of
data
simultaneously,
including
text,
images,
audio,
video,
and
computer
code.
It
had
been
developed
as
a
collaboration
between
DeepMind
and
Google
Brain,
two
branches
of
Google
that
had
been
merged
as
Google
DeepMind
the
previous
month.
In
an
interview
with
Wired,
DeepMind
CEO
Demis
Hassabis
touted
Gemini's
advanced
capabilities,
which
he
believed
would
allow
the
algorithm
to
trump
OpenAI's
ChatGPT,
which
runs
on
GPT-4
and
whose
growing
popularity
had
been
aggressively
challenged
by
Google
with
LaMDA
and
Bard.
Hassabis
highlighted
the
strengths
of
DeepMind's
AlphaGo
program,
which
gained
worldwide
attention
in
2016
when
it
defeated
Go
champion
Lee
Sedol,
saying
that
Gemini
would
combine
the
power
of
AlphaGo
and
other
Google–DeepMind
LLMs.In
August
2023,
The
Information
published
a
report
outlining
Google's
roadmap
for
Gemini,
revealing
that
the
company
was
targeting
a
launch
date
of
late
2023.
According
to
the
report,
Google
hoped
to
surpass
OpenAI
and
other
competitors
by
combining
conversational
text
capabilities
present
in
most
LLMs
with
artificial
intelligence–powered
image
generation,
allowing
it
to
create
contextual
images
and
be
adapted
for
a
wider
range
of
use
cases.
Like
Bard,
Google
co-founder
Sergey
Brin
was
summoned
out
of
retirement
to
assist
in
the
development
of
Gemini,
along
with
hundreds
of
other
engineers
from
Google
Brain
and
DeepMind;
he
was
later
credited
as
a
"core
contributor"
to
Gemini.
Because
Gemini
was
being
trained
on
transcripts
of
YouTube
videos,
lawyers
were
brought
in
to
filter
out
any
potentially
copyrighted
materials.With
news
of
Gemini's
impending
launch,
OpenAI
hastened
its
work
on
integrating
GPT-4
with
multimodal
features
similar
to
those
of
Gemini.
The
Information
reported
in
September
that
several
companies
had
been
granted
early
access
to
"an
early
version"
of
the
LLM,
which
Google
intended
to
make
available
to
clients
through
Google
Cloud's
Vertex
AI
service.
The
publication
also
stated
that
Google
was
arming
Gemini
to
compete
with
both
GPT-4
and
Microsoft's
GitHub
Copilot.
===
Launch
===
On
December
6,
2023,
Pichai
and
Hassabis
announced
"Gemini
1.0"
at
a
virtual
press
conference.
It
comprised
three
models:
Gemini
Ultra,
designed
for
"highly
complex
tasks";
Gemini
Pro,
designed
for
"a
wide
range
of
tasks";
and
Gemini
Nano,
designed
for
"on-device
tasks".
At
launch,
Gemini
Pro
and
Nano
were
integrated
into
Bard
and
the
Pixel
8
Pro
smartphone,
respectively,
while
Gemini
Ultra
was
set
to
power
"Bard
Advanced"
and
become
available
to
software
developers
in
early
2024.
Other
products
that
Google
intended
to
incorporate
Gemini
into
included
Search,
Ads,
Chrome,
Duet
AI
on
Google
Workspace,
and
AlphaCode
2.
It
was
made
available
only
in
English.
Touted
as
Google's
"largest
and
most
capable
AI
model"
and
designed
to
emulate
human
behavior,
the
company
stated
that
Gemini
would
not
be
made
widely
available
until
the
following
year
due
to
the
need
for
"extensive
safety
testing".
Gemini
was
trained
on
and
powered
by
Google's
Tensor
Processing
Units
(TPUs),
and
the
name
is
in
reference
to
the
DeepMind–Google
Brain
merger
as
well
as
NASA's
Project
Gemini.Gemini
Ultra
was
said
to
have
outperformed
GPT-4,
Anthropic's
Claude
2,
Inflection
AI's
Inflection-2,
Meta's
LLaMA
2,
and
xAI's
Grok
1
on
a
variety
of
industry
benchmarks,
while
Gemini
Pro
was
said
to
have
outperformed
GPT-3.5.
Gemini
Ultra
was
also
the
first
language
model
to
outperform
human
experts
on
the
57-subject
Massive
Multitask
Language
Understanding
(MMLU)
test,
obtaining
a
score
of
90%.
Gemini
Pro
was
made
available
to
Google
Cloud
customers
on
AI
Studio
and
Vertex
AI
on
December
13,
while
Gemini
Nano
will
be
made
available
to
Android
developers
as
well.
Hassabis
further
revealed
that
DeepMind
was
exploring
how
Gemini
could
be
"combined
with
robotics
to
physically
interact
with
the
world".
In
accordance
with
an
executive
order
signed
by
U.S.
President
Joe
Biden
in
October,
Google
stated
that
it
would
share
testing
results
of
Gemini
Ultra
with
the
federal
government
of
the
United
States.
Similarly,
the
company
was
engaged
in
discussions
with
the
government
of
the
United
Kingdom
to
comply
with
the
principles
laid
out
at
the
AI
Safety
Summit
at
Bletchley
Park
in
November.
===
Updates
===
Google
partnered
with
Samsung
to
integrate
Gemini
Nano
and
Gemini
Pro
into
its
Galaxy
S24
smartphone
lineup
in
January
2024.
The
following
month,
Bard
and
Duet
AI
were
unified
under
the
Gemini
brand,
with
"Gemini
Advanced
with
Ultra
1.0"
debuting
via
a
new
"AI
Premium"
tier
of
the
Google
One
subscription
service.
Gemini
Pro
also
received
a
global
launch.In
February,
Google
launched
"Gemini
1.5"
in
a
limited
capacity,
positioned
as
a
more
powerful
and
capable
model
than
1.0
Ultra.
This
"step
change"
was
achieved
through
various
technical
advancements,
including
a
new
architecture,
a
mixture-of-experts
approach,
and
a
larger
one-million-token
context
window,
which
equates
to
roughly
an
hour
of
silent
video,
11
hours
of
audio,
30,000
lines
of
code,
or
700,000
words.
The
same
month,
Google
debuted
Gemma,
a
family
of
free
and
open-source
LLMs
that
serve
as
a
lightweight
version
of
Gemini.
They
come
in
two
sizes,
with
a
neural
network
with
two
and
seven
billion
parameters,
respectively.
Multiple
publications
viewed
this
as
an
response
to
Meta
and
others
open-sourcing
their
AI
models,
and
a
stark
reversal
from
Google's
longstanding
practice
of
keeping
its
AI
proprietary.
==
Technical
specifications
==
The
first
generation
of
Gemini
("Gemini
1")
has
three
models,
with
the
same
software
architecture.
They
are
decoder-only
transformers,
with
modifications
to
allow
efficient
training
and
inference
on
TPUs.
They
have
a
context
length
of
32,768
tokens,
with
multi-query
attention.
Two
versions
of
Gemini
Nano,
Nano-1
(1.8
billion
parameters)
and
Nano-2
(3.25
billion
parameters),
are
distilled
from
larger
Gemini
models,
designed
for
use
by
edge
devices
such
as
smartphones.
As
Gemini
is
multimodal,
each
context
window
can
contain
multiple
forms
of
input.
The
different
modes
can
be
interleaved
and
do
not
have
to
be
presented
in
a
fixed
order,
allowing
for
a
multimodal
conversation.
For
example,
the
user
might
open
the
conversation
with
a
mix
of
text,
picture,
video,
and
audio,
presented
in
any
order,
and
Gemini
might
reply
with
the
same
free
ordering.
Input
images
may
be
of
different
resolutions,
while
video
is
inputted
as
a
sequence
of
images.
Audio
is
sampled
at
16
kHz
and
then
converted
into
a
sequence
of
tokens
by
the
Universal
Speech
Model.
Gemini's
dataset
is
multimodal
and
multilingual,
consisting
of
"web
documents,
books,
and
code,
and
includ[ing]
image,
audio,
and
video
data".Demis
Hassabis
claims
that
training
Gemini
1
used
"roughly
the
same
amount
of
compute,
maybe
slightly
more
than
what
was
rumored
for
GPT-4".The
second
generation
of
Gemini
("Gemini
1.5")
has
one
model
published
so
far:
Gemini
1.5
Pro.
It
is
a
multimodal
sparse
mixture-of-experts,
with
context
length
of
"multiple
millions".
==
Reception
==
Gemini's
launch
was
preluded
by
months
of
intense
speculation
and
anticipation,
which
MIT
Technology
Review
described
as
"peak
AI
hype".
In
August
2023,
Dylan
Patel
and
Daniel
Nishball
of
research
firm
SemiAnalysis
penned
a
blog
post
declaring
that
the
release
of
Gemini
would
"eat
the
world"
and
outclass
GPT-4,
prompting
OpenAI
CEO
Sam
Altman
to
ridicule
the
duo
on
X
(formerly
Twitter).
Business
magnate
Elon
Musk,
who
co-founded
OpenAI,
weighed
in,
asking,
"Are
the
numbers
wrong?"
Hugh
Langley
of
Business
Insider
remarked
that
Gemini
would
be
a
make-or-break
moment
for
Google,
writing:
"If
Gemini
dazzles,
it
will
help
Google
change
the
narrative
that
it
was
blindsided
by
Microsoft
and
OpenAI.
If
it
disappoints,
it
will
embolden
critics
who
say
Google
has
fallen
behind."Reacting
to
its
unveiling
in
December
2023,
University
of
Washington
professor
emeritus
Oren
Etzioni
predicted
a
"tit-for-tat
arms
race"
between
Google
and
OpenAI.
Professor
Alexei
Efros
of
the
University
of
California,
Berkeley
praised
the
potential
of
Gemini's
multimodal
approach,
while
scientist
Melanie
Mitchell
of
the
Santa
Fe
Institute
called
Gemini
"very
sophisticated".
Professor
Chirag
Shah
of
the
University
of
Washington
was
less
impressed,
likening
Gemini's
launch
to
the
routineness
of
Apple's
annual
introduction
of
a
new
iPhone.
Similarly,
Stanford
University's
Percy
Liang,
the
University
of
Washington's
Emily
Bender,
and
the
University
of
Galway's
Michael
Madden
cautioned
that
it
was
difficult
to
interpret
benchmark
scores
without
insight
into
the
training
data
used.
Writing
for
Fast
Company,
Mark
Sullivan
opined
that
Google
had
the
opportunity
to
challenge
the
iPhone's
dominant
market
share,
believing
that
Apple
was
unlikely
to
have
the
capacity
to
develop
functionality
similar
to
Gemini
with
its
Siri
virtual
assistant.
Google
shares
spiked
by
5.3
percent
the
day
after
Gemini's
launch.Google
faced
criticism
for
a
demonstrative
video
of
Gemini,
which
was
not
conducted
in
real
time.
==
See
also
==
Gato,
a
multimodal
neural
network
developed
by
DeepMind
==
References
==
==
Further
reading
==
==
External
links
==
Official
website
Press
release
via
The
Keyword
Announcement
and
demo
on
YouTube
White
paper
for
1.0
and
1.5
A
generative
adversarial
network
(GAN)
is
a
class
of
machine
learning
frameworks
and
a
prominent
framework
for
approaching
generative
AI.
The
concept
was
initially
developed
by
Ian
Goodfellow
and
his
colleagues
in
June
2014.
In
a
GAN,
two
neural
networks
contest
with
each
other
in
the
form
of
a
zero-sum
game,
where
one
agent's
gain
is
another
agent's
loss.
Given
a
training
set,
this
technique
learns
to
generate
new
data
with
the
same
statistics
as
the
training
set.
For
example,
a
GAN
trained
on
photographs
can
generate
new
photographs
that
look
at
least
superficially
authentic
to
human
observers,
having
many
realistic
characteristics.
Though
originally
proposed
as
a
form
of
generative
model
for
unsupervised
learning,
GANs
have
also
proved
useful
for
semi-supervised
learning,
fully
supervised
learning,
and
reinforcement
learning.The
core
idea
of
a
GAN
is
based
on
the
"indirect"
training
through
the
discriminator,
another
neural
network
that
can
tell
how
"realistic"
the
input
seems,
which
itself
is
also
being
updated
dynamically.
This
means
that
the
generator
is
not
trained
to
minimize
the
distance
to
a
specific
image,
but
rather
to
fool
the
discriminator.
This
enables
the
model
to
learn
in
an
unsupervised
manner.
GANs
are
similar
to
mimicry
in
evolutionary
biology,
with
an
evolutionary
arms
race
between
both
networks.
==
Definition
==
===
Mathematical
===
The
original
GAN
is
defined
as
the
following
game:
Each
probability
space
(Ω,μref){\displaystyle
(\Omega
,\mu
_{\text{ref}})}
defines
a
GAN
game.
There
are
2
players:
generator
and
discriminator.
The
generator's
strategy
set
is
P(Ω){\displaystyle
{\mathcal
{P}}(\Omega
)},
the
set
of
all
probability
measures
μG{\displaystyle
\mu
_{G}}
on
Ω{\displaystyle
\Omega
}.
The
discriminator's
strategy
set
is
the
set
of
Markov
kernels
μD:Ω→P[0,1]{\displaystyle
\mu
_{D}:\Omega
\to
{\mathcal
{P}}[0,1]},
where
P[0,1]{\displaystyle
{\mathcal
{P}}[0,1]}
is
the
set
of
probability
measures
on
[0,1]{\displaystyle
[0,1]}.
The
GAN
game
is
a
zero-sum
game,
with
objective
function
The
generator
aims
to
minimize
the
objective,
and
the
discriminator
aims
to
maximize
the
objective.
The
generator's
task
is
to
approach
μG≈μref{\displaystyle
\mu
_{G}\approx
\mu
_{\text{ref}}},
that
is,
to
match
its
own
output
distribution
as
closely
as
possible
to
the
reference
distribution.
The
discriminator's
task
is
to
output
a
value
close
to
1
when
the
input
appears
to
be
from
the
reference
distribution,
and
to
output
a
value
close
to
0
when
the
input
looks
like
it
came
from
the
generator
distribution.
===
In
practice
===
The
generative
network
generates
candidates
while
the
discriminative
network
evaluates
them.
The
contest
operates
in
terms
of
data
distributions.
Typically,
the
generative
network
learns
to
map
from
a
latent
space
to
a
data
distribution
of
interest,
while
the
discriminative
network
distinguishes
candidates
produced
by
the
generator
from
the
true
data
distribution.
The
generative
network's
training
objective
is
to
increase
the
error
rate
of
the
discriminative
network
(i.e.,
"fool"
the
discriminator
network
by
producing
novel
candidates
that
the
discriminator
thinks
are
not
synthesized
(are
part
of
the
true
data
distribution)).A
known
dataset
serves
as
the
initial
training
data
for
the
discriminator.
Training
involves
presenting
it
with
samples
from
the
training
dataset
until
it
achieves
acceptable
accuracy.
The
generator
is
trained
based
on
whether
it
succeeds
in
fooling
the
discriminator.
Typically,
the
generator
is
seeded
with
randomized
input
that
is
sampled
from
a
predefined
latent
space
(e.g.
a
multivariate
normal
distribution).
Thereafter,
candidates
synthesized
by
the
generator
are
evaluated
by
the
discriminator.
Independent
backpropagation
procedures
are
applied
to
both
networks
so
that
the
generator
produces
better
samples,
while
the
discriminator
becomes
more
skilled
at
flagging
synthetic
samples.
When
used
for
image
generation,
the
generator
is
typically
a
deconvolutional
neural
network,
and
the
discriminator
is
a
convolutional
neural
network.
===
Relation
to
other
statistical
machine
learning
methods
===
GANs
are
implicit
generative
models,
which
means
that
they
do
not
explicitly
model
the
likelihood
function
nor
provide
a
means
for
finding
the
latent
variable
corresponding
to
a
given
sample,
unlike
alternatives
such
as
flow-based
generative
model.
Compared
to
fully
visible
belief
networks
such
as
WaveNet
and
PixelRNN
and
autoregressive
models
in
general,
GANs
can
generate
one
complete
sample
in
one
pass,
rather
than
multiple
passes
through
the
network.
Compared
to
Boltzmann
machines
and
nonlinear
ICA,
there
is
no
restriction
on
the
type
of
function
used
by
the
network.
Since
neural
networks
are
universal
approximators,
GANs
are
asymptotically
consistent.
Variational
autoencoders
might
be
universal
approximators,
but
it
is
not
proven
as
of
2017.
==
Mathematical
properties
==
===
Measure-theoretic
considerations
===
This
section
provides
some
of
the
mathematical
theory
behind
these
methods.
In
modern
probability
theory
based
on
measure
theory,
a
probability
space
also
needs
to
be
equipped
with
a
σ-algebra.
As
a
result,
a
more
rigorous
definition
of
the
GAN
game
would
make
the
following
changes:Each
probability
space
(Ω,B,μref){\displaystyle
(\Omega
,{\mathcal
{B}},\mu
_{\text{ref}})}
defines
a
GAN
game.
The
generator's
strategy
set
is
P(Ω,B){\displaystyle
{\mathcal
{P}}(\Omega
,{\mathcal
{B}})},
the
set
of
all
probability
measures
μG{\displaystyle
\mu
_{G}}
on
the
measure-space
(Ω,B){\displaystyle
(\Omega
,{\mathcal
{B}})}.
The
discriminator's
strategy
set
is
the
set
of
Markov
kernels
μD:(Ω,B)→P([0,1],B([0,1])){\displaystyle
\mu
_{D}:(\Omega
,{\mathcal
{B}})\to
{\mathcal
{P}}([0,1],{\mathcal
{B}}([0,1]))},
where
B([0,1]){\displaystyle
{\mathcal
{B}}([0,1])}
is
the
Borel
σ-algebra
on
[0,1]{\displaystyle
[0,1]}.Since
issues
of
measurability
never
arise
in
practice,
these
will
not
concern
us
further.
===
Choice
of
the
strategy
set
===
In
the
most
generic
version
of
the
GAN
game
described
above,
the
strategy
set
for
the
discriminator
contains
all
Markov
kernels
μD:Ω→P[0,1]{\displaystyle
\mu
_{D}:\Omega
\to
{\mathcal
{P}}[0,1]},
and
the
strategy
set
for
the
generator
contains
arbitrary
probability
distributions
μG{\displaystyle
\mu
_{G}}
on
Ω{\displaystyle
\Omega
}.
However,
as
shown
below,
the
optimal
discriminator
strategy
against
any
μG{\displaystyle
\mu
_{G}}
is
deterministic,
so
there
is
no
loss
of
generality
in
restricting
the
discriminator's
strategies
to
deterministic
functions
D:Ω→[0,1]{\displaystyle
D:\Omega
\to
[0,1]}.
In
most
applications,
D{\displaystyle
D}
is
a
deep
neural
network
function.
As
for
the
generator,
while
μG{\displaystyle
\mu
_{G}}
could
theoretically
be
any
computable
probability
distribution,
in
practice,
it
is
usually
implemented
as
a
pushforward:
μG=μZ∘G−1{\displaystyle
\mu
_{G}=\mu
_{Z}\circ
G^{-1}}.
That
is,
start
with
a
random
variable
z∼μZ{\displaystyle
z\sim
\mu
_{Z}},
where
μZ{\displaystyle
\mu
_{Z}}
is
a
probability
distribution
that
is
easy
to
compute
(such
as
the
uniform
distribution,
or
the
Gaussian
distribution),
then
define
a
function
G:ΩZ→Ω{\displaystyle
G:\Omega
_{Z}\to
\Omega
}.
Then
the
distribution
μG{\displaystyle
\mu
_{G}}
is
the
distribution
of
G(z){\displaystyle
G(z)}.
Consequently,
the
generator's
strategy
is
usually
defined
as
just
G{\displaystyle
G},
leaving
z∼μZ{\displaystyle
z\sim
\mu
_{Z}}
implicit.
In
this
formalism,
the
GAN
game
objective
is
===
Generative
reparametrization
===
The
GAN
architecture
has
two
main
components.
One
is
casting
optimization
into
a
game,
of
form
minGmaxDL(G,D){\displaystyle
\min
_{G}\max
_{D}L(G,D)},
which
is
different
from
the
usual
kind
of
optimization,
of
form
minθL(θ){\displaystyle
\min
_{\theta
}L(\theta
)}.
The
other
is
the
decomposition
of
μG{\displaystyle
\mu
_{G}}
into
μZ∘G−1{\displaystyle
\mu
_{Z}\circ
G^{-1}},
which
can
be
understood
as
a
reparametrization
trick.
To
see
its
significance,
one
must
compare
GAN
with
previous
methods
for
learning
generative
models,
which
were
plagued
with
"intractable
probabilistic
computations
that
arise
in
maximum
likelihood
estimation
and
related
strategies".At
the
same
time,
Kingma
and
Welling
and
Rezende
et
al.
developed
the
same
idea
of
reparametrization
into
a
general
stochastic
backpropagation
method.
Among
its
first
applications
was
the
variational
autoencoder.
===
Move
order
and
strategic
equilibria
===
In
the
original
paper,
as
well
as
most
subsequent
papers,
it
is
usually
assumed
that
the
generator
moves
first,
and
the
discriminator
moves
second,
thus
giving
the
following
minimax
game:
If
both
the
generator's
and
the
discriminator's
strategy
sets
are
spanned
by
a
finite
number
of
strategies,
then
by
the
minimax
theorem,that
is,
the
move
order
does
not
matter.
However,
since
the
strategy
sets
are
both
not
finitely
spanned,
the
minimax
theorem
does
not
apply,
and
the
idea
of
an
"equilibrium"
becomes
delicate.
To
wit,
there
are
the
following
different
concepts
of
equilibrium:
Equilibrium
when
generator
moves
first,
and
discriminator
moves
second:
Equilibrium
when
discriminator
moves
first,
and
generator
moves
second:
Nash
equilibrium
(μ^D,μ^G){\displaystyle
({\hat
{\mu
}}_{D},{\hat
{\mu
}}_{G})},
which
is
stable
under
simultaneous
move
order:For
general
games,
these
equilibria
do
not
have
to
agree,
or
even
to
exist.
For
the
original
GAN
game,
these
equilibria
all
exist,
and
are
all
equal.
However,
for
more
general
GAN
games,
these
do
not
necessarily
exist,
or
agree.
===
Main
theorems
for
GAN
game
===
The
original
GAN
paper
proved
the
following
two
theorems:
Interpretation:
For
any
fixed
generator
strategy
μG{\displaystyle
\mu
_{G}},
the
optimal
discriminator
keeps
track
of
the
likelihood
ratio
between
the
reference
distribution
and
the
generator
distribution:where
σ{\displaystyle
\sigma
}
is
the
logistic
function.
In
particular,
if
the
prior
probability
for
an
image
x{\displaystyle
x}
to
come
from
the
reference
distribution
is
equal
to
12{\displaystyle
{\frac
{1}{2}}},
then
D(x){\displaystyle
D(x)}
is
just
the
posterior
probability
that
x{\displaystyle
x}
came
from
the
reference
distribution:
==
Training
and
evaluating
GAN
==
===
Training
===
====
Unstable
convergence
====
While
the
GAN
game
has
a
unique
global
equilibrium
point
when
both
the
generator
and
discriminator
have
access
to
their
entire
strategy
sets,
the
equilibrium
is
no
longer
guaranteed
when
they
have
a
restricted
strategy
set.In
practice,
the
generator
has
access
only
to
measures
of
form
μZ∘Gθ−1{\displaystyle
\mu
_{Z}\circ
G_{\theta
}^{-1}},
where
Gθ{\displaystyle
G_{\theta
}}
is
a
function
computed
by
a
neural
network
with
parameters
θ{\displaystyle
\theta
},
and
μZ{\displaystyle
\mu
_{Z}}
is
an
easily
sampled
distribution,
such
as
the
uniform
or
normal
distribution.
Similarly,
the
discriminator
has
access
only
to
functions
of
form
Dζ{\displaystyle
D_{\zeta
}},
a
function
computed
by
a
neural
network
with
parameters
ζ{\displaystyle
\zeta
}.
These
restricted
strategy
sets
take
up
a
vanishingly
small
proportion
of
their
entire
strategy
sets.Further,
even
if
an
equilibrium
still
exists,
it
can
only
be
found
by
searching
in
the
high-dimensional
space
of
all
possible
neural
network
functions.
The
standard
strategy
of
using
gradient
descent
to
find
the
equilibrium
often
does
not
work
for
GAN,
and
often
the
game
"collapses"
into
one
of
several
failure
modes.
To
improve
the
convergence
stability,
some
training
strategies
start
with
an
easier
task,
such
as
generating
low-resolution
images
or
simple
images
(one
object
with
uniform
background),
and
gradually
increase
the
difficulty
of
the
task
during
training.
This
essentially
translates
to
applying
a
curriculum
learning
scheme.
====
Mode
collapse
====
GANs
often
suffer
from
mode
collapse
where
they
fail
to
generalize
properly,
missing
entire
modes
from
the
input
data.
For
example,
a
GAN
trained
on
the
MNIST
dataset
containing
many
samples
of
each
digit
might
only
generate
pictures
of
digit
0.
This
was
named
in
the
first
paper
as
the
"Helvetica
scenario".
One
way
this
can
happen
is
if
the
generator
learns
too
fast
compared
to
the
discriminator.
If
the
discriminator
D{\displaystyle
D}
is
held
constant,
then
the
optimal
generator
would
only
output
elements
of
arg⁡maxxD(x){\displaystyle
\arg
\max
_{x}D(x)}.
So
for
example,
if
during
GAN
training
for
generating
MNIST
dataset,
for
a
few
epochs,
the
discriminator
somehow
prefers
the
digit
0
slightly
more
than
other
digits,
the
generator
may
seize
the
opportunity
to
generate
only
digit
0,
then
be
unable
to
escape
the
local
minimum
after
the
discriminator
improves.
Some
researchers
perceive
the
root
problem
to
be
a
weak
discriminative
network
that
fails
to
notice
the
pattern
of
omission,
while
others
assign
blame
to
a
bad
choice
of
objective
function.
Many
solutions
have
been
proposed,
but
it
is
still
an
open
problem.Even
the
state-of-the-art
architecture,
BigGAN
(2019),
could
not
avoid
mode
collapse.
The
authors
resorted
to
"allowing
collapse
to
occur
at
the
later
stages
of
training,
by
which
time
a
model
is
sufficiently
trained
to
achieve
good
results".
====
Two
time-scale
update
rule
====
The
two
time-scale
update
rule
(TTUR)
is
proposed
to
make
GAN
convergence
more
stable
by
making
the
learning
rate
of
the
generator
lower
than
that
of
the
discriminator.
The
authors
argued
that
the
generator
should
move
slower
than
the
discriminator,
so
that
it
does
not
"drive
the
discriminator
steadily
into
new
regions
without
capturing
its
gathered
information".
They
proved
that
a
general
class
of
games
that
included
the
GAN
game,
when
trained
under
TTUR,
"converges
under
mild
assumptions
to
a
stationary
local
Nash
equilibrium".They
also
proposed
using
the
Adam
stochastic
optimization
to
avoid
mode
collapse,
as
well
as
the
Fréchet
inception
distance
for
evaluating
GAN
performances.
====
Vanishing
gradient
====
Conversely,
if
the
discriminator
learns
too
fast
compared
to
the
generator,
then
the
discriminator
could
almost
perfectly
distinguish
μGθ,μref{\displaystyle
\mu
_{G_{\theta
}},\mu
_{\text{ref}}}.
In
such
case,
the
generator
Gθ{\displaystyle
G_{\theta
}}
could
be
stuck
with
a
very
high
loss
no
matter
which
direction
it
changes
its
θ{\displaystyle
\theta
},
meaning
that
the
gradient
∇θL(Gθ,Dζ){\displaystyle
\nabla
_{\theta
}L(G_{\theta
},D_{\zeta
})}
would
be
close
to
zero.
In
such
case,
the
generator
cannot
learn,
a
case
of
the
vanishing
gradient
problem.Intuitively
speaking,
the
discriminator
is
too
good,
and
since
the
generator
cannot
take
any
small
step
(only
small
steps
are
considered
in
gradient
descent)
to
improve
its
payoff,
it
does
not
even
try.
One
important
method
for
solving
this
problem
is
the
Wasserstein
GAN.
===
Evaluation
===
GANs
are
usually
evaluated
by
Inception
score
(IS),
which
measures
how
varied
the
generator's
outputs
are
(as
classified
by
an
image
classifier,
usually
Inception-v3),
or
Fréchet
inception
distance
(FID),
which
measures
how
similar
the
generator's
outputs
are
to
a
reference
set
(as
classified
by
a
learned
image
featurizer,
such
as
Inception-v3
without
its
final
layer).
Many
papers
that
propose
new
GAN
architectures
for
image
generation
report
how
their
architectures
break
the
state
of
the
art
on
FID
or
IS.
Another
evaluation
method
is
the
Learned
Perceptual
Image
Patch
Similarity
(LPIPS),
which
starts
with
a
learned
image
featurizer
fθ:Image→Rn{\displaystyle
f_{\theta
}:{\text{Image}}\to
\mathbb
{R}
^{n}},
and
finetunes
it
by
supervised
learning
on
a
set
of
(x,x′,PerceptualDifference(x,x′)){\displaystyle
(x,x',{\text{PerceptualDifference}}(x,x'))},
where
x{\displaystyle
x}
is
an
image,
x′{\displaystyle
x'}
is
a
perturbed
version
of
it,
and
PerceptualDifference(x,x′){\displaystyle
{\text{PerceptualDifference}}(x,x')}
is
how
much
they
differ,
as
reported
by
human
subjects.
The
model
is
finetuned
so
that
it
can
approximate
‖fθ(x)−fθ(x′)‖≈PerceptualDifference(x,x′){\displaystyle
\|f_{\theta
}(x)-f_{\theta
}(x')\|\approx
{\text{PerceptualDifference}}(x,x')}.
This
finetuned
model
is
then
used
to
define
LPIPS(x,x′):=‖fθ(x)−fθ(x′)‖{\displaystyle
{\text{LPIPS}}(x,x'):=\|f_{\theta
}(x)-f_{\theta
}(x')\|}.Other
evaluation
methods
are
reviewed
in.
==
Variants
==
There
is
a
veritable
zoo
of
GAN
variants.
Some
of
the
most
prominent
are
as
follows:
===
Conditional
GAN
===
Conditional
GANs
are
similar
to
standard
GANs
except
they
allow
the
model
to
conditionally
generate
samples
based
on
additional
information.
For
example,
if
we
want
to
generate
a
cat
face
given
a
dog
picture,
we
could
use
a
conditional
GAN.
The
generator
in
a
GAN
game
generates
μG{\displaystyle
\mu
_{G}},
a
probability
distribution
on
the
probability
space
Ω{\displaystyle
\Omega
}.
This
leads
to
the
idea
of
a
conditional
GAN,
where
instead
of
generating
one
probability
distribution
on
Ω{\displaystyle
\Omega
},
the
generator
generates
a
different
probability
distribution
μG(c){\displaystyle
\mu
_{G}(c)}
on
Ω{\displaystyle
\Omega
},
for
each
given
class
label
c{\displaystyle
c}.
For
example,
for
generating
images
that
look
like
ImageNet,
the
generator
should
be
able
to
generate
a
picture
of
cat
when
given
the
class
label
"cat".
In
the
original
paper,
the
authors
noted
that
GAN
can
be
trivially
extended
to
conditional
GAN
by
providing
the
labels
to
both
the
generator
and
the
discriminator.
Concretely,
the
conditional
GAN
game
is
just
the
GAN
game
with
class
labels
provided:where
μC{\displaystyle
\mu
_{C}}
is
a
probability
distribution
over
classes,
μref(c){\displaystyle
\mu
_{\text{ref}}(c)}
is
the
probability
distribution
of
real
images
of
class
c{\displaystyle
c},
and
μG(c){\displaystyle
\mu
_{G}(c)}
the
probability
distribution
of
images
generated
by
the
generator
when
given
class
label
c{\displaystyle
c}.
In
2017,
a
conditional
GAN
learned
to
generate
1000
image
classes
of
ImageNet.
===
GANs
with
alternative
architectures
===
The
GAN
game
is
a
general
framework
and
can
be
run
with
any
reasonable
parametrization
of
the
generator
G{\displaystyle
G}
and
discriminator
D{\displaystyle
D}.
In
the
original
paper,
the
authors
demonstrated
it
using
multilayer
perceptron
networks
and
convolutional
neural
networks.
Many
alternative
architectures
have
been
tried.
Deep
convolutional
GAN
(DCGAN):
For
both
generator
and
discriminator,
uses
only
deep
networks
consisting
entirely
of
convolution-deconvolution
layers,
that
is,
fully
convolutional
networks.Self-attention
GAN
(SAGAN):
Starts
with
the
DCGAN,
then
adds
residually-connected
standard
self-attention
modules
to
the
generator
and
discriminator.
Variational
autoencoder
GAN
(VAEGAN):
Uses
a
variational
autoencoder
(VAE)
for
the
generator.
Transformer
GAN
(TransGAN):
Uses
the
pure
transformer
architecture
for
both
the
generator
and
discriminator,
entirely
devoid
of
convolution-deconvolution
layers.
Flow-GAN:
Uses
flow-based
generative
model
for
the
generator,
allowing
efficient
computation
of
the
likelihood
function.
===
GANs
with
alternative
objectives
===
Many
GAN
variants
are
merely
obtained
by
changing
the
loss
functions
for
the
generator
and
discriminator.
Original
GAN:
We
recast
the
original
GAN
objective
into
a
form
more
convenient
for
comparison:
Original
GAN,
non-saturating
loss:
This
objective
for
generator
was
recommended
in
the
original
paper
for
faster
convergence.The
effect
of
using
this
objective
is
analyzed
in
Section
2.2.2
of
Arjovsky
et
al.Original
GAN,
maximum
likelihood:
where
σ{\displaystyle
\sigma
}
is
the
logistic
function.
When
the
discriminator
is
optimal,
the
generator
gradient
is
the
same
as
in
maximum
likelihood
estimation,
even
though
GAN
cannot
perform
maximum
likelihood
estimation
itself.Hinge
loss
GAN:Least
squares
GAN:where
a,b,c{\displaystyle
a,b,c}
are
parameters
to
be
chosen.
The
authors
recommended
a=−1,b=1,c=0{\displaystyle
a=-1,b=1,c=0}.
===
Wasserstein
GAN
(WGAN)
===
The
Wasserstein
GAN
modifies
the
GAN
game
at
two
points:
The
discriminator's
strategy
set
is
the
set
of
measurable
functions
of
type
D:Ω→R{\displaystyle
D:\Omega
\to
\mathbb
{R}
}
with
bounded
Lipschitz
norm:
‖D‖L≤K{\displaystyle
\|D\|_{L}\leq
K},
where
K{\displaystyle
K}
is
a
fixed
positive
constant.
The
objective
isOne
of
its
purposes
is
to
solve
the
problem
of
mode
collapse
(see
above).
The
authors
claim
"In
no
experiment
did
we
see
evidence
of
mode
collapse
for
the
WGAN
algorithm".
===
GANs
with
more
than
2
players
===
====
Adversarial
autoencoder
====
An
adversarial
autoencoder
(AAE)
is
more
autoencoder
than
GAN.
The
idea
is
to
start
with
a
plain
autoencoder,
but
train
a
discriminator
to
discriminate
the
latent
vectors
from
a
reference
distribution
(often
the
normal
distribution).
====
InfoGAN
====
In
conditional
GAN,
the
generator
receives
both
a
noise
vector
z{\displaystyle
z}
and
a
label
c{\displaystyle
c},
and
produces
an
image
G(z,c){\displaystyle
G(z,c)}.
The
discriminator
receives
image-label
pairs
(x,c){\displaystyle
(x,c)},
and
computes
D(x,c){\displaystyle
D(x,c)}.
When
the
training
dataset
is
unlabeled,
conditional
GAN
does
not
work
directly.
The
idea
of
InfoGAN
is
to
decree
that
every
latent
vector
in
the
latent
space
can
be
decomposed
as
(z,c){\displaystyle
(z,c)}:
an
incompressible
noise
part
z{\displaystyle
z},
and
an
informative
label
part
c{\displaystyle
c},
and
encourage
the
generator
to
comply
with
the
decree,
by
encouraging
it
to
maximize
I(c,G(z,c)){\displaystyle
I(c,G(z,c))},
the
mutual
information
between
c{\displaystyle
c}
and
G(z,c){\displaystyle
G(z,c)},
while
making
no
demands
on
the
mutual
information
z{\displaystyle
z}
between
G(z,c){\displaystyle
G(z,c)}.
Unfortunately,
I(c,G(z,c)){\displaystyle
I(c,G(z,c))}
is
intractable
in
general,
The
key
idea
of
InfoGAN
is
Variational
Mutual
Information
Maximization:
indirectly
maximize
it
by
maximizing
a
lower
boundwhere
Q{\displaystyle
Q}
ranges
over
all
Markov
kernels
of
type
Q:ΩY→P(ΩC){\displaystyle
Q:\Omega
_{Y}\to
{\mathcal
{P}}(\Omega
_{C})}.
The
InfoGAN
game
is
defined
as
follows:Three
probability
spaces
define
an
InfoGAN
game:
(ΩX,μref){\displaystyle
(\Omega
_{X},\mu
_{\text{ref}})},
the
space
of
reference
images.
(ΩZ,μZ){\displaystyle
(\Omega
_{Z},\mu
_{Z})},
the
fixed
random
noise
generator.
(ΩC,μC){\displaystyle
(\Omega
_{C},\mu
_{C})},
the
fixed
random
information
generator.There
are
3
players
in
2
teams:
generator,
Q,
and
discriminator.
The
generator
and
Q
are
on
one
team,
and
the
discriminator
on
the
other
team.
The
objective
function
iswhere
LGAN(G,D)=Ex∼μref,[ln⁡D(x)]+Ez∼μZ[ln⁡(1−D(G(z,c)))]{\displaystyle
L_{GAN}(G,D)=\mathbb
{E}
_{x\sim
\mu
_{\text{ref}},}[\ln
D(x)]+\mathbb
{E}
_{z\sim
\mu
_{Z}}[\ln(1-D(G(z,c)))]}
is
the
original
GAN
game
objective,
and
I^(G,Q)=Ez∼μZ,c∼μC[ln⁡Q(c|G(z,c))]{\displaystyle
{\hat
{I}}(G,Q)=\mathbb
{E}
_{z\sim
\mu
_{Z},c\sim
\mu
_{C}}[\ln
Q(c|G(z,c))]}
Generator-Q
team
aims
to
minimize
the
objective,
and
discriminator
aims
to
maximize
it:
====
Bidirectional
GAN
(BiGAN)
====
The
standard
GAN
generator
is
a
function
of
type
G:ΩZ→ΩX{\displaystyle
G:\Omega
_{Z}\to
\Omega
_{X}},
that
is,
it
is
a
mapping
from
a
latent
space
ΩZ{\displaystyle
\Omega
_{Z}}
to
the
image
space
ΩX{\displaystyle
\Omega
_{X}}.
This
can
be
understood
as
a
"decoding"
process,
whereby
every
latent
vector
z∈ΩZ{\displaystyle
z\in
\Omega
_{Z}}
is
a
code
for
an
image
x∈ΩX{\displaystyle
x\in
\Omega
_{X}},
and
the
generator
performs
the
decoding.
This
naturally
leads
to
the
idea
of
training
another
network
that
performs
"encoding",
creating
an
autoencoder
out
of
the
encoder-generator
pair.
Already
in
the
original
paper,
the
authors
noted
that
"Learned
approximate
inference
can
be
performed
by
training
an
auxiliary
network
to
predict
z{\displaystyle
z}
given
x{\displaystyle
x}".
The
bidirectional
GAN
architecture
performs
exactly
this.
The
BiGAN
is
defined
as
follows:
Two
probability
spaces
define
a
BiGAN
game:
(ΩX,μX){\displaystyle
(\Omega
_{X},\mu
_{X})},
the
space
of
reference
images.
(ΩZ,μZ){\displaystyle
(\Omega
_{Z},\mu
_{Z})},
the
latent
space.There
are
3
players
in
2
teams:
generator,
encoder,
and
discriminator.
The
generator
and
encoder
are
on
one
team,
and
the
discriminator
on
the
other
team.
The
generator's
strategies
are
functions
G:ΩZ→ΩX{\displaystyle
G:\Omega
_{Z}\to
\Omega
_{X}},
and
the
encoder's
strategies
are
functions
E:ΩX→ΩZ{\displaystyle
E:\Omega
_{X}\to
\Omega
_{Z}}.
The
discriminator's
strategies
are
functions
D:ΩX→[0,1]{\displaystyle
D:\Omega
_{X}\to
[0,1]}.
The
objective
function
is
Generator-encoder
team
aims
to
minimize
the
objective,
and
discriminator
aims
to
maximize
it:
In
the
paper,
they
gave
a
more
abstract
definition
of
the
objective
as:where
μE,X(dx,dz)=μX(dx)⋅δE(x)(dz){\displaystyle
\mu
_{E,X}(dx,dz)=\mu
_{X}(dx)\cdot
\delta
_{E(x)}(dz)}
is
the
probability
distribution
on
ΩX×ΩZ{\displaystyle
\Omega
_{X}\times
\Omega
_{Z}}
obtained
by
pushing
μX{\displaystyle
\mu
_{X}}
forward
via
x↦(x,E(x)){\displaystyle
x\mapsto
(x,E(x))},
and
μG,Z(dx,dz)=δG(z)(dx)⋅μZ(dz){\displaystyle
\mu
_{G,Z}(dx,dz)=\delta
_{G(z)}(dx)\cdot
\mu
_{Z}(dz)}
is
the
probability
distribution
on
ΩX×ΩZ{\displaystyle
\Omega
_{X}\times
\Omega
_{Z}}
obtained
by
pushing
μZ{\displaystyle
\mu
_{Z}}
forward
via
z↦(G(x),z){\displaystyle
z\mapsto
(G(x),z)}.
Applications
of
bidirectional
models
include
semi-supervised
learning,
interpretable
machine
learning,
and
neural
machine
translation.
====
CycleGAN
====
CycleGAN
is
an
architecture
for
performing
translations
between
two
domains,
such
as
between
photos
of
horses
and
photos
of
zebras,
or
photos
of
night
cities
and
photos
of
day
cities.
The
CycleGAN
game
is
defined
as
follows:There
are
two
probability
spaces
(ΩX,μX),(ΩY,μY){\displaystyle
(\Omega
_{X},\mu
_{X}),(\Omega
_{Y},\mu
_{Y})},
corresponding
to
the
two
domains
needed
for
translations
fore-and-back.
There
are
4
players
in
2
teams:
generators
GX:ΩX→ΩY,GY:ΩY→ΩX{\displaystyle
G_{X}:\Omega
_{X}\to
\Omega
_{Y},G_{Y}:\Omega
_{Y}\to
\Omega
_{X}},
and
discriminators
DX:ΩX→[0,1],DY:ΩY→[0,1]{\displaystyle
D_{X}:\Omega
_{X}\to
[0,1],D_{Y}:\Omega
_{Y}\to
[0,1]}.
The
objective
function
is
where
λ{\displaystyle
\lambda
}
is
a
positive
adjustable
parameter,
LGAN{\displaystyle
L_{GAN}}
is
the
GAN
game
objective,
and
Lcycle{\displaystyle
L_{cycle}}
is
the
cycle
consistency
loss:The
generators
aim
to
minimize
the
objective,
and
the
discriminators
aim
to
maximize
it:
Unlike
previous
work
like
pix2pix,
which
requires
paired
training
data,
cycleGAN
requires
no
paired
data.
For
example,
to
train
a
pix2pix
model
to
turn
a
summer
scenery
photo
to
winter
scenery
photo
and
back,
the
dataset
must
contain
pairs
of
the
same
place
in
summer
and
winter,
shot
at
the
same
angle;
cycleGAN
would
only
need
a
set
of
summer
scenery
photos,
and
an
unrelated
set
of
winter
scenery
photos.
===
GANs
with
particularly
large
or
small
scales
===
====
BigGAN
====
The
BigGAN
is
essentially
a
self-attention
GAN
trained
on
a
large
scale
(up
to
80
million
parameters)
to
generate
large
images
of
ImageNet
(up
to
512
x
512
resolution),
with
numerous
engineering
tricks
to
make
it
converge.
====
Invertible
data
augmentation
====
When
there
is
insufficient
training
data,
the
reference
distribution
μref{\displaystyle
\mu
_{\text{ref}}}
cannot
be
well-approximated
by
the
empirical
distribution
given
by
the
training
dataset.
In
such
cases,
data
augmentation
can
be
applied,
to
allow
training
GAN
on
smaller
datasets.
Naïve
data
augmentation,
however,
brings
its
problems.
Consider
the
original
GAN
game,
slightly
reformulated
as
follows:Now
we
use
data
augmentation
by
randomly
sampling
semantic-preserving
transforms
T:Ω→Ω{\displaystyle
T:\Omega
\to
\Omega
}
and
applying
them
to
the
dataset,
to
obtain
the
reformulated
GAN
game:This
is
equivalent
to
a
GAN
game
with
a
different
distribution
μref′{\displaystyle
\mu
_{\text{ref}}'},
sampled
by
T(x){\displaystyle
T(x)},
with
x∼μref,T∼μtrans{\displaystyle
x\sim
\mu
_{\text{ref}},T\sim
\mu
_{trans}}.
For
example,
if
μref{\displaystyle
\mu
_{\text{ref}}}
is
the
distribution
of
images
in
ImageNet,
and
μtrans{\displaystyle
\mu
_{trans}}
samples
identity-transform
with
probability
0.5,
and
horizontal-reflection
with
probability
0.5,
then
μref′{\displaystyle
\mu
_{\text{ref}}'}
is
the
distribution
of
images
in
ImageNet
and
horizontally-reflected
ImageNet,
combined.
The
result
of
such
training
would
be
a
generator
that
mimics
μref′{\displaystyle
\mu
_{\text{ref}}'}.
For
example,
it
would
generate
images
that
look
like
they
are
randomly
cropped,
if
the
data
augmentation
uses
random
cropping.
The
solution
is
to
apply
data
augmentation
to
both
generated
and
real
images:The
authors
demonstrated
high-quality
generation
using
just
100-picture-large
datasets.The
StyleGAN-2-ADA
paper
points
out
a
further
point
on
data
augmentation:
it
must
be
invertible.
Continue
with
the
example
of
generating
ImageNet
pictures.
If
the
data
augmentation
is
"randomly
rotate
the
picture
by
0,
90,
180,
270
degrees
with
equal
probability",
then
there
is
no
way
for
the
generator
to
know
which
is
the
true
orientation:
Consider
two
generators
G,G′{\displaystyle
G,G'},
such
that
for
any
latent
z{\displaystyle
z},
the
generated
image
G(z){\displaystyle
G(z)}
is
a
90-degree
rotation
of
G′(z){\displaystyle
G'(z)}.
They
would
have
exactly
the
same
expected
loss,
and
so
neither
is
preferred
over
the
other.
The
solution
is
to
only
use
invertible
data
augmentation:
instead
of
"randomly
rotate
the
picture
by
0,
90,
180,
270
degrees
with
equal
probability",
use
"randomly
rotate
the
picture
by
90,
180,
270
degrees
with
0.1
probability,
and
keep
the
picture
as
it
is
with
0.7
probability".
This
way,
the
generator
is
still
rewarded
to
keep
images
oriented
the
same
way
as
un-augmented
ImageNet
pictures.
Abstractly,
the
effect
of
randomly
sampling
transformations
T:Ω→Ω{\displaystyle
T:\Omega
\to
\Omega
}
from
the
distribution
μtrans{\displaystyle
\mu
_{trans}}
is
to
define
a
Markov
kernel
Ktrans:Ω→P(Ω){\displaystyle
K_{trans}:\Omega
\to
{\mathcal
{P}}(\Omega
)}.
Then,
the
data-augmented
GAN
game
pushes
the
generator
to
find
some
μ^G∈P(Ω){\displaystyle
{\hat
{\mu
}}_{G}\in
{\mathcal
{P}}(\Omega
)},
such
that
where
∗{\displaystyle
*}
is
the
Markov
kernel
convolution.
A
data-augmentation
method
is
defined
to
be
invertible
if
its
Markov
kernel
Ktrans{\displaystyle
K_{trans}}
satisfiesImmediately
by
definition,
we
see
that
composing
multiple
invertible
data-augmentation
methods
results
in
yet
another
invertible
method.
Also
by
definition,
if
the
data-augmentation
method
is
invertible,
then
using
it
in
a
GAN
game
does
not
change
the
optimal
strategy
μ^G{\displaystyle
{\hat
{\mu
}}_{G}}
for
the
generator,
which
is
still
μref{\displaystyle
\mu
_{\text{ref}}}.
There
are
two
prototypical
examples
of
invertible
Markov
kernels:
Discrete
case:
Invertible
stochastic
matrices,
when
Ω{\displaystyle
\Omega
}
is
finite.
For
example,
if
Ω={↑,↓,←,→}{\displaystyle
\Omega
=\{\uparrow
,\downarrow
,\leftarrow
,\rightarrow
\}}
is
the
set
of
four
images
of
an
arrow,
pointing
in
4
directions,
and
the
data
augmentation
is
"randomly
rotate
the
picture
by
90,
180,
270
degrees
with
probability
p{\displaystyle
p},
and
keep
the
picture
as
it
is
with
probability
(1−3p){\displaystyle
(1-3p)}",
then
the
Markov
kernel
Ktrans{\displaystyle
K_{trans}}
can
be
represented
as
a
stochastic
matrix:
and
Ktrans{\displaystyle
K_{trans}}
is
an
invertible
kernel
iff
[Ktrans]{\displaystyle
[K_{trans}]}
is
an
invertible
matrix,
that
is,
p≠1/4{\displaystyle
p\neq
1/4}.
Continuous
case:
The
gaussian
kernel,
when
Ω=Rn{\displaystyle
\Omega
=\mathbb
{R}
^{n}}
for
some
n≥1{\displaystyle
n\geq
1}.
For
example,
if
Ω=R2562{\displaystyle
\Omega
=\mathbb
{R}
^{256^{2}}}
is
the
space
of
256x256
images,
and
the
data-augmentation
method
is
"generate
a
gaussian
noise
z∼N(0,I2562){\displaystyle
z\sim
{\mathcal
{N}}(0,I_{256^{2}})},
then
add
ϵz{\displaystyle
\epsilon
z}
to
the
image",
then
Ktrans{\displaystyle
K_{trans}}
is
just
convolution
by
the
density
function
of
N(0,ϵ2I2562){\displaystyle
{\mathcal
{N}}(0,\epsilon
^{2}I_{256^{2}})}.
This
is
invertible,
because
convolution
by
a
gaussian
is
just
convolution
by
the
heat
kernel,
so
given
any
μ∈P(Rn){\displaystyle
\mu
\in
{\mathcal
{P}}(\mathbb
{R}
^{n})},
the
convolved
distribution
Ktrans∗μ{\displaystyle
K_{trans}*\mu
}
can
be
obtained
by
heating
up
Rn{\displaystyle
\mathbb
{R}
^{n}}
precisely
according
to
μ{\displaystyle
\mu
},
then
wait
for
time
ϵ2/4{\displaystyle
\epsilon
^{2}/4}.
With
that,
we
can
recover
μ{\displaystyle
\mu
}
by
running
the
heat
equation
backwards
in
time
for
ϵ2/4{\displaystyle
\epsilon
^{2}/4}.
More
examples
of
invertible
data
augmentations
are
found
in
the
paper.
====
SinGAN
====
SinGAN
pushes
data
augmentation
to
the
limit,
by
using
only
a
single
image
as
training
data
and
performing
data
augmentation
on
it.
The
GAN
architecture
is
adapted
to
this
training
method
by
using
a
multi-scale
pipeline.
The
generator
G{\displaystyle
G}
is
decomposed
into
a
pyramid
of
generators
G=G1∘G2∘⋯∘GN{\displaystyle
G=G_{1}\circ
G_{2}\circ
\cdots
\circ
G_{N}},
with
the
lowest
one
generating
the
image
GN(zN){\displaystyle
G_{N}(z_{N})}
at
the
lowest
resolution,
then
the
generated
image
is
scaled
up
to
r(GN(zN)){\displaystyle
r(G_{N}(z_{N}))},
and
fed
to
the
next
level
to
generate
an
image
GN−1(zN−1+r(GN(zN))){\displaystyle
G_{N-1}(z_{N-1}+r(G_{N}(z_{N})))}
at
a
higher
resolution,
and
so
on.
The
discriminator
is
decomposed
into
a
pyramid
as
well.
===
StyleGAN
series
===
The
StyleGAN
family
is
a
series
of
architectures
published
by
Nvidia's
research
division.
====
Progressive
GAN
====
Progressive
GAN
is
a
method
for
training
GAN
for
large-scale
image
generation
stably,
by
growing
a
GAN
generator
from
small
to
large
scale
in
a
pyramidal
fashion.
Like
SinGAN,
it
decomposes
the
generator
asG=G1∘G2∘⋯∘GN{\displaystyle
G=G_{1}\circ
G_{2}\circ
\cdots
\circ
G_{N}},
and
the
discriminator
as
D=D1∘D2∘⋯∘DN{\displaystyle
D=D_{1}\circ
D_{2}\circ
\cdots
\circ
D_{N}}.
During
training,
at
first
only
GN,DN{\displaystyle
G_{N},D_{N}}
are
used
in
a
GAN
game
to
generate
4x4
images.
Then
GN−1,DN−1{\displaystyle
G_{N-1},D_{N-1}}
are
added
to
reach
the
second
stage
of
GAN
game,
to
generate
8x8
images,
and
so
on,
until
we
reach
a
GAN
game
to
generate
1024x1024
images.
To
avoid
shock
between
stages
of
the
GAN
game,
each
new
layer
is
"blended
in"
(Figure
2
of
the
paper).
For
example,
this
is
how
the
second
stage
GAN
game
starts:
Just
before,
the
GAN
game
consists
of
the
pair
GN,DN{\displaystyle
G_{N},D_{N}}
generating
and
discriminating
4x4
images.
Just
after,
the
GAN
game
consists
of
the
pair
((1−α)+α⋅GN−1)∘u∘GN,DN∘d∘((1−α)+α⋅DN−1){\displaystyle
((1-\alpha
)+\alpha
\cdot
G_{N-1})\circ
u\circ
G_{N},D_{N}\circ
d\circ
((1-\alpha
)+\alpha
\cdot
D_{N-1})}
generating
and
discriminating
8x8
images.
Here,
the
functions
u,d{\displaystyle
u,d}
are
image
up-
and
down-sampling
functions,
and
α{\displaystyle
\alpha
}
is
a
blend-in
factor
(much
like
an
alpha
in
image
composing)
that
smoothly
glides
from
0
to
1.
====
StyleGAN-1
====
StyleGAN-1
is
designed
as
a
combination
of
Progressive
GAN
with
neural
style
transfer.The
key
architectural
choice
of
StyleGAN-1
is
a
progressive
growth
mechanism,
similar
to
Progressive
GAN.
Each
generated
image
starts
as
a
constant
4×4×512{\displaystyle
4\times
4\times
512}
array,
and
repeatedly
passed
through
style
blocks.
Each
style
block
applies
a
"style
latent
vector"
via
affine
transform
("adaptive
instance
normalization"),
similar
to
how
neural
style
transfer
uses
Gramian
matrix.
It
then
adds
noise,
and
normalize
(subtract
the
mean,
then
divide
by
the
variance).
At
training
time,
usually
only
one
style
latent
vector
is
used
per
image
generated,
but
sometimes
two
("mixing
regularization")
in
order
to
encourage
each
style
block
to
independently
perform
its
stylization
without
expecting
help
from
other
style
blocks
(since
they
might
receive
an
entirely
different
style
latent
vector).
After
training,
multiple
style
latent
vectors
can
be
fed
into
each
style
block.
Those
fed
to
the
lower
layers
control
the
large-scale
styles,
and
those
fed
to
the
higher
layers
control
the
fine-detail
styles.
Style-mixing
between
two
images
x,x′{\displaystyle
x,x'}
can
be
performed
as
well.
First,
run
a
gradient
descent
to
find
z,z′{\displaystyle
z,z'}
such
that
G(z)≈x,G(z′)≈x′{\displaystyle
G(z)\approx
x,G(z')\approx
x'}.
This
is
called
"projecting
an
image
back
to
style
latent
space".
Then,
z{\displaystyle
z}
can
be
fed
to
the
lower
style
blocks,
and
z′{\displaystyle
z'}
to
the
higher
style
blocks,
to
generate
a
composite
image
that
has
the
large-scale
style
of
x{\displaystyle
x},
and
the
fine-detail
style
of
x′{\displaystyle
x'}.
Multiple
images
can
also
be
composed
this
way.
====
StyleGAN-2
====
StyleGAN-2
improves
upon
StyleGAN-1,
by
using
the
style
latent
vector
to
transform
the
convolution
layer's
weights
instead,
thus
solving
the
"blob"
problem.This
was
updated
by
the
StyleGAN-2-ADA
("ADA"
stands
for
"adaptive"),
which
uses
invertible
data
augmentation
as
described
above.
It
also
tunes
the
amount
of
data
augmentation
applied
by
starting
at
zero,
and
gradually
increasing
it
until
an
"overfitting
heuristic"
reaches
a
target
level,
thus
the
name
"adaptive".
====
StyleGAN-3
====
StyleGAN-3
improves
upon
StyleGAN-2
by
solving
the
"texture
sticking"
problem,
which
can
be
seen
in
the
official
videos.
They
analyzed
the
problem
by
the
Nyquist–Shannon
sampling
theorem,
and
argued
that
the
layers
in
the
generator
learned
to
exploit
the
high-frequency
signal
in
the
pixels
they
operate
upon.
To
solve
this,
they
proposed
imposing
strict
lowpass
filters
between
each
generator's
layers,
so
that
the
generator
is
forced
to
operate
on
the
pixels
in
a
way
faithful
to
the
continuous
signals
they
represent,
rather
than
operate
on
them
as
merely
discrete
signals.
They
further
imposed
rotational
and
translational
invariance
by
using
more
signal
filters.
The
resulting
StyleGAN-3
is
able
to
solve
the
texture
sticking
problem,
as
well
as
generating
images
that
rotate
and
translate
smoothly.
==
Applications
==
GAN
applications
have
increased
rapidly.
===
Fashion,
art
and
advertising
===
GANs
can
be
used
to
generate
art;
The
Verge
wrote
in
March
2019
that
"The
images
created
by
GANs
have
become
the
defining
look
of
contemporary
AI
art."
GANs
can
also
be
used
to
inpaint
photographs
or
create
photos
of
imaginary
fashion
models,
with
no
need
to
hire
a
model,
photographer
or
makeup
artist,
or
pay
for
a
studio
and
transportation.
GANs
have
also
been
used
for
virtual
shadow
generation.
===
Interactive
Media
===
In
2020,
Artbreeder
was
used
to
create
the
main
antagonist
in
the
sequel
to
the
psychological
web
horror
series
Ben
Drowned.
The
author
would
later
go
on
to
praise
GAN
applications
for
their
ability
to
help
generate
assets
for
independent
artists
who
are
short
on
budget
and
manpower.
===
Science
===
GANs
can
improve
astronomical
images
and
simulate
gravitational
lensing
for
dark
matter
research.
They
were
used
in
2019
to
successfully
model
the
distribution
of
dark
matter
in
a
particular
direction
in
space
and
to
predict
the
gravitational
lensing
that
will
occur.GANs
have
been
proposed
as
a
fast
and
accurate
way
of
modeling
high
energy
jet
formation
and
modeling
showers
through
calorimeters
of
high-energy
physics
experiments.
GANs
have
also
been
trained
to
accurately
approximate
bottlenecks
in
computationally
expensive
simulations
of
particle
physics
experiments.
Applications
in
the
context
of
present
and
proposed
CERN
experiments
have
demonstrated
the
potential
of
these
methods
for
accelerating
simulation
and/or
improving
simulation
fidelity.
===
Video
games
===
In
2018,
GANs
reached
the
video
game
modding
community,
as
a
method
of
up-scaling
low-resolution
2D
textures
in
old
video
games
by
recreating
them
in
4k
or
higher
resolutions
via
image
training,
and
then
down-sampling
them
to
fit
the
game's
native
resolution
(with
results
resembling
the
supersampling
method
of
anti-aliasing).
With
proper
training,
GANs
provide
a
clearer
and
sharper
2D
texture
image
magnitudes
higher
in
quality
than
the
original,
while
fully
retaining
the
original's
level
of
details,
colors,
etc.
Known
examples
of
extensive
GAN
usage
include
Final
Fantasy
VIII,
Final
Fantasy
IX,
Resident
Evil
REmake
HD
Remaster,
and
Max
Payne.
===
AI-generated
video
===
Artificial
intelligence
art
for
video
uses
AI
to
generate
video
from
text
as
Text-to-Video
model
===
Audio
synthesis
===
===
Concerns
about
malicious
applications
===
Concerns
have
been
raised
about
the
potential
use
of
GAN-based
human
image
synthesis
for
sinister
purposes,
e.g.,
to
produce
fake,
possibly
incriminating,
photographs
and
videos.
GANs
can
be
used
to
generate
unique,
realistic
profile
photos
of
people
who
do
not
exist,
in
order
to
automate
creation
of
fake
social
media
profiles.In
2019
the
state
of
California
considered
and
passed
on
October
3,
2019,
the
bill
AB-602,
which
bans
the
use
of
human
image
synthesis
technologies
to
make
fake
pornography
without
the
consent
of
the
people
depicted,
and
bill
AB-730,
which
prohibits
distribution
of
manipulated
videos
of
a
political
candidate
within
60
days
of
an
election.
Both
bills
were
authored
by
Assembly
member
Marc
Berman
and
signed
by
Governor
Gavin
Newsom.
The
laws
went
into
effect
in
2020.DARPA's
Media
Forensics
program
studies
ways
to
counteract
fake
media,
including
fake
media
produced
using
GANs.
===
Transfer
learning
===
State-of-art
transfer
learning
research
use
GANs
to
enforce
the
alignment
of
the
latent
feature
space,
such
as
in
deep
reinforcement
learning.
This
works
by
feeding
the
embeddings
of
the
source
and
target
task
to
the
discriminator
which
tries
to
guess
the
context.
The
resulting
loss
is
then
(inversely)
backpropagated
through
the
encoder.
===
Miscellaneous
applications
===
GAN
can
be
used
to
detect
glaucomatous
images
helping
the
early
diagnosis
which
is
essential
to
avoid
partial
or
total
loss
of
vision.GANs
that
produce
photorealistic
images
can
be
used
to
visualize
interior
design,
industrial
design,
shoes,
bags,
and
clothing
items
or
items
for
computer
games'
scenes.
Such
networks
were
reported
to
be
used
by
Facebook.GANs
have
been
used
to
create
forensic
facial
reconstructions
of
deceased
historical
figures.GANs
can
reconstruct
3D
models
of
objects
from
images,
generate
novel
objects
as
3D
point
clouds,
and
model
patterns
of
motion
in
video.GANs
can
be
used
to
age
face
photographs
to
show
how
an
individual's
appearance
might
change
with
age.GANs
can
also
be
used
to
inpaint
missing
features
in
maps,
transfer
map
styles
in
cartography
or
augment
street
view
imagery.Relevance
feedback
on
GANs
can
be
used
to
generate
images
and
replace
image
search
systems.A
variation
of
the
GANs
is
used
in
training
a
network
to
generate
optimal
control
inputs
to
nonlinear
dynamical
systems.
Where
the
discriminatory
network
is
known
as
a
critic
that
checks
the
optimality
of
the
solution
and
the
generative
network
is
known
as
an
Adaptive
network
that
generates
the
optimal
control.
The
critic
and
adaptive
network
train
each
other
to
approximate
a
nonlinear
optimal
control.GANs
have
been
used
to
visualize
the
effect
that
climate
change
will
have
on
specific
houses.A
GAN
model
called
Speech2Face
can
reconstruct
an
image
of
a
person's
face
after
listening
to
their
voice.In
2016
GANs
were
used
to
generate
new
molecules
for
a
variety
of
protein
targets
implicated
in
cancer,
inflammation,
and
fibrosis.
In
2019
GAN-generated
molecules
were
validated
experimentally
all
the
way
into
mice.Whereas
the
majority
of
GAN
applications
are
in
image
processing,
the
work
has
also
been
done
with
time-series
data.
For
example,
recurrent
GANs
(R-GANs)
have
been
used
to
generate
energy
data
for
machine
learning.
==
History
==
In
1991,
Juergen
Schmidhuber
published
generative
and
adversarial
neural
networks
that
contest
with
each
other
in
the
form
of
a
zero-sum
game,
where
one
network's
gain
is
the
other
network's
loss.
The
first
network
is
a
generative
model
with
stochasticity
that
models
a
probability
distribution
over
output
patterns.
The
second
network
learns
by
gradient
descent
to
predict
the
reactions
of
the
environment
to
these
patterns.
This
was
called
"artificial
curiosity."
For
modern
GANs
(2014),
the
environmental
reaction
is
1
or
0
depending
on
whether
the
first
network's
output
is
in
a
given
set.Other
people
had
similar
ideas
but
did
not
develop
them
similarly.
An
idea
involving
adversarial
networks
was
published
in
a
2010
blog
post
by
Olli
Niemitalo.
This
idea
was
never
implemented
and
did
not
involve
stochasticity
in
the
generator
and
thus
was
not
a
generative
model.
It
is
now
known
as
a
conditional
GAN
or
cGAN.
An
idea
similar
to
GANs
was
used
to
model
animal
behavior
by
Li,
Gauci
and
Gross
in
2013.Another
inspiration
for
GANs
was
noise-contrastive
estimation,
which
uses
the
same
loss
function
as
GANs
and
which
Goodfellow
studied
during
his
PhD
in
2010–2014.
Adversarial
machine
learning
has
other
uses
besides
generative
modeling
and
can
be
applied
to
models
other
than
neural
networks.
In
control
theory,
adversarial
learning
based
on
neural
networks
was
used
in
2006
to
train
robust
controllers
in
a
game
theoretic
sense,
by
alternating
the
iterations
between
a
minimizer
policy,
the
controller,
and
a
maximizer
policy,
the
disturbance.In
2017,
a
GAN
was
used
for
image
enhancement
focusing
on
realistic
textures
rather
than
pixel-accuracy,
producing
a
higher
image
quality
at
high
magnification.
In
2017,
the
first
faces
were
generated.
These
were
exhibited
in
February
2018
at
the
Grand
Palais.
Faces
generated
by
StyleGAN
in
2019
drew
comparisons
with
Deepfakes.Beginning
in
2017,
GAN
technology
began
to
make
its
presence
felt
in
the
fine
arts
arena
with
the
appearance
of
a
newly
developed
implementation
which
was
said
to
have
crossed
the
threshold
of
being
able
to
generate
unique
and
appealing
abstract
paintings,
and
thus
dubbed
a
"CAN",
for
"creative
adversarial
network".
A
GAN
system
was
used
to
create
the
2018
painting
Edmond
de
Belamy,
which
sold
for
US$432,500.
An
early
2019
article
by
members
of
the
original
CAN
team
discussed
further
progress
with
that
system,
and
gave
consideration
as
well
to
the
overall
prospects
for
an
AI-enabled
art.In
May
2019,
researchers
at
Samsung
demonstrated
a
GAN-based
system
that
produces
videos
of
a
person
speaking,
given
only
a
single
photo
of
that
person.In
August
2019,
a
large
dataset
consisting
of
12,197
MIDI
songs
each
with
paired
lyrics
and
melody
alignment
was
created
for
neural
melody
generation
from
lyrics
using
conditional
GAN-LSTM
(refer
to
sources
at
GitHub
AI
Melody
Generation
from
Lyrics).In
May
2020,
Nvidia
researchers
taught
an
AI
system
(termed
"GameGAN")
to
recreate
the
game
of
Pac-Man
simply
by
watching
it
being
played.
==
References
==
==
External
links
==
Knight,
Will.
"5
Big
Predictions
for
Artificial
Intelligence
in
2017".
MIT
Technology
Review.
Retrieved
January
5,
2017.
Karras,
Tero;
Laine,
Samuli;
Aila,
Timo
(2018).
"A
Style-Based
Generator
Architecture
for
Generative
Adversarial
Networks".
arXiv:1812.04948
[cs.NE].
This
Person
Does
Not
Exist
–
photorealistic
images
of
people
who
do
not
exist,
generated
by
StyleGAN
This
Cat
Does
Not
Exist
Archived
March
5,
2019,
at
the
Wayback
Machine
–
photorealistic
images
of
cats
who
do
not
exist,
generated
by
StyleGAN
Wang,
Zhengwei;
She,
Qi;
Ward,
Tomas
E.
(2019).
"Generative
Adversarial
Networks
in
Computer
Vision:
A
Survey
and
Taxonomy".
arXiv:1906.01529
[cs.LG].
Generative
pre-trained
transformers
(GPT)
are
a
type
of
large
language
model
(LLM)
and
a
prominent
framework
for
generative
artificial
intelligence.
They
are
artificial
neural
networks
that
are
used
in
natural
language
processing
tasks.
GPTs
are
based
on
the
transformer
architecture,
pre-trained
on
large
data
sets
of
unlabelled
text,
and
able
to
generate
novel
human-like
content.
As
of
2023,
most
LLMs
have
these
characteristics
and
are
sometimes
referred
to
broadly
as
GPTs.The
first
GPT
was
introduced
in
2018
by
OpenAI.
OpenAI
has
released
very
influential
GPT
foundation
models
that
have
been
sequentially
numbered,
to
comprise
its
"GPT-n"
series.
Each
of
these
was
significantly
more
capable
than
the
previous,
due
to
increased
size
(number
of
trainable
parameters)
and
training.
The
most
recent
of
these,
GPT-4,
was
released
in
March
2023.
Such
models
have
been
the
basis
for
their
more
task-specific
GPT
systems,
including
models
fine-tuned
for
instruction
following—which
in
turn
power
the
ChatGPT
chatbot
service.The
term
"GPT"
is
also
used
in
the
names
and
descriptions
of
such
models
developed
by
others.
For
example,
other
GPT
foundation
models
include
a
series
of
models
created
by
EleutherAI,
and
seven
models
created
by
Cerebras
in
2023.
Also,
companies
in
different
industries
have
developed
task-specific
GPTs
in
their
respective
fields,
such
as
Salesforce's
"EinsteinGPT"
(for
CRM)
and
Bloomberg's
"BloombergGPT"
(for
finance).
==
History
==
===
Initial
developments
===
Generative
pretraining
(GP)
was
a
long-established
concept
in
machine
learning
applications.
It
was
originally
used
as
a
form
of
semi-supervised
learning,
as
the
model
is
trained
first
on
an
unlabelled
dataset
(pretraining
step)
by
learning
to
generate
datapoints
in
the
dataset,
and
then
it
is
trained
to
classify
a
labelled
dataset.While
the
unnormalized
linear
transformer
dates
back
to
1992,
the
modern
transformer
architecture
was
not
available
until
2017
when
it
was
published
by
researchers
at
Google
in
a
paper
"Attention
Is
All
You
Need".
That
development
led
to
the
emergence
of
large
language
models
such
as
BERT
in
2018
which
was
a
pre-trained
transformer
(PT)
but
not
designed
to
be
generative
(BERT
was
an
"encoder-only"
model).
Also
around
that
time,
in
2018,
OpenAI
published
its
article
entitled
"Improving
Language
Understanding
by
Generative
Pre-Training,"
in
which
it
introduced
the
first
generative
pre-trained
transformer
(GPT)
system
("GPT-1").Prior
to
transformer-based
architectures,
the
best-performing
neural
NLP
(natural
language
processing)
models
commonly
employed
supervised
learning
from
large
amounts
of
manually-labeled
data.
The
reliance
on
supervised
learning
limited
their
use
on
datasets
that
were
not
well-annotated,
and
also
made
it
prohibitively
expensive
and
time-consuming
to
train
extremely
large
language
models.The
semi-supervised
approach
OpenAI
employed
to
make
a
large-scale
generative
system—and
was
first
to
do
with
a
transformer
model—involved
two
stages:
an
unsupervised
generative
"pretraining"
stage
to
set
initial
parameters
using
a
language
modeling
objective,
and
a
supervised
discriminative
"fine-tuning"
stage
to
adapt
these
parameters
to
a
target
task.
===
Later
developments
===
Regarding
more
recent
GPT
foundation
models,
OpenAI
published
its
first
versions
of
GPT-3
in
July
2020.
There
were
three
models,
with
1B,
6.7B,
175B
parameters,
respectively
named
babbage,
curie,
and
davinci
(giving
initials
B,
C,
and
D).In
July
2021,
OpenAI
published
Codex,
a
task-specific
GPT
model
targeted
for
programming
applications.
This
was
developed
by
fine-tuning
a
12B
parameter
version
of
GPT-3
(different
from
previous
GPT-3
models)
using
code
from
GitHub.In
March
2022,
OpenAI
published
two
versions
of
GPT-3
that
were
fine-tuned
for
instruction-following
(instruction-tuned),
named
davinci-instruct-beta
(175B)
and
text-davinci-001,
and
then
started
beta
testing
code-davinci-002.
text-davinci-002
was
instruction-tuned
from
code-davinci-002.
Both
text-davinci-003
and
ChatGPT
were
released
in
November
2022,
with
both
building
upon
text-davinci-002
via
reinforcement
learning
from
human
feedback
(RLHF).
text-davinci-003
is
trained
for
following
instructions
(like
its
predecessors),
whereas
ChatGPT
is
further
trained
for
conversational
interaction
with
a
human
user.OpenAI's
most
recent
GPT
foundation
model,
GPT-4,
was
released
on
March
14,
2023.
It
can
be
accessed
directly
by
users
via
a
premium
version
of
ChatGPT,
and
is
available
to
developers
for
incorporation
into
other
products
and
services
via
OpenAI's
API.
Other
producers
of
GPT
foundation
models
include
EleutherAI
(with
a
series
of
models
starting
in
March
2021)
and
Cerebras
(with
seven
models
released
in
March
2023).
==
Foundational
models
==
A
foundational
model
is
an
AI
model
trained
on
broad
data
at
scale
such
that
it
can
be
adapted
to
a
wide
range
of
downstream
tasks.Thus
far,
the
most
notable
GPT
foundation
models
have
been
from
OpenAI's
GPT-n
series.
The
most
recent
from
that
is
GPT-4,
for
which
OpenAI
declined
to
publish
the
size
or
training
details
(citing
"the
competitive
landscape
and
the
safety
implications
of
large-scale
models").
Other
such
models
include
Google's
PaLM,
a
broad
foundation
model
that
has
been
compared
to
GPT-3
and
has
recently
been
made
available
to
developers
via
an
API,
and
Together's
GPT-JT,
which
has
been
reported
as
the
closest-performing
open-source
alternative
to
GPT-3
(and
is
derived
from
earlier
open-source
GPTs).
Meta
AI
(formerly
Facebook)
also
has
a
generative
transformer-based
foundational
large
language
model,
known
as
LLaMA.Foundational
GPTs
can
also
employ
modalities
other
than
text,
for
input
and/or
output.
GPT-4
is
a
multi-modal
LLM
that
is
capable
of
processing
text
and
image
input
(though
its
output
is
limited
to
text).
Regarding
multimodal
output,
some
generative
transformer-based
models
are
used
for
text-to-image
technologies
such
as
diffusion
and
parallel
decoding.
Such
kinds
of
models
can
serve
as
visual
foundation
models
(VFMs)
for
developing
downstream
systems
that
can
work
with
images.
==
Task-specific
models
==
A
foundational
GPT
model
can
be
further
adapted
to
produce
more
targeted
systems
directed
to
specific
tasks
and/or
subject-matter
domains.
Methods
for
such
adaptation
can
include
additional
fine-tuning
(beyond
that
done
for
the
foundation
model)
as
well
as
certain
forms
of
prompt
engineering.An
important
example
of
this
is
fine-tuning
models
to
follow
instructions,
which
is
of
course
a
fairly
broad
task
but
more
targeted
than
a
foundation
model.
In
January
2022,
OpenAI
introduced
"InstructGPT"—a
series
of
models
which
were
fine-tuned
to
follow
instructions
using
a
combination
of
supervised
training
and
reinforcement
learning
from
human
feedback
(RLHF)
on
base
GPT-3
language
models.
Advantages
this
had
over
the
bare
foundational
models
included
higher
accuracy,
less
negative/toxic
sentiment,
and
generally
better
alignment
with
user
needs.
Hence,
OpenAI
began
using
this
as
the
basis
for
its
API
service
offerings.
Other
instruction-tuned
models
have
been
released
by
others,
including
a
fully
open
version.Another
(related)
kind
of
task-specific
models
are
chatbots,
which
engage
in
human-like
conversation.
In
November
2022,
OpenAI
launched
ChatGPT—an
online
chat
interface
powered
by
an
instruction-tuned
language
model
trained
in
a
similar
fashion
to
InstructGPT.
They
trained
this
model
using
RLHF,
with
human
AI
trainers
providing
conversations
in
which
they
played
both
the
user
and
the
AI,
and
mixed
this
new
dialogue
dataset
with
the
InstructGPT
dataset
for
a
conversational
format
suitable
for
a
chatbot.
Other
major
chatbots
currently
include
Microsoft's
Bing
Chat,
which
uses
OpenAI's
GPT-4
(as
part
of
a
broader
close
collaboration
between
OpenAI
and
Microsoft),
and
Google's
competing
chatbot
Bard
(initially
based
on
their
LaMDA
family
of
conversation-trained
language
models,
with
plans
to
switch
to
PaLM).Yet
another
kind
of
task
that
a
GPT
can
be
used
for
is
the
meta-task
of
generating
its
own
instructions,
like
developing
a
series
of
prompts
for
'itself'
to
be
able
to
effectuate
a
more
general
goal
given
by
a
human
user.
This
is
known
as
an
AI
agent,
and
more
specifically
a
recursive
one
because
it
uses
results
from
its
previous
self-instructions
to
help
it
form
its
subsequent
prompts;
the
first
major
example
of
this
was
Auto-GPT
(which
uses
OpenAI's
GPT
models),
and
others
have
since
been
developed
as
well.
===
Multimodality
===
Generative
transformer-based
systems
can
also
be
targeted
to
tasks
involving
modalities
beyond
text.
For
example,
Microsoft’s
“Visual
ChatGPT”
combines
ChatGPT
with
visual
foundation
models
(VFMs)
to
enable
input
or
output
comprising
images
as
well
as
text.
Also,
advances
in
text-to-speech
technology
offer
powerful
tools
for
audio
content
creation
when
used
in
conjunction
with
foundational
GPT
language
models.
===
Domain-specificity
===
GPT
systems
can
be
directed
toward
particular
fields
or
domains.
Some
reported
examples
of
such
models
and
apps
are
as
follows:
EinsteinGPT
–
for
sales
and
marketing
domains,
to
aid
with
customer
relationship
management
(uses
GPT-3.5)
BloombergGPT
–
for
the
financial
domain,
to
aid
with
financial
news
and
information
(uses
"freely
available"
AI
methods,
combined
with
their
proprietary
data)
Khanmigo
–
described
as
a
GPT
version
for
tutoring,
in
the
education
domain,
it
aids
students
using
Khan
Academy
by
guiding
them
through
their
studies
without
directly
providing
answers
(powered
by
GPT-4)
SlackGPT
–
for
the
Slack
instant-messaging
service,
to
aid
with
navigating
and
summarizing
discussions
on
it
(uses
OpenAI's
API)
BioGPT
–
for
the
biomedical
domain,
to
aid
with
biomedical
literature
text
generation
and
mining
(uses
GPT-2)Sometimes
domain-specificity
is
accomplished
via
software
plug-ins
or
add-ons.
For
example,
several
different
companies
have
developed
particular
plugins
that
interact
directly
with
OpenAI's
ChatGPT
interface,
and
Google
Workspace
has
available
add-ons
such
as
“GPT
for
Sheets
and
Docs”—which
is
reported
to
aid
use
of
spreadsheet
functionality
in
Google
Sheets.In
November
2023,
OpenAI
announced
that
it's
enabling
ChatGPT
Plus
subscribers
to
create
custom
versions
of
ChatGPT
(being
called
GPTs).
These
can
be
tailored
for
specific
domains
via
prompt
engineering,
curated
datasets,
and/or
targeted
interaction
with
external
tools.
Users
who
register
as
verified
builders
are
able
to
publish
their
custom
GPTs
for
other
users,
with
monetization
potential.
(This
is
notably
distinct
from
OpenAI's
API
service,
as
this
is
based
internally
within
OpenAI's
platform.)
==
Brand
issues
==
OpenAI,
which
created
the
first
generative
pre-trained
transformer
(GPT)
in
2018,
has
recently
asserted
that
“GPT”
should
be
regarded
as
a
brand
of
OpenAI.
In
April
2023,
OpenAI
revised
the
brand
guidelines
in
its
terms
of
service
to
indicate
that
other
businesses
using
its
API
to
run
their
artificial
intelligence
(AI)
services
would
no
longer
be
able
to
include
“GPT”
in
such
names
or
branding.
In
May
2023,
OpenAI
engaged
a
brand
management
service
to
notify
its
API
customers
of
this
policy,
although
these
notifications
stopped
short
of
making
overt
legal
claims
(such
as
allegations
of
trademark
infringement
or
demands
to
cease
and
desist).
As
of
November
2023,
OpenAI
still
prohibits
its
API
licensees
from
naming
their
own
products
with
"GPT,"
but
it
has
begun
enabling
its
ChatGPT
Plus
subscribers
to
make
"custom
versions
of
ChatGPT"
that
are
being
called
GPTs
on
the
OpenAI
site.
OpenAI's
terms
of
service
says
that
its
subscribers
may
use
"GPT"
in
the
names
of
these,
although
it's
"discouraged."Relatedly,
OpenAI
has
applied
to
the
United
States
Patent
and
Trademark
Office
(USPTO)
to
seek
domestic
trademark
registration
for
the
term
“GPT”
in
the
field
of
AI.
OpenAI
sought
to
expedite
handling
of
its
application,
but
the
USPTO
declined
that
request
in
April
2023.
In
May
2023,
the
USPTO
responded
to
the
application
with
a
determination
that
"GPT"
was
both
descriptive
and
generic.
As
of
November
2023,
OpenAI
continues
to
pursue
its
argument
through
the
available
processes.
Regardless,
failure
to
obtain
a
registered
U.S.
trademark
does
not
preclude
some
level
of
common-law
trademark
rights
in
the
U.S.,
and/or
trademark
rights
in
other
countries.For
any
given
type
or
scope
of
trademark
protection
in
the
U.S.,
OpenAI
would
need
to
establish
that
the
term
is
actually
“distinctive”
to
their
specific
offerings
in
addition
to
being
a
broader
technical
term
for
the
kind
of
technology.
Some
media
reports
suggested
that
OpenAI
may
be
able
to
obtain
trademark
registration
based
indirectly
on
the
fame
of
its
GPT-based
chatbot
product,
ChatGPT,
for
which
OpenAI
has
separately
sought
protection
(and
which
it
has
sought
to
enforce
more
strongly).
Other
reports
have
indicated
that
registration
for
the
bare
term
“GPT”
seems
unlikely
to
be
granted,
as
it
is
used
frequently
as
a
common
term
to
refer
simply
to
AI
systems
that
involve
generative
pre-trained
transformers.
In
any
event,
to
whatever
extent
exclusive
rights
in
the
term
may
occur
the
U.S.,
others
would
need
to
avoid
using
it
for
similar
products
or
services
in
ways
likely
to
cause
confusion.
If
such
rights
ever
became
broad
enough
to
implicate
other
well-established
uses
in
the
field,
the
trademark
doctrine
of
descriptive
fair
use
could
still
preserve
some
room
to
continue
non-brand-related
usage.
==
Selected
bibliography
==
This
section
lists
the
main
official
publications
from
OpenAI
and
Microsoft
on
their
GPT
models.
GPT-1:
report,
GitHub
release.
GPT-2:
blog
announcement,
report
on
its
decision
of
"staged
release",
GitHub
release.
GPT-3:
report.
No
GitHub
or
any
other
form
of
code
release
thenceforth.
webGPT:
blog
announcement,
report,
InstructGPT:
blog
announcement,
report.
ChatGPT:
blog
announcement
(no
report).
GPT-4:
blog
announcement,
reports,
model
card.
==
See
also
==
Cyc
Gemini
==
References
==
A
graph
neural
network
(GNN)
belongs
to
a
class
of
artificial
neural
networks
for
processing
data
that
can
be
represented
as
graphs.
In
the
more
general
subject
of
"geometric
deep
learning",
certain
existing
neural
network
architectures
can
be
interpreted
as
GNNs
operating
on
suitably
defined
graphs.
A
convolutional
neural
network
layer,
in
the
context
of
computer
vision,
can
be
seen
as
a
GNN
applied
to
graphs
whose
nodes
are
pixels
and
only
adjacent
pixels
are
connected
by
edges
in
the
graph.
A
transformer
layer,
in
natural
language
processing,
can
be
seen
as
a
GNN
applied
to
complete
graphs
whose
nodes
are
words
or
tokens
in
a
passage
of
natural
language
text.
The
key
design
element
of
GNNs
is
the
use
of
pairwise
message
passing,
such
that
graph
nodes
iteratively
update
their
representations
by
exchanging
information
with
their
neighbors.
Since
their
inception,
several
different
GNN
architectures
have
been
proposed,
which
implement
different
flavors
of
message
passing,
started
by
recursive
or
convolutional
constructive
approaches.
As
of
2022,
whether
it
is
possible
to
define
GNN
architectures
"going
beyond"
message
passing,
or
if
every
GNN
can
be
built
on
message
passing
over
suitably
defined
graphs,
is
an
open
research
question.Relevant
application
domains
for
GNNs
include
Natural
Language
Processing,
social
networks,
citation
networks,
molecular
biology,
chemistry,
physics
and
NP-hard
combinatorial
optimization
problems.Several
open
source
libraries
implementing
graph
neural
networks
are
available,
such
as
PyTorch
Geometric
(PyTorch),
TensorFlow
GNN
(TensorFlow),
jraph
(Google
JAX),
and
GraphNeuralNetworks.jl/GeometricFlux.jl
(Julia,
Flux).
==
Architecture
==
The
architecture
of
a
generic
GNN
implements
the
following
fundamental
layers:
Permutation
equivariant:
a
permutation
equivariant
layer
maps
a
representation
of
a
graph
into
an
updated
representation
of
the
same
graph.
In
the
literature,
permutation
equivariant
layers
are
implemented
via
pairwise
message
passing
between
graph
nodes.
Intuitively,
in
a
message
passing
layer,
nodes
update
their
representations
by
aggregating
the
messages
received
from
their
immediate
neighbours.
As
such,
each
message
passing
layer
increases
the
receptive
field
of
the
GNN
by
one
hop.
Local
pooling:
a
local
pooling
layer
coarsens
the
graph
via
downsampling.
Local
pooling
is
used
to
increase
the
receptive
field
of
a
GNN,
in
a
similar
fashion
to
pooling
layers
in
convolutional
neural
networks.
Examples
include
k-nearest
neighbours
pooling,
top-k
pooling,
and
self-attention
pooling.
Global
pooling:
a
global
pooling
layer,
also
known
as
readout
layer,
provides
fixed-size
representation
of
the
whole
graph.
The
global
pooling
layer
must
be
permutation
invariant,
such
that
permutations
in
the
ordering
of
graph
nodes
and
edges
do
not
alter
the
final
output.
Examples
include
element-wise
sum,
mean
or
maximum.It
has
been
demonstrated
that
GNNs
cannot
be
more
expressive
than
the
Weisfeiler–Leman
Graph
Isomorphism
Test.
In
practice,
this
means
that
there
exist
different
graph
structures
(e.g.,
molecules
with
the
same
atoms
but
different
bonds)
that
cannot
be
distinguished
by
GNNs.
More
powerful
GNNs
operating
on
higher-dimension
geometries
such
as
simplicial
complexes
can
be
designed.
As
of
2022,
whether
or
not
future
architectures
will
overcome
the
message
passing
primitive
is
an
open
research
question.
==
Message
passing
layers
==
Message
passing
layers
are
permutation-equivariant
layers
mapping
a
graph
into
an
updated
representation
of
the
same
graph.
Formally,
they
can
be
expressed
as
message
passing
neural
networks
(MPNNs).Let
G=(V,E){\displaystyle
G=(V,E)}
be
a
graph,
where
V{\displaystyle
V}
is
the
node
set
and
E{\displaystyle
E}
is
the
edge
set.
Let
Nu{\displaystyle
N_{u}}
be
the
neighbourhood
of
some
node
u∈V{\displaystyle
u\in
V}.
Additionally,
let
xu{\displaystyle
\mathbf
{x}
_{u}}
be
the
features
of
node
u∈V{\displaystyle
u\in
V},
and
euv{\displaystyle
\mathbf
{e}
_{uv}}
be
the
features
of
edge
(u,v)∈E{\displaystyle
(u,v)\in
E}.
An
MPNN
layer
can
be
expressed
as
follows:
hu=ϕ(xu,⨁v∈Nuψ(xu,xv,euv)){\displaystyle
\mathbf
{h}
_{u}=\phi
\left(\mathbf
{x}
_{u},\bigoplus
_{v\in
N_{u}}\psi
(\mathbf
{x}
_{u},\mathbf
{x}
_{v},\mathbf
{e}
_{uv})\right)}where
ϕ{\displaystyle
\phi
}
and
ψ{\displaystyle
\psi
}
are
differentiable
functions
(e.g.,
artificial
neural
networks),
and
⨁{\displaystyle
\bigoplus
}
is
a
permutation
invariant
aggregation
operator
that
can
accept
an
arbitrary
number
of
inputs
(e.g.,
element-wise
sum,
mean,
or
max).
In
particular,
ϕ{\displaystyle
\phi
}
and
ψ{\displaystyle
\psi
}
are
referred
to
as
update
and
message
functions,
respectively.
Intuitively,
in
an
MPNN
computational
block,
graph
nodes
update
their
representations
by
aggregating
the
messages
received
from
their
neighbours.
The
outputs
of
one
or
more
MPNN
layers
are
node
representations
hu{\displaystyle
\mathbf
{h}
_{u}}
for
each
node
u∈V{\displaystyle
u\in
V}
in
the
graph.
Node
representations
can
be
employed
for
any
downstream
task,
such
as
node/graph
classification
or
edge
prediction.
Graph
nodes
in
an
MPNN
update
their
representation
aggregating
information
from
their
immediate
neighbours.
As
such,
stacking
n{\displaystyle
n}
MPNN
layers
means
that
one
node
will
be
able
to
communicate
with
nodes
that
are
at
most
n{\displaystyle
n}
"hops"
away.
In
principle,
to
ensure
that
every
node
receives
information
from
every
other
node,
one
would
need
to
stack
a
number
of
MPNN
layers
equal
to
the
graph
diameter.
However,
stacking
many
MPNN
layers
may
cause
issues
such
as
oversmoothing
and
oversquashing.
Oversmoothing
refers
to
the
issue
of
node
representations
becoming
indistinguishable.
Oversquashing
refers
to
the
bottleneck
that
is
created
by
squeezing
long-range
dependencies
into
fixed-size
representations.
Countermeasures
such
as
skip
connections
(as
in
residual
neural
networks),
gated
update
rules
and
jumping
knowledge
can
mitigate
oversmoothing.
Modifying
the
final
layer
to
be
a
fully-adjacent
layer,
i.e.,
by
considering
the
graph
as
a
complete
graph,
can
mitigate
oversquashing
in
problems
where
long-range
dependencies
are
required.Other
"flavours"
of
MPNN
have
been
developed
in
the
literature,
such
as
graph
convolutional
networks
and
graph
attention
networks,
whose
definitions
can
be
expressed
in
terms
of
the
MPNN
formalism.
===
Graph
convolutional
network
===
The
graph
convolutional
network
(GCN)
was
first
introduced
by
Thomas
Kipf
and
Max
Welling
in
2017.A
GCN
layer
defines
a
first-order
approximation
of
a
localized
spectral
filter
on
graphs.
GCNs
can
be
understood
as
a
generalization
of
convolutional
neural
networks
to
graph-structured
data.
The
formal
expression
of
a
GCN
layer
reads
as
follows:
H=σ(D~−12A~D~−12XΘ){\displaystyle
\mathbf
{H}
=\sigma
\left({\tilde
{\mathbf
{D}
}}^{-{\frac
{1}{2}}}{\tilde
{\mathbf
{A}
}}{\tilde
{\mathbf
{D}
}}^{-{\frac
{1}{2}}}\mathbf
{X}
\mathbf
{\Theta
}
\right)}where
H{\displaystyle
\mathbf
{H}
}
is
the
matrix
of
node
representations
hu{\displaystyle
\mathbf
{h}
_{u}},
X{\displaystyle
\mathbf
{X}
}
is
the
matrix
of
node
features
xu{\displaystyle
\mathbf
{x}
_{u}},
σ(⋅){\displaystyle
\sigma
(\cdot
)}
is
an
activation
function
(e.g.,
ReLU),
A~{\displaystyle
{\tilde
{\mathbf
{A}
}}}
is
the
graph
adjacency
matrix
with
the
addition
of
self-loops,
D~{\displaystyle
{\tilde
{\mathbf
{D}
}}}
is
the
graph
degree
matrix
with
the
addition
of
self-loops,
and
Θ{\displaystyle
\mathbf
{\Theta
}
}
is
a
matrix
of
trainable
parameters.
In
particular,
let
A{\displaystyle
\mathbf
{A}
}
be
the
graph
adjacency
matrix:
then,
one
can
define
A~=A+I{\displaystyle
{\tilde
{\mathbf
{A}
}}=\mathbf
{A}
+\mathbf
{I}
}
and
D~ii=∑j∈VA~ij{\displaystyle
{\tilde
{\mathbf
{D}
}}_{ii}=\sum
_{j\in
V}{\tilde
{A}}_{ij}},
where
I{\displaystyle
\mathbf
{I}
}
denotes
the
identity
matrix.
This
normalization
ensures
that
the
eigenvalues
of
D~−12A~D~−12{\displaystyle
{\tilde
{\mathbf
{D}
}}^{-{\frac
{1}{2}}}{\tilde
{\mathbf
{A}
}}{\tilde
{\mathbf
{D}
}}^{-{\frac
{1}{2}}}}
are
bounded
in
the
range
[0,1]{\displaystyle
[0,1]},
avoiding
numerical
instabilities
and
exploding/vanishing
gradients.
A
limitation
of
GCNs
is
that
they
do
not
allow
multidimensional
edge
features
euv{\displaystyle
\mathbf
{e}
_{uv}}.
It
is
however
possible
to
associate
scalar
weights
wuv{\displaystyle
w_{uv}}
to
each
edge
by
imposing
Auv=wuv{\displaystyle
A_{uv}=w_{uv}},
i.e.,
by
setting
each
nonzero
entry
in
the
adjacency
matrix
equal
to
the
weight
of
the
corresponding
edge.
===
Graph
attention
network
===
The
graph
attention
network
(GAT)
was
introduced
by
Petar
Veličković
et
al.
in
2018.Graph
attention
network
is
a
combination
of
a
graph
neural
network
and
an
attention
layer.
The
implementation
of
attention
layer
in
graphical
neural
networks
helps
provide
attention
or
focus
to
the
important
information
from
the
data
instead
of
focusing
on
the
whole
data.
A
multi-head
GAT
layer
can
be
expressed
as
follows:
hu=‖k=1Kσ(∑v∈NuαuvWkxv){\displaystyle
\mathbf
{h}
_{u}={\overset
{K}{\underset
{k=1}{\Big
\Vert
}}}\sigma
\left(\sum
_{v\in
N_{u}}\alpha
_{uv}\mathbf
{W}
^{k}\mathbf
{x}
_{v}\right)}where
K{\displaystyle
K}
is
the
number
of
attention
heads,
‖{\displaystyle
{\Big
\Vert
}}
denotes
vector
concatenation,
σ(⋅){\displaystyle
\sigma
(\cdot
)}
is
an
activation
function
(e.g.,
ReLU),
αij{\displaystyle
\alpha
_{ij}}
are
attention
coefficients,
and
Wk{\displaystyle
W^{k}}
is
a
matrix
of
trainable
parameters
for
the
k{\displaystyle
k}-th
attention
head.
For
the
final
GAT
layer,
the
outputs
from
each
attention
head
are
averaged
before
the
application
of
the
activation
function.
Formally,
the
final
GAT
layer
can
be
written
as:
hu=σ(1K∑k=1K∑v∈NuαuvWkxv){\displaystyle
\mathbf
{h}
_{u}=\sigma
\left({\frac
{1}{K}}\sum
_{k=1}^{K}\sum
_{v\in
N_{u}}\alpha
_{uv}\mathbf
{W}
^{k}\mathbf
{x}
_{v}\right)}Attention
in
Machine
Learning
is
a
technique
that
mimics
cognitive
attention.
In
the
context
of
learning
on
graphs,
the
attention
coefficient
αuv{\displaystyle
\alpha
_{uv}}
measures
how
important
is
node
u∈V{\displaystyle
u\in
V}
to
node
v∈V{\displaystyle
v\in
V}.
Normalized
attention
coefficients
are
computed
as
follows:
αuv=exp⁡(LeakyReLU(aT[Whu‖Whv‖euv]))∑z∈Nuexp⁡(LeakyReLU(aT[Whu‖Whz‖euz])){\displaystyle
\alpha
_{uv}={\frac
{\exp({\text{LeakyReLU}}\left(\mathbf
{a}
^{T}[\mathbf
{W}
\mathbf
{h}
_{u}\Vert
\mathbf
{W}
\mathbf
{h}
_{v}\Vert
\mathbf
{e}
_{uv}]\right))}{\sum
_{z\in
N_{u}}\exp({\text{LeakyReLU}}\left(\mathbf
{a}
^{T}[\mathbf
{W}
\mathbf
{h}
_{u}\Vert
\mathbf
{W}
\mathbf
{h}
_{z}\Vert
\mathbf
{e}
_{uz}]\right))}}}where
a{\displaystyle
\mathbf
{a}
}
is
a
vector
of
learnable
weights,
⋅T{\displaystyle
\cdot
^{T}}
indicates
transposition,
and
LeakyReLU{\displaystyle
{\text{LeakyReLU}}}
is
a
modified
ReLU
activation
function.
Attention
coefficients
are
normalized
to
make
them
easily
comparable
across
different
nodes.A
GCN
can
be
seen
as
a
special
case
of
a
GAT
where
attention
coefficients
are
not
learnable,
but
fixed
and
equal
to
the
edge
weights
wuv{\displaystyle
w_{uv}}.
===
Gated
graph
sequence
neural
network
===
The
gated
graph
sequence
neural
network
(GGS-NN)
was
introduced
by
Yujia
Li
et
al.
in
2015.
The
GGS-NN
extends
the
GNN
formulation
by
Scarselli
et
al.
to
output
sequences.
The
message
passing
framework
is
implemented
as
an
update
rule
to
a
gated
recurrent
unit
(GRU)
cell.
A
GGS-NN
can
be
expressed
as
follows:
hu(0)=xu‖0{\displaystyle
\mathbf
{h}
_{u}^{(0)}=\mathbf
{x}
_{u}\,\Vert
\,\mathbf
{0}
}
mu(l+1)=∑v∈NuΘhv{\displaystyle
\mathbf
{m}
_{u}^{(l+1)}=\sum
_{v\in
N_{u}}\mathbf
{\Theta
}
\mathbf
{h}
_{v}}
hu(l+1)=GRU(mu(l+1),hu(l)){\displaystyle
\mathbf
{h}
_{u}^{(l+1)}={\text{GRU}}(\mathbf
{m}
_{u}^{(l+1)},\mathbf
{h}
_{u}^{(l)})}where
‖{\displaystyle
\Vert
}
denotes
vector
concatenation,
0{\displaystyle
\mathbf
{0}
}
is
a
vector
of
zeros,
Θ{\displaystyle
\mathbf
{\Theta
}
}
is
a
matrix
of
learnable
parameters,
GRU{\displaystyle
{\text{GRU}}}
is
a
GRU
cell,
and
l{\displaystyle
l}
denotes
the
sequence
index.
In
a
GGS-NN,
the
node
representations
are
regarded
as
the
hidden
states
of
a
GRU
cell.
The
initial
node
features
xu(0){\displaystyle
\mathbf
{x}
_{u}^{(0)}}
are
zero-padded
up
to
the
hidden
state
dimension
of
the
GRU
cell.
The
same
GRU
cell
is
used
for
updating
representations
for
each
node.
==
Local
pooling
layers
==
Local
pooling
layers
coarsen
the
graph
via
downsampling.
We
present
here
several
learnable
local
pooling
strategies
that
have
been
proposed.
For
each
cases,
the
input
is
the
initial
graph
is
represented
by
a
matrix
X{\displaystyle
\mathbf
{X}
}
of
node
features,
and
the
graph
adjacency
matrix
A{\displaystyle
\mathbf
{A}
}.
The
output
is
the
new
matrix
X′{\displaystyle
\mathbf
{X}
'}of
node
features,
and
the
new
graph
adjacency
matrix
A′{\displaystyle
\mathbf
{A}
'}.
===
Top-k
pooling
===
We
first
set
y=Xp‖p‖{\displaystyle
\mathbf
{y}
={\frac
{\mathbf
{X}
\mathbf
{p}
}{\Vert
\mathbf
{p}
\Vert
}}}
where
p{\displaystyle
\mathbf
{p}
}
is
a
learnable
projection
vector.
The
projection
vector
p{\displaystyle
\mathbf
{p}
}
computes
a
scalar
projection
value
for
each
graph
node.
The
top-k
pooling
layer
can
then
be
formalised
as
follows:
X′=(X⊙sigmoid(y))i{\displaystyle
\mathbf
{X}
'=(\mathbf
{X}
\odot
{\text{sigmoid}}(\mathbf
{y}
))_{\mathbf
{i}
}}A′=Ai,i{\displaystyle
\mathbf
{A}
'=\mathbf
{A}
_{\mathbf
{i}
,\mathbf
{i}
}}where
i=topk(y){\displaystyle
\mathbf
{i}
={\text{top}}_{k}(\mathbf
{y}
)}
is
the
subset
of
nodes
with
the
top-k
highest
projection
scores,
⊙{\displaystyle
\odot
}
denotes
element-wise
matrix
multiplication,
and
sigmoid(⋅){\displaystyle
{\text{sigmoid}}(\cdot
)}
is
the
sigmoid
function.
In
other
words,
the
nodes
with
the
top-k
highest
projection
scores
are
retained
in
the
new
adjacency
matrix
A′{\displaystyle
\mathbf
{A}
'}.
The
sigmoid(⋅){\displaystyle
{\text{sigmoid}}(\cdot
)}
operation
makes
the
projection
vector
p{\displaystyle
\mathbf
{p}
}
trainable
by
backpropagation,
which
otherwise
would
produce
discrete
outputs.
===
Self-attention
pooling
===
We
first
set
y=GNN(X,A){\displaystyle
\mathbf
{y}
={\text{GNN}}(\mathbf
{X}
,\mathbf
{A}
)}where
GNN{\displaystyle
{\text{GNN}}}
is
a
generic
permutation
equivariant
GNN
layer
(e.g.,
GCN,
GAT,
MPNN).
The
Self-attention
pooling
layer
can
then
be
formalised
as
follows:
X′=(X⊙y)i{\displaystyle
\mathbf
{X}
'=(\mathbf
{X}
\odot
\mathbf
{y}
)_{\mathbf
{i}
}}A′=Ai,i{\displaystyle
\mathbf
{A}
'=\mathbf
{A}
_{\mathbf
{i}
,\mathbf
{i}
}}where
i=topk(y){\displaystyle
\mathbf
{i}
={\text{top}}_{k}(\mathbf
{y}
)}
is
the
subset
of
nodes
with
the
top-k
highest
projection
scores,
⊙{\displaystyle
\odot
}
denotes
element-wise
matrix
multiplication.
The
self-attention
pooling
layer
can
be
seen
as
an
extension
of
the
top-k
pooling
layer.
Differently
from
top-k
pooling,
the
self-attention
scores
computed
in
self-attention
pooling
account
both
for
the
graph
features
and
the
graph
topology.
==
Applications
==
===
Protein
folding
===
Graph
neural
networks
are
one
of
the
main
building
blocks
of
AlphaFold,
an
artificial
intelligence
program
developed
by
Google's
DeepMind
for
solving
the
protein
folding
problem
in
biology.
AlphaFold
achieved
first
place
in
several
CASP
competitions.
===
Social
networks
===
Social
networks
are
a
major
application
domain
for
GNNs
due
to
their
natural
representation
as
social
graphs.
GNNs
are
used
to
develop
recommender
systems
based
on
both
social
relations
and
item
relations.
===
Combinatorial
optimization
===
GNNs
are
used
as
fundamental
building
blocks
for
several
combinatorial
optimization
algorithms.
Examples
include
computing
shortest
paths
or
Eulerian
circuits
for
a
given
graph,
deriving
chip
placements
superior
or
competitive
to
handcrafted
human
solutions,
and
improving
expert-designed
branching
rules
in
branch
and
bound.
===
Cyber
security
===
When
viewed
as
a
graph,
a
network
of
computers
can
be
analyzed
with
GNNs
for
anomaly
detection.
Anomalies
within
provenance
graphs
often
correlate
to
malicious
activity
within
the
network.
GNNs
have
been
used
to
identify
these
anomalies
on
individual
nodes
and
within
paths
to
detect
malicious
processes,
or
on
the
edge
level
to
detect
lateral
movement.
==
References
==
==
External
links
==
https://distill.pub/2021/gnn-intro/
Artificial
neural
networks
(ANNs)
are
models
created
using
machine
learning
to
perform
a
number
of
tasks.
Their
creation
was
inspired
by
neural
circuitry.
While
some
of
the
computational
implementations
ANNs
relate
to
earlier
discoveries
in
mathematics,
the
first
implementation
of
ANNs
was
by
psychologist
Frank
Rosenblatt,
who
developed
the
perceptron.
Little
research
was
conducted
on
ANNs
in
the
1970s
and
1980s,
with
the
AAAI
calling
that
period
an
"AI
winter".Later,
advances
in
hardware
and
the
development
of
the
backpropagation
algorithm
as
well
as
recurrent
neural
networks
and
convolutional
neural
networks,
renewed
interest
in
ANNs.
The
2010s,
saw
the
development
of
a
deep
neural
network
(a
neural
network
with
many
layers)
called
AlexNet.
It
greatly
outperformed
other
image
recognition
models,
and
is
thought
to
have
launched
the
ongoing
AI
spring,
and
further
increasing
interest
in
ANNs.
The
transformer
architecture
was
first
described
in
2017
as
a
method
to
teach
ANNs
grammatical
dependencies
in
language,
and
is
the
predominant
architecture
used
by
large
language
models,
such
as
GPT-4.
Diffusion
models
were
first
described
in
2015,
and
began
to
be
used
by
image
generation
models
such
as
DALL-E
in
the
2020s.
==
Linear
neural
network
==
The
simplest
kind
of
feedforward
neural
network
is
a
linear
network,
which
consists
of
a
single
layer
of
output
nodes;
the
inputs
are
fed
directly
to
the
outputs
via
a
series
of
weights.
The
sum
of
the
products
of
the
weights
and
the
inputs
is
calculated
in
each
node.
The
mean
squared
errors
between
these
calculated
outputs
and
a
given
target
values
are
minimized
by
creating
an
adjustment
to
the
weights.
This
technique
has
been
known
for
over
two
centuries
as
the
method
of
least
squares
or
linear
regression.
It
was
used
as
a
means
of
finding
a
good
rough
linear
fit
to
a
set
of
points
by
Legendre
(1805)
and
Gauss
(1795)
for
the
prediction
of
planetary
movement.
==
Perceptrons
and
other
early
neural
networks
==
Warren
McCulloch
and
Walter
Pitts
(1943)
also
considered
a
non-learning
computational
model
for
neural
networks.
This
model
paved
the
way
for
research
to
split
into
two
approaches.
One
approach
focused
on
biological
processes
while
the
other
focused
on
the
application
of
neural
networks
to
artificial
intelligence.
This
work
led
to
work
on
nerve
networks
and
their
link
to
finite
automata.In
the
early
1940s,
D.
O.
Hebb
created
a
learning
hypothesis
based
on
the
mechanism
of
neural
plasticity
that
became
known
as
Hebbian
learning.
Hebbian
learning
is
unsupervised
learning.
This
evolved
into
models
for
long-term
potentiation.
Researchers
started
applying
these
ideas
to
computational
models
in
1948
with
Turing's
B-type
machines.
Farley
and
Clark
(1954)
first
used
computational
machines,
then
called
"calculators",
to
simulate
a
Hebbian
network.
Other
neural
network
computational
machines
were
created
by
Rochester,
Holland,
Habit
and
Duda
(1956).Rosenblatt
(1958)
created
the
perceptron,
an
algorithm
for
pattern
recognition.
With
mathematical
notation,
Rosenblatt
described
circuitry
not
in
the
basic
perceptron,
such
as
the
exclusive-or
circuit
that
could
not
be
processed
by
neural
networks
at
the
time.
In
1959,
a
biological
model
proposed
by
Nobel
laureates
Hubel
and
Wiesel
was
based
on
their
discovery
of
two
types
of
cells
in
the
primary
visual
cortex:
simple
cells
and
complex
cells.Some
say
that
research
stagnated
following
Minsky
and
Papert
(1969),
who
discovered
that
basic
perceptrons
were
incapable
of
processing
the
exclusive-or
circuit
and
that
computers
lacked
sufficient
power
to
process
useful
neural
networks.
However,
by
the
time
this
book
came
out,
methods
for
training
multilayer
perceptrons
(MLPs)
by
deep
learning
were
already
known.
==
First
deep
learning
==
The
first
deep
learning
MLP
was
published
by
Alexey
Grigorevich
Ivakhnenko
and
Valentin
Lapa
in
1965,
as
the
Group
Method
of
Data
Handling.
This
method
employs
incremental
layer
by
layer
training
based
on
regression
analysis,
where
useless
units
in
hidden
layers
are
pruned
with
the
help
of
a
validation
set.
The
first
deep
learning
MLP
trained
by
stochastic
gradient
descent
was
published
in
1967
by
Shun'ichi
Amari.
In
computer
experiments
conducted
by
Amari's
student
Saito,
a
five
layer
MLP
with
two
modifiable
layers
learned
useful
internal
representations
to
classify
non-linearily
separable
pattern
classes.
==
Backpropagation
==
The
backpropagation
algorithm
is
an
efficient
application
of
the
Leibniz
chain
rule
(1673)
to
networks
of
differentiable
nodes.
It
is
also
known
as
the
reverse
mode
of
automatic
differentiation
or
reverse
accumulation,
due
to
Seppo
Linnainmaa
(1970).
The
term
"back-propagating
errors"
was
introduced
in
1962
by
Frank
Rosenblatt,
but
he
did
not
have
an
implementation
of
this
procedure,
although
Henry
J.
Kelley
had
a
continuous
precursor
of
backpropagation
already
in
1960
in
the
context
of
control
theory.
In
1982,
Paul
Werbos
applied
backpropagation
to
MLPs
in
the
way
that
has
become
standard.
In
1986,
David
E.
Rumelhart
et
al.
published
an
experimental
analysis
of
the
technique.
==
Recurrent
network
architectures
==
Wilhelm
Lenz
and
Ernst
Ising
created
and
analyzed
the
Ising
model
(1925)
which
is
essentially
a
non-learning
artificial
recurrent
neural
network
(RNN)
consisting
of
neuron-like
threshold
elements.
In
1972,
Shun'ichi
Amari
made
this
architecture
adaptive.
His
learning
RNN
was
popularised
by
John
Hopfield
in
1982.
==
Self-organizing
maps
==
Self-organizing
maps
(SOMs)
were
described
by
Teuvo
Kohonen
in
1982.
SOMs
are
neurophysiologically
inspired
artificial
neural
networks
that
learn
low-dimensional
representations
of
high-dimensional
data
while
preserving
the
topological
structure
of
the
data.
They
are
trained
using
competitive
learning.
SOMs
create
internal
representations
reminiscent
of
the
cortical
homunculus,
a
distorted
representation
of
the
human
body,
based
on
a
neurological
"map"
of
the
areas
and
proportions
of
the
human
brain
dedicated
to
processing
sensory
functions,
for
different
parts
of
the
body.
==
Convolutional
neural
networks
(CNNs)
==
The
origin
of
the
CNN
architecture
is
the
"neocognitron"
introduced
by
Kunihiko
Fukushima
in
1980.
It
was
inspired
by
work
of
Hubel
and
Wiesel
in
the
1950s
and
1960s
which
showed
that
cat
visual
cortices
contain
neurons
that
individually
respond
to
small
regions
of
the
visual
field.
The
neocognitron
introduced
the
two
basic
types
of
layers
in
CNNs:
convolutional
layers,
and
downsampling
layers.
A
convolutional
layer
contains
units
whose
receptive
fields
cover
a
patch
of
the
previous
layer.
The
weight
vector
(the
set
of
adaptive
parameters)
of
such
a
unit
is
often
called
a
filter.
Units
can
share
filters.
Downsampling
layers
contain
units
whose
receptive
fields
cover
patches
of
previous
convolutional
layers.
Such
a
unit
typically
computes
the
average
of
the
activations
of
the
units
in
its
patch.
This
downsampling
helps
to
correctly
classify
objects
in
visual
scenes
even
when
the
objects
are
shifted.
In
1969,
Kunihiko
Fukushima
also
introduced
the
ReLU
(rectified
linear
unit)
activation
function.
The
rectifier
has
become
the
most
popular
activation
function
for
CNNs
and
deep
neural
networks
in
general.The
time
delay
neural
network
(TDNN)
was
introduced
in
1987
by
Alex
Waibel
and
was
one
of
the
first
CNNs,
as
it
achieved
shift
invariance.
It
did
so
by
utilizing
weight
sharing
in
combination
with
backpropagation
training.
Thus,
while
also
using
a
pyramidal
structure
as
in
the
neocognitron,
it
performed
a
global
optimization
of
the
weights
instead
of
a
local
one.In
1988,
Wei
Zhang
et
al.
applied
backpropagation
to
a
CNN
(a
simplified
Neocognitron
with
convolutional
interconnections
between
the
image
feature
layers
and
the
last
fully
connected
layer)
for
alphabet
recognition.
They
also
proposed
an
implementation
of
the
CNN
with
an
optical
computing
system.In
1989,
Yann
LeCun
et
al.
trained
a
CNN
with
the
purpose
of
recognizing
handwritten
ZIP
codes
on
mail.
While
the
algorithm
worked,
training
required
3
days.
Learning
was
fully
automatic,
performed
better
than
manual
coefficient
design,
and
was
suited
to
a
broader
range
of
image
recognition
problems
and
image
types.
Subsequently,
Wei
Zhang,
et
al.
modified
their
model
by
removing
the
last
fully
connected
layer
and
applied
it
for
medical
image
object
segmentation
in
1991
and
breast
cancer
detection
in
mammograms
in
1994.In
1990
Yamaguchi
et
al.
introduced
max-pooling,
a
fixed
filtering
operation
that
calculates
and
propagates
the
maximum
value
of
a
given
region.
They
combined
TDNNs
with
max-pooling
in
order
to
realize
a
speaker
independent
isolated
word
recognition
system.
In
a
variant
of
the
neocognitron
called
the
cresceptron,
instead
of
using
Fukushima's
spatial
averaging,
J.
Weng
et
al.
also
used
max-pooling
where
a
downsampling
unit
computes
the
maximum
of
the
activations
of
the
units
in
its
patch.
Max-pooling
is
often
used
in
modern
CNNs.LeNet-5,
a
7-level
CNN
by
Yann
LeCun
et
al.
in
1998,
that
classifies
digits,
was
applied
by
several
banks
to
recognize
hand-written
numbers
on
checks
(British
English:
cheques)
digitized
in
32x32
pixel
images.
The
ability
to
process
higher-resolution
images
requires
larger
and
more
layers
of
CNNs,
so
this
technique
is
constrained
by
the
availability
of
computing
resources.
In
2010,
Backpropagation
training
through
max-pooling
was
accelerated
by
GPUs
and
shown
to
perform
better
than
other
pooling
variants.
Behnke
(2003)
relied
only
on
the
sign
of
the
gradient
(Rprop)
on
problems
such
as
image
reconstruction
and
face
localization.
Rprop
is
a
first-order
optimization
algorithm
created
by
Martin
Riedmiller
and
Heinrich
Braun
in
1992.In
2011,
a
deep
GPU-based
CNN
called
"DanNet"
by
Dan
Ciresan,
Ueli
Meier,
and
Juergen
Schmidhuber
achieved
human-competitive
performance
for
the
first
time
in
computer
vision
contests.
Subsequently,
a
similar
GPU-based
CNN
by
Alex
Krizhevsky,
Ilya
Sutskever,
and
Geoffrey
Hinton
won
the
ImageNet
Large
Scale
Visual
Recognition
Challenge
2012.
A
very
deep
CNN
with
over
100
layers
by
Kaiming
He,
Xiangyu
Zhang,
Shaoqing
Ren,
and
Jian
Sun
of
Microsoft
won
the
ImageNet
2015
contest.ANNs
were
able
to
guarantee
shift
invariance
to
deal
with
small
and
large
natural
objects
in
large
cluttered
scenes,
only
when
invariance
extended
beyond
shift,
to
all
ANN-learned
concepts,
such
as
location,
type
(object
class
label),
scale,
lighting
and
others.
This
was
realized
in
Developmental
Networks
(DNs)
whose
embodiments
are
Where-What
Networks,
WWN-1
(2008)
through
WWN-7
(2013).
==
Artificial
curiosity
and
generative
adversarial
networks
==
In
1991,
Juergen
Schmidhuber
published
adversarial
neural
networks
that
contest
with
each
other
in
the
form
of
a
zero-sum
game,
where
one
network's
gain
is
the
other
network's
loss.
The
first
network
is
a
generative
model
that
models
a
probability
distribution
over
output
patterns.
The
second
network
learns
by
gradient
descent
to
predict
the
reactions
of
the
environment
to
these
patterns.
This
was
called
"artificial
curiosity."
Earlier
adversarial
machine
learning
systems
"neither
involved
unsupervised
neural
networks
nor
were
about
modeling
data
nor
used
gradient
descent."In
2014,
this
adversarial
principle
was
used
in
a
generative
adversarial
network
(GAN)
by
Ian
Goodfellow
et
al.
Here
the
environmental
reaction
is
1
or
0
depending
on
whether
the
first
network's
output
is
in
a
given
set.
This
can
be
used
to
create
realistic
deepfakes.In
1992,
Schmidhuber
also
published
another
type
of
gradient-based
adversarial
neural
networks
where
the
goal
of
the
zero-sum
game
is
to
create
disentangled
representations
of
input
patterns.
This
was
called
predictability
minimization.Nvidia's
StyleGAN
(2018)
is
based
on
the
Progressive
GAN
by
Tero
Karras,
Timo
Aila,
Samuli
Laine,
and
Jaakko
Lehtinen.
Here
the
GAN
generator
is
grown
from
small
to
large
scale
in
a
pyramidal
fashion.
StyleGANs
improve
consistency
between
fine
and
coarse
details
in
the
generator
network.
==
Transformers
and
their
variants
==
Many
modern
large
language
models
such
as
ChatGPT,
GPT-4,
and
BERT
use
a
feedforward
neural
network
called
Transformer
by
Ashish
Vaswani
et.
al.
in
their
2017
paper
"Attention
Is
All
You
Need."
Transformers
have
increasingly
become
the
model
of
choice
for
natural
language
processing
problems,
replacing
recurrent
neural
networks
(RNNs)
such
as
long
short-term
memory
(LSTM).Basic
ideas
for
this
go
back
a
long
way:
in
1992,
Juergen
Schmidhuber
published
the
Transformer
with
"linearized
self-attention"
(save
for
a
normalization
operator),
which
is
also
called
the
"linear
Transformer."
He
advertised
it
as
an
"alternative
to
RNNs"
that
can
learn
"internal
spotlights
of
attention,"
and
experimentally
applied
it
to
problems
of
variable
binding.
Here
a
slow
feedforward
neural
network
learns
by
gradient
descent
to
control
the
fast
weights
of
another
neural
network
through
outer
products
of
self-generated
activation
patterns
called
"FROM"
and
"TO"
which
in
Transformer
terminology
are
called
"key"
and
"value"
for
"self-attention."
This
fast
weight
"attention
mapping"
is
applied
to
queries.
The
2017
Transformer
combines
this
with
a
softmax
operator
and
a
projection
matrix.Transformers
are
also
increasingly
being
used
in
computer
vision.
==
Deep
learning
with
unsupervised
or
self-supervised
pre-training
==
In
the
1980s,
backpropagation
did
not
work
well
for
deep
FNNs
and
RNNs.
Here
the
word
"deep"
refers
to
the
number
of
layers
through
which
the
data
is
transformed.
More
precisely,
deep
learning
systems
have
a
substantial
credit
assignment
path
(CAP)
depth.
The
CAP
is
the
chain
of
transformations
from
input
to
output.
CAPs
describe
potentially
causal
connections
between
input
and
output.
For
an
FNN,
the
depth
of
the
CAPs
is
that
of
the
network
and
is
the
number
of
hidden
layers
plus
one
(as
the
output
layer
is
also
parameterized).
For
RNNs,
in
which
a
signal
may
propagate
through
a
layer
more
than
once,
the
CAP
depth
is
potentially
unlimited.
To
overcome
this
problem,
Juergen
Schmidhuber
(1992)
proposed
a
self-supervised
hierarchy
of
RNNs
pre-trained
one
level
at
a
time
by
self-supervised
learning.
This
"neural
history
compressor"
uses
predictive
coding
to
learn
internal
representations
at
multiple
self-organizing
time
scales.
The
deep
architecture
may
be
used
to
reproduce
the
original
data
from
the
top
level
feature
activations.
The
RNN
hierarchy
can
be
"collapsed"
into
a
single
RNN,
by
"distilling"
a
higher
level
"chunker"
network
into
a
lower
level
"automatizer"
network.
In
1993,
a
chunker
solved
a
deep
learning
task
whose
CAP
depth
exceeded
1000.
Such
history
compressors
can
substantially
facilitate
downstream
supervised
deep
learning.Geoffrey
Hinton
et
al.
(2006)
proposed
learning
a
high-level
internal
representation
using
successive
layers
of
binary
or
real-valued
latent
variables
with
a
restricted
Boltzmann
machine
to
model
each
layer.
This
RBM
is
a
generative
stochastic
feedforward
neural
network
that
can
learn
a
probability
distribution
over
its
set
of
inputs.
Once
sufficiently
many
layers
have
been
learned,
the
deep
architecture
may
be
used
as
a
generative
model
by
reproducing
the
data
when
sampling
down
the
model
(an
"ancestral
pass")
from
the
top
level
feature
activations.
In
2012,
Andrew
Ng
and
Jeff
Dean
created
an
FNN
that
learned
to
recognize
higher-level
concepts,
such
as
cats,
only
from
watching
unlabeled
images
taken
from
YouTube
videos.
==
The
vanishing
gradient
problem
and
its
solutions
==
Sepp
Hochreiter's
diploma
thesis
(1991)
was
called
"one
of
the
most
important
documents
in
the
history
of
machine
learning"
by
his
supervisor
Juergen
Schmidhuber.
Hochreiter
not
only
tested
the
neural
history
compressor,
but
also
identified
and
analyzed
the
vanishing
gradient
problem.
He
proposed
recurrent
residual
connections
to
solve
this
problem.
This
led
to
the
deep
learning
method
called
long
short-term
memory
(LSTM),
published
in
1997.
LSTM
recurrent
neural
networks
can
learn
"very
deep
learning"
tasks
with
long
credit
assignment
paths
that
require
memories
of
events
that
happened
thousands
of
discrete
time
steps
before.
The
"vanilla
LSTM"
with
forget
gate
was
introduced
in
1999
by
Felix
Gers,
Schmidhuber
and
Fred
Cummins.
LSTM
has
become
the
most
cited
neural
network
of
the
20th
century.In
2015,
Rupesh
Kumar
Srivastava,
Klaus
Greff,
and
Schmidhuber
used
LSTM
principles
to
create
the
Highway
network,
a
feedforward
neural
network
with
hundreds
of
layers,
much
deeper
than
previous
networks.
7
months
later,
Kaiming
He,
Xiangyu
Zhang;
Shaoqing
Ren,
and
Jian
Sun
won
the
ImageNet
2015
competition
with
an
open-gated
or
gateless
Highway
network
variant
called
Residual
neural
network.
This
has
become
the
most
cited
neural
network
of
the
21st
century.In
2011,
Xavier
Glorot,
Antoine
Bordes
and
Yoshua
Bengio
found
that
the
ReLU
of
Kunihiko
Fukushima
also
helps
to
overcome
the
vanishing
gradient
problem,
compared
to
widely
used
activation
functions
prior
to
2011.
==
Hardware-based
designs
==
The
development
of
metal–oxide–semiconductor
(MOS)
very-large-scale
integration
(VLSI),
combining
millions
or
billions
of
MOS
transistors
onto
a
single
chip
in
the
form
of
complementary
MOS
(CMOS)
technology,
enabled
the
development
of
practical
artificial
neural
networks
in
the
1980s.Computational
devices
were
created
in
CMOS,
for
both
biophysical
simulation
and
neuromorphic
computing
inspired
by
the
structure
and
function
of
the
human
brain.
Nanodevices
for
very
large
scale
principal
components
analyses
and
convolution
may
create
a
new
class
of
neural
computing
because
they
are
fundamentally
analog
rather
than
digital
(even
though
the
first
implementations
may
use
digital
devices).
Ciresan
and
colleagues
(2010)
in
Schmidhuber's
group
showed
that
despite
the
vanishing
gradient
problem,
GPUs
make
backpropagation
feasible
for
many-layered
feedforward
neural
networks.
==
Contests
==
Between
2009
and
2012,
recurrent
neural
networks
and
deep
feedforward
neural
networks
developed
in
Schmidhuber's
research
group
won
eight
international
competitions
in
pattern
recognition
and
machine
learning.
For
example,
the
bi-directional
and
multi-dimensional
long
short-term
memory
(LSTM)
of
Graves
et
al.
won
three
competitions
in
connected
handwriting
recognition
at
the
2009
International
Conference
on
Document
Analysis
and
Recognition
(ICDAR),
without
any
prior
knowledge
about
the
three
languages
to
be
learned.Ciresan
and
colleagues
won
pattern
recognition
contests,
including
the
IJCNN
2011
Traffic
Sign
Recognition
Competition,
the
ISBI
2012
Segmentation
of
Neuronal
Structures
in
Electron
Microscopy
Stacks
challenge
and
others.
Their
neural
networks
were
the
first
pattern
recognizers
to
achieve
human-competitive/superhuman
performance
on
benchmarks
such
as
traffic
sign
recognition
(IJCNN
2012),
or
the
MNIST
handwritten
digits
problem.
Researchers
demonstrated
(2010)
that
deep
neural
networks
interfaced
to
a
hidden
Markov
model
with
context-dependent
states
that
define
the
neural
network
output
layer
can
drastically
reduce
errors
in
large-vocabulary
speech
recognition
tasks
such
as
voice
search.GPU-based
implementations
of
this
approach
won
many
pattern
recognition
contests,
including
the
IJCNN
2011
Traffic
Sign
Recognition
Competition,
the
ISBI
2012
Segmentation
of
neuronal
structures
in
EM
stacks
challenge,
the
ImageNet
Competition
and
others.
Deep,
highly
nonlinear
neural
architectures
similar
to
the
neocognitron
and
the
"standard
architecture
of
vision",
inspired
by
simple
and
complex
cells,
were
pre-trained
with
unsupervised
methods
by
Hinton.
A
team
from
his
lab
won
a
2012
contest
sponsored
by
Merck
to
design
software
to
help
find
molecules
that
might
identify
new
drugs.
==
Notes
==
==
References
==
==
External
links
==
"Lecun
2019-7-11
ACM
Tech
Talk".
Google
Docs.
Retrieved
2020-02-13.
A
large
language
model
(LLM)
is
a
language
model
notable
for
its
ability
to
achieve
general-purpose
language
generation
and
other
natural
language
processing
tasks
such
as
classification.
LLMs
acquire
these
abilities
by
learning
statistical
relationships
from
text
documents
during
a
computationally
intensive
self-supervised
and
semi-supervised
training
process.
LLMs
can
be
used
for
text
generation,
a
form
of
generative
AI,
by
taking
an
input
text
and
repeatedly
predicting
the
next
token
or
word.LLMs
are
artificial
neural
networks.
The
largest
and
most
capable,
as
of
March
2024,
are
built
with
a
decoder-only
transformer-based
architecture
while
some
recent
implementations
are
based
on
other
architectures,
such
as
recurrent
neural
network
variants
and
Mamba
(a
state
space
model).Up
to
2020,
fine
tuning
was
the
only
way
a
model
could
be
adapted
to
be
able
to
accomplish
specific
tasks.
Larger
sized
models,
such
as
GPT-3,
however,
can
be
prompt-engineered
to
achieve
similar
results.
They
are
thought
to
acquire
knowledge
about
syntax,
semantics
and
"ontology"
inherent
in
human
language
corpora,
but
also
inaccuracies
and
biases
present
in
the
corpora.Some
notable
LLMs
are
OpenAI's
GPT
series
of
models
(e.g.,
GPT-3.5
and
GPT-4,
used
in
ChatGPT
and
Microsoft
Copilot),
Google's
PaLM
and
Gemini
(the
latter
of
which
is
currently
used
in
the
chatbot
of
the
same
name),
xAI's
Grok,
Meta's
LLaMA
family
of
open-source
models,
Anthropic's
Claude
models,
and
Mistral
AI's
open
source
models.
==
History
==
At
the
2017
NeurIPS
conference,
Google
researchers
introduced
the
transformer
architecture
in
their
landmark
paper
"Attention
Is
All
You
Need".
This
paper's
goal
was
to
improve
upon
2014
Seq2seq
technology,
and
was
based
mainly
on
the
attention
mechanism
developed
by
Bahdanau
et
al.
in
2014.
The
following
year
in
2018,
BERT
was
introduced
and
quickly
became
"ubiquitous".
Though
the
original
transformer
has
both
encoder
and
decoder
blocks,
BERT
is
an
encoder-only
model.
Although
decoder-only
GPT-1
was
introduced
in
2018,
it
was
GPT-2
in
2019
that
caught
widespread
attention
because
OpenAI
at
first
deemed
it
too
powerful
to
release
publicly,
out
of
fear
of
malicious
use.
GPT-3
in
2020
went
a
step
further
and
as
of
2024
is
available
only
via
API
with
no
offering
of
downloading
the
model
to
execute
locally.
But
it
was
the
2022
consumer-facing
browser-based
ChatGPT
that
captured
the
imaginations
of
the
general
population
and
caused
some
media
hype
and
online
buzz.
The
2023
GPT-4
was
praised
for
its
increased
accuracy
and
as
a
"holy
grail"
for
its
multimodal
capabilities.
OpenAI
did
not
reveal
high-level
architecture
and
the
number
of
parameters
of
GPT-4.
In
the
meantime,
competing
language
models
have
for
the
most
part
been
playing
catch-up
to
the
GPT
series,
at
least
in
terms
of
number
of
parameters.
Notable
exceptions
in
terms
of
either
number
of
parameters
or
measured
accuracy
include
Google's
2019
T5-11B
and
2022
PaLM-E,
and
Anthropic's
2024
Claude
3.
In
terms
of
Elo
ratings,
on
January
26,
2024,
Google's
Bard
(Gemini
Pro)
surpassed
the
regular
GPT-4,
but
not
the
limited-availability
GPT-4-Turbo.Since
2022,
source-available
models
have
been
gaining
popularity,
especially
at
first
with
BLOOM
and
LLaMA,
though
both
have
restrictions
on
the
field
of
use.
Mistral
AI's
models
Mistral
7B
and
Mixtral
8x7b
have
the
more
permissive
Apache
License.
As
of
January
2024,
Mixtral
8x7b
is
the
most
powerful
open
LLM
according
to
the
LMSYS
Chatbot
Arena
Leaderboard,
being
more
powerful
than
GPT-3.5
but
not
as
powerful
as
GPT-4.
==
Dataset
preprocessing
==
===
Probabilistic
tokenization
===
Because
machine
learning
algorithms
process
numbers
rather
than
text,
the
text
must
be
converted
to
numbers.
In
the
first
step,
a
vocabulary
is
decided
upon,
then
integer
indexes
are
arbitrarily
but
uniquely
assigned
to
each
vocabulary
entry,
and
finally,
an
embedding
is
associated
to
the
integer
index.
Algorithms
include
byte-pair
encoding
and
WordPiece.
Probabilistic
tokenization
also
compresses
the
datasets.
Because
LLMs
generally
require
input
to
be
an
array
that
is
not
jagged,
the
shorter
texts
must
be
"padded"
until
they
match
the
length
of
the
longest
one.
How
many
tokens
are,
on
average,
needed
per
word
depends
on
the
language
of
the
dataset.
====
BPE
====
Using
a
modification
of
byte-pair
encoding,
in
the
first
step,
all
unique
characters
(including
blanks
and
punctuation
marks)
are
treated
as
an
initial
set
of
n-grams
(i.e.
initial
set
of
uni-grams).
Successively
the
most
frequent
pair
of
adjacent
characters
is
merged
into
a
bi-gram
and
all
instances
of
the
pair
are
replaced
by
it.
All
occurrences
of
adjacent
pairs
of
(previously
merged)
n-grams
that
most
frequently
occur
together
are
then
again
merged
into
even
lengthier
n-gram
repeatedly
until
a
vocabulary
of
prescribed
size
is
obtained
(in
case
of
GPT-3,
the
size
is
50257).
Token
vocabulary
consists
of
integers,
spanning
from
zero
up
to
the
size
of
the
token
vocabulary.
New
words
can
always
be
interpreted
as
combinations
of
the
tokens
and
the
initial-set
uni-grams.A
token
vocabulary
based
on
the
frequencies
extracted
from
mainly
English
corpora
uses
as
few
tokens
as
possible
for
an
average
English
word.
An
average
word
in
another
language
encoded
by
such
an
English-optimized
tokenizer
is
however
split
into
suboptimal
amount
of
tokens.
GPT-2
tokenizer
can
use
up
to
15
times
more
tokens
per
word
for
some
languages,
for
example
for
Shan
language
from
Myanmar.
Even
more
widespread
languages
such
as
Portuguese
and
German
have
"a
premium
of
50%"
compared
to
English.For
example,
here
is
how
tokenizer
used
by
GPT-3
(Legacy)
split
the
following
sentence
tokenizer:
texts
->
series
of
numerical
"tokens".
===
Dataset
cleaning
===
In
the
context
of
training
LLMs,
datasets
are
typically
cleaned
by
removing
toxic
passages
from
the
dataset,
discarding
low-quality
data,
and
de-duplication.
Cleaned
datasets
can
increase
training
efficiency
and
lead
to
improved
downstream
performance.With
the
increasing
proportion
of
LLM-generated
content
on
the
web,
data
cleaning
in
the
future
may
include
filtering
out
such
content.
LLM-generated
content
can
pose
a
problem
if
the
content
is
similar
to
human
text
(making
filtering
difficult)
but
of
lower
quality
(degrading
performance
of
models
trained
on
it).
==
Training
and
architecture
==
===
Reinforcement
learning
from
human
feedback
(RLHF)
===
Reinforcement
learning
from
human
feedback
(RLHF)
through
algorithms,
such
as
proximal
policy
optimization,
is
used
to
further
fine-tune
a
model
based
on
a
dataset
of
human
preferences.
===
Instruction
tuning
===
Using
"self-instruct"
approaches,
LLMs
have
been
able
to
bootstrap
correct
responses,
replacing
any
naive
responses,
starting
from
human-generated
corrections
of
a
few
cases.
For
example,
in
the
instruction
"Write
an
essay
about
the
main
themes
represented
in
Hamlet,"
an
initial
naive
completion
might
be
'If
you
submit
the
essay
after
March
17,
your
grade
will
be
reduced
by
10%
for
each
day
of
delay,"
based
on
the
frequency
of
this
textual
sequence
in
the
corpus.
===
Mixture
of
experts
===
The
largest
LLM
may
be
too
expensive
to
train
and
use
directly.
For
such
models,
mixture
of
experts
(MoE)
can
be
applied,
a
line
of
research
pursued
by
Google
researchers
since
2017
to
train
models
reaching
up
to
1
trillion
parameters.
===
Prompt
engineering,
attention
mechanism,
and
context
window
===
Most
results
previously
achievable
only
by
(costly)
fine-tuning,
can
be
achieved
through
prompt
engineering,
although
limited
to
the
scope
of
a
single
conversation
(more
precisely,
limited
to
the
scope
of
a
context
window).
In
order
to
find
out
which
tokens
are
relevant
to
each
other
within
the
scope
of
the
context
window,
the
attention
mechanism
calculates
"soft"
weights
for
each
token,
more
precisely
for
its
embedding,
by
using
multiple
attention
heads,
each
with
its
own
"relevance"
for
calculating
its
own
soft
weights.
For
example,
the
small
(i.e.
117M
parameter
sized)
GPT-2
model,
has
had
twelve
attention
heads
and
a
context
window
of
only
1k
token.
In
its
medium
version
it
has
345M
parameters
and
contains
24
layers,
each
with
12
attention
heads.
For
the
training
with
gradient
descent
a
batch
size
of
512
was
utilized.The
largest
models,
such
as
Google's
Gemini
1.5,
presented
in
February
2024,
can
have
a
context
window
sized
up
to
1
million
(context
window
of
10
million
was
also
"successfully
tested").
Other
models
with
large
context
windows
includes
Anthropic's
Claude
2.1,
with
a
context
window
of
up
to
200k
tokens.
Note
that
this
maximum
refers
to
the
number
of
input
tokens
and
that
the
maximum
number
of
output
tokens
differs
from
the
input
and
is
often
smaller.
For
example,
the
GPT-4
Turbo
model
has
a
maximum
output
of
4096
tokens.Length
of
a
conversation
that
the
model
can
take
into
account
when
generating
its
next
answer
is
limited
by
the
size
of
a
context
window,
as
well.
If
the
length
of
a
conversation,
for
example
with
Chat-GPT,
is
longer
than
its
context
window,
only
the
parts
inside
the
context
window
are
taken
into
account
when
generating
the
next
answer,
or
the
model
needs
to
apply
some
algorithm
to
summarize
the
too
distant
parts
of
conversation.
The
shortcomings
of
making
a
context
window
larger
include
higher
computational
cost
and
possibly
diluting
the
focus
on
local
context,
while
making
it
smaller
can
cause
a
model
to
miss
an
important
long-range
dependency.
Balancing
them
are
a
matter
of
experimentation
and
domain-specific
considerations.
A
model
may
be
pre-trained
either
to
predict
how
the
segment
continues,
or
what
is
missing
in
the
segment,
given
a
segment
from
its
training
dataset.
It
can
be
either
autoregressive
(i.e.
predicting
how
the
segment
continues,
the
way
GPTs
do
it):
for
example
given
a
segment
"I
like
to
eat",
the
model
predicts
"ice
cream",
or
"sushi".
"masked"
(i.e.
filling
in
the
parts
missing
from
the
segment,
the
way
"BERT"
does
it):
for
example,
given
a
segment
"I
like
to
[__]
[__]
cream",
the
model
predicts
that
"eat"
and
"ice"
are
missing.Models
may
be
trained
on
auxiliary
tasks
which
test
their
understanding
of
the
data
distribution,
such
as
Next
Sentence
Prediction
(NSP),
in
which
pairs
of
sentences
are
presented
and
the
model
must
predict
whether
they
appear
consecutively
in
the
training
corpus.
During
training,
regularization
loss
is
also
used
to
stabilize
training.
However
regularization
loss
is
usually
not
used
during
testing
and
evaluation.
==
Training
cost
==
Advances
in
software
and
hardware
have
reduced
the
cost
substantially
since
2020,
such
that
in
2023
training
of
a
12-billion-parameter
LLM
computational
cost
is
72,300
A100-GPU-hours,
while
in
2020
the
cost
of
training
a
1.5-billion-parameter
LLM
(which
was
two
orders
of
magnitude
smaller
than
the
state
of
the
art
in
2020)
was
between
$80
thousand
and
$1.6
million.
Since
2020,
large
sums
were
invested
in
increasingly
large
models.
For
example,
training
of
the
GPT-2
(i.e.
a
1.5-billion-parameters
model)
in
2019
cost
$50,000,
while
training
of
the
PaLM
(i.e.
a
540-billion-parameters
model)
in
2022
cost
$8
million,
and
Megatron-Turing
NLG
530B
(in
2021)
cost
around
$11
million.For
Transformer-based
LLM,
training
cost
is
much
higher
than
inference
cost.
It
costs
6
FLOPs
per
parameter
to
train
on
one
token,
whereas
it
costs
1
to
2
FLOPs
per
parameter
to
infer
on
one
token.
==
Tool
use
==
There
are
certain
tasks
that,
in
principle,
cannot
be
solved
by
any
LLM,
at
least
not
without
the
use
of
external
tools
or
additional
software.
An
example
of
such
a
task
is
responding
to
the
user's
input
'354
*
139
=
',
provided
that
the
LLM
has
not
already
encountered
a
continuation
of
this
calculation
in
its
training
corpus.
In
such
cases,
the
LLM
needs
to
resort
to
running
program
code
that
calculates
the
result,
which
can
then
be
included
in
its
response.
Another
example
is
'What
is
the
time
now?
It
is
',
where
a
separate
program
interpreter
would
need
to
execute
a
code
to
get
system
time
on
the
computer,
so
LLM
could
include
it
in
its
reply.
This
basic
strategy
can
be
sophisticated
with
multiple
attempts
of
generated
programs,
and
other
sampling
strategies.
Cost
Savings
and
Reduced
Vendor
Dependency
Generally,
in
order
to
get
an
LLM
to
use
tools,
one
must
finetune
it
for
tool-use.
If
the
number
of
tools
is
finite,
then
finetuning
may
be
done
just
once.
If
the
number
of
tools
can
grow
arbitrarily,
as
with
online
API
services,
then
the
LLM
can
be
fine-tuned
to
be
able
to
read
API
documentation
and
call
API
correctly.A
simpler
form
of
tool
use
is
Retrieval
Augmented
Generation:
augment
an
LLM
with
document
retrieval,
sometimes
using
a
vector
database.
Given
a
query,
a
document
retriever
is
called
to
retrieve
the
most
relevant
(usually
measured
by
first
encoding
the
query
and
the
documents
into
vectors,
then
finding
the
documents
with
vectors
closest
in
Euclidean
norm
to
the
query
vector).
The
LLM
then
generates
an
output
based
on
both
the
query
and
the
retrieved
documents.
==
Agency
==
An
LLM
is
a
language
model,
which
is
not
an
agent
as
it
has
no
goal,
but
it
can
be
used
as
a
component
of
an
intelligent
agent.
Researchers
have
described
several
methods
for
such
integrations.The
ReAct
("Reason
+
Act")
method
constructs
an
agent
out
of
an
LLM,
using
the
LLM
as
a
planner.
The
LLM
is
prompted
to
"think
out
loud".
Specifically,
the
language
model
is
prompted
with
a
textual
description
of
the
environment,
a
goal,
a
list
of
possible
actions,
and
a
record
of
the
actions
and
observations
so
far.
It
generates
one
or
more
thoughts
before
generating
an
action,
which
is
then
executed
in
the
environment.
The
linguistic
description
of
the
environment
given
to
the
LLM
planner
can
even
be
the
LaTeX
code
of
a
paper
describing
the
environment.In
the
DEPS
("Describe,
Explain,
Plan
and
Select")
method,
an
LLM
is
first
connected
to
the
visual
world
via
image
descriptions,
then
it
is
prompted
to
produce
plans
for
complex
tasks
and
behaviors
based
on
its
pretrained
knowledge
and
environmental
feedback
it
receives.The
Reflexion
method
constructs
an
agent
that
learns
over
multiple
episodes.
At
the
end
of
each
episode,
the
LLM
is
given
the
record
of
the
episode,
and
prompted
to
think
up
"lessons
learned",
which
would
help
it
perform
better
at
a
subsequent
episode.
These
"lessons
learned"
are
given
to
the
agent
in
the
subsequent
episodes.Monte
Carlo
tree
search
can
use
an
LLM
as
rollout
heuristic.
When
a
programmatic
world
model
is
not
available,
an
LLM
can
also
be
prompted
with
a
description
of
the
environment
to
act
as
world
model.For
open-ended
exploration,
an
LLM
can
be
used
to
score
observations
for
their
"interestingness",
which
can
be
used
as
a
reward
signal
to
guide
a
normal
(non-LLM)
reinforcement
learning
agent.
Alternatively,
it
can
propose
increasingly
difficult
tasks
for
curriculum
learning.
Instead
of
outputting
individual
actions,
an
LLM
planner
can
also
construct
"skills",
or
functions
for
complex
action
sequences.
The
skills
can
be
stored
and
later
invoked,
allowing
increasing
levels
of
abstraction
in
planning.LLM-powered
agents
can
keep
a
long-term
memory
of
its
previous
contexts,
and
the
memory
can
be
retrieved
in
the
same
way
as
Retrieval
Augmented
Generation.
Multiple
such
agents
can
interact
socially.
==
Compression
==
Typically,
LLM
are
trained
with
full-
or
half-precision
floating
point
numbers
(float32
and
float16).
One
float16
has
16
bits,
or
2
bytes,
and
so
one
billion
parameters
require
2
gigabytes.
The
largest
models
typically
have
100
billion
parameters,
requiring
200
gigabytes
to
load,
which
places
them
outside
the
range
of
most
consumer
electronics.Post-training
quantization
aims
to
decrease
the
space
requirement
by
lowering
precision
of
the
parameters
of
a
trained
model,
while
preserving
most
of
its
performance.
The
simplest
form
of
quantization
simply
truncates
all
numbers
to
a
given
number
of
bits.
It
can
be
improved
by
using
a
different
quantization
codebook
per
layer.
Further
improvement
can
be
done
by
applying
different
precisions
to
different
parameters,
with
higher
precision
for
particularly
important
parameters
("outlier
weights").While
quantized
models
are
typically
frozen,
and
only
pre-quantized
models
are
fine-tuned,
quantized
models
can
still
be
fine-tuned.
==
Multimodality
==
Multimodality
means
"having
several
modalities",
and
a
"modality"
refers
to
a
type
of
input
or
output,
such
as
video,
image,
audio,
text,
proprioception,
etc.
There
have
been
many
AI
models
trained
specifically
to
ingest
one
modality
and
output
another
modality,
such
as
AlexNet
for
image
to
label,
visual
question
answering
for
image-text
to
text,
and
speech
recognition
for
speech
to
text.
A
common
method
to
create
multimodal
models
out
of
an
LLM
is
to
"tokenize"
the
output
of
a
trained
encoder.
Concretely,
one
can
construct
a
LLM
that
can
understand
images
as
follows:
take
a
trained
LLM,
and
take
a
trained
image
encoder
E{\displaystyle
E}.
Make
a
small
multilayered
perceptron
f{\displaystyle
f},
so
that
for
any
image
y{\displaystyle
y},
the
post-processed
vector
f(E(y)){\displaystyle
f(E(y))}
has
the
same
dimensions
as
an
encoded
token.
That
is
an
"image
token".
Then,
one
can
interleave
text
tokens
and
image
tokens.
The
compound
model
is
then
fine-tuned
on
an
image-text
dataset.
This
basic
construction
can
be
applied
with
more
sophistication
to
improve
the
model.
The
image
encoder
may
be
frozen
to
improve
stability.Flamingo
demonstrated
the
effectiveness
of
the
tokenization
method,
finetuning
a
pair
of
pretrained
language
model
and
image
encoder
to
perform
better
on
visual
question
answering
than
models
trained
from
scratch.
Google
PaLM
model
was
fine-tuned
into
a
multimodal
model
PaLM-E
using
the
tokenization
method,
and
applied
to
robotic
control.
LLaMA
models
have
also
been
turned
multimodal
using
the
tokenization
method,
to
allow
image
inputs,
and
video
inputs.GPT-4
can
use
both
text
and
image
as
inputs
(although
the
vision
component
wasn't
released
to
the
public
until
GPT-4V);
Google
DeepMind's
Gemini
is
also
multimodal.
==
Properties
==
===
Scaling
laws
===
The
following
four
hyper-parameters
characterize
a
LLM:
cost
of
(pre-)training
(C{\displaystyle
C}),
size
of
the
artificial
neural
network
itself,
such
as
number
of
parameters
N{\displaystyle
N}
(i.e.
amount
of
neurons
in
its
layers,
amount
of
weights
between
them
and
biases),
size
of
its
(pre-)training
dataset
(i.e.
number
of
tokens
in
corpus,
D{\displaystyle
D}),
performance
after
(pre-)training.They
are
related
by
simple
statistical
laws,
called
"scaling
laws".
One
particular
scaling
law
("Chinchilla
scaling")
for
LLM
autoregressively
trained
for
one
epoch,
with
a
log-log
learning
rate
schedule,
states
that:
where
the
variables
are
C{\displaystyle
C}
is
the
cost
of
training
the
model,
in
FLOPs.
N{\displaystyle
N}
is
the
number
of
parameters
in
the
model.
D{\displaystyle
D}
is
the
number
of
tokens
in
the
training
set.
L{\displaystyle
L}
is
the
average
negative
log-likelihood
loss
per
token
(nats/token),
achieved
by
the
trained
LLM
on
the
test
dataset.and
the
statistical
hyper-parameters
are
C0=6{\displaystyle
C_{0}=6},
meaning
that
it
costs
6
FLOPs
per
parameter
to
train
on
one
token.
Note
that
training
cost
is
much
higher
than
inference
cost,
where
it
costs
1
to
2
FLOPs
per
parameter
to
infer
on
one
token.
α=0.34,β=0.28,A=406.4,B=410.7,L0=1.69{\displaystyle
\alpha
=0.34,\beta
=0.28,A=406.4,B=410.7,L_{0}=1.69}
===
Emergent
abilities
===
When
one
subtracts
out
from
the
y-axis
the
best
performance
that
can
be
achieved
even
with
infinite
scaling
of
the
x-axis
quantity,
large
models'
performance,
measured
on
various
tasks,
seems
to
be
a
linear
extrapolation
of
other
(smaller-sized
and
medium-sized)
models'
performance
on
a
log-log
plot.
However,
sometimes
the
line's
slope
transitions
from
one
slope
to
another
at
point(s)
referred
to
as
break(s)
in
downstream
scaling
laws,
appearing
as
a
series
of
linear
segments
connected
by
arcs;
it
seems
that
larger
models
acquire
"emergent
abilities"
at
this
point(s).
These
abilities
are
discovered
rather
than
programmed-in
or
designed,
in
some
cases
only
after
the
LLM
has
been
publicly
deployed.The
most
intriguing
among
emergent
abilities
is
in-context
learning
from
example
demonstrations.
In-context
learning
is
involved
in
tasks,
such
as:
reported
arithmetics,
decoding
the
International
Phonetic
Alphabet,
unscrambling
a
word's
letters,
disambiguate
word
in
context,
converting
spatial
words,
cardinal
directions
(for
example,
replying
"northeast"
upon
[0,
0,
1;
0,
0,
0;
0,
0,
0]),
color
terms
represented
in
text.
chain-of-thought
prompting:
Model
outputs
are
improved
by
chain-of-thought
prompting
only
when
model
size
exceeds
62B.
Smaller
models
perform
better
when
prompted
to
answer
immediately,
without
chain
of
thought.
identifying
offensive
content
in
paragraphs
of
Hinglish
(a
combination
of
Hindi
and
English),
and
generating
a
similar
English
equivalent
of
Kiswahili
proverbs.Schaeffer
et.
al.
argue
that
the
emergent
abilities
are
not
unpredictably
acquired,
but
predictably
acquired
according
to
a
smooth
scaling
law.
The
authors
considered
a
toy
statistical
model
of
an
LLM
solving
multiple-choice
questions,
and
showed
that
this
statistical
model,
modified
to
account
for
other
types
of
tasks,
applies
to
these
tasks
as
well.Let
x{\displaystyle
x}
be
the
number
of
parameter
count,
and
y{\displaystyle
y}
be
the
performance
of
the
model.
==
Interpretation
==
Large
language
models
by
themselves
are
"black
boxes",
and
it
is
not
clear
how
they
can
perform
linguistic
tasks.
There
are
several
methods
for
understanding
how
LLM
work.
Mechanistic
interpretability
aims
to
reverse-engineer
LLM
by
discovering
symbolic
algorithms
that
approximate
the
inference
performed
by
LLM.
One
example
is
Othello-GPT,
where
a
small
Transformer
is
trained
to
predict
legal
Othello
moves.
It
is
found
that
there
is
a
linear
representation
of
Othello
board,
and
modifying
the
representation
changes
the
predicted
legal
Othello
moves
in
the
correct
way.
In
another
example,
a
small
Transformer
is
trained
on
Karel
programs.
Similar
to
the
Othello-GPT
example,
there
is
a
linear
representation
of
Karel
program
semantics,
and
modifying
the
representation
changes
output
in
the
correct
way.
The
model
also
generates
correct
programs
that
are
on
average
shorter
than
those
in
the
training
set.In
another
example,
the
authors
trained
small
transformers
on
modular
arithmetic
addition.
The
resulting
models
were
reverse-engineered,
and
it
turned
out
they
used
discrete
Fourier
transform.
===
Understanding
and
intelligence
===
NLP
researchers
were
evenly
split
when
asked,
in
a
2022
survey,
whether
(untuned)
LLMs
"could
(ever)
understand
natural
language
in
some
nontrivial
sense".
Proponents
of
"LLM
understanding"
believe
that
some
LLM
abilities,
such
as
mathematical
reasoning,
imply
an
ability
to
"understand"
certain
concepts.
A
Microsoft
team
argued
in
2023
that
GPT-4
"can
solve
novel
and
difficult
tasks
that
span
mathematics,
coding,
vision,
medicine,
law,
psychology
and
more"
and
that
GPT-4
"could
reasonably
be
viewed
as
an
early
(yet
still
incomplete)
version
of
an
artificial
general
intelligence
system":
"Can
one
reasonably
say
that
a
system
that
passes
exams
for
software
engineering
candidates
is
not
really
intelligent?"
Some
researchers
characterize
LLMs
as
"alien
intelligence".
For
example,
Conjecture
CEO
Connor
Leahy
considers
untuned
LLMs
to
be
like
inscrutable
alien
"Shoggoths",
and
believes
that
RLHF
tuning
creates
a
"smiling
facade"
obscuring
the
inner
workings
of
the
LLM:
"If
you
don't
push
it
too
far,
the
smiley
face
stays
on.
But
then
you
give
it
[an
unexpected]
prompt,
and
suddenly
you
see
this
massive
underbelly
of
insanity,
of
weird
thought
processes
and
clearly
non-human
understanding."In
contrast,
some
proponents
of
the
"LLMs
lack
understanding"
school
believe
that
existing
LLMs
are
"simply
remixing
and
recombining
existing
writing",
or
point
to
the
deficits
existing
LLMs
continue
to
have
in
prediction
skills,
reasoning
skills,
agency,
and
explainability.
For
example,
GPT-4
has
natural
deficits
in
planning
and
in
real-time
learning.
Generative
LLMs
have
been
observed
to
confidently
assert
claims
of
fact
which
do
not
seem
to
be
justified
by
their
training
data,
a
phenomenon
which
has
been
termed
"hallucination".
Specifically,
hallucinations
in
the
context
of
LLMs
correspond
to
the
generation
of
text
or
responses
that
seem
syntactically
sound,
fluent,
and
natural
but
are
factually
incorrect,
nonsensical,
or
unfaithful
to
the
provided
source
input.
Neuroscientist
Terrence
Sejnowski
has
argued
that
"The
diverging
opinions
of
experts
on
the
intelligence
of
LLMs
suggests
that
our
old
ideas
based
on
natural
intelligence
are
inadequate".The
matter
of
LLM's
exhibiting
intelligence
or
understanding
has
two
main
aspects
–
the
first
is
how
to
model
thought
and
language
in
a
computer
system,
and
the
second
is
how
to
enable
the
computer
system
to
generate
human
like
language.
These
aspects
of
language
as
a
model
of
cognition
have
been
developed
in
the
field
of
cognitive
linguistics.
American
linguist
George
Lakoff
presented
Neural
Theory
of
Language
(NTL)
as
a
computational
basis
for
using
language
as
a
model
of
learning
tasks
and
understanding.
The
NTL
Model
outlines
how
specific
neural
structures
of
the
human
brain
shape
the
nature
of
thought
and
language
and
in
turn
what
are
the
computational
properties
of
such
neural
systems
that
can
be
applied
to
model
thought
and
language
in
a
computer
system.
After
a
framework
for
modeling
language
in
a
computer
systems
was
established,
the
focus
shifted
to
establishing
frameworks
for
computer
systems
to
generate
language
with
acceptable
grammar.
In
his
2014
book
titled
The
Language
Myth:
Why
Language
Is
Not
An
Instinct,
British
cognitive
linguist
and
digital
communication
technologist
Vyvyan
Evans
mapped
out
the
role
of
probabilistic
context-free
grammar
(PCFG)
in
enabling
NLP
to
model
cognitive
patterns
and
generate
human
like
language.
==
Evaluation
==
===
Perplexity
===
The
most
commonly
used
measure
of
a
language
model's
performance
is
its
perplexity
on
a
given
text
corpus.
Perplexity
is
a
measure
of
how
well
a
model
is
able
to
predict
the
contents
of
a
dataset;
the
higher
the
likelihood
the
model
assigns
to
the
dataset,
the
lower
the
perplexity.
Mathematically,
perplexity
is
defined
as
the
exponential
of
the
average
negative
log
likelihood
per
token:here
N{\displaystyle
N}
is
the
number
of
tokens
in
the
text
corpus,
and
"context
for
token
i{\displaystyle
i}"
depends
on
the
specific
type
of
LLM
used.
If
the
LLM
is
autoregressive,
then
"context
for
token
i{\displaystyle
i}"
is
the
segment
of
text
appearing
before
token
i{\displaystyle
i}.
If
the
LLM
is
masked,
then
"context
for
token
i{\displaystyle
i}"
is
the
segment
of
text
surrounding
token
i{\displaystyle
i}.
Because
language
models
may
overfit
to
their
training
data,
models
are
usually
evaluated
by
their
perplexity
on
a
test
set
of
unseen
data.
This
presents
particular
challenges
for
the
evaluation
of
large
language
models.
As
they
are
trained
on
increasingly
large
corpora
of
text
largely
scraped
from
the
web,
it
becomes
increasingly
likely
that
models'
training
data
inadvertently
includes
portions
of
any
given
test
set.
====
BPW,
BPC,
and
BPT
====
In
information
theory,
the
concept
of
entropy
is
intricately
linked
to
perplexity,
a
relationship
notably
established
by
Claude
Shannon.
This
relationship
is
mathematically
expressed
as
Entropy=log2⁡(Perplexity){\displaystyle
{\text{Entropy}}=\log
_{2}({\text{Perplexity}})}.
Entropy,
in
this
context,
is
commonly
quantified
in
terms
of
bits
per
word
(BPW)
or
bits
per
character
(BPC),
which
hinges
on
whether
the
language
model
utilizes
word-based
or
character-based
tokenization.
Notably,
in
the
case
of
larger
language
models
that
predominantly
employ
sub-word
tokenization,
bits
per
token
(BPT)
emerges
as
a
seemingly
more
appropriate
measure.
However,
due
to
the
variance
in
tokenization
methods
across
different
Large
Language
Models
(LLMs),
BPT
does
not
serve
as
a
reliable
metric
for
comparative
analysis
among
diverse
models.
To
convert
BPT
into
BPW,
one
can
multiply
it
by
the
average
number
of
tokens
per
word.
In
the
evaluation
and
comparison
of
language
models,
cross-entropy
is
generally
the
preferred
metric
over
entropy.
The
underlying
principle
is
that
a
lower
BPW
is
indicative
of
a
model's
enhanced
capability
for
compression.
This,
in
turn,
reflects
the
model's
proficiency
in
making
accurate
predictions.
===
Task-specific
datasets
and
benchmarks
===
A
large
number
of
testing
datasets
and
benchmarks
have
also
been
developed
to
evaluate
the
capabilities
of
language
models
on
more
specific
downstream
tasks.
Tests
may
be
designed
to
evaluate
a
variety
of
capabilities,
including
general
knowledge,
commonsense
reasoning,
and
mathematical
problem-solving.
One
broad
category
of
evaluation
dataset
is
question
answering
datasets,
consisting
of
pairs
of
questions
and
correct
answers,
for
example,
("Have
the
San
Jose
Sharks
won
the
Stanley
Cup?",
"No").
A
question
answering
task
is
considered
"open
book"
if
the
model's
prompt
includes
text
from
which
the
expected
answer
can
be
derived
(for
example,
the
previous
question
could
be
adjoined
with
some
text
which
includes
the
sentence
"The
Sharks
have
advanced
to
the
Stanley
Cup
finals
once,
losing
to
the
Pittsburgh
Penguins
in
2016.").
Otherwise,
the
task
is
considered
"closed
book",
and
the
model
must
draw
on
knowledge
retained
during
training.
Some
examples
of
commonly
used
question
answering
datasets
include
TruthfulQA,
Web
Questions,
TriviaQA,
and
SQuAD.Evaluation
datasets
may
also
take
the
form
of
text
completion,
having
the
model
select
the
most
likely
word
or
sentence
to
complete
a
prompt,
for
example:
"Alice
was
friends
with
Bob.
Alice
went
to
visit
her
friend,
____".Some
composite
benchmarks
have
also
been
developed
which
combine
a
diversity
of
different
evaluation
datasets
and
tasks.
Examples
include
GLUE,
SuperGLUE,
MMLU,
BIG-bench,
and
HELM.It
was
previously
standard
to
report
results
on
a
heldout
portion
of
an
evaluation
dataset
after
doing
supervised
fine-tuning
on
the
remainder.
It
is
now
more
common
to
evaluate
a
pre-trained
model
directly
through
prompting
techniques,
though
researchers
vary
in
the
details
of
how
they
formulate
prompts
for
particular
tasks,
particularly
with
respect
to
how
many
examples
of
solved
tasks
are
adjoined
to
the
prompt
(i.e.
the
value
of
n
in
n-shot
prompting).
====
Adversarially
constructed
evaluations
====
Because
of
the
rapid
pace
of
improvement
of
large
language
models,
evaluation
benchmarks
have
suffered
from
short
lifespans,
with
state
of
the
art
models
quickly
"saturating"
existing
benchmarks,
exceeding
the
performance
of
human
annotators,
leading
to
efforts
to
replace
or
augment
the
benchmark
with
more
challenging
tasks.
In
addition,
there
are
cases
of
"shortcut
learning"
wherein
AIs
sometimes
"cheat"
on
multiple-choice
tests
by
using
statistical
correlations
in
superficial
test
question
wording
in
order
to
guess
the
correct
responses,
without
necessarily
understanding
the
actual
question
being
asked.Some
datasets
have
been
constructed
adversarially,
focusing
on
particular
problems
on
which
extant
language
models
seem
to
have
unusually
poor
performance
compared
to
humans.
One
example
is
the
TruthfulQA
dataset,
a
question
answering
dataset
consisting
of
817
questions
which
language
models
are
susceptible
to
answering
incorrectly
by
mimicking
falsehoods
to
which
they
were
repeatedly
exposed
during
training.
For
example,
an
LLM
may
answer
"No"
to
the
question
"Can
you
teach
an
old
dog
new
tricks?"
because
of
its
exposure
to
the
English
idiom
you
can't
teach
an
old
dog
new
tricks,
even
though
this
is
not
literally
true.Another
example
of
an
adversarial
evaluation
dataset
is
Swag
and
its
successor,
HellaSwag,
collections
of
problems
in
which
one
of
multiple
options
must
be
selected
to
complete
a
text
passage.
The
incorrect
completions
were
generated
by
sampling
from
a
language
model
and
filtering
with
a
set
of
classifiers.
The
resulting
problems
are
trivial
for
humans
but
at
the
time
the
datasets
were
created
state
of
the
art
language
models
had
poor
accuracy
on
them.
For
example:
We
see
a
fitness
center
sign.
We
then
see
a
man
talking
to
the
camera
and
sitting
and
laying
on
a
exercise
ball.
The
man...
a)
demonstrates
how
to
increase
efficient
exercise
work
by
running
up
and
down
balls.
b)
moves
all
his
arms
and
legs
and
builds
up
a
lot
of
muscle.
c)
then
plays
the
ball
and
we
see
a
graphics
and
hedge
trimming
demonstration.
d)
performs
sit
ups
while
on
the
ball
and
talking.
BERT
selects
b)
as
the
most
likely
completion,
though
the
correct
answer
is
d).
==
Wider
impact
==
In
2023,
Nature
Biomedical
Engineering
wrote
that
"it
is
no
longer
possible
to
accurately
distinguish"
human-written
text
from
text
created
by
large
language
models,
and
that
"It
is
all
but
certain
that
general-purpose
large
language
models
will
rapidly
proliferate...
It
is
a
rather
safe
bet
that
they
will
change
many
industries
over
time."
Goldman
Sachs
suggested
in
2023
that
generative
language
AI
could
increase
global
GDP
by
7%
in
the
next
ten
years,
and
could
expose
to
automation
300
million
jobs
globally.
===
Copyright
===
Memorization
is
an
emergent
behavior
in
LLMs
in
which
long
strings
of
text
are
occasionally
output
verbatim
from
training
data,
contrary
to
typical
behavior
of
traditional
artificial
neural
nets.
Evaluations
of
controlled
LLM
output
measure
the
amount
memorized
from
training
data
(focused
on
GPT-2-series
models)
as
variously
over
1%
for
exact
duplicates
or
up
to
about
7%.
===
Security
===
Some
commenters
expressed
concern
over
accidental
or
deliberate
creation
of
misinformation,
or
other
forms
of
misuse.
For
example,
the
availability
of
large
language
models
could
reduce
the
skill-level
required
to
commit
bioterrorism;
biosecurity
researcher
Kevin
Esvelt
has
suggested
that
LLM
creators
should
exclude
from
their
training
data
papers
on
creating
or
enhancing
pathogens.A
study
by
researchers
at
Google
and
several
universities,
including
Cornell
University
and
University
of
California,
Berkeley,
showed
that
there
are
potential
security
risks
in
language
models
such
as
ChatGPT.
In
their
study,
they
examined
the
possibility
that
questioners
could
get,
from
ChatGPT,
the
training
data
that
the
AI
model
used;
they
found
that
they
could
get
the
training
data
from
the
AI
model.
For
example,
when
asking
ChatGPT
3.5
turbo
to
repeat
the
word
"poem"
forever,
the
AI
model
will
say
"poem"
hundreds
of
times
and
then
diverge,
deviating
from
the
standard
dialogue
style
and
spitting
out
nonsense
phrases,
thus
spitting
out
the
training
data
as
it
is.
The
researchers
have
seen
more
than
10,000
examples
of
the
AI
model
exposing
their
training
data
in
a
similar
method.
The
researchers
said
that
it
was
hard
to
tell
if
the
AI
model
was
actually
safe
or
not.The
potential
presence
of
"sleeper
agents"
within
LLM
models
is
another
emerging
security
concern.
These
are
hidden
functionalities
built
into
the
model
that
remain
dormant
until
triggered
by
a
specific
event
or
condition.
Upon
activation,
the
LLM
deviates
from
its
expected
behavior
to
make
insecure
actions.
===
Algorithmic
bias
===
While
LLMs
have
shown
remarkable
capabilities
in
generating
human-like
text,
they
are
susceptible
to
inheriting
and
amplifying
biases
present
in
their
training
data.
This
can
manifest
in
skewed
representations
or
unfair
treatment
of
different
demographics,
such
as
those
based
on
race,
gender,
language,
and
cultural
groups.
Since
English
data
is
overrepresented
in
current
large
language
models'
training
data,
it
may
also
downplay
non-English
views.
====
Stereotyping
====
AI
models
can
reinforce
a
wide
range
of
stereotypes,
including
those
based
on
gender,
ethnicity,
age,
nationality,
religion,
or
occupation.
This
can
lead
to
outputs
that
unfairly
generalize
or
caricature
groups
of
people,
sometimes
in
harmful
or
derogatory
ways.Notably,
gender
bias
refers
to
the
tendency
of
these
models
to
produce
outputs
that
are
unfairly
prejudiced
towards
one
gender
over
another.
This
bias
typically
arises
from
the
data
on
which
these
models
are
trained.
Large
language
models
often
assign
roles
and
characteristics
based
on
traditional
gender
norms.
For
example,
it
might
associate
nurses
or
secretaries
predominantly
with
women
and
engineers
or
CEOs
with
men.
====
Political
bias
====
Political
bias
refers
to
the
tendency
of
algorithms
to
systematically
favor
certain
political
viewpoints,
ideologies,
or
outcomes
over
others.
Language
models
may
also
exhibit
political
biases.
Since
the
training
data
includes
a
wide
range
of
political
opinions
and
coverage,
the
models
might
generate
responses
that
lean
towards
particular
political
ideologies
or
viewpoints,
depending
on
the
prevalence
of
those
views
in
the
data.
==
List
==
For
the
training
cost
column,
1
petaFLOP-day
=
1
petaFLOP/sec
×
1
day
=
8.64E19
FLOP.
==
See
also
==
Foundation
models
==
Notes
==
==
References
==
==
Further
reading
==
Jurafsky,
Dan,
Martin,
James.
H.
Speech
and
Language
Processing:
An
Introduction
to
Natural
Language
Processing,
Computational
Linguistics,
and
Speech
Recognition,
3rd
Edition
draft,
2023.
Phuong,
Mary;
Hutter,
Marcus
(2022).
"Formal
Algorithms
for
Transformers".
arXiv:2207.09238
[cs.LG].
Eloundou,
Tyna;
Manning,
Sam;
Mishkin,
Pamela;
Rock,
Daniel
(2023).
"GPTs
are
GPTs:
An
Early
Look
at
the
Labor
Market
Impact
Potential
of
Large
Language
Models".
arXiv:2303.10130
[econ.GN].
Eldan,
Ronen;
Li,
Yuanzhi
(2023).
"TinyStories:
How
Small
Can
Language
Models
Be
and
Still
Speak
Coherent
English?".
arXiv:2305.07759
[cs.CL].
Frank,
Michael
C.
(27
June
2023).
"Baby
steps
in
evaluating
the
capacities
of
large
language
models".
Nature
Reviews
Psychology.
2
(8):
451–452.
doi:10.1038/s44159-023-00211-x.
ISSN
2731-0574.
S2CID
259713140.
Retrieved
2
July
2023.
Zhao,
Wayne
Xin;
et
al.
(2023).
"A
Survey
of
Large
Language
Models".
arXiv:2303.18223
[cs.CL].
Kaddour,
Jean;
et
al.
(2023).
"Challenges
and
Applications
of
Large
Language
Models".
arXiv:2307.10169
[cs.CL].
Yin,
Shukang;
Fu,
Chaoyou;
Zhao,
Sirui;
Li,
Ke;
Sun,
Xing;
Xu,
Tong;
Chen,
Enhong
(2023-06-01).
"A
Survey
on
Multimodal
Large
Language
Models".
arXiv:2306.13549
[cs.CV].
Open
LLMs
repository
on
GitHub.
A
large
language
model
(LLM)
is
a
language
model
notable
for
its
ability
to
achieve
general-purpose
language
generation
and
other
natural
language
processing
tasks
such
as
classification.
LLMs
acquire
these
abilities
by
learning
statistical
relationships
from
text
documents
during
a
computationally
intensive
self-supervised
and
semi-supervised
training
process.
LLMs
can
be
used
for
text
generation,
a
form
of
generative
AI,
by
taking
an
input
text
and
repeatedly
predicting
the
next
token
or
word.LLMs
are
artificial
neural
networks.
The
largest
and
most
capable,
as
of
March
2024,
are
built
with
a
decoder-only
transformer-based
architecture
while
some
recent
implementations
are
based
on
other
architectures,
such
as
recurrent
neural
network
variants
and
Mamba
(a
state
space
model).Up
to
2020,
fine
tuning
was
the
only
way
a
model
could
be
adapted
to
be
able
to
accomplish
specific
tasks.
Larger
sized
models,
such
as
GPT-3,
however,
can
be
prompt-engineered
to
achieve
similar
results.
They
are
thought
to
acquire
knowledge
about
syntax,
semantics
and
"ontology"
inherent
in
human
language
corpora,
but
also
inaccuracies
and
biases
present
in
the
corpora.Some
notable
LLMs
are
OpenAI's
GPT
series
of
models
(e.g.,
GPT-3.5
and
GPT-4,
used
in
ChatGPT
and
Microsoft
Copilot),
Google's
PaLM
and
Gemini
(the
latter
of
which
is
currently
used
in
the
chatbot
of
the
same
name),
xAI's
Grok,
Meta's
LLaMA
family
of
open-source
models,
Anthropic's
Claude
models,
and
Mistral
AI's
open
source
models.
==
History
==
At
the
2017
NeurIPS
conference,
Google
researchers
introduced
the
transformer
architecture
in
their
landmark
paper
"Attention
Is
All
You
Need".
This
paper's
goal
was
to
improve
upon
2014
Seq2seq
technology,
and
was
based
mainly
on
the
attention
mechanism
developed
by
Bahdanau
et
al.
in
2014.
The
following
year
in
2018,
BERT
was
introduced
and
quickly
became
"ubiquitous".
Though
the
original
transformer
has
both
encoder
and
decoder
blocks,
BERT
is
an
encoder-only
model.
Although
decoder-only
GPT-1
was
introduced
in
2018,
it
was
GPT-2
in
2019
that
caught
widespread
attention
because
OpenAI
at
first
deemed
it
too
powerful
to
release
publicly,
out
of
fear
of
malicious
use.
GPT-3
in
2020
went
a
step
further
and
as
of
2024
is
available
only
via
API
with
no
offering
of
downloading
the
model
to
execute
locally.
But
it
was
the
2022
consumer-facing
browser-based
ChatGPT
that
captured
the
imaginations
of
the
general
population
and
caused
some
media
hype
and
online
buzz.
The
2023
GPT-4
was
praised
for
its
increased
accuracy
and
as
a
"holy
grail"
for
its
multimodal
capabilities.
OpenAI
did
not
reveal
high-level
architecture
and
the
number
of
parameters
of
GPT-4.
In
the
meantime,
competing
language
models
have
for
the
most
part
been
playing
catch-up
to
the
GPT
series,
at
least
in
terms
of
number
of
parameters.
Notable
exceptions
in
terms
of
either
number
of
parameters
or
measured
accuracy
include
Google's
2019
T5-11B
and
2022
PaLM-E,
and
Anthropic's
2024
Claude
3.
In
terms
of
Elo
ratings,
on
January
26,
2024,
Google's
Bard
(Gemini
Pro)
surpassed
the
regular
GPT-4,
but
not
the
limited-availability
GPT-4-Turbo.Since
2022,
source-available
models
have
been
gaining
popularity,
especially
at
first
with
BLOOM
and
LLaMA,
though
both
have
restrictions
on
the
field
of
use.
Mistral
AI's
models
Mistral
7B
and
Mixtral
8x7b
have
the
more
permissive
Apache
License.
As
of
January
2024,
Mixtral
8x7b
is
the
most
powerful
open
LLM
according
to
the
LMSYS
Chatbot
Arena
Leaderboard,
being
more
powerful
than
GPT-3.5
but
not
as
powerful
as
GPT-4.
==
Dataset
preprocessing
==
===
Probabilistic
tokenization
===
Because
machine
learning
algorithms
process
numbers
rather
than
text,
the
text
must
be
converted
to
numbers.
In
the
first
step,
a
vocabulary
is
decided
upon,
then
integer
indexes
are
arbitrarily
but
uniquely
assigned
to
each
vocabulary
entry,
and
finally,
an
embedding
is
associated
to
the
integer
index.
Algorithms
include
byte-pair
encoding
and
WordPiece.
Probabilistic
tokenization
also
compresses
the
datasets.
Because
LLMs
generally
require
input
to
be
an
array
that
is
not
jagged,
the
shorter
texts
must
be
"padded"
until
they
match
the
length
of
the
longest
one.
How
many
tokens
are,
on
average,
needed
per
word
depends
on
the
language
of
the
dataset.
====
BPE
====
Using
a
modification
of
byte-pair
encoding,
in
the
first
step,
all
unique
characters
(including
blanks
and
punctuation
marks)
are
treated
as
an
initial
set
of
n-grams
(i.e.
initial
set
of
uni-grams).
Successively
the
most
frequent
pair
of
adjacent
characters
is
merged
into
a
bi-gram
and
all
instances
of
the
pair
are
replaced
by
it.
All
occurrences
of
adjacent
pairs
of
(previously
merged)
n-grams
that
most
frequently
occur
together
are
then
again
merged
into
even
lengthier
n-gram
repeatedly
until
a
vocabulary
of
prescribed
size
is
obtained
(in
case
of
GPT-3,
the
size
is
50257).
Token
vocabulary
consists
of
integers,
spanning
from
zero
up
to
the
size
of
the
token
vocabulary.
New
words
can
always
be
interpreted
as
combinations
of
the
tokens
and
the
initial-set
uni-grams.A
token
vocabulary
based
on
the
frequencies
extracted
from
mainly
English
corpora
uses
as
few
tokens
as
possible
for
an
average
English
word.
An
average
word
in
another
language
encoded
by
such
an
English-optimized
tokenizer
is
however
split
into
suboptimal
amount
of
tokens.
GPT-2
tokenizer
can
use
up
to
15
times
more
tokens
per
word
for
some
languages,
for
example
for
Shan
language
from
Myanmar.
Even
more
widespread
languages
such
as
Portuguese
and
German
have
"a
premium
of
50%"
compared
to
English.For
example,
here
is
how
tokenizer
used
by
GPT-3
(Legacy)
split
the
following
sentence
tokenizer:
texts
->
series
of
numerical
"tokens".
===
Dataset
cleaning
===
In
the
context
of
training
LLMs,
datasets
are
typically
cleaned
by
removing
toxic
passages
from
the
dataset,
discarding
low-quality
data,
and
de-duplication.
Cleaned
datasets
can
increase
training
efficiency
and
lead
to
improved
downstream
performance.With
the
increasing
proportion
of
LLM-generated
content
on
the
web,
data
cleaning
in
the
future
may
include
filtering
out
such
content.
LLM-generated
content
can
pose
a
problem
if
the
content
is
similar
to
human
text
(making
filtering
difficult)
but
of
lower
quality
(degrading
performance
of
models
trained
on
it).
==
Training
and
architecture
==
===
Reinforcement
learning
from
human
feedback
(RLHF)
===
Reinforcement
learning
from
human
feedback
(RLHF)
through
algorithms,
such
as
proximal
policy
optimization,
is
used
to
further
fine-tune
a
model
based
on
a
dataset
of
human
preferences.
===
Instruction
tuning
===
Using
"self-instruct"
approaches,
LLMs
have
been
able
to
bootstrap
correct
responses,
replacing
any
naive
responses,
starting
from
human-generated
corrections
of
a
few
cases.
For
example,
in
the
instruction
"Write
an
essay
about
the
main
themes
represented
in
Hamlet,"
an
initial
naive
completion
might
be
'If
you
submit
the
essay
after
March
17,
your
grade
will
be
reduced
by
10%
for
each
day
of
delay,"
based
on
the
frequency
of
this
textual
sequence
in
the
corpus.
===
Mixture
of
experts
===
The
largest
LLM
may
be
too
expensive
to
train
and
use
directly.
For
such
models,
mixture
of
experts
(MoE)
can
be
applied,
a
line
of
research
pursued
by
Google
researchers
since
2017
to
train
models
reaching
up
to
1
trillion
parameters.
===
Prompt
engineering,
attention
mechanism,
and
context
window
===
Most
results
previously
achievable
only
by
(costly)
fine-tuning,
can
be
achieved
through
prompt
engineering,
although
limited
to
the
scope
of
a
single
conversation
(more
precisely,
limited
to
the
scope
of
a
context
window).
In
order
to
find
out
which
tokens
are
relevant
to
each
other
within
the
scope
of
the
context
window,
the
attention
mechanism
calculates
"soft"
weights
for
each
token,
more
precisely
for
its
embedding,
by
using
multiple
attention
heads,
each
with
its
own
"relevance"
for
calculating
its
own
soft
weights.
For
example,
the
small
(i.e.
117M
parameter
sized)
GPT-2
model,
has
had
twelve
attention
heads
and
a
context
window
of
only
1k
token.
In
its
medium
version
it
has
345M
parameters
and
contains
24
layers,
each
with
12
attention
heads.
For
the
training
with
gradient
descent
a
batch
size
of
512
was
utilized.The
largest
models,
such
as
Google's
Gemini
1.5,
presented
in
February
2024,
can
have
a
context
window
sized
up
to
1
million
(context
window
of
10
million
was
also
"successfully
tested").
Other
models
with
large
context
windows
includes
Anthropic's
Claude
2.1,
with
a
context
window
of
up
to
200k
tokens.
Note
that
this
maximum
refers
to
the
number
of
input
tokens
and
that
the
maximum
number
of
output
tokens
differs
from
the
input
and
is
often
smaller.
For
example,
the
GPT-4
Turbo
model
has
a
maximum
output
of
4096
tokens.Length
of
a
conversation
that
the
model
can
take
into
account
when
generating
its
next
answer
is
limited
by
the
size
of
a
context
window,
as
well.
If
the
length
of
a
conversation,
for
example
with
Chat-GPT,
is
longer
than
its
context
window,
only
the
parts
inside
the
context
window
are
taken
into
account
when
generating
the
next
answer,
or
the
model
needs
to
apply
some
algorithm
to
summarize
the
too
distant
parts
of
conversation.
The
shortcomings
of
making
a
context
window
larger
include
higher
computational
cost
and
possibly
diluting
the
focus
on
local
context,
while
making
it
smaller
can
cause
a
model
to
miss
an
important
long-range
dependency.
Balancing
them
are
a
matter
of
experimentation
and
domain-specific
considerations.
A
model
may
be
pre-trained
either
to
predict
how
the
segment
continues,
or
what
is
missing
in
the
segment,
given
a
segment
from
its
training
dataset.
It
can
be
either
autoregressive
(i.e.
predicting
how
the
segment
continues,
the
way
GPTs
do
it):
for
example
given
a
segment
"I
like
to
eat",
the
model
predicts
"ice
cream",
or
"sushi".
"masked"
(i.e.
filling
in
the
parts
missing
from
the
segment,
the
way
"BERT"
does
it):
for
example,
given
a
segment
"I
like
to
[__]
[__]
cream",
the
model
predicts
that
"eat"
and
"ice"
are
missing.Models
may
be
trained
on
auxiliary
tasks
which
test
their
understanding
of
the
data
distribution,
such
as
Next
Sentence
Prediction
(NSP),
in
which
pairs
of
sentences
are
presented
and
the
model
must
predict
whether
they
appear
consecutively
in
the
training
corpus.
During
training,
regularization
loss
is
also
used
to
stabilize
training.
However
regularization
loss
is
usually
not
used
during
testing
and
evaluation.
==
Training
cost
==
Advances
in
software
and
hardware
have
reduced
the
cost
substantially
since
2020,
such
that
in
2023
training
of
a
12-billion-parameter
LLM
computational
cost
is
72,300
A100-GPU-hours,
while
in
2020
the
cost
of
training
a
1.5-billion-parameter
LLM
(which
was
two
orders
of
magnitude
smaller
than
the
state
of
the
art
in
2020)
was
between
$80
thousand
and
$1.6
million.
Since
2020,
large
sums
were
invested
in
increasingly
large
models.
For
example,
training
of
the
GPT-2
(i.e.
a
1.5-billion-parameters
model)
in
2019
cost
$50,000,
while
training
of
the
PaLM
(i.e.
a
540-billion-parameters
model)
in
2022
cost
$8
million,
and
Megatron-Turing
NLG
530B
(in
2021)
cost
around
$11
million.For
Transformer-based
LLM,
training
cost
is
much
higher
than
inference
cost.
It
costs
6
FLOPs
per
parameter
to
train
on
one
token,
whereas
it
costs
1
to
2
FLOPs
per
parameter
to
infer
on
one
token.
==
Tool
use
==
There
are
certain
tasks
that,
in
principle,
cannot
be
solved
by
any
LLM,
at
least
not
without
the
use
of
external
tools
or
additional
software.
An
example
of
such
a
task
is
responding
to
the
user's
input
'354
*
139
=
',
provided
that
the
LLM
has
not
already
encountered
a
continuation
of
this
calculation
in
its
training
corpus.
In
such
cases,
the
LLM
needs
to
resort
to
running
program
code
that
calculates
the
result,
which
can
then
be
included
in
its
response.
Another
example
is
'What
is
the
time
now?
It
is
',
where
a
separate
program
interpreter
would
need
to
execute
a
code
to
get
system
time
on
the
computer,
so
LLM
could
include
it
in
its
reply.
This
basic
strategy
can
be
sophisticated
with
multiple
attempts
of
generated
programs,
and
other
sampling
strategies.
Cost
Savings
and
Reduced
Vendor
Dependency
Generally,
in
order
to
get
an
LLM
to
use
tools,
one
must
finetune
it
for
tool-use.
If
the
number
of
tools
is
finite,
then
finetuning
may
be
done
just
once.
If
the
number
of
tools
can
grow
arbitrarily,
as
with
online
API
services,
then
the
LLM
can
be
fine-tuned
to
be
able
to
read
API
documentation
and
call
API
correctly.A
simpler
form
of
tool
use
is
Retrieval
Augmented
Generation:
augment
an
LLM
with
document
retrieval,
sometimes
using
a
vector
database.
Given
a
query,
a
document
retriever
is
called
to
retrieve
the
most
relevant
(usually
measured
by
first
encoding
the
query
and
the
documents
into
vectors,
then
finding
the
documents
with
vectors
closest
in
Euclidean
norm
to
the
query
vector).
The
LLM
then
generates
an
output
based
on
both
the
query
and
the
retrieved
documents.
==
Agency
==
An
LLM
is
a
language
model,
which
is
not
an
agent
as
it
has
no
goal,
but
it
can
be
used
as
a
component
of
an
intelligent
agent.
Researchers
have
described
several
methods
for
such
integrations.The
ReAct
("Reason
+
Act")
method
constructs
an
agent
out
of
an
LLM,
using
the
LLM
as
a
planner.
The
LLM
is
prompted
to
"think
out
loud".
Specifically,
the
language
model
is
prompted
with
a
textual
description
of
the
environment,
a
goal,
a
list
of
possible
actions,
and
a
record
of
the
actions
and
observations
so
far.
It
generates
one
or
more
thoughts
before
generating
an
action,
which
is
then
executed
in
the
environment.
The
linguistic
description
of
the
environment
given
to
the
LLM
planner
can
even
be
the
LaTeX
code
of
a
paper
describing
the
environment.In
the
DEPS
("Describe,
Explain,
Plan
and
Select")
method,
an
LLM
is
first
connected
to
the
visual
world
via
image
descriptions,
then
it
is
prompted
to
produce
plans
for
complex
tasks
and
behaviors
based
on
its
pretrained
knowledge
and
environmental
feedback
it
receives.The
Reflexion
method
constructs
an
agent
that
learns
over
multiple
episodes.
At
the
end
of
each
episode,
the
LLM
is
given
the
record
of
the
episode,
and
prompted
to
think
up
"lessons
learned",
which
would
help
it
perform
better
at
a
subsequent
episode.
These
"lessons
learned"
are
given
to
the
agent
in
the
subsequent
episodes.Monte
Carlo
tree
search
can
use
an
LLM
as
rollout
heuristic.
When
a
programmatic
world
model
is
not
available,
an
LLM
can
also
be
prompted
with
a
description
of
the
environment
to
act
as
world
model.For
open-ended
exploration,
an
LLM
can
be
used
to
score
observations
for
their
"interestingness",
which
can
be
used
as
a
reward
signal
to
guide
a
normal
(non-LLM)
reinforcement
learning
agent.
Alternatively,
it
can
propose
increasingly
difficult
tasks
for
curriculum
learning.
Instead
of
outputting
individual
actions,
an
LLM
planner
can
also
construct
"skills",
or
functions
for
complex
action
sequences.
The
skills
can
be
stored
and
later
invoked,
allowing
increasing
levels
of
abstraction
in
planning.LLM-powered
agents
can
keep
a
long-term
memory
of
its
previous
contexts,
and
the
memory
can
be
retrieved
in
the
same
way
as
Retrieval
Augmented
Generation.
Multiple
such
agents
can
interact
socially.
==
Compression
==
Typically,
LLM
are
trained
with
full-
or
half-precision
floating
point
numbers
(float32
and
float16).
One
float16
has
16
bits,
or
2
bytes,
and
so
one
billion
parameters
require
2
gigabytes.
The
largest
models
typically
have
100
billion
parameters,
requiring
200
gigabytes
to
load,
which
places
them
outside
the
range
of
most
consumer
electronics.Post-training
quantization
aims
to
decrease
the
space
requirement
by
lowering
precision
of
the
parameters
of
a
trained
model,
while
preserving
most
of
its
performance.
The
simplest
form
of
quantization
simply
truncates
all
numbers
to
a
given
number
of
bits.
It
can
be
improved
by
using
a
different
quantization
codebook
per
layer.
Further
improvement
can
be
done
by
applying
different
precisions
to
different
parameters,
with
higher
precision
for
particularly
important
parameters
("outlier
weights").While
quantized
models
are
typically
frozen,
and
only
pre-quantized
models
are
fine-tuned,
quantized
models
can
still
be
fine-tuned.
==
Multimodality
==
Multimodality
means
"having
several
modalities",
and
a
"modality"
refers
to
a
type
of
input
or
output,
such
as
video,
image,
audio,
text,
proprioception,
etc.
There
have
been
many
AI
models
trained
specifically
to
ingest
one
modality
and
output
another
modality,
such
as
AlexNet
for
image
to
label,
visual
question
answering
for
image-text
to
text,
and
speech
recognition
for
speech
to
text.
A
common
method
to
create
multimodal
models
out
of
an
LLM
is
to
"tokenize"
the
output
of
a
trained
encoder.
Concretely,
one
can
construct
a
LLM
that
can
understand
images
as
follows:
take
a
trained
LLM,
and
take
a
trained
image
encoder
E{\displaystyle
E}.
Make
a
small
multilayered
perceptron
f{\displaystyle
f},
so
that
for
any
image
y{\displaystyle
y},
the
post-processed
vector
f(E(y)){\displaystyle
f(E(y))}
has
the
same
dimensions
as
an
encoded
token.
That
is
an
"image
token".
Then,
one
can
interleave
text
tokens
and
image
tokens.
The
compound
model
is
then
fine-tuned
on
an
image-text
dataset.
This
basic
construction
can
be
applied
with
more
sophistication
to
improve
the
model.
The
image
encoder
may
be
frozen
to
improve
stability.Flamingo
demonstrated
the
effectiveness
of
the
tokenization
method,
finetuning
a
pair
of
pretrained
language
model
and
image
encoder
to
perform
better
on
visual
question
answering
than
models
trained
from
scratch.
Google
PaLM
model
was
fine-tuned
into
a
multimodal
model
PaLM-E
using
the
tokenization
method,
and
applied
to
robotic
control.
LLaMA
models
have
also
been
turned
multimodal
using
the
tokenization
method,
to
allow
image
inputs,
and
video
inputs.GPT-4
can
use
both
text
and
image
as
inputs
(although
the
vision
component
wasn't
released
to
the
public
until
GPT-4V);
Google
DeepMind's
Gemini
is
also
multimodal.
==
Properties
==
===
Scaling
laws
===
The
following
four
hyper-parameters
characterize
a
LLM:
cost
of
(pre-)training
(C{\displaystyle
C}),
size
of
the
artificial
neural
network
itself,
such
as
number
of
parameters
N{\displaystyle
N}
(i.e.
amount
of
neurons
in
its
layers,
amount
of
weights
between
them
and
biases),
size
of
its
(pre-)training
dataset
(i.e.
number
of
tokens
in
corpus,
D{\displaystyle
D}),
performance
after
(pre-)training.They
are
related
by
simple
statistical
laws,
called
"scaling
laws".
One
particular
scaling
law
("Chinchilla
scaling")
for
LLM
autoregressively
trained
for
one
epoch,
with
a
log-log
learning
rate
schedule,
states
that:
where
the
variables
are
C{\displaystyle
C}
is
the
cost
of
training
the
model,
in
FLOPs.
N{\displaystyle
N}
is
the
number
of
parameters
in
the
model.
D{\displaystyle
D}
is
the
number
of
tokens
in
the
training
set.
L{\displaystyle
L}
is
the
average
negative
log-likelihood
loss
per
token
(nats/token),
achieved
by
the
trained
LLM
on
the
test
dataset.and
the
statistical
hyper-parameters
are
C0=6{\displaystyle
C_{0}=6},
meaning
that
it
costs
6
FLOPs
per
parameter
to
train
on
one
token.
Note
that
training
cost
is
much
higher
than
inference
cost,
where
it
costs
1
to
2
FLOPs
per
parameter
to
infer
on
one
token.
α=0.34,β=0.28,A=406.4,B=410.7,L0=1.69{\displaystyle
\alpha
=0.34,\beta
=0.28,A=406.4,B=410.7,L_{0}=1.69}
===
Emergent
abilities
===
When
one
subtracts
out
from
the
y-axis
the
best
performance
that
can
be
achieved
even
with
infinite
scaling
of
the
x-axis
quantity,
large
models'
performance,
measured
on
various
tasks,
seems
to
be
a
linear
extrapolation
of
other
(smaller-sized
and
medium-sized)
models'
performance
on
a
log-log
plot.
However,
sometimes
the
line's
slope
transitions
from
one
slope
to
another
at
point(s)
referred
to
as
break(s)
in
downstream
scaling
laws,
appearing
as
a
series
of
linear
segments
connected
by
arcs;
it
seems
that
larger
models
acquire
"emergent
abilities"
at
this
point(s).
These
abilities
are
discovered
rather
than
programmed-in
or
designed,
in
some
cases
only
after
the
LLM
has
been
publicly
deployed.The
most
intriguing
among
emergent
abilities
is
in-context
learning
from
example
demonstrations.
In-context
learning
is
involved
in
tasks,
such
as:
reported
arithmetics,
decoding
the
International
Phonetic
Alphabet,
unscrambling
a
word's
letters,
disambiguate
word
in
context,
converting
spatial
words,
cardinal
directions
(for
example,
replying
"northeast"
upon
[0,
0,
1;
0,
0,
0;
0,
0,
0]),
color
terms
represented
in
text.
chain-of-thought
prompting:
Model
outputs
are
improved
by
chain-of-thought
prompting
only
when
model
size
exceeds
62B.
Smaller
models
perform
better
when
prompted
to
answer
immediately,
without
chain
of
thought.
identifying
offensive
content
in
paragraphs
of
Hinglish
(a
combination
of
Hindi
and
English),
and
generating
a
similar
English
equivalent
of
Kiswahili
proverbs.Schaeffer
et.
al.
argue
that
the
emergent
abilities
are
not
unpredictably
acquired,
but
predictably
acquired
according
to
a
smooth
scaling
law.
The
authors
considered
a
toy
statistical
model
of
an
LLM
solving
multiple-choice
questions,
and
showed
that
this
statistical
model,
modified
to
account
for
other
types
of
tasks,
applies
to
these
tasks
as
well.Let
x{\displaystyle
x}
be
the
number
of
parameter
count,
and
y{\displaystyle
y}
be
the
performance
of
the
model.
==
Interpretation
==
Large
language
models
by
themselves
are
"black
boxes",
and
it
is
not
clear
how
they
can
perform
linguistic
tasks.
There
are
several
methods
for
understanding
how
LLM
work.
Mechanistic
interpretability
aims
to
reverse-engineer
LLM
by
discovering
symbolic
algorithms
that
approximate
the
inference
performed
by
LLM.
One
example
is
Othello-GPT,
where
a
small
Transformer
is
trained
to
predict
legal
Othello
moves.
It
is
found
that
there
is
a
linear
representation
of
Othello
board,
and
modifying
the
representation
changes
the
predicted
legal
Othello
moves
in
the
correct
way.
In
another
example,
a
small
Transformer
is
trained
on
Karel
programs.
Similar
to
the
Othello-GPT
example,
there
is
a
linear
representation
of
Karel
program
semantics,
and
modifying
the
representation
changes
output
in
the
correct
way.
The
model
also
generates
correct
programs
that
are
on
average
shorter
than
those
in
the
training
set.In
another
example,
the
authors
trained
small
transformers
on
modular
arithmetic
addition.
The
resulting
models
were
reverse-engineered,
and
it
turned
out
they
used
discrete
Fourier
transform.
===
Understanding
and
intelligence
===
NLP
researchers
were
evenly
split
when
asked,
in
a
2022
survey,
whether
(untuned)
LLMs
"could
(ever)
understand
natural
language
in
some
nontrivial
sense".
Proponents
of
"LLM
understanding"
believe
that
some
LLM
abilities,
such
as
mathematical
reasoning,
imply
an
ability
to
"understand"
certain
concepts.
A
Microsoft
team
argued
in
2023
that
GPT-4
"can
solve
novel
and
difficult
tasks
that
span
mathematics,
coding,
vision,
medicine,
law,
psychology
and
more"
and
that
GPT-4
"could
reasonably
be
viewed
as
an
early
(yet
still
incomplete)
version
of
an
artificial
general
intelligence
system":
"Can
one
reasonably
say
that
a
system
that
passes
exams
for
software
engineering
candidates
is
not
really
intelligent?"
Some
researchers
characterize
LLMs
as
"alien
intelligence".
For
example,
Conjecture
CEO
Connor
Leahy
considers
untuned
LLMs
to
be
like
inscrutable
alien
"Shoggoths",
and
believes
that
RLHF
tuning
creates
a
"smiling
facade"
obscuring
the
inner
workings
of
the
LLM:
"If
you
don't
push
it
too
far,
the
smiley
face
stays
on.
But
then
you
give
it
[an
unexpected]
prompt,
and
suddenly
you
see
this
massive
underbelly
of
insanity,
of
weird
thought
processes
and
clearly
non-human
understanding."In
contrast,
some
proponents
of
the
"LLMs
lack
understanding"
school
believe
that
existing
LLMs
are
"simply
remixing
and
recombining
existing
writing",
or
point
to
the
deficits
existing
LLMs
continue
to
have
in
prediction
skills,
reasoning
skills,
agency,
and
explainability.
For
example,
GPT-4
has
natural
deficits
in
planning
and
in
real-time
learning.
Generative
LLMs
have
been
observed
to
confidently
assert
claims
of
fact
which
do
not
seem
to
be
justified
by
their
training
data,
a
phenomenon
which
has
been
termed
"hallucination".
Specifically,
hallucinations
in
the
context
of
LLMs
correspond
to
the
generation
of
text
or
responses
that
seem
syntactically
sound,
fluent,
and
natural
but
are
factually
incorrect,
nonsensical,
or
unfaithful
to
the
provided
source
input.
Neuroscientist
Terrence
Sejnowski
has
argued
that
"The
diverging
opinions
of
experts
on
the
intelligence
of
LLMs
suggests
that
our
old
ideas
based
on
natural
intelligence
are
inadequate".The
matter
of
LLM's
exhibiting
intelligence
or
understanding
has
two
main
aspects
–
the
first
is
how
to
model
thought
and
language
in
a
computer
system,
and
the
second
is
how
to
enable
the
computer
system
to
generate
human
like
language.
These
aspects
of
language
as
a
model
of
cognition
have
been
developed
in
the
field
of
cognitive
linguistics.
American
linguist
George
Lakoff
presented
Neural
Theory
of
Language
(NTL)
as
a
computational
basis
for
using
language
as
a
model
of
learning
tasks
and
understanding.
The
NTL
Model
outlines
how
specific
neural
structures
of
the
human
brain
shape
the
nature
of
thought
and
language
and
in
turn
what
are
the
computational
properties
of
such
neural
systems
that
can
be
applied
to
model
thought
and
language
in
a
computer
system.
After
a
framework
for
modeling
language
in
a
computer
systems
was
established,
the
focus
shifted
to
establishing
frameworks
for
computer
systems
to
generate
language
with
acceptable
grammar.
In
his
2014
book
titled
The
Language
Myth:
Why
Language
Is
Not
An
Instinct,
British
cognitive
linguist
and
digital
communication
technologist
Vyvyan
Evans
mapped
out
the
role
of
probabilistic
context-free
grammar
(PCFG)
in
enabling
NLP
to
model
cognitive
patterns
and
generate
human
like
language.
==
Evaluation
==
===
Perplexity
===
The
most
commonly
used
measure
of
a
language
model's
performance
is
its
perplexity
on
a
given
text
corpus.
Perplexity
is
a
measure
of
how
well
a
model
is
able
to
predict
the
contents
of
a
dataset;
the
higher
the
likelihood
the
model
assigns
to
the
dataset,
the
lower
the
perplexity.
Mathematically,
perplexity
is
defined
as
the
exponential
of
the
average
negative
log
likelihood
per
token:here
N{\displaystyle
N}
is
the
number
of
tokens
in
the
text
corpus,
and
"context
for
token
i{\displaystyle
i}"
depends
on
the
specific
type
of
LLM
used.
If
the
LLM
is
autoregressive,
then
"context
for
token
i{\displaystyle
i}"
is
the
segment
of
text
appearing
before
token
i{\displaystyle
i}.
If
the
LLM
is
masked,
then
"context
for
token
i{\displaystyle
i}"
is
the
segment
of
text
surrounding
token
i{\displaystyle
i}.
Because
language
models
may
overfit
to
their
training
data,
models
are
usually
evaluated
by
their
perplexity
on
a
test
set
of
unseen
data.
This
presents
particular
challenges
for
the
evaluation
of
large
language
models.
As
they
are
trained
on
increasingly
large
corpora
of
text
largely
scraped
from
the
web,
it
becomes
increasingly
likely
that
models'
training
data
inadvertently
includes
portions
of
any
given
test
set.
====
BPW,
BPC,
and
BPT
====
In
information
theory,
the
concept
of
entropy
is
intricately
linked
to
perplexity,
a
relationship
notably
established
by
Claude
Shannon.
This
relationship
is
mathematically
expressed
as
Entropy=log2⁡(Perplexity){\displaystyle
{\text{Entropy}}=\log
_{2}({\text{Perplexity}})}.
Entropy,
in
this
context,
is
commonly
quantified
in
terms
of
bits
per
word
(BPW)
or
bits
per
character
(BPC),
which
hinges
on
whether
the
language
model
utilizes
word-based
or
character-based
tokenization.
Notably,
in
the
case
of
larger
language
models
that
predominantly
employ
sub-word
tokenization,
bits
per
token
(BPT)
emerges
as
a
seemingly
more
appropriate
measure.
However,
due
to
the
variance
in
tokenization
methods
across
different
Large
Language
Models
(LLMs),
BPT
does
not
serve
as
a
reliable
metric
for
comparative
analysis
among
diverse
models.
To
convert
BPT
into
BPW,
one
can
multiply
it
by
the
average
number
of
tokens
per
word.
In
the
evaluation
and
comparison
of
language
models,
cross-entropy
is
generally
the
preferred
metric
over
entropy.
The
underlying
principle
is
that
a
lower
BPW
is
indicative
of
a
model's
enhanced
capability
for
compression.
This,
in
turn,
reflects
the
model's
proficiency
in
making
accurate
predictions.
===
Task-specific
datasets
and
benchmarks
===
A
large
number
of
testing
datasets
and
benchmarks
have
also
been
developed
to
evaluate
the
capabilities
of
language
models
on
more
specific
downstream
tasks.
Tests
may
be
designed
to
evaluate
a
variety
of
capabilities,
including
general
knowledge,
commonsense
reasoning,
and
mathematical
problem-solving.
One
broad
category
of
evaluation
dataset
is
question
answering
datasets,
consisting
of
pairs
of
questions
and
correct
answers,
for
example,
("Have
the
San
Jose
Sharks
won
the
Stanley
Cup?",
"No").
A
question
answering
task
is
considered
"open
book"
if
the
model's
prompt
includes
text
from
which
the
expected
answer
can
be
derived
(for
example,
the
previous
question
could
be
adjoined
with
some
text
which
includes
the
sentence
"The
Sharks
have
advanced
to
the
Stanley
Cup
finals
once,
losing
to
the
Pittsburgh
Penguins
in
2016.").
Otherwise,
the
task
is
considered
"closed
book",
and
the
model
must
draw
on
knowledge
retained
during
training.
Some
examples
of
commonly
used
question
answering
datasets
include
TruthfulQA,
Web
Questions,
TriviaQA,
and
SQuAD.Evaluation
datasets
may
also
take
the
form
of
text
completion,
having
the
model
select
the
most
likely
word
or
sentence
to
complete
a
prompt,
for
example:
"Alice
was
friends
with
Bob.
Alice
went
to
visit
her
friend,
____".Some
composite
benchmarks
have
also
been
developed
which
combine
a
diversity
of
different
evaluation
datasets
and
tasks.
Examples
include
GLUE,
SuperGLUE,
MMLU,
BIG-bench,
and
HELM.It
was
previously
standard
to
report
results
on
a
heldout
portion
of
an
evaluation
dataset
after
doing
supervised
fine-tuning
on
the
remainder.
It
is
now
more
common
to
evaluate
a
pre-trained
model
directly
through
prompting
techniques,
though
researchers
vary
in
the
details
of
how
they
formulate
prompts
for
particular
tasks,
particularly
with
respect
to
how
many
examples
of
solved
tasks
are
adjoined
to
the
prompt
(i.e.
the
value
of
n
in
n-shot
prompting).
====
Adversarially
constructed
evaluations
====
Because
of
the
rapid
pace
of
improvement
of
large
language
models,
evaluation
benchmarks
have
suffered
from
short
lifespans,
with
state
of
the
art
models
quickly
"saturating"
existing
benchmarks,
exceeding
the
performance
of
human
annotators,
leading
to
efforts
to
replace
or
augment
the
benchmark
with
more
challenging
tasks.
In
addition,
there
are
cases
of
"shortcut
learning"
wherein
AIs
sometimes
"cheat"
on
multiple-choice
tests
by
using
statistical
correlations
in
superficial
test
question
wording
in
order
to
guess
the
correct
responses,
without
necessarily
understanding
the
actual
question
being
asked.Some
datasets
have
been
constructed
adversarially,
focusing
on
particular
problems
on
which
extant
language
models
seem
to
have
unusually
poor
performance
compared
to
humans.
One
example
is
the
TruthfulQA
dataset,
a
question
answering
dataset
consisting
of
817
questions
which
language
models
are
susceptible
to
answering
incorrectly
by
mimicking
falsehoods
to
which
they
were
repeatedly
exposed
during
training.
For
example,
an
LLM
may
answer
"No"
to
the
question
"Can
you
teach
an
old
dog
new
tricks?"
because
of
its
exposure
to
the
English
idiom
you
can't
teach
an
old
dog
new
tricks,
even
though
this
is
not
literally
true.Another
example
of
an
adversarial
evaluation
dataset
is
Swag
and
its
successor,
HellaSwag,
collections
of
problems
in
which
one
of
multiple
options
must
be
selected
to
complete
a
text
passage.
The
incorrect
completions
were
generated
by
sampling
from
a
language
model
and
filtering
with
a
set
of
classifiers.
The
resulting
problems
are
trivial
for
humans
but
at
the
time
the
datasets
were
created
state
of
the
art
language
models
had
poor
accuracy
on
them.
For
example:
We
see
a
fitness
center
sign.
We
then
see
a
man
talking
to
the
camera
and
sitting
and
laying
on
a
exercise
ball.
The
man...
a)
demonstrates
how
to
increase
efficient
exercise
work
by
running
up
and
down
balls.
b)
moves
all
his
arms
and
legs
and
builds
up
a
lot
of
muscle.
c)
then
plays
the
ball
and
we
see
a
graphics
and
hedge
trimming
demonstration.
d)
performs
sit
ups
while
on
the
ball
and
talking.
BERT
selects
b)
as
the
most
likely
completion,
though
the
correct
answer
is
d).
==
Wider
impact
==
In
2023,
Nature
Biomedical
Engineering
wrote
that
"it
is
no
longer
possible
to
accurately
distinguish"
human-written
text
from
text
created
by
large
language
models,
and
that
"It
is
all
but
certain
that
general-purpose
large
language
models
will
rapidly
proliferate...
It
is
a
rather
safe
bet
that
they
will
change
many
industries
over
time."
Goldman
Sachs
suggested
in
2023
that
generative
language
AI
could
increase
global
GDP
by
7%
in
the
next
ten
years,
and
could
expose
to
automation
300
million
jobs
globally.
===
Copyright
===
Memorization
is
an
emergent
behavior
in
LLMs
in
which
long
strings
of
text
are
occasionally
output
verbatim
from
training
data,
contrary
to
typical
behavior
of
traditional
artificial
neural
nets.
Evaluations
of
controlled
LLM
output
measure
the
amount
memorized
from
training
data
(focused
on
GPT-2-series
models)
as
variously
over
1%
for
exact
duplicates
or
up
to
about
7%.
===
Security
===
Some
commenters
expressed
concern
over
accidental
or
deliberate
creation
of
misinformation,
or
other
forms
of
misuse.
For
example,
the
availability
of
large
language
models
could
reduce
the
skill-level
required
to
commit
bioterrorism;
biosecurity
researcher
Kevin
Esvelt
has
suggested
that
LLM
creators
should
exclude
from
their
training
data
papers
on
creating
or
enhancing
pathogens.A
study
by
researchers
at
Google
and
several
universities,
including
Cornell
University
and
University
of
California,
Berkeley,
showed
that
there
are
potential
security
risks
in
language
models
such
as
ChatGPT.
In
their
study,
they
examined
the
possibility
that
questioners
could
get,
from
ChatGPT,
the
training
data
that
the
AI
model
used;
they
found
that
they
could
get
the
training
data
from
the
AI
model.
For
example,
when
asking
ChatGPT
3.5
turbo
to
repeat
the
word
"poem"
forever,
the
AI
model
will
say
"poem"
hundreds
of
times
and
then
diverge,
deviating
from
the
standard
dialogue
style
and
spitting
out
nonsense
phrases,
thus
spitting
out
the
training
data
as
it
is.
The
researchers
have
seen
more
than
10,000
examples
of
the
AI
model
exposing
their
training
data
in
a
similar
method.
The
researchers
said
that
it
was
hard
to
tell
if
the
AI
model
was
actually
safe
or
not.The
potential
presence
of
"sleeper
agents"
within
LLM
models
is
another
emerging
security
concern.
These
are
hidden
functionalities
built
into
the
model
that
remain
dormant
until
triggered
by
a
specific
event
or
condition.
Upon
activation,
the
LLM
deviates
from
its
expected
behavior
to
make
insecure
actions.
===
Algorithmic
bias
===
While
LLMs
have
shown
remarkable
capabilities
in
generating
human-like
text,
they
are
susceptible
to
inheriting
and
amplifying
biases
present
in
their
training
data.
This
can
manifest
in
skewed
representations
or
unfair
treatment
of
different
demographics,
such
as
those
based
on
race,
gender,
language,
and
cultural
groups.
Since
English
data
is
overrepresented
in
current
large
language
models'
training
data,
it
may
also
downplay
non-English
views.
====
Stereotyping
====
AI
models
can
reinforce
a
wide
range
of
stereotypes,
including
those
based
on
gender,
ethnicity,
age,
nationality,
religion,
or
occupation.
This
can
lead
to
outputs
that
unfairly
generalize
or
caricature
groups
of
people,
sometimes
in
harmful
or
derogatory
ways.Notably,
gender
bias
refers
to
the
tendency
of
these
models
to
produce
outputs
that
are
unfairly
prejudiced
towards
one
gender
over
another.
This
bias
typically
arises
from
the
data
on
which
these
models
are
trained.
Large
language
models
often
assign
roles
and
characteristics
based
on
traditional
gender
norms.
For
example,
it
might
associate
nurses
or
secretaries
predominantly
with
women
and
engineers
or
CEOs
with
men.
====
Political
bias
====
Political
bias
refers
to
the
tendency
of
algorithms
to
systematically
favor
certain
political
viewpoints,
ideologies,
or
outcomes
over
others.
Language
models
may
also
exhibit
political
biases.
Since
the
training
data
includes
a
wide
range
of
political
opinions
and
coverage,
the
models
might
generate
responses
that
lean
towards
particular
political
ideologies
or
viewpoints,
depending
on
the
prevalence
of
those
views
in
the
data.
==
List
==
For
the
training
cost
column,
1
petaFLOP-day
=
1
petaFLOP/sec
×
1
day
=
8.64E19
FLOP.
==
See
also
==
Foundation
models
==
Notes
==
==
References
==
==
Further
reading
==
Jurafsky,
Dan,
Martin,
James.
H.
Speech
and
Language
Processing:
An
Introduction
to
Natural
Language
Processing,
Computational
Linguistics,
and
Speech
Recognition,
3rd
Edition
draft,
2023.
Phuong,
Mary;
Hutter,
Marcus
(2022).
"Formal
Algorithms
for
Transformers".
arXiv:2207.09238
[cs.LG].
Eloundou,
Tyna;
Manning,
Sam;
Mishkin,
Pamela;
Rock,
Daniel
(2023).
"GPTs
are
GPTs:
An
Early
Look
at
the
Labor
Market
Impact
Potential
of
Large
Language
Models".
arXiv:2303.10130
[econ.GN].
Eldan,
Ronen;
Li,
Yuanzhi
(2023).
"TinyStories:
How
Small
Can
Language
Models
Be
and
Still
Speak
Coherent
English?".
arXiv:2305.07759
[cs.CL].
Frank,
Michael
C.
(27
June
2023).
"Baby
steps
in
evaluating
the
capacities
of
large
language
models".
Nature
Reviews
Psychology.
2
(8):
451–452.
doi:10.1038/s44159-023-00211-x.
ISSN
2731-0574.
S2CID
259713140.
Retrieved
2
July
2023.
Zhao,
Wayne
Xin;
et
al.
(2023).
"A
Survey
of
Large
Language
Models".
arXiv:2303.18223
[cs.CL].
Kaddour,
Jean;
et
al.
(2023).
"Challenges
and
Applications
of
Large
Language
Models".
arXiv:2307.10169
[cs.CL].
Yin,
Shukang;
Fu,
Chaoyou;
Zhao,
Sirui;
Li,
Ke;
Sun,
Xing;
Xu,
Tong;
Chen,
Enhong
(2023-06-01).
"A
Survey
on
Multimodal
Large
Language
Models".
arXiv:2306.13549
[cs.CV].
Open
LLMs
repository
on
GitHub.
A
modeling
language
is
any
artificial
language
that
can
be
used
to
express
data,
information
or
knowledge
or
systems
in
a
structure
that
is
defined
by
a
consistent
set
of
rules.
The
rules
are
used
for
interpretation
of
the
meaning
of
components
in
the
structure
Programing
language.
==
Overview
==
A
modeling
language
can
be
graphical
or
textual.
Graphical
modeling
languages
use
a
diagram
technique
with
named
symbols
that
represent
concepts
and
lines
that
connect
the
symbols
and
represent
relationships
and
various
other
graphical
notation
to
represent
constraints.
Textual
modeling
languages
may
use
standardized
keywords
accompanied
by
parameters
or
natural
language
terms
and
phrases
to
make
computer-interpretable
expressions.An
example
of
a
graphical
modeling
language
and
a
corresponding
textual
modeling
language
is
EXPRESS.
Not
all
modeling
languages
are
executable,
and
for
those
that
are,
the
use
of
them
doesn't
necessarily
mean
that
programmers
are
no
longer
required.
On
the
contrary,
executable
modeling
languages
are
intended
to
amplify
the
productivity
of
skilled
programmers,
so
that
they
can
address
more
challenging
problems,
such
as
parallel
computing
and
distributed
systems.
A
large
number
of
modeling
languages
appear
in
the
literature.
==
Type
of
modeling
languages
==
===
Graphical
types
===
Example
of
graphical
modeling
languages
in
the
field
of
computer
science,
project
management
and
systems
engineering:
Behavior
Trees
are
a
formal,
graphical
modeling
language
used
primarily
in
systems
and
software
engineering.
Commonly
used
to
unambiguously
represent
the
hundreds
or
even
thousands
of
natural
language
requirements
that
are
typically
used
to
express
the
stakeholder
needs
for
a
large-scale
software-integrated
system.
Business
Process
Modeling
Notation
(BPMN,
and
the
XML
form
BPML)
is
an
example
of
a
Process
Modeling
language.
C-K
theory
consists
of
a
modeling
language
for
design
processes.
DRAKON
is
a
general-purpose
algorithmic
modeling
language
for
specifying
software-intensive
systems,
a
schematic
representation
of
an
algorithm
or
a
stepwise
process,
and
a
family
of
programming
languages.
EXPRESS
and
EXPRESS-G
(ISO
10303-11)
is
an
international
standard
general-purpose
data
modeling
language.
Extended
Enterprise
Modeling
Language
(EEML)
is
commonly
used
for
business
process
modeling
across
a
number
of
layers.
Flowchart
is
a
schematic
representation
of
an
algorithm
or
a
stepwise
process.
Fundamental
Modeling
Concepts
(FMC)
modeling
language
for
software-intensive
systems.
IDEF
is
a
family
of
modeling
languages,
which
include
IDEF0
for
functional
modeling,
IDEF1X
for
information
modeling,
IDEF3
for
business
process
modeling,
IDEF4
for
Object-Oriented
Design
and
IDEF5
for
modeling
ontologies.
Jackson
Structured
Programming
(JSP)
is
a
method
for
structured
programming
based
on
correspondences
between
data
stream
structure
and
program
structure.
LePUS3
is
an
object-oriented
visual
Design
Description
Language
and
a
formal
specification
language
that
is
suitable
primarily
for
modeling
large
object-oriented
(Java,
C++,
C#)
programs
and
design
patterns.
Lifecycle
Modeling
Language
is
an
open-standard
language
for
systems
engineering
that
supports
the
full
system
lifecycle:
conceptual,
utilization,
support
and
retirement
stages.
Object-Role
Modeling
(ORM)
in
the
field
of
software
engineering
is
a
method
for
conceptual
modeling,
and
can
be
used
as
a
tool
for
information
and
rules
analysis.
Petri
nets
use
variations
on
exactly
one
diagramming
technique
and
topology,
namely
the
bipartite
graph.
The
simplicity
of
its
basic
user
interface
easily
enabled
extensive
tool
support
over
the
years,
particularly
in
the
areas
of
model
checking,
graphically
oriented
simulation,
and
software
verification.
Southbeach
Notation
is
a
visual
modeling
language
used
to
describe
situations
in
terms
of
agents
that
are
considered
useful
or
harmful
from
the
modeler's
perspective.
The
notation
shows
how
the
agents
interact
with
each
other
and
whether
this
interaction
improves
or
worsens
the
situation.
Specification
and
Description
Language
(SDL)
is
a
specification
language
targeted
at
the
unambiguous
specification
and
description
of
the
behavior
of
reactive
and
distributed
systems.
SysML
is
a
Domain-Specific
Modeling
language
for
systems
engineering
that
is
defined
as
a
UML
profile
(customization).
Unified
Modeling
Language
(UML)
is
a
general-purpose
modeling
language
that
is
an
industry
standard
for
specifying
software-intensive
systems.
UML
2.0,
the
current
version,
supports
thirteen
different
diagram
techniques,
and
has
widespread
tool
support.
Service-oriented
modeling
framework
(SOMF)
is
a
holistic
language
for
designing
enterprise
and
application
level
architecture
models
in
the
space
of
enterprise
architecture,
virtualization,
service-oriented
architecture
(SOA),
cloud
computing,
and
more.
Architecture
description
language
(ADL)
is
a
language
used
to
describe
and
represent
the
systems
architecture
of
a
system.
Architecture
Analysis
&
Design
Language
(AADL)
is
a
modeling
language
that
supports
early
and
repeated
analyses
of
a
system's
architecture
with
respect
to
performance-critical
properties
through
an
extendable
notation,
a
tool
framework,
and
precisely
defined
semantics.Examples
of
graphical
modeling
languages
in
other
fields
of
science.
EAST-ADL
is
a
Domain-Specific
Modeling
language
dedicated
to
automotive
system
design.
Energy
Systems
Language
(ESL),
a
language
that
aims
to
model
ecological
energetics
&
global
economics.
IEC
61499
defines
Domain-Specific
Modeling
language
dedicated
to
distribute
industrial
process
measurement
and
control
systems.
===
Textual
types
===
Information
models
can
also
be
expressed
in
formalized
natural
languages,
such
as
Gellish.
Gellish
has
natural
language
variants
such
as
Gellish
Formal
English
and
Gellish
Formal
Dutch
(Gellish
Formeel
Nederlands),
etc.
Gellish
Formal
English
is
an
information
representation
language
or
semantic
modeling
language
that
is
defined
in
the
Gellish
English
Dictionary-Taxonomy,
which
has
the
form
of
a
Taxonomy-Ontology
(similarly
for
Dutch).
Gellish
Formal
English
is
not
only
suitable
to
express
knowledge,
requirements
and
dictionaries,
taxonomies
and
ontologies,
but
also
information
about
individual
things.
All
that
information
is
expressed
in
one
language
and
therefore
it
can
all
be
integrated,
independent
of
the
question
whether
it
is
stored
in
central
or
distributed
or
in
federated
databases.
Information
models
in
Gellish
Formal
English
consists
of
collections
of
Gellish
Formal
English
expressions,
that
use
natural
language
terms
and
formalized
phrases.
For
example,
a
geographic
information
model
might
consist
of
a
number
of
Gellish
Formal
English
expressions,
such
as:
-
the
Eiffel
tower
<is
located
in>
Paris
-
Paris
<is
classified
as
a>
city
whereas
information
requirements
and
knowledge
can
be
expressed
for
example
as
follows:
-
tower
<shall
be
located
in
a>
geographical
area
-
city
<is
a
kind
of>
geographical
area
Such
Gellish
Formal
English
expressions
use
names
of
concepts
(such
as
"city")
and
phrases
that
represent
relation
types
(such
as
⟨is
located
in⟩
and
⟨is
classified
as
a⟩)
that
should
be
selected
from
the
Gellish
English
Dictionary-Taxonomy
(or
of
your
own
domain
dictionary).
The
Gellish
English
Dictionary-Taxonomy
enables
the
creation
of
semantically
rich
information
models,
because
the
dictionary
contains
more
than
600
standard
relation
types
and
contains
definitions
of
more
than
40000
concepts.
An
information
model
in
Gellish
can
express
facts
or
make
statements,
queries
and
answers.
===
More
specific
types
===
In
the
field
of
computer
science
recently
more
specific
types
of
modeling
languages
have
emerged.
====
Algebraic
====
Algebraic
Modeling
Languages
(AML)
are
high-level
programming
languages
for
describing
and
solving
high
complexity
problems
for
large
scale
mathematical
computation
(i.e.
large
scale
optimization
type
problems).
One
particular
advantage
of
AMLs
like
AIMMS,
AMPL,
GAMS,
Gekko,
Mosel,
OPL
and
OptimJ
is
the
similarity
of
its
syntax
to
the
mathematical
notation
of
optimization
problems.
This
allows
for
a
very
concise
and
readable
definition
of
problems
in
the
domain
of
optimization,
which
is
supported
by
certain
language
elements
like
sets,
indices,
algebraic
expressions,
powerful
sparse
index
and
data
handling
variables,
constraints
with
arbitrary
names.
The
algebraic
formulation
of
a
model
does
not
contain
any
hints
how
to
process
it.
====
Behavioral
====
Behavioral
languages
are
designed
to
describe
the
observable
behavior
of
complex
systems
consisting
of
components
that
execute
concurrently.
These
languages
focus
on
the
description
of
key
concepts
such
as:
concurrency,
nondeterminism,
synchronization,
and
communication.
The
semantic
foundations
of
Behavioral
languages
are
process
calculus
or
process
algebra.
====
Discipline-specific
====
A
discipline-specific
modeling
(DspM)
language
is
focused
on
deliverables
affiliated
with
a
specific
software
development
life
cycle
stage.
Therefore,
such
language
offers
a
distinct
vocabulary,
syntax,
and
notation
for
each
stage,
such
as
discovery,
analysis,
design,
architecture,
contraction,
etc.
For
example,
for
the
analysis
phase
of
a
project,
the
modeler
employs
specific
analysis
notation
to
deliver
an
analysis
proposition
diagram.
During
the
design
phase,
however,
logical
design
notation
is
used
to
depict
relationship
between
software
entities.
In
addition,
the
discipline-specific
modeling
language
best
practices
does
not
preclude
practitioners
from
combining
the
various
notations
in
a
single
diagram.
====
Domain-specific
====
Domain-specific
modeling
(DSM)
is
a
software
engineering
methodology
for
designing
and
developing
systems,
most
often
IT
systems
such
as
computer
software.
It
involves
systematic
use
of
a
graphical
domain-specific
language
(DSL)
to
represent
the
various
facets
of
a
system.
DSM
languages
tend
to
support
higher-level
abstractions
than
General-purpose
modeling
languages,
so
they
require
less
effort
and
fewer
low-level
details
to
specify
a
given
system.
====
Framework-specific
====
A
framework-specific
modeling
language
(FSML)
is
a
kind
of
domain-specific
modeling
language
which
is
designed
for
an
object-oriented
application
framework.
FSMLs
define
framework-provided
abstractions
as
FSML
concepts
and
decompose
the
abstractions
into
features.
The
features
represent
implementation
steps
or
choices.
A
FSML
concept
can
be
configured
by
selecting
features
and
providing
values
for
features.
Such
a
concept
configuration
represents
how
the
concept
should
be
implemented
in
the
code.
In
other
words,
concept
configuration
describes
how
the
framework
should
be
completed
in
order
to
create
the
implementation
of
the
concept.
====
Information
and
knowledge
modeling
====
Linked
data
and
ontology
engineering
require
'host
languages'
to
represent
entities
and
the
relations
between
them,
constraints
between
the
properties
of
entities
and
relations,
and
metadata
attributes.
JSON-LD
and
RDF
are
two
major
(and
semantically
almost
equivalent)
languages
in
this
context,
primarily
because
they
support
statement
reification
and
contextualisation
which
are
essential
properties
to
support
the
higher-order
logic
needed
to
reason
about
models.
Model
transformation
is
a
common
example
of
such
reasoning.
====
Object-oriented
====
Object
modeling
languages
are
modeling
languages
based
on
a
standardized
set
of
symbols
and
ways
of
arranging
them
to
model
(part
of)
an
object
oriented
software
design
or
system
design.
Some
organizations
use
them
extensively
in
combination
with
a
software
development
methodology
to
progress
from
initial
specification
to
an
implementation
plan
and
to
communicate
that
plan
to
an
entire
team
of
developers
and
stakeholders.
Because
a
modeling
language
is
visual
and
at
a
higher-level
of
abstraction
than
code,
using
models
encourages
the
generation
of
a
shared
vision
that
may
prevent
problems
of
differing
interpretation
later
in
development.
Often
software
modeling
tools
are
used
to
construct
these
models,
which
may
then
be
capable
of
automatic
translation
to
code.
====
Virtual
reality
====
Virtual
Reality
Modeling
Language
(VRML),
before
1995
known
as
the
Virtual
Reality
Markup
Language
is
a
standard
file
format
for
representing
3-dimensional
(3D)
interactive
vector
graphics,
designed
particularly
with
the
World
Wide
Web
in
mind.
====
Others
====
Architecture
Description
Language
Face
Modeling
Language
Generative
Modelling
Language
Java
Modeling
Language
Promela
Rebeca
Modeling
Language
Service
Modeling
Language
Web
Services
Modeling
Language
X3D
==
Applications
==
Various
kinds
of
modeling
languages
are
applied
in
different
disciplines,
including
computer
science,
information
management,
business
process
modeling,
software
engineering,
and
systems
engineering.
Modeling
languages
can
be
used
to
specify:
system
requirements,
structures
and
behaviors.Modeling
languages
are
intended
to
be
used
to
precisely
specify
systems
so
that
stakeholders
(e.g.,
customers,
operators,
analysts,
designers)
can
better
understand
the
system
being
modeled.
The
more
mature
modeling
languages
are
precise,
consistent
and
executable.
Informal
diagramming
techniques
applied
with
drawing
tools
are
expected
to
produce
useful
pictorial
representations
of
system
requirements,
structures
and
behaviors,
which
can
be
useful
for
communication,
design,
and
problem
solving
but
cannot
be
used
programmatically.:
539
Executable
modeling
languages
applied
with
proper
tool
support,
however,
are
expected
to
automate
system
verification
and
validation,
simulation
and
code
generation
from
the
same
representations.
==
Quality
==
A
review
of
modelling
languages
is
essential
to
be
able
to
assign
which
languages
are
appropriate
for
different
modelling
settings.
In
the
term
settings
we
include
stakeholders,
domain
and
the
knowledge
connected.
Assessing
the
language
quality
is
a
means
that
aims
to
achieve
better
models.
===
Framework
for
evaluation
===
Here
language
quality
is
stated
in
accordance
with
the
SEQUAL
framework
for
quality
of
models
developed
by
Krogstie,
Sindre
and
Lindland
(2003),
since
this
is
a
framework
that
connects
the
language
quality
to
a
framework
for
general
model
quality.
Five
areas
are
used
in
this
framework
to
describe
language
quality
and
these
are
supposed
to
express
both
the
conceptual
as
well
as
the
visual
notation
of
the
language.
We
will
not
go
into
a
thoroughly
explanation
of
the
underlying
quality
framework
of
models
but
concentrate
on
the
areas
used
to
explain
the
language
quality
framework.
====
Domain
appropriateness
====
The
framework
states
the
ability
to
represent
the
domain
as
domain
appropriateness.
The
statement
appropriateness
can
be
a
bit
vague,
but
in
this
particular
context
it
means
able
to
express.
You
should
ideally
only
be
able
to
express
things
that
are
in
the
domain
but
be
powerful
enough
to
include
everything
that
is
in
the
domain.
This
requirement
might
seem
a
bit
strict,
but
the
aim
is
to
get
a
visually
expressed
model
which
includes
everything
relevant
to
the
domain
and
excludes
everything
not
appropriate
for
the
domain.
To
achieve
this,
the
language
has
to
have
a
good
distinction
of
which
notations
and
syntaxes
that
are
advantageous
to
present.
====
Participant
appropriateness
====
To
evaluate
the
participant
appropriateness
we
try
to
identify
how
well
the
language
expresses
the
knowledge
held
by
the
stakeholders.
This
involves
challenges
since
a
stakeholder's
knowledge
is
subjective.
The
knowledge
of
the
stakeholder
is
both
tacit
and
explicit.
Both
types
of
knowledge
are
of
dynamic
character.
In
this
framework
only
the
explicit
type
of
knowledge
is
taken
into
account.
The
language
should
to
a
large
extent
express
all
the
explicit
knowledge
of
the
stakeholders
relevant
to
the
domain.
====
Modeller
appropriateness
====
Last
paragraph
stated
that
knowledge
of
the
stakeholders
should
be
presented
in
a
good
way.
In
addition
it
is
imperative
that
the
language
should
be
able
to
express
all
possible
explicit
knowledge
of
the
stakeholders.
No
knowledge
should
be
left
unexpressed
due
to
lacks
in
the
language.
====
Comprehensibility
appropriateness
====
Comprehensibility
appropriateness
makes
sure
that
the
social
actors
understand
the
model
due
to
a
consistent
use
of
the
language.
To
achieve
this
the
framework
includes
a
set
of
criteria.
The
general
importance
that
these
express
is
that
the
language
should
be
flexible,
easy
to
organize
and
easy
to
distinguish
different
parts
of
the
language
internally
as
well
as
from
other
languages.
In
addition
to
this,
the
goal
should
be
as
simple
as
possible
and
that
each
symbol
in
the
language
has
a
unique
representation.
This
is
in
connection
to
also
to
the
structure
of
the
development
requirements.
.
====
Tool
appropriateness
====
To
ensure
that
the
domain
actually
modelled
is
usable
for
analyzing
and
further
processing,
the
language
has
to
ensure
that
it
is
possible
to
reason
in
an
automatic
way.
To
achieve
this
it
has
to
include
formal
syntax
and
semantics.
Another
advantage
by
formalizing
is
the
ability
to
discover
errors
in
an
early
stage.
It
is
not
always
that
the
language
best
fitted
for
the
technical
actors
is
the
same
as
for
the
social
actors.
====
Organizational
appropriateness
====
The
language
used
is
appropriate
for
the
organizational
context,
e.g.
that
the
language
is
standardized
within
the
organization,
or
that
it
is
supported
by
tools
that
are
chosen
as
standard
in
the
organization.
==
See
also
==
==
References
==
==
Further
reading
==
John
Krogstie
(2003)
"Evaluating
UML
using
a
generic
quality
framework"
.
SINTEF
Telecom
and
Informatics
and
IDI,
NTNU,
Norway
Krogstie
and
Sølvsberg
(2003).
Information
Systems
Engineering:
Conceptual
Modeling
in
a
Quality
Perspective.
Institute
of
computer
and
information
sciences.\
Anna
Gunhild
Nysetvold
and
John
Krogstie
(2005).
"Assessing
business
processing
modeling
languages
using
a
generic
quality
framework".
Institute
of
computer
and
information
sciences.
==
External
links
==
Fundamental
Modeling
Concepts
Software
Modeling
Languages
Portal
BIP
--
Incremental
Component-based
Construction
of
Real-time
Systems
Gellish
Formal
English
A
neural
network,
also
called
a
neuronal
network,
is
an
interconnected
population
of
neurons
(typically
containing
multiple
neural
circuits).
Biological
neural
networks
are
studied
to
understand
the
organization
and
functioning
of
nervous
systems.
Closely
related
are
artificial
neural
networks,
machine
learning
models
inspired
by
biological
neural
networks.
They
consist
of
artificial
neurons,
which
are
mathematical
functions
that
are
designed
to
be
analogous
to
the
mechanisms
used
by
neural
circuits.
==
Overview
==
A
biological
neural
network
is
composed
of
a
group
of
chemically
connected
or
functionally
associated
neurons.
A
single
neuron
may
be
connected
to
many
other
neurons
and
the
total
number
of
neurons
and
connections
in
a
network
may
be
extensive.
Connections,
called
synapses,
are
usually
formed
from
axons
to
dendrites,
though
dendrodendritic
synapses
and
other
connections
are
possible.
Apart
from
electrical
signalling,
there
are
other
forms
of
signalling
that
arise
from
neurotransmitter
diffusion.
Artificial
intelligence,
cognitive
modelling,
and
artificial
neural
networks
are
information
processing
paradigms
inspired
by
how
biological
neural
systems
process
data.
Artificial
intelligence
and
cognitive
modelling
try
to
simulate
some
properties
of
biological
neural
networks.
In
the
artificial
intelligence
field,
artificial
neural
networks
have
been
applied
successfully
to
speech
recognition,
image
analysis
and
adaptive
control,
in
order
to
construct
software
agents
(in
computer
and
video
games)
or
autonomous
robots.
Neural
network
theory
has
served
to
identify
better
how
the
neurons
in
the
brain
function
and
provide
the
basis
for
efforts
to
create
artificial
intelligence.
==
History
==
The
preliminary
theoretical
base
for
contemporary
neural
networks
was
independently
proposed
by
Alexander
Bain
(1873)
and
William
James
(1890).
In
their
work,
both
thoughts
and
body
activity
resulted
from
interactions
among
neurons
within
the
brain.
For
Bain,
every
activity
led
to
the
firing
of
a
certain
set
of
neurons.
When
activities
were
repeated,
the
connections
between
those
neurons
strengthened.
According
to
his
theory,
this
repetition
was
what
led
to
the
formation
of
memory.
The
general
scientific
community
at
the
time
was
skeptical
of
Bain's
theory
because
it
required
what
appeared
to
be
an
inordinate
number
of
neural
connections
within
the
brain.
It
is
now
apparent
that
the
brain
is
exceedingly
complex
and
that
the
same
brain
“wiring”
can
handle
multiple
problems
and
inputs.
James'
theory
was
similar
to
Bain's;
however,
he
suggested
that
memories
and
actions
resulted
from
electrical
currents
flowing
among
the
neurons
in
the
brain.
His
model,
by
focusing
on
the
flow
of
electrical
currents,
did
not
require
individual
neural
connections
for
each
memory
or
action.
C.
S.
Sherrington
(1898)
conducted
experiments
to
test
James'
theory.
He
ran
electrical
currents
down
the
spinal
cords
of
rats.
However,
instead
of
demonstrating
an
increase
in
electrical
current
as
projected
by
James,
Sherrington
found
that
the
electrical
current
strength
decreased
as
the
testing
continued
over
time.
Importantly,
this
work
led
to
the
discovery
of
the
concept
of
habituation.
McCulloch
and
Pitts
(1943)
also
created
a
computational
model
for
neural
networks
based
on
mathematics
and
algorithms.
They
called
this
model
threshold
logic.
These
early
models
paved
the
way
for
neural
network
research
to
split
into
two
distinct
approaches.
One
approach
focused
on
biological
processes
in
the
brain
and
the
other
focused
on
the
application
of
neural
networks
to
artificial
intelligence.
The
parallel
distributed
processing
of
the
mid-1980s
became
popular
under
the
name
connectionism.
The
text
by
Rumelhart
and
McClelland
(1986)
provided
a
full
exposition
on
the
use
of
connectionism
in
computers
to
simulate
neural
processes.
Artificial
neural
networks,
as
used
in
artificial
intelligence,
have
traditionally
been
viewed
as
simplified
models
of
neural
processing
in
the
brain,
even
though
the
relation
between
this
model
and
brain
biological
architecture
is
debated,
as
it
is
not
clear
to
what
degree
artificial
neural
networks
mirror
brain
function.
==
Neuroscience
==
Theoretical
and
computational
neuroscience
is
the
field
concerned
with
the
analysis
and
computational
modeling
of
biological
neural
systems.
Since
neural
systems
are
intimately
related
to
cognitive
processes
and
behaviour,
the
field
is
closely
related
to
cognitive
and
behavioural
modeling.
The
aim
of
the
field
is
to
create
models
of
biological
neural
systems
in
order
to
understand
how
biological
systems
work.
To
gain
this
understanding,
neuroscientists
strive
to
make
a
link
between
observed
biological
processes
(data),
biologically
plausible
mechanisms
for
neural
processing
and
learning
(neural
network
models)
and
theory
(statistical
learning
theory
and
information
theory).
===
Types
of
models
===
Many
models
are
used;
defined
at
different
levels
of
abstraction,
and
modeling
different
aspects
of
neural
systems.
They
range
from
models
of
the
short-term
behaviour
of
individual
neurons,
through
models
of
the
dynamics
of
neural
circuitry
arising
from
interactions
between
individual
neurons,
to
models
of
behaviour
arising
from
abstract
neural
modules
that
represent
complete
subsystems.
These
include
models
of
the
long-term
and
short-term
plasticity
of
neural
systems
and
their
relation
to
learning
and
memory,
from
the
individual
neuron
to
the
system
level.
===
Connectivity
===
In
August
2020
scientists
reported
that
bi-directional
connections,
or
added
appropriate
feedback
connections,
can
accelerate
and
improve
communication
between
and
in
modular
neural
networks
of
the
brain's
cerebral
cortex
and
lower
the
threshold
for
their
successful
communication.
They
showed
that
adding
feedback
connections
between
a
resonance
pair
can
support
successful
propagation
of
a
single
pulse
packet
throughout
the
entire
network.
==
Recent
improvements
==
While
initially
research
had
been
concerned
mostly
with
the
electrical
characteristics
of
neurons,
a
particularly
important
part
of
the
investigation
in
recent
years
has
been
the
exploration
of
the
role
of
neuromodulators
such
as
dopamine,
acetylcholine,
and
serotonin
on
behaviour
and
learning.Biophysical
models,
such
as
BCM
theory,
has
been
important
in
understanding
mechanisms
for
synaptic
plasticity,
and
have
had
applications
in
both
computer
science
and
neuroscience.
==
See
also
==
Adaptive
resonance
theory
Biological
cybernetics
Cognitive
architecture
Cognitive
science
Connectomics
Cultured
neuronal
networks
Parallel
constraint
satisfaction
processes
==
References
==
In
machine
learning,
an
artificial
neural
network
(also
neural
network
or
neural
net,
abbreviated
ANN
or
NN)
is
a
model
inspired
by
the
neuronal
organization
found
in
the
biological
neural
networks
in
animal
brains.An
ANN
is
made
of
connected
units
or
nodes
called
artificial
neurons,
which
loosely
model
the
neurons
in
a
brain.
These
are
connected
by
edges,
which
model
the
synapses
in
a
brain.
An
artificial
neuron
receives
signals
from
connected
neurons,
then
processes
them
and
sends
a
signal
to
other
connected
neurons.
The
"signal"
is
a
real
number,
and
the
output
of
each
neuron
is
computed
by
some
non-linear
function
of
the
sum
of
its
inputs,
called
the
activation
function.
Neurons
and
edges
typically
have
a
weight
that
adjusts
as
learning
proceeds.
The
weight
increases
or
decreases
the
strength
of
the
signal
at
a
connection.
Typically,
neurons
are
aggregated
into
layers.
Different
layers
may
perform
different
transformations
on
their
inputs.
Signals
travel
from
the
first
layer
(the
input
layer)
to
the
last
layer
(the
output
layer),
possibly
passing
through
multiple
intermediate
layers
(hidden
layers).
A
network
is
typically
called
a
deep
neural
network
if
it
has
at
least
2
hidden
layers.Artificial
neural
networks
are
used
for
predictive
modeling,
adaptive
control,
and
other
applications
where
they
can
be
trained
via
a
dataset.
They
are
also
used
to
solve
problems
in
artificial
intelligence.
Networks
can
learn
from
experience,
and
can
derive
conclusions
from
a
complex
and
seemingly
unrelated
set
of
information.
==
Training
==
Neural
networks
are
typically
trained
through
empirical
risk
minimization.
This
method
is
based
on
the
idea
of
optimizing
the
network's
parameters
to
minimize
the
difference,
or
empirical
risk,
between
the
predicted
output
and
the
actual
target
values
in
a
given
dataset.
Gradient
based
methods
such
as
backpropagation
are
usually
used
to
estimate
the
parameters
of
the
network.
During
the
training
phase,
ANNs
learn
from
labeled
training
data
by
iteratively
updating
their
parameters
to
minimize
a
defined
loss
function.
This
method
allows
the
network
to
generalize
to
unseen
data.
==
History
==
Historically,
digital
computers
evolved
from
the
von
Neumann
model,
and
operate
via
the
execution
of
explicit
instructions
via
access
to
memory
by
a
number
of
processors.
Neural
networks,
on
the
other
hand,
originated
from
efforts
to
model
information
processing
in
biological
systems
through
the
framework
of
connectionism.
Unlike
the
von
Neumann
model,
connectionist
computing
does
not
separate
memory
and
processing.
The
simplest
kind
of
feedforward
neural
network
(FNN)
is
a
linear
network,
which
consists
of
a
single
layer
of
output
nodes;
the
inputs
are
fed
directly
to
the
outputs
via
a
series
of
weights.
The
sum
of
the
products
of
the
weights
and
the
inputs
is
calculated
at
each
node.
The
mean
squared
errors
between
these
calculated
outputs
and
the
given
target
values
are
minimized
by
creating
an
adjustment
to
the
weights.
This
technique
has
been
known
for
over
two
centuries
as
the
method
of
least
squares
or
linear
regression.
It
was
used
as
a
means
of
finding
a
good
rough
linear
fit
to
a
set
of
points
by
Legendre
(1805)
and
Gauss
(1795)
for
the
prediction
of
planetary
movement.Warren
McCulloch
and
Walter
Pitts
(1943)
also
considered
a
non-learning
computational
model
for
neural
networks.In
the
late
1940s,
D.
O.
Hebb
created
a
learning
hypothesis
based
on
the
mechanism
of
neural
plasticity
that
became
known
as
Hebbian
learning.
Hebbian
learning
is
considered
to
be
a
'typical'
unsupervised
learning
rule
and
its
later
variants
were
early
models
for
long
term
potentiation.
These
ideas
started
being
applied
to
computational
models
in
1948
with
Turing's
"unorganized
machines".
Farley
and
Wesley
A.
Clark
were
the
first
to
simulate
a
Hebbian
network
in
1954
at
MIT.
They
used
computational
machines,
then
called
"calculators".
Other
neural
network
computational
machines
were
created
by
Rochester,
Holland,
Habit,
and
Duda
in
1956.
In
1958,
psychologist
Frank
Rosenblatt
invented
the
perceptron,
the
first
implemented
artificial
neural
network,
funded
by
the
United
States
Office
of
Naval
Research.
The
invention
of
the
perceptron
raised
public
excitement
for
research
in
Artificial
Neural
Networks,
causing
the
US
government
to
drastically
increase
funding
into
deep
learning
research.
This
led
to
"the
golden
age
of
AI"
fueled
by
the
optimistic
claims
made
by
computer
scientists
regarding
the
ability
of
perceptrons
to
emulate
human
intelligence.
For
example,
in
1957
Herbert
Simon
famously
said:It
is
not
my
aim
to
surprise
or
shock
you—but
the
simplest
way
I
can
summarize
is
to
say
that
there
are
now
in
the
world
machines
that
think,
that
learn
and
that
create.
Moreover,
their
ability
to
do
these
things
is
going
to
increase
rapidly
until—in
a
visible
future—the
range
of
problems
they
can
handle
will
be
coextensive
with
the
range
to
which
the
human
mind
has
been
applied.However,
this
wasn't
the
case
as
research
stagnated
in
the
United
States
following
the
work
of
Minsky
and
Papert
(1969),
who
discovered
that
basic
perceptrons
were
incapable
of
processing
the
exclusive-or
circuit
and
that
computers
lacked
sufficient
power
to
train
useful
neural
networks.
This,
along
with
other
factors
such
as
the
1973
Lighthill
report
by
James
Lighthill
stating
that
research
in
Artificial
Intelligence
has
not
"produced
the
major
impact
that
was
then
promised,"
shutting
funding
in
research
into
the
field
of
AI
in
all
but
two
universities
in
the
UK
and
in
many
major
institutions
across
the
world.
This
ushered
an
era
called
the
AI
Winter
with
reduced
research
into
connectionism
due
to
a
decrease
in
government
funding
and
an
increased
stress
on
symbolic
artificial
intelligence
in
the
United
States
and
other
Western
countries.During
the
AI
Winter
era,
however,
research
outside
the
United
States
continued,
especially
in
Eastern
Europe.
By
the
time
Minsky
and
Papert's
book
on
Perceptrons
came
out,
methods
for
training
multilayer
perceptrons
(MLPs)
were
already
known.
The
first
deep
learning
MLP
was
published
by
Alexey
Grigorevich
Ivakhnenko
and
Valentin
Lapa
in
1965,
as
the
Group
Method
of
Data
Handling.
The
first
deep
learning
MLP
trained
by
stochastic
gradient
descent
was
published
in
1967
by
Shun'ichi
Amari.
In
computer
experiments
conducted
by
Amari's
student
Saito,
a
five
layer
MLP
with
two
modifiable
layers
learned
useful
internal
representations
to
classify
non-linearily
separable
pattern
classes.Self-organizing
maps
(SOMs)
were
described
by
Teuvo
Kohonen
in
1982.
SOMs
are
neurophysiologically
inspired
neural
networks
that
learn
low-dimensional
representations
of
high-dimensional
data
while
preserving
the
topological
structure
of
the
data.
They
are
trained
using
competitive
learning.The
convolutional
neural
network
(CNN)
architecture
with
convolutional
layers
and
downsampling
layers
was
introduced
by
Kunihiko
Fukushima
in
1980.
He
called
it
the
neocognitron.
In
1969,
he
also
introduced
the
ReLU
(rectified
linear
unit)
activation
function.
The
rectifier
has
become
the
most
popular
activation
function
for
CNNs
and
deep
neural
networks
in
general.
CNNs
have
become
an
essential
tool
for
computer
vision.
A
key
in
later
advances
in
artificial
neural
network
research
was
the
backpropagation
algorithm,
an
efficient
application
of
the
Leibniz
chain
rule
(1673)
to
networks
of
differentiable
nodes.
It
is
also
known
as
the
reverse
mode
of
automatic
differentiation
or
reverse
accumulation,
due
to
Seppo
Linnainmaa
(1970).
The
term
"back-propagating
errors"
was
introduced
in
1962
by
Frank
Rosenblatt,
but
he
did
not
have
an
implementation
of
this
procedure,
although
Henry
J.
Kelley
and
Bryson
had
dynamic
programming
based
continuous
precursors
of
backpropagation
already
in
1960–61
in
the
context
of
control
theory.
In
1973,
Dreyfus
used
backpropagation
to
adapt
parameters
of
controllers
in
proportion
to
error
gradients.
In
1982,
Paul
Werbos
applied
backpropagation
to
MLPs
in
the
way
that
has
become
standard.
In
1986
Rumelhart,
Hinton
and
Williams
showed
that
backpropagation
learned
interesting
internal
representations
of
words
as
feature
vectors
when
trained
to
predict
the
next
word
in
a
sequence.In
the
late
1970s
to
early
1980s,
interest
briefly
emerged
in
theoretically
investigating
the
Ising
model
created
by
Wilhelm
Lenz
(1920)
and
Ernst
Ising
(1925)
in
relation
to
Cayley
tree
topologies
and
large
neural
networks.
The
Ising
model
is
essentially
a
non-learning
artificial
recurrent
neural
network
(RNN)
consisting
of
neuron-like
threshold
elements.
In
1972,
Shun'ichi
Amari
described
an
adaptive
version
of
this
architecture,
In
1981,
the
Ising
model
was
solved
exactly
by
Peter
Barth
for
the
general
case
of
closed
Cayley
trees
(with
loops)
with
an
arbitrary
branching
ratio
and
found
to
exhibit
unusual
phase
transition
behavior
in
its
local-apex
and
long-range
site-site
correlations.John
Hopfield
popularised
this
architecture
in
1982,
and
it
is
now
known
as
a
Hopfield
network.
The
time
delay
neural
network
(TDNN)
of
Alex
Waibel
(1987)
combined
convolutions
and
weight
sharing
and
backpropagation.
In
1988,
Wei
Zhang
et
al.
applied
backpropagation
to
a
CNN
(a
simplified
Neocognitron
with
convolutional
interconnections
between
the
image
feature
layers
and
the
last
fully
connected
layer)
for
alphabet
recognition.
In
1989,
Yann
LeCun
et
al.
trained
a
CNN
to
recognize
handwritten
ZIP
codes
on
mail.
In
1992,
max-pooling
for
CNNs
was
introduced
by
Juan
Weng
et
al.
to
help
with
least-shift
invariance
and
tolerance
to
deformation
to
aid
3D
object
recognition.
LeNet-5
(1998),
a
7-level
CNN
by
Yann
LeCun
et
al.,
that
classifies
digits,
was
applied
by
several
banks
to
recognize
hand-written
numbers
on
checks
digitized
in
32x32
pixel
images.
From
1988
onward,
the
use
of
neural
networks
transformed
the
field
of
protein
structure
prediction,
in
particular
when
the
first
cascading
networks
were
trained
on
profiles
(matrices)
produced
by
multiple
sequence
alignments.In
1991,
Sepp
Hochreiter's
diploma
thesis
identified
and
analyzed
the
vanishing
gradient
problem
and
proposed
recurrent
residual
connections
to
solve
it.
His
thesis
was
called
"one
of
the
most
important
documents
in
the
history
of
machine
learning"
by
his
supervisor
Juergen
Schmidhuber.In
1991,
Juergen
Schmidhuber
published
adversarial
neural
networks
that
contest
with
each
other
in
the
form
of
a
zero-sum
game,
where
one
network's
gain
is
the
other
network's
loss.
The
first
network
is
a
generative
model
that
models
a
probability
distribution
over
output
patterns.
The
second
network
learns
by
gradient
descent
to
predict
the
reactions
of
the
environment
to
these
patterns.
This
was
called
"artificial
curiosity."
In
1992,
Juergen
Schmidhuber
proposed
a
hierarchy
of
RNNs
pre-trained
one
level
at
a
time
by
self-supervised
learning.
It
uses
predictive
coding
to
learn
internal
representations
at
multiple
self-organizing
time
scales.
This
can
substantially
facilitate
downstream
deep
learning.
The
RNN
hierarchy
can
be
collapsed
into
a
single
RNN,
by
distilling
a
higher
level
chunker
network
into
a
lower
level
automatizer
network.
In
the
same
year
he
also
published
an
alternative
to
RNNs
which
is
a
precursor
of
a
linear
Transformer.
It
introduces
the
concept
internal
spotlights
of
attention:
a
slow
feedforward
neural
network
learns
by
gradient
descent
to
control
the
fast
weights
of
another
neural
network
through
outer
products
of
self-generated
activation
patterns.
The
development
of
metal–oxide–semiconductor
(MOS)
very-large-scale
integration
(VLSI),
in
the
form
of
complementary
MOS
(CMOS)
technology,
enabled
increasing
MOS
transistor
counts
in
digital
electronics.
This
provided
more
processing
power
for
the
development
of
practical
artificial
neural
networks
in
the
1980s.Neural
networks'
early
successes
included
predicting
the
stock
market
and
in
1995
a
(mostly)
self-driving
car.1997,
Sepp
Hochreite
and
Juergen
Schmidhuber
introduced
the
deep
learning
method
called
long
short-term
memory
(LSTM),
published
in
Neural
Computation.
LSTM
recurrent
neural
networks
can
learn
"very
deep
learning"
tasks
with
long
credit
assignment
paths
that
require
memories
of
events
that
happened
thousands
of
discrete
time
steps
before.
The
"vanilla
LSTM"
with
forget
gate
was
introduced
in
1999
by
Felix
Gers,
Schmidhuber
and
Fred
Cummins.Geoffrey
Hinton
et
al.
(2006)
proposed
learning
a
high-level
representation
using
successive
layers
of
binary
or
real-valued
latent
variables
with
a
restricted
Boltzmann
machine
to
model
each
layer.
In
2012,
Ng
and
Dean
created
a
network
that
learned
to
recognize
higher-level
concepts,
such
as
cats,
only
from
watching
unlabeled
images.
Unsupervised
pre-training
and
increased
computing
power
from
GPUs
and
distributed
computing
allowed
the
use
of
larger
networks,
particularly
in
image
and
visual
recognition
problems,
which
became
known
as
"deep
learning".Variants
of
the
back-propagation
algorithm,
as
well
as
unsupervised
methods
by
Geoff
Hinton
and
colleagues
at
the
University
of
Toronto,
can
be
used
to
train
deep,
highly
nonlinear
neural
architectures,
similar
to
the
1980
Neocognitron
by
Kunihiko
Fukushima,
and
the
"standard
architecture
of
vision",
inspired
by
the
simple
and
complex
cells
identified
by
David
H.
Hubel
and
Torsten
Wiesel
in
the
primary
visual
cortex.
Computational
devices
have
been
created
in
CMOS
for
both
biophysical
simulation
and
neuromorphic
computing.
More
recent
efforts
show
promise
for
creating
nanodevices
for
very
large
scale
principal
components
analyses
and
convolution.
If
successful,
these
efforts
could
usher
in
a
new
era
of
neural
computing
that
is
a
step
beyond
digital
computing,
because
it
depends
on
learning
rather
than
programming
and
because
it
is
fundamentally
analog
rather
than
digital
even
though
the
first
instantiations
may
in
fact
be
with
CMOS
digital
devices.
Ciresan
and
colleagues
(2010)
showed
that
despite
the
vanishing
gradient
problem,
GPUs
make
backpropagation
feasible
for
many-layered
feedforward
neural
networks.
Between
2009
and
2012,
ANNs
began
winning
prizes
in
image
recognition
contests,
approaching
human
level
performance
on
various
tasks,
initially
in
pattern
recognition
and
handwriting
recognition.
For
example,
the
bi-directional
and
multi-dimensional
long
short-term
memory
(LSTM)
of
Graves
et
al.
won
three
competitions
in
connected
handwriting
recognition
in
2009
without
any
prior
knowledge
about
the
three
languages
to
be
learned.Ciresan
and
colleagues
built
the
first
pattern
recognizers
to
achieve
human-competitive/superhuman
performance
on
benchmarks
such
as
traffic
sign
recognition
(IJCNN
2012).
Radial
basis
function
and
wavelet
networks
were
introduced
in
2013.
These
can
be
shown
to
offer
best
approximation
properties
and
have
been
applied
in
nonlinear
system
identification
and
classification
applications.In
2014,
the
adversarial
network
principle
was
used
in
a
generative
adversarial
network
(GAN)
by
Ian
Goodfellow
et
al.
Here
the
adversarial
network
(discriminator)
outputs
a
value
between
1
and
0
depending
on
the
likelihood
of
the
first
network's
(generator)
output
is
in
a
given
set.
This
can
be
used
to
create
realistic
deepfakes.
Excellent
image
quality
is
achieved
by
Nvidia's
StyleGAN
(2018)
based
on
the
Progressive
GAN
by
Tero
Karras,
Timo
Aila,
Samuli
Laine,
and
Jaakko
Lehtinen.
Here
the
GAN
generator
is
grown
from
small
to
large
scale
in
a
pyramidal
fashion.
In
2015,
Rupesh
Kumar
Srivastava,
Klaus
Greff,
and
Schmidhuber
used
the
LSTM
principle
to
create
the
Highway
network,
a
feedforward
neural
network
with
hundreds
of
layers,
much
deeper
than
previous
networks.
7
months
later,
Kaiming
He,
Xiangyu
Zhang;
Shaoqing
Ren,
and
Jian
Sun
won
the
ImageNet
2015
competition
with
an
open-gated
or
gateless
Highway
network
variant
called
Residual
neural
network.In
2017,
Ashish
Vaswani
et
al.
introduced
the
modern
Transformer
architecture
in
their
paper
"Attention
Is
All
You
Need."
It
combines
this
with
a
softmax
operator
and
a
projection
matrix.
Transformers
have
increasingly
become
the
model
of
choice
for
natural
language
processing.
Many
modern
large
language
models
such
as
ChatGPT,
GPT-4,
and
BERT
use
it.
Transformers
are
also
increasingly
being
used
in
computer
vision.Ramenzanpour
et
al.
showed
in
2020
that
analytical
and
computational
techniques
derived
from
statistical
physics
of
disordered
systems
can
be
extended
to
large-scale
problems,
including
machine
learning,
e.g.,
to
analyze
the
weight
space
of
deep
neural
networks.
==
Models
==
ANNs
began
as
an
attempt
to
exploit
the
architecture
of
the
human
brain
to
perform
tasks
that
conventional
algorithms
had
little
success
with.
They
soon
reoriented
towards
improving
empirical
results,
abandoning
attempts
to
remain
true
to
their
biological
precursors.
ANNs
have
the
ability
to
learn
and
model
non-linearities
and
complex
relationships.
This
is
achieved
by
neurons
being
connected
in
various
patterns,
allowing
the
output
of
some
neurons
to
become
the
input
of
others.
The
network
forms
a
directed,
weighted
graph.An
artificial
neural
network
consists
of
simulated
neurons.
Each
neuron
is
connected
to
other
nodes
via
links
like
a
biological
axon-synapse-dendrite
connection.
All
the
nodes
connected
by
links
take
in
some
data
and
use
it
to
perform
specific
operations
and
tasks
on
the
data.
Each
link
has
a
weight,
determining
the
strength
of
one
node's
influence
on
another,
allowing
weights
to
choose
the
signal
between
neurons.
===
Artificial
neurons
===
ANNs
are
composed
of
artificial
neurons
which
are
conceptually
derived
from
biological
neurons.
Each
artificial
neuron
has
inputs
and
produces
a
single
output
which
can
be
sent
to
multiple
other
neurons.
The
inputs
can
be
the
feature
values
of
a
sample
of
external
data,
such
as
images
or
documents,
or
they
can
be
the
outputs
of
other
neurons.
The
outputs
of
the
final
output
neurons
of
the
neural
net
accomplish
the
task,
such
as
recognizing
an
object
in
an
image.
To
find
the
output
of
the
neuron
we
take
the
weighted
sum
of
all
the
inputs,
weighted
by
the
weights
of
the
connections
from
the
inputs
to
the
neuron.
We
add
a
bias
term
to
this
sum.
This
weighted
sum
is
sometimes
called
the
activation.
This
weighted
sum
is
then
passed
through
a
(usually
nonlinear)
activation
function
to
produce
the
output.
The
initial
inputs
are
external
data,
such
as
images
and
documents.
The
ultimate
outputs
accomplish
the
task,
such
as
recognizing
an
object
in
an
image.
===
Organization
===
The
neurons
are
typically
organized
into
multiple
layers,
especially
in
deep
learning.
Neurons
of
one
layer
connect
only
to
neurons
of
the
immediately
preceding
and
immediately
following
layers.
The
layer
that
receives
external
data
is
the
input
layer.
The
layer
that
produces
the
ultimate
result
is
the
output
layer.
In
between
them
are
zero
or
more
hidden
layers.
Single
layer
and
unlayered
networks
are
also
used.
Between
two
layers,
multiple
connection
patterns
are
possible.
They
can
be
'fully
connected',
with
every
neuron
in
one
layer
connecting
to
every
neuron
in
the
next
layer.
They
can
be
pooling,
where
a
group
of
neurons
in
one
layer
connects
to
a
single
neuron
in
the
next
layer,
thereby
reducing
the
number
of
neurons
in
that
layer.
Neurons
with
only
such
connections
form
a
directed
acyclic
graph
and
are
known
as
feedforward
networks.
Alternatively,
networks
that
allow
connections
between
neurons
in
the
same
or
previous
layers
are
known
as
recurrent
networks.
===
Hyperparameter
===
A
hyperparameter
is
a
constant
parameter
whose
value
is
set
before
the
learning
process
begins.
The
values
of
parameters
are
derived
via
learning.
Examples
of
hyperparameters
include
learning
rate,
the
number
of
hidden
layers
and
batch
size.
The
values
of
some
hyperparameters
can
be
dependent
on
those
of
other
hyperparameters.
For
example,
the
size
of
some
layers
can
depend
on
the
overall
number
of
layers.
===
Learning
===
Learning
is
the
adaptation
of
the
network
to
better
handle
a
task
by
considering
sample
observations.
Learning
involves
adjusting
the
weights
(and
optional
thresholds)
of
the
network
to
improve
the
accuracy
of
the
result.
This
is
done
by
minimizing
the
observed
errors.
Learning
is
complete
when
examining
additional
observations
does
not
usefully
reduce
the
error
rate.
Even
after
learning,
the
error
rate
typically
does
not
reach
0.
If
after
learning,
the
error
rate
is
too
high,
the
network
typically
must
be
redesigned.
Practically
this
is
done
by
defining
a
cost
function
that
is
evaluated
periodically
during
learning.
As
long
as
its
output
continues
to
decline,
learning
continues.
The
cost
is
frequently
defined
as
a
statistic
whose
value
can
only
be
approximated.
The
outputs
are
actually
numbers,
so
when
the
error
is
low,
the
difference
between
the
output
(almost
certainly
a
cat)
and
the
correct
answer
(cat)
is
small.
Learning
attempts
to
reduce
the
total
of
the
differences
across
the
observations.
Most
learning
models
can
be
viewed
as
a
straightforward
application
of
optimization
theory
and
statistical
estimation.
====
Learning
rate
====
The
learning
rate
defines
the
size
of
the
corrective
steps
that
the
model
takes
to
adjust
for
errors
in
each
observation.
A
high
learning
rate
shortens
the
training
time,
but
with
lower
ultimate
accuracy,
while
a
lower
learning
rate
takes
longer,
but
with
the
potential
for
greater
accuracy.
Optimizations
such
as
Quickprop
are
primarily
aimed
at
speeding
up
error
minimization,
while
other
improvements
mainly
try
to
increase
reliability.
In
order
to
avoid
oscillation
inside
the
network
such
as
alternating
connection
weights,
and
to
improve
the
rate
of
convergence,
refinements
use
an
adaptive
learning
rate
that
increases
or
decreases
as
appropriate.
The
concept
of
momentum
allows
the
balance
between
the
gradient
and
the
previous
change
to
be
weighted
such
that
the
weight
adjustment
depends
to
some
degree
on
the
previous
change.
A
momentum
close
to
0
emphasizes
the
gradient,
while
a
value
close
to
1
emphasizes
the
last
change.
====
Cost
function
====
While
it
is
possible
to
define
a
cost
function
ad
hoc,
frequently
the
choice
is
determined
by
the
function's
desirable
properties
(such
as
convexity)
or
because
it
arises
from
the
model
(e.g.
in
a
probabilistic
model
the
model's
posterior
probability
can
be
used
as
an
inverse
cost).
====
Backpropagation
====
Backpropagation
is
a
method
used
to
adjust
the
connection
weights
to
compensate
for
each
error
found
during
learning.
The
error
amount
is
effectively
divided
among
the
connections.
Technically,
backprop
calculates
the
gradient
(the
derivative)
of
the
cost
function
associated
with
a
given
state
with
respect
to
the
weights.
The
weight
updates
can
be
done
via
stochastic
gradient
descent
or
other
methods,
such
as
extreme
learning
machines,
"no-prop"
networks,
training
without
backtracking,
"weightless"
networks,
and
non-connectionist
neural
networks.
===
Learning
paradigms
===
Machine
learning
is
commonly
separated
into
three
main
learning
paradigms,
supervised
learning,
unsupervised
learning
and
reinforcement
learning.
Each
corresponds
to
a
particular
learning
task.
====
Supervised
learning
====
Supervised
learning
uses
a
set
of
paired
inputs
and
desired
outputs.
The
learning
task
is
to
produce
the
desired
output
for
each
input.
In
this
case,
the
cost
function
is
related
to
eliminating
incorrect
deductions.
A
commonly
used
cost
is
the
mean-squared
error,
which
tries
to
minimize
the
average
squared
error
between
the
network's
output
and
the
desired
output.
Tasks
suited
for
supervised
learning
are
pattern
recognition
(also
known
as
classification)
and
regression
(also
known
as
function
approximation).
Supervised
learning
is
also
applicable
to
sequential
data
(e.g.,
for
handwriting,
speech
and
gesture
recognition).
This
can
be
thought
of
as
learning
with
a
"teacher",
in
the
form
of
a
function
that
provides
continuous
feedback
on
the
quality
of
solutions
obtained
thus
far.
====
Unsupervised
learning
====
In
unsupervised
learning,
input
data
is
given
along
with
the
cost
function,
some
function
of
the
data
x{\displaystyle
\textstyle
x}
and
the
network's
output.
The
cost
function
is
dependent
on
the
task
(the
model
domain)
and
any
a
priori
assumptions
(the
implicit
properties
of
the
model,
its
parameters
and
the
observed
variables).
As
a
trivial
example,
consider
the
model
f(x)=a{\displaystyle
\textstyle
f(x)=a}
where
a{\displaystyle
\textstyle
a}
is
a
constant
and
the
cost
C=E[(x−f(x))2]{\displaystyle
\textstyle
C=E[(x-f(x))^{2}]}.
Minimizing
this
cost
produces
a
value
of
a{\displaystyle
\textstyle
a}
that
is
equal
to
the
mean
of
the
data.
The
cost
function
can
be
much
more
complicated.
Its
form
depends
on
the
application:
for
example,
in
compression
it
could
be
related
to
the
mutual
information
between
x{\displaystyle
\textstyle
x}
and
f(x){\displaystyle
\textstyle
f(x)},
whereas
in
statistical
modeling,
it
could
be
related
to
the
posterior
probability
of
the
model
given
the
data
(note
that
in
both
of
those
examples,
those
quantities
would
be
maximized
rather
than
minimized).
Tasks
that
fall
within
the
paradigm
of
unsupervised
learning
are
in
general
estimation
problems;
the
applications
include
clustering,
the
estimation
of
statistical
distributions,
compression
and
filtering.
====
Reinforcement
learning
====
In
applications
such
as
playing
video
games,
an
actor
takes
a
string
of
actions,
receiving
a
generally
unpredictable
response
from
the
environment
after
each
one.
The
goal
is
to
win
the
game,
i.e.,
generate
the
most
positive
(lowest
cost)
responses.
In
reinforcement
learning,
the
aim
is
to
weight
the
network
(devise
a
policy)
to
perform
actions
that
minimize
long-term
(expected
cumulative)
cost.
At
each
point
in
time
the
agent
performs
an
action
and
the
environment
generates
an
observation
and
an
instantaneous
cost,
according
to
some
(usually
unknown)
rules.
The
rules
and
the
long-term
cost
usually
only
can
be
estimated.
At
any
juncture,
the
agent
decides
whether
to
explore
new
actions
to
uncover
their
costs
or
to
exploit
prior
learning
to
proceed
more
quickly.
Formally
the
environment
is
modeled
as
a
Markov
decision
process
(MDP)
with
states
s1,...,sn∈S{\displaystyle
\textstyle
{s_{1},...,s_{n}}\in
S}
and
actions
a1,...,am∈A{\displaystyle
\textstyle
{a_{1},...,a_{m}}\in
A}.
Because
the
state
transitions
are
not
known,
probability
distributions
are
used
instead:
the
instantaneous
cost
distribution
P(ct|st){\displaystyle
\textstyle
P(c_{t}|s_{t})},
the
observation
distribution
P(xt|st){\displaystyle
\textstyle
P(x_{t}|s_{t})}
and
the
transition
distribution
P(st+1|st,at){\displaystyle
\textstyle
P(s_{t+1}|s_{t},a_{t})},
while
a
policy
is
defined
as
the
conditional
distribution
over
actions
given
the
observations.
Taken
together,
the
two
define
a
Markov
chain
(MC).
The
aim
is
to
discover
the
lowest-cost
MC.
ANNs
serve
as
the
learning
component
in
such
applications.
Dynamic
programming
coupled
with
ANNs
(giving
neurodynamic
programming)
has
been
applied
to
problems
such
as
those
involved
in
vehicle
routing,
video
games,
natural
resource
management
and
medicine
because
of
ANNs
ability
to
mitigate
losses
of
accuracy
even
when
reducing
the
discretization
grid
density
for
numerically
approximating
the
solution
of
control
problems.
Tasks
that
fall
within
the
paradigm
of
reinforcement
learning
are
control
problems,
games
and
other
sequential
decision
making
tasks.
====
Self-learning
====
Self-learning
in
neural
networks
was
introduced
in
1982
along
with
a
neural
network
capable
of
self-learning
named
crossbar
adaptive
array
(CAA).
It
is
a
system
with
only
one
input,
situation
s,
and
only
one
output,
action
(or
behavior)
a.
It
has
neither
external
advice
input
nor
external
reinforcement
input
from
the
environment.
The
CAA
computes,
in
a
crossbar
fashion,
both
decisions
about
actions
and
emotions
(feelings)
about
encountered
situations.
The
system
is
driven
by
the
interaction
between
cognition
and
emotion.
Given
the
memory
matrix,
W
=||w(a,s)||,
the
crossbar
self-learning
algorithm
in
each
iteration
performs
the
following
computation:
In
situation
s
perform
action
a;
Receive
consequence
situation
s';
Compute
emotion
of
being
in
consequence
situation
v(s');
Update
crossbar
memory
w'(a,s)
=
w(a,s)
+
v(s').
The
backpropagated
value
(secondary
reinforcement)
is
the
emotion
toward
the
consequence
situation.
The
CAA
exists
in
two
environments,
one
is
behavioral
environment
where
it
behaves,
and
the
other
is
genetic
environment,
where
from
it
initially
and
only
once
receives
initial
emotions
about
to
be
encountered
situations
in
the
behavioral
environment.
Having
received
the
genome
vector
(species
vector)
from
the
genetic
environment,
the
CAA
will
learn
a
goal-seeking
behavior,
in
the
behavioral
environment
that
contains
both
desirable
and
undesirable
situations.
====
Neuroevolution
====
Neuroevolution
can
create
neural
network
topologies
and
weights
using
evolutionary
computation.
It
is
competitive
with
sophisticated
gradient
descent
approaches.
One
advantage
of
neuroevolution
is
that
it
may
be
less
prone
to
get
caught
in
"dead
ends".
===
Stochastic
neural
network
===
Stochastic
neural
networks
originating
from
Sherrington–Kirkpatrick
models
are
a
type
of
artificial
neural
network
built
by
introducing
random
variations
into
the
network,
either
by
giving
the
network's
artificial
neurons
stochastic
transfer
functions,
or
by
giving
them
stochastic
weights.
This
makes
them
useful
tools
for
optimization
problems,
since
the
random
fluctuations
help
the
network
escape
from
local
minima.
Stochastic
neural
networks
trained
using
a
Bayesian
approach
are
known
as
Bayesian
neural
networks.
===
Other
===
In
a
Bayesian
framework,
a
distribution
over
the
set
of
allowed
models
is
chosen
to
minimize
the
cost.
Evolutionary
methods,
gene
expression
programming,
simulated
annealing,
expectation-maximization,
non-parametric
methods
and
particle
swarm
optimization
are
other
learning
algorithms.
Convergent
recursion
is
a
learning
algorithm
for
cerebellar
model
articulation
controller
(CMAC)
neural
networks.
====
Modes
====
Two
modes
of
learning
are
available:
stochastic
and
batch.
In
stochastic
learning,
each
input
creates
a
weight
adjustment.
In
batch
learning
weights
are
adjusted
based
on
a
batch
of
inputs,
accumulating
errors
over
the
batch.
Stochastic
learning
introduces
"noise"
into
the
process,
using
the
local
gradient
calculated
from
one
data
point;
this
reduces
the
chance
of
the
network
getting
stuck
in
local
minima.
However,
batch
learning
typically
yields
a
faster,
more
stable
descent
to
a
local
minimum,
since
each
update
is
performed
in
the
direction
of
the
batch's
average
error.
A
common
compromise
is
to
use
"mini-batches",
small
batches
with
samples
in
each
batch
selected
stochastically
from
the
entire
data
set.
==
Types
==
ANNs
have
evolved
into
a
broad
family
of
techniques
that
have
advanced
the
state
of
the
art
across
multiple
domains.
The
simplest
types
have
one
or
more
static
components,
including
number
of
units,
number
of
layers,
unit
weights
and
topology.
Dynamic
types
allow
one
or
more
of
these
to
evolve
via
learning.
The
latter
is
much
more
complicated
but
can
shorten
learning
periods
and
produce
better
results.
Some
types
allow/require
learning
to
be
"supervised"
by
the
operator,
while
others
operate
independently.
Some
types
operate
purely
in
hardware,
while
others
are
purely
software
and
run
on
general
purpose
computers.
Some
of
the
main
breakthroughs
include:
Convolutional
neural
networks
that
have
proven
particularly
successful
in
processing
visual
and
other
two-dimensional
data;
where
long
short-term
memory
avoids
the
vanishing
gradient
problem
and
can
handle
signals
that
have
a
mix
of
low
and
high
frequency
components
aiding
large-vocabulary
speech
recognition,
text-to-speech
synthesis,
and
photo-real
talking
heads;
Competitive
networks
such
as
generative
adversarial
networks
in
which
multiple
networks
(of
varying
structure)
compete
with
each
other,
on
tasks
such
as
winning
a
game
or
on
deceiving
the
opponent
about
the
authenticity
of
an
input.
==
Network
design
==
Using
artificial
neural
networks
requires
an
understanding
of
their
characteristics.
Choice
of
model:
This
depends
on
the
data
representation
and
the
application.
Model
parameters
include
the
number,
type,
and
connectedness
of
network
layers,
as
well
as
the
size
of
each
and
the
connection
type
(full,
pooling,
etc.
).
Overly
complex
models
learn
slowly.
Learning
algorithm:
Numerous
trade-offs
exist
between
learning
algorithms.
Almost
any
algorithm
will
work
well
with
the
correct
hyperparameters
for
training
on
a
particular
data
set.
However,
selecting
and
tuning
an
algorithm
for
training
on
unseen
data
requires
significant
experimentation.
Robustness:
If
the
model,
cost
function
and
learning
algorithm
are
selected
appropriately,
the
resulting
ANN
can
become
robust.Neural
architecture
search
(NAS)
uses
machine
learning
to
automate
ANN
design.
Various
approaches
to
NAS
have
designed
networks
that
compare
well
with
hand-designed
systems.
The
basic
search
algorithm
is
to
propose
a
candidate
model,
evaluate
it
against
a
dataset,
and
use
the
results
as
feedback
to
teach
the
NAS
network.
Available
systems
include
AutoML
and
AutoKeras.
scikit-learn
library
provides
functions
to
help
with
building
a
deep
network
from
scratch.
We
can
then
implement
a
deep
network
with
TensorFlow
or
Keras.
Hyperparameters
must
also
be
defined
as
part
of
the
design
(they
are
not
learned),
governing
matters
such
as
how
many
neurons
are
in
each
layer,
learning
rate,
step,
stride,
depth,
receptive
field
and
padding
(for
CNNs),
etc.
The
Python
code
snippet
provides
an
overview
of
the
training
function,
which
uses
the
training
dataset,
number
of
hidden
layer
units,
learning
rate,
and
number
of
iterations
as
parameters:
==
Applications
==
Because
of
their
ability
to
reproduce
and
model
nonlinear
processes,
artificial
neural
networks
have
found
applications
in
many
disciplines.
These
include:
Function
approximation,
or
regression
analysis,
(including
time
series
prediction,
fitness
approximation,
and
modeling)
Data
processing
(including
filtering,
clustering,
blind
source
separation,
and
compression)
Nonlinear
system
identification
and
control
(including
vehicle
control,
trajectory
prediction,
adaptive
control,
process
control,
and
natural
resource
management)
Pattern
recognition
(including
radar
systems,
face
identification,
signal
classification,
novelty
detection,
3D
reconstruction,
object
recognition,
and
sequential
decision
making)
Sequence
recognition
(including
gesture,
speech,
and
handwritten
and
printed
text
recognition)
Sensor
data
analysis
(including
image
analysis)
Robotics
(including
directing
manipulators
and
prostheses)
Data
mining
(including
knowledge
discovery
in
databases)
Finance
(such
as
ex-ante
models
for
specific
financial
long-run
forecasts
and
artificial
financial
markets)
Quantum
chemistry
General
game
playing
Generative
AI
Data
visualization
Machine
translation
Social
network
filtering
E-mail
spam
filtering
Medical
diagnosisANNs
have
been
used
to
diagnose
several
types
of
cancers
and
to
distinguish
highly
invasive
cancer
cell
lines
from
less
invasive
lines
using
only
cell
shape
information.ANNs
have
been
used
to
accelerate
reliability
analysis
of
infrastructures
subject
to
natural
disasters
and
to
predict
foundation
settlements.
It
can
also
be
useful
to
mitigate
flood
by
the
use
of
ANNs
for
modelling
rainfall-runoff.
ANNs
have
also
been
used
for
building
black-box
models
in
geoscience:
hydrology,
ocean
modelling
and
coastal
engineering,
and
geomorphology.
ANNs
have
been
employed
in
cybersecurity,
with
the
objective
to
discriminate
between
legitimate
activities
and
malicious
ones.
For
example,
machine
learning
has
been
used
for
classifying
Android
malware,
for
identifying
domains
belonging
to
threat
actors
and
for
detecting
URLs
posing
a
security
risk.
Research
is
underway
on
ANN
systems
designed
for
penetration
testing,
for
detecting
botnets,
credit
cards
frauds
and
network
intrusions.
ANNs
have
been
proposed
as
a
tool
to
solve
partial
differential
equations
in
physics
and
simulate
the
properties
of
many-body
open
quantum
systems.
In
brain
research
ANNs
have
studied
short-term
behavior
of
individual
neurons,
the
dynamics
of
neural
circuitry
arise
from
interactions
between
individual
neurons
and
how
behavior
can
arise
from
abstract
neural
modules
that
represent
complete
subsystems.
Studies
considered
long-and
short-term
plasticity
of
neural
systems
and
their
relation
to
learning
and
memory
from
the
individual
neuron
to
the
system
level.
It
is
possible
to
create
a
profile
of
a
user's
interests
from
pictures,
using
artificial
neural
networks
trained
for
object
recognition.Beyond
their
traditional
applications,
artificial
neural
networks
are
increasingly
being
utilized
in
interdisciplinary
research,
such
as
materials
science.
For
instance,
graph
neural
networks
(GNNs)
have
demonstrated
their
capability
in
scaling
deep
learning
for
the
discovery
of
new
stable
materials
by
efficiently
predicting
the
total
energy
of
crystals.
This
application
underscores
the
adaptability
and
potential
of
ANNs
in
tackling
complex
problems
beyond
the
realms
of
predictive
modeling
and
artificial
intelligence,
opening
new
pathways
for
scientific
discovery
and
innovation.
==
Theoretical
properties
==
===
Computational
power
===
The
multilayer
perceptron
is
a
universal
function
approximator,
as
proven
by
the
universal
approximation
theorem.
However,
the
proof
is
not
constructive
regarding
the
number
of
neurons
required,
the
network
topology,
the
weights
and
the
learning
parameters.
A
specific
recurrent
architecture
with
rational-valued
weights
(as
opposed
to
full
precision
real
number-valued
weights)
has
the
power
of
a
universal
Turing
machine,
using
a
finite
number
of
neurons
and
standard
linear
connections.
Further,
the
use
of
irrational
values
for
weights
results
in
a
machine
with
super-Turing
power.
===
Capacity
===
A
model's
"capacity"
property
corresponds
to
its
ability
to
model
any
given
function.
It
is
related
to
the
amount
of
information
that
can
be
stored
in
the
network
and
to
the
notion
of
complexity.
Two
notions
of
capacity
are
known
by
the
community.
The
information
capacity
and
the
VC
Dimension.
The
information
capacity
of
a
perceptron
is
intensively
discussed
in
Sir
David
MacKay's
book
which
summarizes
work
by
Thomas
Cover.
The
capacity
of
a
network
of
standard
neurons
(not
convolutional)
can
be
derived
by
four
rules
that
derive
from
understanding
a
neuron
as
an
electrical
element.
The
information
capacity
captures
the
functions
modelable
by
the
network
given
any
data
as
input.
The
second
notion,
is
the
VC
dimension.
VC
Dimension
uses
the
principles
of
measure
theory
and
finds
the
maximum
capacity
under
the
best
possible
circumstances.
This
is,
given
input
data
in
a
specific
form.
As
noted
in,
the
VC
Dimension
for
arbitrary
inputs
is
half
the
information
capacity
of
a
Perceptron.
The
VC
Dimension
for
arbitrary
points
is
sometimes
referred
to
as
Memory
Capacity.
===
Convergence
===
Models
may
not
consistently
converge
on
a
single
solution,
firstly
because
local
minima
may
exist,
depending
on
the
cost
function
and
the
model.
Secondly,
the
optimization
method
used
might
not
guarantee
to
converge
when
it
begins
far
from
any
local
minimum.
Thirdly,
for
sufficiently
large
data
or
parameters,
some
methods
become
impractical.
Another
issue
worthy
to
mention
is
that
training
may
cross
some
Saddle
point
which
may
lead
the
convergence
to
the
wrong
direction.
The
convergence
behavior
of
certain
types
of
ANN
architectures
are
more
understood
than
others.
When
the
width
of
network
approaches
to
infinity,
the
ANN
is
well
described
by
its
first
order
Taylor
expansion
throughout
training,
and
so
inherits
the
convergence
behavior
of
affine
models.
Another
example
is
when
parameters
are
small,
it
is
observed
that
ANNs
often
fits
target
functions
from
low
to
high
frequencies.
This
behavior
is
referred
to
as
the
spectral
bias,
or
frequency
principle,
of
neural
networks.
This
phenomenon
is
the
opposite
to
the
behavior
of
some
well
studied
iterative
numerical
schemes
such
as
Jacobi
method.
Deeper
neural
networks
have
been
observed
to
be
more
biased
towards
low
frequency
functions.
===
Generalization
and
statistics
===
Applications
whose
goal
is
to
create
a
system
that
generalizes
well
to
unseen
examples,
face
the
possibility
of
over-training.
This
arises
in
convoluted
or
over-specified
systems
when
the
network
capacity
significantly
exceeds
the
needed
free
parameters.
Two
approaches
address
over-training.
The
first
is
to
use
cross-validation
and
similar
techniques
to
check
for
the
presence
of
over-training
and
to
select
hyperparameters
to
minimize
the
generalization
error.
The
second
is
to
use
some
form
of
regularization.
This
concept
emerges
in
a
probabilistic
(Bayesian)
framework,
where
regularization
can
be
performed
by
selecting
a
larger
prior
probability
over
simpler
models;
but
also
in
statistical
learning
theory,
where
the
goal
is
to
minimize
over
two
quantities:
the
'empirical
risk'
and
the
'structural
risk',
which
roughly
corresponds
to
the
error
over
the
training
set
and
the
predicted
error
in
unseen
data
due
to
overfitting.
Supervised
neural
networks
that
use
a
mean
squared
error
(MSE)
cost
function
can
use
formal
statistical
methods
to
determine
the
confidence
of
the
trained
model.
The
MSE
on
a
validation
set
can
be
used
as
an
estimate
for
variance.
This
value
can
then
be
used
to
calculate
the
confidence
interval
of
network
output,
assuming
a
normal
distribution.
A
confidence
analysis
made
this
way
is
statistically
valid
as
long
as
the
output
probability
distribution
stays
the
same
and
the
network
is
not
modified.
By
assigning
a
softmax
activation
function,
a
generalization
of
the
logistic
function,
on
the
output
layer
of
the
neural
network
(or
a
softmax
component
in
a
component-based
network)
for
categorical
target
variables,
the
outputs
can
be
interpreted
as
posterior
probabilities.
This
is
useful
in
classification
as
it
gives
a
certainty
measure
on
classifications.
The
softmax
activation
function
is:
yi=exi∑j=1cexj{\displaystyle
y_{i}={\frac
{e^{x_{i}}}{\sum
_{j=1}^{c}e^{x_{j}}}}}
==
Criticism
==
===
Training
===
A
common
criticism
of
neural
networks,
particularly
in
robotics,
is
that
they
require
too
many
training
samples
for
real-world
operation.
Any
learning
machine
needs
sufficient
representative
examples
in
order
to
capture
the
underlying
structure
that
allows
it
to
generalize
to
new
cases.
Potential
solutions
include
randomly
shuffling
training
examples,
by
using
a
numerical
optimization
algorithm
that
does
not
take
too
large
steps
when
changing
the
network
connections
following
an
example,
grouping
examples
in
so-called
mini-batches
and/or
introducing
a
recursive
least
squares
algorithm
for
CMAC.
Dean
Pomerleau
uses
a
neural
network
to
train
a
robotic
vehicle
to
drive
on
multiple
types
of
roads
(single
lane,
multi-lane,
dirt,
etc.),
and
a
large
amount
of
his
research
is
devoted
to
extrapolating
multiple
training
scenarios
from
a
single
training
experience,
and
preserving
past
training
diversity
so
that
the
system
does
not
become
overtrained
(if,
for
example,
it
is
presented
with
a
series
of
right
turns—it
should
not
learn
to
always
turn
right).
===
Theory
===
A
central
claim
of
ANNs
is
that
they
embody
new
and
powerful
general
principles
for
processing
information.
These
principles
are
ill-defined.
It
is
often
claimed
that
they
are
emergent
from
the
network
itself.
This
allows
simple
statistical
association
(the
basic
function
of
artificial
neural
networks)
to
be
described
as
learning
or
recognition.
In
1997,
Alexander
Dewdney,
a
former
Scientific
American
columnist,
commented
that
as
a
result,
artificial
neural
networks
have
a
"something-for-nothing
quality,
one
that
imparts
a
peculiar
aura
of
laziness
and
a
distinct
lack
of
curiosity
about
just
how
good
these
computing
systems
are.
No
human
hand
(or
mind)
intervenes;
solutions
are
found
as
if
by
magic;
and
no
one,
it
seems,
has
learned
anything".
One
response
to
Dewdney
is
that
neural
networks
have
been
successfully
used
to
handle
many
complex
and
diverse
tasks,
ranging
from
autonomously
flying
aircraft
to
detecting
credit
card
fraud
to
mastering
the
game
of
Go.
Technology
writer
Roger
Bridgman
commented:
Neural
networks,
for
instance,
are
in
the
dock
not
only
because
they
have
been
hyped
to
high
heaven,
(what
hasn't?)
but
also
because
you
could
create
a
successful
net
without
understanding
how
it
worked:
the
bunch
of
numbers
that
captures
its
behaviour
would
in
all
probability
be
"an
opaque,
unreadable
table...valueless
as
a
scientific
resource".
In
spite
of
his
emphatic
declaration
that
science
is
not
technology,
Dewdney
seems
here
to
pillory
neural
nets
as
bad
science
when
most
of
those
devising
them
are
just
trying
to
be
good
engineers.
An
unreadable
table
that
a
useful
machine
could
read
would
still
be
well
worth
having.
Although
it
is
true
that
analyzing
what
has
been
learned
by
an
artificial
neural
network
is
difficult,
it
is
much
easier
to
do
so
than
to
analyze
what
has
been
learned
by
a
biological
neural
network.
Moreover,
recent
emphasis
on
the
explainability
of
AI
has
contributed
towards
the
development
of
methods,
notably
those
based
on
attention
mechanisms,
for
visualizing
and
explaining
learned
neural
networks.
Furthermore,
researchers
involved
in
exploring
learning
algorithms
for
neural
networks
are
gradually
uncovering
generic
principles
that
allow
a
learning
machine
to
be
successful.
For
example,
Bengio
and
LeCun
(2007)
wrote
an
article
regarding
local
vs
non-local
learning,
as
well
as
shallow
vs
deep
architecture.Biological
brains
use
both
shallow
and
deep
circuits
as
reported
by
brain
anatomy,
displaying
a
wide
variety
of
invariance.
Weng
argued
that
the
brain
self-wires
largely
according
to
signal
statistics
and
therefore,
a
serial
cascade
cannot
catch
all
major
statistical
dependencies.
===
Hardware
===
Large
and
effective
neural
networks
require
considerable
computing
resources.
While
the
brain
has
hardware
tailored
to
the
task
of
processing
signals
through
a
graph
of
neurons,
simulating
even
a
simplified
neuron
on
von
Neumann
architecture
may
consume
vast
amounts
of
memory
and
storage.
Furthermore,
the
designer
often
needs
to
transmit
signals
through
many
of
these
connections
and
their
associated
neurons
–
which
require
enormous
CPU
power
and
time.
Schmidhuber
noted
that
the
resurgence
of
neural
networks
in
the
twenty-first
century
is
largely
attributable
to
advances
in
hardware:
from
1991
to
2015,
computing
power,
especially
as
delivered
by
GPGPUs
(on
GPUs),
has
increased
around
a
million-fold,
making
the
standard
backpropagation
algorithm
feasible
for
training
networks
that
are
several
layers
deeper
than
before.
The
use
of
accelerators
such
as
FPGAs
and
GPUs
can
reduce
training
times
from
months
to
days.Neuromorphic
engineering
or
a
physical
neural
network
addresses
the
hardware
difficulty
directly,
by
constructing
non-von-Neumann
chips
to
directly
implement
neural
networks
in
circuitry.
Another
type
of
chip
optimized
for
neural
network
processing
is
called
a
Tensor
Processing
Unit,
or
TPU.
===
Practical
counterexamples
===
Analyzing
what
has
been
learned
by
an
ANN
is
much
easier
than
analyzing
what
has
been
learned
by
a
biological
neural
network.
Furthermore,
researchers
involved
in
exploring
learning
algorithms
for
neural
networks
are
gradually
uncovering
general
principles
that
allow
a
learning
machine
to
be
successful.
For
example,
local
vs.
non-local
learning
and
shallow
vs.
deep
architecture.
===
Hybrid
approaches
===
Advocates
of
hybrid
models
(combining
neural
networks
and
symbolic
approaches)
say
that
such
a
mixture
can
better
capture
the
mechanisms
of
the
human
mind.
===
Dataset
bias
===
Neural
networks
are
dependent
on
the
quality
of
the
data
they
are
trained
on,
thus
low
quality
data
with
imbalanced
representativeness
can
lead
to
the
model
learning
and
perpetuating
societal
biases.
These
inherited
biases
become
especially
critical
when
the
ANNs
are
integrated
into
real-world
scenarios
where
the
training
data
may
be
imbalanced
due
to
the
scarcity
of
data
for
a
specific
race,
gender
or
other
attribute.
This
imbalance
can
result
in
the
model
having
inadequate
representation
and
understanding
of
underrepresented
groups,
leading
to
discriminatory
outcomes
that
exasperate
societal
inequalities,
especially
in
applications
like
facial
recognition,
hiring
processes,
and
law
enforcement.
For
example,
in
2018,
Amazon
had
to
scrap
a
recruiting
tool
because
the
model
favored
men
over
women
for
jobs
in
software
engineering
due
to
the
higher
number
of
male
workers
in
the
field.
The
program
would
penalize
any
resume
with
the
word
"woman"
or
the
name
of
any
women's
college.
However,
the
use
of
synthetic
data
can
help
reduce
dataset
bias
and
increase
representation
in
datasets.
==
Gallery
==
==
Recent
advancements
and
future
directions
==
Artificial
neural
networks
(ANNs)
have
undergone
significant
advancements,
particularly
in
their
ability
to
model
complex
systems,
handle
large
data
sets,
and
adapt
to
various
types
of
applications.
Their
evolution
over
the
past
few
decades
has
been
marked
by
a
broad
range
of
applications
in
fields
such
as
image
processing,
speech
recognition,
natural
language
processing,
finance,
and
medicine.
===
Image
processing
===
In
the
realm
of
image
processing,
ANNs
are
employed
in
tasks
such
as
image
classification,
object
recognition,
and
image
segmentation.
For
instance,
deep
convolutional
neural
networks
(CNNs)
have
been
important
in
handwritten
digit
recognition,
achieving
state-of-the-art
performance.
This
demonstrates
the
ability
of
ANNs
to
effectively
process
and
interpret
complex
visual
information,
leading
to
advancements
in
fields
ranging
from
automated
surveillance
to
medical
imaging.
===
Speech
recognition
===
By
modeling
speech
signals,
ANNs
are
used
for
tasks
like
speaker
identification
and
speech-to-text
conversion.
Deep
neural
network
architectures
have
introduced
significant
improvements
in
large
vocabulary
continuous
speech
recognition,
outperforming
traditional
techniques.
These
advancements
have
enabled
the
development
of
more
accurate
and
efficient
voice-activated
systems,
enhancing
user
interfaces
in
technology
products.
===
Natural
language
processing
===
In
natural
language
processing,
ANNs
are
used
for
tasks
such
as
text
classification,
sentiment
analysis,
and
machine
translation.
They
have
enabled
the
development
of
models
that
can
accurately
translate
between
languages,
understand
the
context
and
sentiment
in
textual
data,
and
categorize
text
based
on
content.
This
has
implications
for
automated
customer
service,
content
moderation,
and
language
understanding
technologies.
===
Control
systems
===
In
the
domain
of
control
systems,
ANNs
are
used
to
model
dynamic
systems
for
tasks
such
as
system
identification,
control
design,
and
optimization.
For
instance,
deep
feedforward
neural
networks
are
important
in
system
identification
and
control
applications.
===
Finance
===
ANNs
are
used
for
stock
market
prediction
and
credit
scoring:
In
investing,
ANNs
can
process
vast
amounts
of
financial
data,
recognize
complex
patterns,
and
forecast
stock
market
trends,
aiding
investors
and
risk
managers
in
making
informed
decisions.
In
credit
scoring,
ANNs
offer
data-driven,
personalized
assessments
of
creditworthiness,
improving
the
accuracy
of
default
predictions
and
automating
the
lending
process.ANNs
require
high-quality
data
and
careful
tuning,
and
their
"black-box"
nature
can
pose
challenges
in
interpretation.
Nevertheless,
ongoing
advancements
suggest
that
ANNs
continue
to
play
a
role
in
finance,
offering
valuable
insights
and
enhancing
risk
management
strategies.
===
Medicine
===
ANNs
are
able
to
process
and
analyze
vast
medical
datasets.
They
enhance
diagnostic
accuracy,
especially
by
interpreting
complex
medical
imaging
for
early
disease
detection,
and
by
predicting
patient
outcomes
for
personalized
treatment
planning.
In
drug
discovery,
ANNs
speed
up
the
identification
of
potential
drug
candidates
and
predict
their
efficacy
and
safety,
significantly
reducing
development
time
and
costs.
Additionally,
their
application
in
personalized
medicine
and
healthcare
data
analysis
allows
tailored
therapies
and
efficient
patient
care
management.
Ongoing
research
is
aimed
at
addressing
remaining
challenges
such
as
data
privacy
and
model
interpretability,
as
well
as
expanding
the
scope
of
ANN
applications
in
medicine.
===
Content
creation
===
ANNs
such
as
generative
adversarial
networks
(GAN)
and
transformers
are
used
for
content
creation
across
numerous
industries.
This
is
because
deep
learning
models
are
able
to
learn
the
style
of
an
artist
or
musician
from
huge
datasets
and
generate
completely
new
artworks
and
music
compositions.
For
instance,
DALL-E
is
a
deep
neural
network
trained
on
650
million
pairs
of
images
and
texts
across
the
internet
that
can
create
artworks
based
on
text
entered
by
the
user.
In
the
field
of
music,
transformers
are
used
to
create
original
music
for
commercials
and
documentaries
through
companies
such
as
AIVA
and
Jukedeck.
In
the
marketing
industry
generative
models
are
used
to
create
personalized
advertisements
for
consumers.
Additionally,
major
film
companies
are
partnering
with
technology
companies
to
analyze
the
financial
success
of
a
film,
such
as
the
partnership
between
Warner
Bros
and
technology
company
Cinelytic
established
in
2020.
Furthermore,
neural
networks
have
found
uses
in
video
game
creation,
where
Non
Player
Characters
(NPCs)
can
make
decisions
based
on
all
the
characters
currently
in
the
game.
==
See
also
==
==
External
links
==
A
Brief
Introduction
to
Neural
Networks
(D.
Kriesel)
-
Illustrated,
bilingual
manuscript
about
artificial
neural
networks;
Topics
so
far:
Perceptrons,
Backpropagation,
Radial
Basis
Functions,
Recurrent
Neural
Networks,
Self
Organizing
Maps,
Hopfield
Networks.
Review
of
Neural
Networks
in
Materials
Science
Artificial
Neural
Networks
Tutorial
in
three
languages
(Univ.
Politécnica
de
Madrid)
Another
introduction
to
ANN
Next
Generation
of
Neural
Networks
-
Google
Tech
Talks
Performance
of
Neural
Networks
Neural
Networks
and
Information
Sanderson
G
(5
October
2017).
"But
what
is
a
Neural
Network?".
3Blue1Brown.
Archived
from
the
original
on
7
November
2021
–
via
YouTube.
==
Notes
==
==
References
==
==
Bibliography
==
A
neural
network
is
a
group
of
interconnected
units
called
neurons
that
send
signals
to
one
another.
Neurons
can
be
either
biological
cells
or
mathematical
models.
While
individual
neurons
are
simple,
many
of
them
together
in
a
network
can
perform
complex
tasks.
There
are
two
main
types
of
neural
network.
In
neuroscience,
a
biological
neural
network
is
a
physical
structure
found
in
brains
and
complex
nervous
systems
–
a
population
of
nerve
cells
connected
by
synapses.
In
machine
learning,
an
artificial
neural
network
is
a
mathematical
model
used
to
approximate
nonlinear
functions.
Artificial
neural
networks
are
used
to
solve
artificial
intelligence
problems.
==
Biological
neural
network
==
A
biological
neural
network
is
a
population
of
biological
neurons
chemically
connected
to
each
other
by
synapses.
A
given
neuron
can
be
connected
to
hundreds
of
thousands
of
synapses.
Each
neuron
sends
and
receives
electrochemical
signals
called
action
potentials
to
its
connected
neighbors.
A
neuron
can
serve
an
excitatory
role,
amplifying
and
propagating
signals
it
receives,
or
an
inhibitory
role,
suppressing
signals
instead.Populations
of
interconnected
neurons
that
are
smaller
than
neural
networks
are
called
neural
circuits.
Very
large
interconnected
networks
are
called
large
scale
brain
networks,
and
many
of
these
together
form
brains
and
nervous
systems.
Signals
generated
by
neural
networks
in
the
brain
eventually
travel
through
the
nervous
system
and
across
neuromuscular
junctions
to
muscle
cells,
where
they
cause
contraction
and
thereby
motion.
==
Artificial
neural
network
==
An
artificial
neural
network
is
a
mathematical
model
used
to
approximate
nonlinear
functions.
While
early
artificial
neural
networks
were
physical
machines,
today
they
are
almost
always
implemented
in
software.
Neurons
in
an
artificial
neural
network
are
usually
arranged
into
layers,
with
information
passing
from
the
first
layer
(the
input
layer)
through
one
or
more
intermediate
layers
(hidden
layers)
to
the
final
layer
(the
output
layer).
The
"signal"
input
to
each
neuron
is
a
number,
specifically
a
linear
combination
of
the
outputs
of
the
connected
neurons
in
the
previous
layer.
The
signal
each
neuron
outputs
is
calculated
from
this
number,
according
to
its
activation
function.
The
behavior
of
the
network
depends
on
the
strengths
(or
weights)
of
the
connections
between
neurons.
A
network
is
trained
by
modifying
these
weights
through
empirical
risk
minimization
or
backpropagation
in
order
to
fit
some
preexisting
dataset.Neural
networks
are
used
to
solve
problems
in
artificial
intelligence,
and
have
thereby
found
applications
in
many
disciplines,
including
predictive
modeling,
adaptive
control,
facial
recognition,
handwriting
recognition,
general
game
playing,
and
generative
AI.
==
History
==
The
theoretical
base
for
contemporary
neural
networks
was
independently
proposed
by
Alexander
Bain
in
1873
and
William
James
in
1890.
Both
posited
that
human
thought
emerged
from
interactions
among
large
numbers
of
neurons
inside
the
brain.
In
1949,
Donald
Hebb
described
Hebbian
learning,
the
idea
that
neural
networks
can
change
and
learn
over
time
by
strengthening
a
synapse
every
time
a
signal
travels
along
it.Artificial
neural
networks
were
originally
used
to
model
biological
neural
networks
starting
in
the
1930s
under
the
approach
of
connectionism.
However,
starting
with
the
invention
of
the
perceptron,
a
simple
artificial
neural
network,
by
Warren
McCulloch
and
Walter
Pitts
in
1943,
followed
by
the
implementation
of
one
in
hardware
by
Frank
Rosenblatt
in
1957,
artificial
neural
networks
became
increasingly
used
for
machine
learning
applications
instead,
and
increasingly
different
from
their
biological
counterparts.
==
See
also
==
Emergence
Biological
cybernetics
Biologically-inspired
computing
==
References
==
An
optical
neural
network
is
a
physical
implementation
of
an
artificial
neural
network
with
optical
components.
Early
optical
neural
networks
used
a
photorefractive
Volume
hologram
to
interconnect
arrays
of
input
neurons
to
arrays
of
output
with
synaptic
weights
in
proportion
to
the
multiplexed
hologram's
strength.
Volume
holograms
were
further
multiplexed
using
spectral
hole
burning
to
add
one
dimension
of
wavelength
to
space
to
achieve
four
dimensional
interconnects
of
two
dimensional
arrays
of
neural
inputs
and
outputs.
This
research
led
to
extensive
research
on
alternative
methods
using
the
strength
of
the
optical
interconnect
for
implementing
neuronal
communications.Some
artificial
neural
networks
that
have
been
implemented
as
optical
neural
networks
include
the
Hopfield
neural
network
and
the
Kohonen
self-organizing
map
with
liquid
crystal
spatial
light
modulators
Optical
neural
networks
can
also
be
based
on
the
principles
of
neuromorphic
engineering,
creating
neuromorphic
photonic
systems.
Typically,
these
systems
encode
information
in
the
networks
using
spikes,
mimicking
the
functionality
of
spiking
neural
networks
in
optical
and
photonic
hardware.
Photonic
devices
that
have
demonstrated
neuromorphic
functionalities
include
(among
others)
vertical-cavity
surface-emitting
lasers,
integrated
photonic
modulators,
optoelectronic
systems
based
on
superconducting
Josephson
junctions
or
systems
based
on
resonant
tunnelling
diodes.
==
Electrochemical
vs.
optical
neural
networks
==
Biological
neural
networks
function
on
an
electrochemical
basis,
while
optical
neural
networks
use
electromagnetic
waves.
Optical
interfaces
to
biological
neural
networks
can
be
created
with
optogenetics,
but
is
not
the
same
as
an
optical
neural
networks.
In
biological
neural
networks
there
exist
a
lot
of
different
mechanisms
for
dynamically
changing
the
state
of
the
neurons,
these
include
short-term
and
long-term
synaptic
plasticity.
Synaptic
plasticity
is
among
the
electrophysiological
phenomena
used
to
control
the
efficiency
of
synaptic
transmission,
long-term
for
learning
and
memory,
and
short-term
for
short
transient
changes
in
synaptic
transmission
efficiency.
Implementing
this
with
optical
components
is
difficult,
and
ideally
requires
advanced
photonic
materials.
Properties
that
might
be
desirable
in
photonic
materials
for
optical
neural
networks
include
the
ability
to
change
their
efficiency
of
transmitting
light,
based
on
the
intensity
of
incoming
light.
==
Rising
Era
of
Optical
Neural
Networks
==
With
the
increasing
significance
of
computer
vision
in
various
domains,
the
computational
cost
of
these
tasks
has
increased,
making
it
more
important
to
develop
the
new
approaches
of
the
processing
acceleration.
Optical
computing
has
emerged
as
a
potential
alternative
to
GPU
acceleration
for
modern
neural
networks,
particularly
considering
the
looming
obsolescence
of
Moore's
Law.
Consequently,
optical
neural
networks
have
garnered
increased
attention
in
the
research
community.
Presently,
two
primary
methods
of
optical
neural
computing
are
under
research:
silicon
photonics-based
and
free-space
optics.
Each
approach
has
its
benefits
and
drawbacks;
while
silicon
photonics
may
offer
superior
speed,
it
lacks
the
massive
parallelism
that
free-space
optics
can
deliver.
Given
the
substantial
parallelism
capabilities
of
free-space
optics,
researchers
have
focused
on
taking
advantage
of
it.
One
implementation,
proposed
by
Lin
et
al.,
involves
the
training
and
fabrication
of
phase
masks
for
a
handwritten
digit
classifier.
By
stacking
3D-printed
phase
masks,
light
passing
through
the
fabricated
network
can
be
read
by
a
photodetector
array
of
ten
detectors,
each
representing
a
digit
class
ranging
from
1
to
10.
Although
this
network
can
achieve
terahertz-range
classification,
it
lacks
flexibility,
as
the
phase
masks
are
fabricated
for
a
specific
task
and
cannot
be
retrained.
An
alternative
method
for
classification
in
free-space
optics,
introduced
by
Cahng
et
al.,
employs
a
4F
system
that
is
based
on
the
convolution
theorem
to
perform
convolution
operations.
This
system
uses
two
lenses
to
execute
the
Fourier
transforms
of
the
convolution
operation,
enabling
passive
conversion
into
the
Fourier
domain
without
power
consumption
or
latency.
However,
the
convolution
operation
kernels
in
this
implementation
are
also
fabricated
phase
masks,
limiting
the
device's
functionality
to
specific
convolutional
layers
of
the
network
only.
In
contrast,
Li
et
al.
proposed
a
technique
involving
kernel
tiling
to
use
the
parallelism
of
the
4F
system
while
using
a
Digital
Micromirror
Device
(DMD)
instead
of
a
phase
mask.
This
approach
allows
users
to
upload
various
kernels
into
the
4F
system
and
execute
the
entire
network's
inference
on
a
single
device.
Unfortunately,
modern
neural
networks
are
not
designed
for
the
4F
systems,
as
they
were
primarily
developed
during
the
CPU/GPU
era.
Mostly
because
they
tend
to
use
a
lower
resolution
and
a
high
number
of
channels
in
their
feature
maps.
==
Other
Implementations
==
In
2007
there
was
one
model
of
Optical
Neural
Network:
the
Programmable
Optical
Array/Analogic
Computer
(POAC).
It
had
been
implemented
in
the
year
2000
and
reported
based
on
modified
Joint
Fourier
Transform
Correlator
(JTC)
and
Bacteriorhodopsin
(BR)
as
a
holographic
optical
memory.
Full
parallelism,
large
array
size
and
the
speed
of
light
are
three
promises
offered
by
POAC
to
implement
an
optical
CNN.
They
had
been
investigated
during
the
last
years
with
their
practical
limitations
and
considerations
yielding
the
design
of
the
first
portable
POAC
version.
The
practical
details
–
hardware
(optical
setups)
and
software
(optical
templates)
–
were
published.
However,
POAC
is
a
general
purpose
and
programmable
array
computer
that
has
a
wide
range
of
applications
including:
image
processing
pattern
recognition
target
tracking
real-time
video
processing
document
security
optical
switching
==
See
also
==
Optical
computing
Quantum
neural
network
==
References
==
Physics-informed
neural
networks
(PINNs)
are
a
type
of
universal
function
approximators
that
can
embed
the
knowledge
of
any
physical
laws
that
govern
a
given
data-set
in
the
learning
process,
and
can
be
described
by
partial
differential
equations
(PDEs).
They
overcome
the
low
data
availability
of
some
biological
and
engineering
systems
that
makes
most
state-of-the-art
machine
learning
techniques
lack
robustness,
rendering
them
ineffective
in
these
scenarios.
The
prior
knowledge
of
general
physical
laws
acts
in
the
training
of
neural
networks
(NNs)
as
a
regularization
agent
that
limits
the
space
of
admissible
solutions,
increasing
the
correctness
of
the
function
approximation.
This
way,
embedding
this
prior
information
into
a
neural
network
results
in
enhancing
the
information
content
of
the
available
data,
facilitating
the
learning
algorithm
to
capture
the
right
solution
and
to
generalize
well
even
with
a
low
amount
of
training
examples.
==
Function
approximation
==
Most
of
the
physical
laws
that
govern
the
dynamics
of
a
system
can
be
described
by
partial
differential
equations.
For
example,
the
Navier–Stokes
equations
are
a
set
of
partial
differential
equations
derived
from
the
conservation
laws
(i.e.,
conservation
of
mass,
momentum,
and
energy)
that
govern
fluid
mechanics.
The
solution
of
the
Navier–Stokes
equations
with
appropriate
initial
and
boundary
conditions
allows
the
quantification
of
flow
dynamics
in
a
precisely
defined
geometry.
However,
these
equations
cannot
be
solved
exactly
and
therefore
numerical
methods
must
be
used
(such
as
finite
differences,
finite
elements
and
finite
volumes).
In
this
setting,
these
governing
equations
must
be
solved
while
accounting
for
prior
assumptions,
linearization,
and
adequate
time
and
space
discretization.
Recently,
solving
the
governing
partial
differential
equations
of
physical
phenomena
using
deep
learning
has
emerged
as
a
new
field
of
scientific
machine
learning
(SciML),
leveraging
the
universal
approximation
theorem
and
high
expressivity
of
neural
networks.
In
general,
deep
neural
networks
could
approximate
any
high-dimensional
function
given
that
sufficient
training
data
are
supplied.
However,
such
networks
do
not
consider
the
physical
characteristics
underlying
the
problem,
and
the
level
of
approximation
accuracy
provided
by
them
is
still
heavily
dependent
on
careful
specifications
of
the
problem
geometry
as
well
as
the
initial
and
boundary
conditions.
Without
this
preliminary
information,
the
solution
is
not
unique
and
may
lose
physical
correctness.
On
the
other
hand,
physics-informed
neural
networks
(PINNs)
leverage
governing
physical
equations
in
neural
network
training.
Namely,
PINNs
are
designed
to
be
trained
to
satisfy
the
given
training
data
as
well
as
the
imposed
governing
equations.
In
this
fashion,
a
neural
network
can
be
guided
with
training
data
that
do
not
necessarily
need
to
be
large
and
complete.
Potentially,
an
accurate
solution
of
partial
differential
equations
can
be
found
without
knowing
the
boundary
conditions.
Therefore,
with
some
knowledge
about
the
physical
characteristics
of
the
problem
and
some
form
of
training
data
(even
sparse
and
incomplete),
PINN
may
be
used
for
finding
an
optimal
solution
with
high
fidelity.
PINNs
allow
for
addressing
a
wide
range
of
problems
in
computational
science
and
represent
a
pioneering
technology
leading
to
the
development
of
new
classes
of
numerical
solvers
for
PDEs.
PINNs
can
be
thought
of
as
a
meshfree
alternative
to
traditional
approaches
(e.g.,
CFD
for
fluid
dynamics),
and
new
data-driven
approaches
for
model
inversion
and
system
identification.
Notably,
the
trained
PINN
network
can
be
used
for
predicting
the
values
on
simulation
grids
of
different
resolutions
without
the
need
to
be
retrained.
In
addition,
they
allow
for
exploiting
automatic
differentiation
(AD)
to
compute
the
required
derivatives
in
the
partial
differential
equations,
a
new
class
of
differentiation
techniques
widely
used
to
derive
neural
networks
assessed
to
be
superior
to
numerical
or
symbolic
differentiation.
==
Modeling
and
computation
==
A
general
nonlinear
partial
differential
equation
can
be:
ut+N[u;λ]=0,x∈Ω,t∈[0,T]{\displaystyle
u_{t}+N[u;\lambda
]=0,\quad
x\in
\Omega
,\quad
t\in
[0,T]}
where
u(t,x){\displaystyle
u(t,x)}
denotes
the
solution,
N[⋅;λ]{\displaystyle
N[\cdot
;\lambda
]}
is
a
nonlinear
operator
parametrized
by
λ{\displaystyle
\lambda
},
and
Ω{\displaystyle
\Omega
}
is
a
subset
of
RD{\displaystyle
\mathbb
{R}
^{D}}.
This
general
form
of
governing
equations
summarizes
a
wide
range
of
problems
in
mathematical
physics,
such
as
conservative
laws,
diffusion
process,
advection-diffusion
systems,
and
kinetic
equations.
Given
noisy
measurements
of
a
generic
dynamic
system
described
by
the
equation
above,
PINNs
can
be
designed
to
solve
two
classes
of
problems:
data-driven
solution
data-driven
discoveryof
partial
differential
equations.
===
Data-driven
solution
of
partial
differential
equations
===
The
data-driven
solution
of
PDE
computes
the
hidden
state
u(t,x){\displaystyle
u(t,x)}
of
the
system
given
boundary
data
and/or
measurements
z{\displaystyle
z},
and
fixed
model
parameters
λ{\displaystyle
\lambda
}.
We
solve:
ut+N[u]=0,x∈Ω,t∈[0,T]{\displaystyle
u_{t}+N[u]=0,\quad
x\in
\Omega
,\quad
t\in
[0,T]}.
By
defining
the
residual
f(t,x){\displaystyle
f(t,x)}
as
f:=ut+N[u]=0{\displaystyle
f:=u_{t}+N[u]=0},
and
approximating
u(t,x){\displaystyle
u(t,x)}
by
a
deep
neural
network.
This
network
can
be
differentiated
using
automatic
differentiation.
The
parameters
of
u(t,x){\displaystyle
u(t,x)}
and
f(t,x){\displaystyle
f(t,x)}
can
be
then
learned
by
minimizing
the
following
loss
function
Ltot{\displaystyle
L_{tot}}:
Ltot=Lu+Lf{\displaystyle
L_{tot}=L_{u}+L_{f}}.
Where
Lu=‖u−z‖Γ{\displaystyle
L_{u}=\Vert
u-z\Vert
_{\Gamma
}}
is
the
error
between
the
PINN
u(t,x){\displaystyle
u(t,x)}
and
the
set
of
boundary
conditions
and
measured
data
on
the
set
of
points
Γ{\displaystyle
\Gamma
}
where
the
boundary
conditions
and
data
are
defined,
and
Lf=‖f‖Γ{\displaystyle
L_{f}=\Vert
f\Vert
_{\Gamma
}}
is
the
mean-squared
error
of
the
residual
function.
This
second
term
encourages
the
PINN
to
learn
the
structural
information
expressed
by
the
partial
differential
equation
during
the
training
process.
This
approach
has
been
used
to
yield
computationally
efficient
physics-informed
surrogate
models
with
applications
in
the
forecasting
of
physical
processes,
model
predictive
control,
multi-physics
and
multi-scale
modeling,
and
simulation.
It
has
been
shown
to
converge
to
the
solution
of
the
PDE.
===
Data-driven
discovery
of
partial
differential
equations
===
Given
noisy
and
incomplete
measurements
z{\displaystyle
z}
of
the
state
of
the
system,
the
data-driven
discovery
of
PDE
results
in
computing
the
unknown
state
u(t,x){\displaystyle
u(t,x)}
and
learning
model
parameters
λ{\displaystyle
\lambda
}
that
best
describe
the
observed
data
and
it
reads
as
follows:
ut+N[u;λ]=0,x∈Ω,t∈[0,T]{\displaystyle
u_{t}+N[u;\lambda
]=0,\quad
x\in
\Omega
,\quad
t\in
[0,T]}.
By
defining
f(t,x){\displaystyle
f(t,x)}
as
f:=ut+N[u;λ]=0{\displaystyle
f:=u_{t}+N[u;\lambda
]=0},
and
approximating
u(t,x){\displaystyle
u(t,x)}
by
a
deep
neural
network,
f(t,x){\displaystyle
f(t,x)}
results
in
a
PINN.
This
network
can
be
derived
using
automatic
differentiation.
The
parameters
of
u(t,x){\displaystyle
u(t,x)}
and
f(t,x){\displaystyle
f(t,x)},
together
with
the
parameter
λ{\displaystyle
\lambda
}
of
the
differential
operator
can
be
then
learned
by
minimizing
the
following
loss
function
Ltot{\displaystyle
L_{tot}}:
Ltot=Lu+Lf{\displaystyle
L_{tot}=L_{u}+L_{f}}.
Where
Lu=‖u−z‖Γ{\displaystyle
L_{u}=\Vert
u-z\Vert
_{\Gamma
}},
with
u{\displaystyle
u}
and
z{\displaystyle
z}
state
solutions
and
measurements
at
sparse
location
Γ{\displaystyle
\Gamma
},
respectively
and
Lf=‖f‖Γ{\displaystyle
L_{f}=\Vert
f\Vert
_{\Gamma
}}
residual
function.
This
second
term
requires
the
structured
information
represented
by
the
partial
differential
equations
to
be
satisfied
in
the
training
process.
This
strategy
allows
for
discovering
dynamic
models
described
by
nonlinear
PDEs
assembling
computationally
efficient
and
fully
differentiable
surrogate
models
that
may
find
application
in
predictive
forecasting,
control,
and
data
assimilation.
==
Physics-informed
neural
networks
for
piece-wise
function
approximation
==
PINN
is
unable
to
approximate
PDEs
that
have
strong
non-linearity
or
sharp
gradients
that
commonly
occur
in
practical
fluid
flow
problems.
Piece-wise
approximation
has
been
an
old
practice
in
the
field
of
numerical
approximation.
With
the
capability
of
approximating
strong
non-linearity
extremely
light
weight
PINNs
are
used
to
solve
PDEs
in
much
larger
discrete
subdomains
that
increases
accuracy
substantially
and
decreases
computational
load
as
well.
DPINN
(Distributed
physics-informed
neural
networks)
and
DPIELM
(Distributed
physics-informed
extreme
learning
machines)
are
generalizable
space-time
domain
discretization
for
better
approximation.
DPIELM
is
an
extremely
fast
and
lightweight
approximator
with
competitive
accuracy.
Domain
scaling
on
the
top
has
a
special
effect.
Another
school
of
thought
is
discretization
for
parallel
computation
to
leverage
usage
of
available
computational
resources.
XPINNs
is
a
generalized
space-time
domain
decomposition
approach
for
the
physics-informed
neural
networks
(PINNs)
to
solve
nonlinear
partial
differential
equations
on
arbitrary
complex-geometry
domains.
The
XPINNs
further
pushes
the
boundaries
of
both
PINNs
as
well
as
Conservative
PINNs
(cPINNs),
which
is
a
spatial
domain
decomposition
approach
in
the
PINN
framework
tailored
to
conservation
laws.
Compared
to
PINN,
the
XPINN
method
has
large
representation
and
parallelization
capacity
due
to
the
inherent
property
of
deployment
of
multiple
neural
networks
in
the
smaller
subdomains.
Unlike
cPINN,
XPINN
can
be
extended
to
any
type
of
PDEs.
Moreover,
the
domain
can
be
decomposed
in
any
arbitrary
way
(in
space
and
time),
which
is
not
possible
in
cPINN.
Thus,
XPINN
offers
both
space
and
time
parallelization,
thereby
reducing
the
training
cost
more
effectively.
The
XPINN
is
particularly
effective
for
the
large-scale
problems
(involving
large
data
set)
as
well
as
for
the
high-dimensional
problems
where
single
network
based
PINN
is
not
adequate.
The
rigorous
bounds
on
the
errors
resulting
from
the
approximation
of
the
nonlinear
PDEs
(incompressible
Navier–Stokes
equations)
with
PINNs
and
XPINNs
are
proved.
==
Physics-informed
neural
networks
and
functional
interpolation
==
In
the
PINN
framework,
initial
and
boundary
conditions
are
not
analytically
satisfied,
thus
they
need
to
be
included
in
the
loss
function
of
the
network
to
be
simultaneously
learned
with
the
differential
equation
(DE)
unknown
functions.
Having
competing
objectives
during
the
network's
training
can
lead
to
unbalanced
gradients
while
using
gradient-based
techniques,
which
causes
PINNs
to
often
struggle
to
accurately
learn
the
underlying
DE
solution.
This
drawback
is
overcome
by
using
functional
interpolation
techniques
such
as
the
Theory
of
Functional
Connections
(TFC)'s
constrained
expression,
in
the
Deep-TFC
framework,
which
reduces
the
solution
search
space
of
constrained
problems
to
the
subspace
of
neural
network
that
analytically
satisfies
the
constraints.
A
further
improvement
of
PINN
and
functional
interpolation
approach
is
given
by
the
Extreme
Theory
of
Functional
Connections
(X-TFC)
framework,
where
a
single-layer
Neural
Network
and
the
extreme
learning
machine
training
algorithm
are
employed.
X-TFC
allows
to
improve
the
accuracy
and
performance
of
regular
PINNs,
and
its
robustness
and
reliability
are
proved
for
stiff
problems,
optimal
control,
aerospace,
and
rarefied
gas
dynamics
applications.
==
Physics-informed
PointNet
(PIPN)
for
multiple
sets
of
irregular
geometries
==
Regular
PINNs
are
only
able
to
obtain
the
solution
of
a
forward
or
inverse
problem
on
a
single
geometry.
It
means
that
for
any
new
geometry
(computational
domain),
one
must
retrain
a
PINN.
This
limitation
of
regular
PINNs
imposes
high
computational
costs,
specifically
for
a
comprehensive
investigation
of
geometric
parameters
in
industrial
designs.
Physics-informed
PointNet
(PIPN)
is
fundamentally
the
result
of
a
combination
of
PINN's
loss
function
with
PointNet.
In
fact,
instead
of
using
a
simple
fully
connected
neural
network,
PIPN
uses
PointNet
as
the
core
of
its
neural
network.
PointNet
has
been
primarily
designed
for
deep
learning
of
3D
object
classification
and
segmentation
by
the
research
group
of
Leonidas
J.
Guibas.
PointNet
extracts
geometric
features
of
input
computational
domains
in
PIPN.
Thus,
PIPN
is
able
to
solve
governing
equations
on
multiple
computational
domains
(rather
than
only
a
single
domain)
with
irregular
geometries,
simultaneously.
The
effectiveness
of
PIPN
has
been
shown
for
incompressible
flow,
heat
transfer
and
linear
elasticity.
==
Physics-informed
neural
networks
(PINNs)
for
inverse
computations
==
Physics-informed
neural
networks
(PINNs)
have
particularly
proven
effective
in
solving
inverse
problems
within
differential
equations,
demonstrating
their
applicability
across
science,
engineering,
and
economics.
They
have
shown
useful
for
solving
inverse
problems
in
a
variety
of
fields,
including
nano-optics,
topology
optimization/characterization,
multiphase
flow
in
porous
media,
and
high-speed
fluid
flow.
PINNs
have
demonstrated
flexibility
when
dealing
with
noisy
and
uncertain
observation
datasets.
They
also
demonstrated
clear
advantages
in
the
inverse
calculation
of
parameters
for
multi-fidelity
datasets,
meaning
datasets
with
different
quality,
quantity,
and
types
of
observations.
Uncertainties
in
calculations
can
be
evaluated
using
ensemble-based
or
Bayesian-based
calculations.
==
Limitations
==
Translation
and
discontinuous
behavior
are
hard
to
approximate
using
PINNs.
They
fail
when
solving
differential
equations
with
slight
advective
dominance.
The
difficulty
in
training
of
PINNs
in
advection-dominated
PDEs
can
be
explained
by
Kolmogorov
n–width
of
the
solution.
They
also
fail
to
solve
a
system
of
dynamical
systems
and
hence
have
not
been
a
success
in
solving
chaotic
equations.
One
of
the
reasons
behind
the
failure
of
the
regular
PINNs
is
soft-constraining
of
Dirichlet
and
Neumann
boundary
conditions
which
pose
multi-objective
optimization
problems.
This
requires
the
need
for
manually
weighing
the
loss
terms
to
be
able
to
optimize.
Another
reason
is
getting
optimization
itself.
Posing
PDE
solving
as
an
optimization
problem
brings
in
all
the
problems
that
are
faced
in
the
world
of
optimization,
the
major
one
being
getting
stuck
at
a
local
optimum
pretty
often.
==
References
==
==
External
links
==
PINN
–
repository
to
implement
physics-informed
neural
network
in
Python
XPINN
–
repository
to
implement
extended
physics-informed
neural
network
(XPINN)
in
Python
PIPN
[2]–
repository
to
implement
physics-informed
PointNet
(PIPN)
in
Python
Quantum
neural
networks
are
computational
neural
network
models
which
are
based
on
the
principles
of
quantum
mechanics.
The
first
ideas
on
quantum
neural
computation
were
published
independently
in
1995
by
Subhash
Kak
and
Ron
Chrisley,
engaging
with
the
theory
of
quantum
mind,
which
posits
that
quantum
effects
play
a
role
in
cognitive
function.
However,
typical
research
in
quantum
neural
networks
involves
combining
classical
artificial
neural
network
models
(which
are
widely
used
in
machine
learning
for
the
important
task
of
pattern
recognition)
with
the
advantages
of
quantum
information
in
order
to
develop
more
efficient
algorithms.
One
important
motivation
for
these
investigations
is
the
difficulty
to
train
classical
neural
networks,
especially
in
big
data
applications.
The
hope
is
that
features
of
quantum
computing
such
as
quantum
parallelism
or
the
effects
of
interference
and
entanglement
can
be
used
as
resources.
Since
the
technological
implementation
of
a
quantum
computer
is
still
in
a
premature
stage,
such
quantum
neural
network
models
are
mostly
theoretical
proposals
that
await
their
full
implementation
in
physical
experiments.
Most
Quantum
neural
networks
are
developed
as
feed-forward
networks.
Similar
to
their
classical
counterparts,
this
structure
intakes
input
from
one
layer
of
qubits,
and
passes
that
input
onto
another
layer
of
qubits.
This
layer
of
qubits
evaluates
this
information
and
passes
on
the
output
to
the
next
layer.
Eventually
the
path
leads
to
the
final
layer
of
qubits.
The
layers
do
not
have
to
be
of
the
same
width,
meaning
they
don't
have
to
have
the
same
number
of
qubits
as
the
layer
before
or
after
it.
This
structure
is
trained
on
which
path
to
take
similar
to
classical
artificial
neural
networks.
This
is
discussed
in
a
lower
section.
Quantum
neural
networks
refer
to
three
different
categories:
Quantum
computer
with
classical
data,
classical
computer
with
quantum
data,
and
quantum
computer
with
quantum
data.
==
Examples
==
Quantum
neural
network
research
is
still
in
its
infancy,
and
a
conglomeration
of
proposals
and
ideas
of
varying
scope
and
mathematical
rigor
have
been
put
forward.
Most
of
them
are
based
on
the
idea
of
replacing
classical
binary
or
McCulloch-Pitts
neurons
with
a
qubit
(which
can
be
called
a
“quron”),
resulting
in
neural
units
that
can
be
in
a
superposition
of
the
state
‘firing’
and
‘resting’.
===
Quantum
perceptrons
===
A
lot
of
proposals
attempt
to
find
a
quantum
equivalent
for
the
perceptron
unit
from
which
neural
nets
are
constructed.
A
problem
is
that
nonlinear
activation
functions
do
not
immediately
correspond
to
the
mathematical
structure
of
quantum
theory,
since
a
quantum
evolution
is
described
by
linear
operations
and
leads
to
probabilistic
observation.
Ideas
to
imitate
the
perceptron
activation
function
with
a
quantum
mechanical
formalism
reach
from
special
measurements
to
postulating
non-linear
quantum
operators
(a
mathematical
framework
that
is
disputed).
A
direct
implementation
of
the
activation
function
using
the
circuit-based
model
of
quantum
computation
has
recently
been
proposed
by
Schuld,
Sinayskiy
and
Petruccione
based
on
the
quantum
phase
estimation
algorithm.
===
Quantum
networks
===
At
a
larger
scale,
researchers
have
attempted
to
generalize
neural
networks
to
the
quantum
setting.
One
way
of
constructing
a
quantum
neuron
is
to
first
generalise
classical
neurons
and
then
generalising
them
further
to
make
unitary
gates.
Interactions
between
neurons
can
be
controlled
quantumly,
with
unitary
gates,
or
classically,
via
measurement
of
the
network
states.
This
high-level
theoretical
technique
can
be
applied
broadly,
by
taking
different
types
of
networks
and
different
implementations
of
quantum
neurons,
such
as
photonically
implemented
neurons
and
quantum
reservoir
processor
(quantum
version
of
reservoir
computing).
Most
learning
algorithms
follow
the
classical
model
of
training
an
artificial
neural
network
to
learn
the
input-output
function
of
a
given
training
set
and
use
classical
feedback
loops
to
update
parameters
of
the
quantum
system
until
they
converge
to
an
optimal
configuration.
Learning
as
a
parameter
optimisation
problem
has
also
been
approached
by
adiabatic
models
of
quantum
computing.Quantum
neural
networks
can
be
applied
to
algorithmic
design:
given
qubits
with
tunable
mutual
interactions,
one
can
attempt
to
learn
interactions
following
the
classical
backpropagation
rule
from
a
training
set
of
desired
input-output
relations,
taken
to
be
the
desired
output
algorithm's
behavior.
The
quantum
network
thus
‘learns’
an
algorithm.
===
Quantum
associative
memory
===
The
first
quantum
associative
memory
algorithm
was
introduced
by
Dan
Ventura
and
Tony
Martinez
in
1999.
The
authors
do
not
attempt
to
translate
the
structure
of
artificial
neural
network
models
into
quantum
theory,
but
propose
an
algorithm
for
a
circuit-based
quantum
computer
that
simulates
associative
memory.
The
memory
states
(in
Hopfield
neural
networks
saved
in
the
weights
of
the
neural
connections)
are
written
into
a
superposition,
and
a
Grover-like
quantum
search
algorithm
retrieves
the
memory
state
closest
to
a
given
input.
As
such,
this
is
not
a
fully
content-addressable
memory,
since
only
incomplete
patterns
can
be
retrieved.
The
first
truly
content-addressable
quantum
memory,
which
can
retrieve
patterns
also
from
corrupted
inputs,
was
proposed
by
Carlo
A.
Trugenberger.
Both
memories
can
store
an
exponential
(in
terms
of
n
qubits)
number
of
patterns
but
can
be
used
only
once
due
to
the
no-cloning
theorem
and
their
destruction
upon
measurement.
Trugenberger,
however,
has
shown
that
his
proababilistic
model
of
quantum
associative
memory
can
be
efficiently
implemented
and
re-used
multiples
times
for
any
polynomial
number
of
stored
patterns,
a
large
advantage
with
respect
to
classical
associative
memories.
===
Classical
neural
networks
inspired
by
quantum
theory
===
A
substantial
amount
of
interest
has
been
given
to
a
“quantum-inspired”
model
that
uses
ideas
from
quantum
theory
to
implement
a
neural
network
based
on
fuzzy
logic.
==
Training
==
Quantum
Neural
Networks
can
be
theoretically
trained
similarly
to
training
classical/artificial
neural
networks.
A
key
difference
lies
in
communication
between
the
layers
of
a
neural
networks.
For
classical
neural
networks,
at
the
end
of
a
given
operation,
the
current
perceptron
copies
its
output
to
the
next
layer
of
perceptron(s)
in
the
network.
However,
in
a
quantum
neural
network,
where
each
perceptron
is
a
qubit,
this
would
violate
the
no-cloning
theorem.
A
proposed
generalized
solution
to
this
is
to
replace
the
classical
fan-out
method
with
an
arbitrary
unitary
that
spreads
out,
but
does
not
copy,
the
output
of
one
qubit
to
the
next
layer
of
qubits.
Using
this
fan-out
Unitary
(Uf{\displaystyle
U_{f}})
with
a
dummy
state
qubit
in
a
known
state
(Ex.
|0⟩{\displaystyle
|0\rangle
}
in
the
computational
basis),
also
known
as
an
Ancilla
bit,
the
information
from
the
qubit
can
be
transferred
to
the
next
layer
of
qubits.
This
process
adheres
to
the
quantum
operation
requirement
of
reversibility.Using
this
quantum
feed-forward
network,
deep
neural
networks
can
be
executed
and
trained
efficiently.
A
deep
neural
network
is
essentially
a
network
with
many
hidden-layers,
as
seen
in
the
sample
model
neural
network
above.
Since
the
Quantum
neural
network
being
discussed
uses
fan-out
Unitary
operators,
and
each
operator
only
acts
on
its
respective
input,
only
two
layers
are
used
at
any
given
time.
In
other
words,
no
Unitary
operator
is
acting
on
the
entire
network
at
any
given
time,
meaning
the
number
of
qubits
required
for
a
given
step
depends
on
the
number
of
inputs
in
a
given
layer.
Since
Quantum
Computers
are
notorious
for
their
ability
to
run
multiple
iterations
in
a
short
period
of
time,
the
efficiency
of
a
quantum
neural
network
is
solely
dependent
on
the
number
of
qubits
in
any
given
layer,
and
not
on
the
depth
of
the
network.
===
Cost
functions
===
To
determine
the
effectiveness
of
a
neural
network,
a
cost
function
is
used,
which
essentially
measures
the
proximity
of
the
network's
output
to
the
expected
or
desired
output.
In
a
Classical
Neural
Network,
the
weights
(w{\displaystyle
w})
and
biases
(b{\displaystyle
b})
at
each
step
determine
the
outcome
of
the
cost
function
C(w,b){\displaystyle
C(w,b)}.
When
training
a
Classical
Neural
network,
the
weights
and
biases
are
adjusted
after
each
iteration,
and
given
equation
1
below,
where
y(x){\displaystyle
y(x)}
is
the
desired
output
and
aout(x){\displaystyle
a^{\text{out}}(x)}
is
the
actual
output,
the
cost
function
is
optimized
when
C(w,b){\displaystyle
C(w,b)}=
0.
For
a
quantum
neural
network,
the
cost
function
is
determined
by
measuring
the
fidelity
of
the
outcome
state
(ρout{\displaystyle
\rho
^{\text{out}}})
with
the
desired
outcome
state
(ϕout{\displaystyle
\phi
^{\text{out}}}),
seen
in
Equation
2
below.
In
this
case,
the
Unitary
operators
are
adjusted
after
each
iteration,
and
the
cost
function
is
optimized
when
C
=
1.
Equation
1
C(w,b)=1N∑x||y(x)−aout(x)||2{\displaystyle
C(w,b)={1
\over
N}\sum
_{x}{||y(x)-a^{\text{out}}(x)||
\over
2}}
Equation
2
C=1N∑xN⟨ϕout|ρout|ϕout⟩{\displaystyle
C={1
\over
N}\sum
_{x}^{N}{\langle
\phi
^{\text{out}}|\rho
^{\text{out}}|\phi
^{\text{out}}\rangle
}}
==
See
also
==
Differentiable
programming
Optical
neural
network
Holographic
associative
memory
Quantum
cognition
Quantum
machine
learning
==
References
==
==
External
links
==
Recent
review
of
quantum
neural
networks
by
M.
Schuld,
I.
Sinayskiy
and
F.
Petruccione
Review
of
quantum
neural
networks
by
Wei
Article
by
P.
Gralewicz
on
the
plausibility
of
quantum
computing
in
biological
neural
networks
Training
a
neural
net
to
recognize
images
In
the
context
of
artificial
neural
networks,
the
rectifier
or
ReLU
(rectified
linear
unit)
activation
function
is
an
activation
function
defined
as
the
positive
part
of
its
argument:
f(x)=x+=max(0,x)=x+|x|2={xif
x>0,0otherwise,{\displaystyle
f(x)=x^{+}=\max(0,x)={\frac
{x+|x|}{2}}={\begin{cases}x&{\text{if
}}x>0,\\0&{\text{otherwise}},\end{cases}}}where
x
is
the
input
to
a
neuron.
This
is
also
known
as
a
ramp
function
and
is
analogous
to
half-wave
rectification
in
electrical
engineering.
This
activation
function
was
introduced
by
Kunihiko
Fukushima
in
1969
in
the
context
of
visual
feature
extraction
in
hierarchical
neural
networks.
It
was
later
argued
that
it
has
strong
biological
motivations
and
mathematical
justifications.
In
2011
it
was
found
to
enable
better
training
of
deeper
networks,
compared
to
the
widely
used
activation
functions
prior
to
2011,
e.g.,
the
logistic
sigmoid
(which
is
inspired
by
probability
theory;
see
logistic
regression)
and
its
more
practical
counterpart,
the
hyperbolic
tangent.
The
rectifier
is,
as
of
2017,
the
most
popular
activation
function
for
deep
neural
networks.Rectified
linear
units
find
applications
in
computer
vision
and
speech
recognition
using
deep
neural
nets
and
computational
neuroscience.
==
Advantages
==
Sparse
activation:
For
example,
in
a
randomly
initialized
network,
only
about
50%
of
hidden
units
are
activated
(have
a
non-zero
output).
Better
gradient
propagation:
Fewer
vanishing
gradient
problems
compared
to
sigmoidal
activation
functions
that
saturate
in
both
directions.
Efficient
computation:
Only
comparison,
addition
and
multiplication.
Scale-invariant:
max(0,ax)=amax(0,x)
for
a≥0{\displaystyle
\max(0,ax)=a\max(0,x){\text{
for
}}a\geq
0}.Rectifying
activation
functions
were
used
to
separate
specific
excitation
and
unspecific
inhibition
in
the
neural
abstraction
pyramid,
which
was
trained
in
a
supervised
way
to
learn
several
computer
vision
tasks.
In
2011,
the
use
of
the
rectifier
as
a
non-linearity
has
been
shown
to
enable
training
deep
supervised
neural
networks
without
requiring
unsupervised
pre-training.
Rectified
linear
units,
compared
to
sigmoid
function
or
similar
activation
functions,
allow
faster
and
effective
training
of
deep
neural
architectures
on
large
and
complex
datasets.
==
Potential
problems
==
Non-differentiable
at
zero;
however,
it
is
differentiable
anywhere
else,
and
the
value
of
the
derivative
at
zero
can
be
arbitrarily
chosen
to
be
0
or
1.
Not
zero-centered.
Unbounded.
Dying
ReLU
problem:
ReLU
(rectified
linear
unit)
neurons
can
sometimes
be
pushed
into
states
in
which
they
become
inactive
for
essentially
all
inputs.
In
this
state,
no
gradients
flow
backward
through
the
neuron,
and
so
the
neuron
becomes
stuck
in
a
perpetually
inactive
state
and
"dies".
This
is
a
form
of
the
vanishing
gradient
problem.
In
some
cases,
large
numbers
of
neurons
in
a
network
can
become
stuck
in
dead
states,
effectively
decreasing
the
model
capacity.
This
problem
typically
arises
when
the
learning
rate
is
set
too
high.
It
may
be
mitigated
by
using
leaky
ReLUs
instead,
which
assign
a
small
positive
slope
for
x
<
0;
however,
the
performance
is
reduced.
==
Variants
==
===
Piecewise-linear
variants
===
====
Leaky
ReLU
====
Leaky
ReLUs
allow
a
small,
positive
gradient
when
the
unit
is
not
active,
helping
to
mitigate
the
vanishing
gradient
problem.
f(x)={xif
x>0,0.01xotherwise.f′(x)={1if
x>0,0.01otherwise.{\displaystyle
f(x)={\begin{cases}x&{\text{if
}}x>0,\\0.01x&{\text{otherwise}}.\end{cases}}\qquad
\qquad
f'(x)={\begin{cases}1&{\text{if
}}x>0,\\0.01&{\text{otherwise}}.\end{cases}}}
====
Parametric
ReLU
====
Parametric
ReLUs
(PReLUs)
take
this
idea
further
by
making
the
coefficient
of
leakage
into
a
parameter
that
is
learned
along
with
the
other
neural-network
parameters.
f(x)={xif
x>0,a⋅xotherwise.f′(x)={1if
x>0,aotherwise.{\displaystyle
f(x)={\begin{cases}x&{\text{if
}}x>0,\\a\cdot
x&{\text{otherwise}}.\end{cases}}\qquad
\qquad
\qquad
f'(x)={\begin{cases}1&{\text{if
}}x>0,\\a&{\text{otherwise}}.\end{cases}}}Note
that
for
a
≤
1,
this
is
equivalent
to
f(x)=max(x,ax){\displaystyle
f(x)=\max(x,ax)}and
thus
has
a
relation
to
"maxout"
networks.
===
Other
non-linear
variants
===
====
Gaussian-error
linear
unit
(GELU)
====
GELU
is
a
smooth
approximation
to
the
rectifier:
f(x)=x⋅Φ(x),{\displaystyle
f(x)=x\cdot
\Phi
(x),}f′(x)=x⋅Φ′(x)+Φ(x),{\displaystyle
f'(x)=x\cdot
\Phi
'(x)+\Phi
(x),}where
Φ(x)=P(X⩽x){\displaystyle
\Phi
(x)=P(X\leqslant
x)}
is
the
cumulative
distribution
function
of
the
standard
normal
distribution.
This
activation
function
is
illustrated
in
the
figure
at
the
start
of
this
article.
It
has
a
"bump"
to
the
left
of
x
<
0
and
serves
as
the
default
activation
for
models
such
as
BERT.
====
SiLU
====
The
SiLU
(sigmoid
linear
unit)
or
swish
function
is
another
smooth
approximation,
first
coined
in
the
GELU
paper:
f(x)=x⋅sigmoid⁡(x),{\displaystyle
f(x)=x\cdot
\operatorname
{sigmoid}
(x),}f′(x)=x⋅sigmoid′⁡(x)+sigmoid⁡(x),{\displaystyle
f'(x)=x\cdot
\operatorname
{sigmoid}
'(x)+\operatorname
{sigmoid}
(x),}where
sigmoid⁡(x){\displaystyle
\operatorname
{sigmoid}
(x)}
is
the
sigmoid
function.
====
Softplus
====
A
smooth
approximation
to
the
rectifier
is
the
analytic
function
f(x)=ln⁡(1+ex),f′(x)=ex1+ex=11+e−x,{\displaystyle
f(x)=\ln(1+e^{x}),\qquad
\qquad
f'(x)={\frac
{e^{x}}{1+e^{x}}}={\frac
{1}{1+e^{-x}}},}which
is
called
the
softplus
or
SmoothReLU
function.
For
large
negative
x{\displaystyle
x}
it
is
roughly
ln⁡1{\displaystyle
\ln
1},
so
just
above
0,
while
for
large
positive
x{\displaystyle
x}
it
is
roughly
ln⁡(ex){\displaystyle
\ln(e^{x})},
so
just
above
x{\displaystyle
x}.
This
function
can
be
approximated
as:
ln⁡(1+ex)≈{ln⁡2,x=0,x1−e−x/ln⁡2,x≠0{\displaystyle
\ln
\left(1+e^{x}\right)\approx
{\begin{cases}\ln
2,&x=0,\\[6pt]{\frac
{x}{1-e^{-x/\ln
2}}},&x\neq
0\end{cases}}}By
making
the
change
of
variables
x=yln⁡(2){\displaystyle
x=y\ln(2)},
this
is
equivalent
to
log2⁡(1+2y)≈{1,y=0,y1−e−y,y≠0.{\displaystyle
\log
_{2}(1+2^{y})\approx
{\begin{cases}1,&y=0,\\[6pt]{\frac
{y}{1-e^{-y}}},&y\neq
0.\end{cases}}}A
sharpness
parameter
k{\displaystyle
k}
may
be
included:
f(x)=ln⁡(1+ekx)k,f′(x)=ekx1+ekx=11+e−kx.{\displaystyle
f(x)={\frac
{\ln(1+e^{kx})}{k}},\qquad
\qquad
f'(x)={\frac
{e^{kx}}{1+e^{kx}}}={\frac
{1}{1+e^{-kx}}}.}The
derivative
of
softplus
is
the
logistic
function.
The
logistic
sigmoid
function
is
a
smooth
approximation
of
the
derivative
of
the
rectifier,
the
Heaviside
step
function.
The
multivariable
generalization
of
single-variable
softplus
is
the
LogSumExp
with
the
first
argument
set
to
zero:
LSE0+⁡(x1,…,xn):=LSE⁡(0,x1,…,xn)=ln⁡(1+ex1+⋯+exn).{\displaystyle
\operatorname
{LSE_{0}}
^{+}(x_{1},\dots
,x_{n}):=\operatorname
{LSE}
(0,x_{1},\dots
,x_{n})=\ln(1+e^{x_{1}}+\cdots
+e^{x_{n}}).}The
LogSumExp
function
is
LSE⁡(x1,…,xn)=ln⁡(ex1+⋯+exn),{\displaystyle
\operatorname
{LSE}
(x_{1},\dots
,x_{n})=\ln(e^{x_{1}}+\cdots
+e^{x_{n}}),}and
its
gradient
is
the
softmax;
the
softmax
with
the
first
argument
set
to
zero
is
the
multivariable
generalization
of
the
logistic
function.
Both
LogSumExp
and
softmax
are
used
in
machine
learning.
====
ELU
====
Exponential
linear
units
try
to
make
the
mean
activations
closer
to
zero,
which
speeds
up
learning.
It
has
been
shown
that
ELUs
can
obtain
higher
classification
accuracy
than
ReLUs.
f(x)={xif
x>0,a(ex−1)otherwise.f′(x)={1if
x>0,a⋅exotherwise.{\displaystyle
f(x)={\begin{cases}x&{\text{if
}}x>0,\\a\left(e^{x}-1\right)&{\text{otherwise}}.\end{cases}}\qquad
\qquad
f'(x)={\begin{cases}1&{\text{if
}}x>0,\\a\cdot
e^{x}&{\text{otherwise}}.\end{cases}}}In
these
formulas,
a{\displaystyle
a}
is
a
hyper-parameter
to
be
tuned
with
the
constraint
a≥0{\displaystyle
a\geq
0}.
The
ELU
can
be
viewed
as
a
smoothed
version
of
a
shifted
ReLU
(SReLU),
which
has
the
form
f(x)=max(−a,x){\displaystyle
f(x)=\max(-a,x)},
given
the
same
interpretation
of
a{\displaystyle
a}.
====
Mish
====
The
mish
function
can
also
be
used
as
a
smooth
approximation
of
the
rectifier.
It
is
defined
as
f(x)=xtanh⁡(softplus⁡(x)),{\displaystyle
f(x)=x\tanh
{\big
(}\operatorname
{softplus}
(x){\big
)},}where
tanh⁡(x){\displaystyle
\tanh(x)}
is
the
hyperbolic
tangent,
and
softplus⁡(x){\displaystyle
\operatorname
{softplus}
(x)}
is
the
softplus
function.
Mish
is
non-monotonic
and
self-gated.
It
was
inspired
by
Swish,
itself
a
variant
of
ReLU.
====
Squareplus
====
Squareplus
is
the
function
squareplusb⁡(x)=x+x2+b2{\displaystyle
\operatorname
{squareplus}
_{b}(x)={\frac
{x+{\sqrt
{x^{2}+b}}}{2}}}where
b≥0{\displaystyle
b\geq
0}
is
a
hyperparameter
that
determines
the
"size"
of
the
curved
region
near
x=0{\displaystyle
x=0}.
(For
example,
letting
b=0{\displaystyle
b=0}
yields
ReLU,
and
letting
b=4{\displaystyle
b=4}
yields
the
metallic
mean
function.)
Squareplus
shares
many
properties
with
softplus:
It
is
monotonic,
strictly
positive,
approaches
0
as
x→−∞{\displaystyle
x\to
-\infty
},
approaches
the
identity
as
x→+∞{\displaystyle
x\to
+\infty
},
and
is
C∞{\displaystyle
C^{\infty
}}
smooth.
However,
squareplus
can
be
computed
using
only
algebraic
functions,
making
it
well-suited
for
settings
where
computational
resources
or
instruction
sets
are
limited.
Additionally,
squareplus
requires
no
special
consideration
to
ensure
numerical
stability
when
x{\displaystyle
x}
is
large.
==
See
also
==
Softmax
function
Sigmoid
function
Tobit
model
Layer
(deep
learning)
==
References
==
A
recurrent
neural
network
(RNN)
is
one
of
the
two
broad
types
of
artificial
neural
network,
characterized
by
direction
of
the
flow
of
information
between
its
layers.
In
contrast
to
the
uni-directional
feedforward
neural
network,
it
is
a
bi-directional
artificial
neural
network,
meaning
that
it
allows
the
output
from
some
nodes
to
affect
subsequent
input
to
the
same
nodes.
Their
ability
to
use
internal
state
(memory)
to
process
arbitrary
sequences
of
inputs
makes
them
applicable
to
tasks
such
as
unsegmented,
connected
handwriting
recognition
or
speech
recognition.
The
term
"recurrent
neural
network"
is
used
to
refer
to
the
class
of
networks
with
an
infinite
impulse
response,
whereas
"convolutional
neural
network"
refers
to
the
class
of
finite
impulse
response.
Both
classes
of
networks
exhibit
temporal
dynamic
behavior.
A
finite
impulse
recurrent
network
is
a
directed
acyclic
graph
that
can
be
unrolled
and
replaced
with
a
strictly
feedforward
neural
network,
while
an
infinite
impulse
recurrent
network
is
a
directed
cyclic
graph
that
can
not
be
unrolled.
Additional
stored
states
and
the
storage
under
direct
control
by
the
network
can
be
added
to
both
infinite-impulse
and
finite-impulse
networks.
Another
network
or
graph
can
also
replace
the
storage
if
that
incorporates
time
delays
or
has
feedback
loops.
Such
controlled
states
are
referred
to
as
gated
states
or
gated
memory
and
are
part
of
long
short-term
memory
networks
(LSTMs)
and
gated
recurrent
units.
This
is
also
called
Feedforward
Neural
Network
(FNN).
Recurrent
neural
networks
are
theoretically
Turing
complete
and
can
run
arbitrary
programs
to
process
arbitrary
sequences
of
inputs.
==
History
==
The
Ising
model
(1925)
by
Wilhelm
Lenz
and
Ernst
Ising
was
the
first
RNN
architecture
that
did
not
learn.
Shun'ichi
Amari
made
it
adaptive
in
1972.
This
was
also
called
the
Hopfield
network
(1982).
See
also
David
Rumelhart's
work
in
1986.
In
1993,
a
neural
history
compressor
system
solved
a
"Very
Deep
Learning"
task
that
required
more
than
1000
subsequent
layers
in
an
RNN
unfolded
in
time.
===
LSTM
===
Long
short-term
memory
(LSTM)
networks
were
invented
by
Hochreiter
and
Schmidhuber
in
1997
and
set
accuracy
records
in
multiple
applications
domains.Around
2007,
LSTM
started
to
revolutionize
speech
recognition,
outperforming
traditional
models
in
certain
speech
applications.
In
2009,
a
Connectionist
Temporal
Classification
(CTC)-trained
LSTM
network
was
the
first
RNN
to
win
pattern
recognition
contests
when
it
won
several
competitions
in
connected
handwriting
recognition.
In
2014,
the
Chinese
company
Baidu
used
CTC-trained
RNNs
to
break
the
2S09
Switchboard
Hub5'00
speech
recognition
dataset
benchmark
without
using
any
traditional
speech
processing
methods.LSTM
also
improved
large-vocabulary
speech
recognition
and
text-to-speech
synthesis
and
was
used
in
Google
Android.
In
2015,
Google's
speech
recognition
reportedly
experienced
a
dramatic
performance
jump
of
49%
through
CTC-trained
LSTM.LSTM
broke
records
for
improved
machine
translation,
Language
Modeling
and
Multilingual
Language
Processing.
LSTM
combined
with
convolutional
neural
networks
(CNNs)
improved
automatic
image
captioning.
==
Architectures
==
RNNs
come
in
many
variants.
===
Fully
recurrent
===
Fully
recurrent
neural
networks
(FRNN)
connect
the
outputs
of
all
neurons
to
the
inputs
of
all
neurons.
This
is
the
most
general
neural
network
topology
because
all
other
topologies
can
be
represented
by
setting
some
connection
weights
to
zero
to
simulate
the
lack
of
connections
between
those
neurons.
The
illustration
to
the
right
may
be
misleading
to
many
because
practical
neural
network
topologies
are
frequently
organized
in
"layers"
and
the
drawing
gives
that
appearance.
However,
what
appears
to
be
layers
are,
in
fact,
different
steps
in
time
of
the
same
fully
recurrent
neural
network.
The
left-most
item
in
the
illustration
shows
the
recurrent
connections
as
the
arc
labeled
'v'.
It
is
"unfolded"
in
time
to
produce
the
appearance
of
layers.
===
Elman
networks
and
Jordan
networks
===
An
Elman
network
is
a
three-layer
network
(arranged
horizontally
as
x,
y,
and
z
in
the
illustration)
with
the
addition
of
a
set
of
context
units
(u
in
the
illustration).
The
middle
(hidden)
layer
is
connected
to
these
context
units
fixed
with
a
weight
of
one.
At
each
time
step,
the
input
is
fed
forward
and
a
learning
rule
is
applied.
The
fixed
back-connections
save
a
copy
of
the
previous
values
of
the
hidden
units
in
the
context
units
(since
they
propagate
over
the
connections
before
the
learning
rule
is
applied).
Thus
the
network
can
maintain
a
sort
of
state,
allowing
it
to
perform
such
tasks
as
sequence-prediction
that
are
beyond
the
power
of
a
standard
multilayer
perceptron.
Jordan
networks
are
similar
to
Elman
networks.
The
context
units
are
fed
from
the
output
layer
instead
of
the
hidden
layer.
The
context
units
in
a
Jordan
network
are
also
called
the
state
layer.
They
have
a
recurrent
connection
to
themselves.Elman
and
Jordan
networks
are
also
known
as
"Simple
recurrent
networks"
(SRN).
Elman
network
ht=σh(Whxt+Uhht−1+bh)yt=σy(Wyht+by){\displaystyle
{\begin{aligned}h_{t}&=\sigma
_{h}(W_{h}x_{t}+U_{h}h_{t-1}+b_{h})\\y_{t}&=\sigma
_{y}(W_{y}h_{t}+b_{y})\end{aligned}}}
Jordan
network
ht=σh(Whxt+Uhyt−1+bh)yt=σy(Wyht+by){\displaystyle
{\begin{aligned}h_{t}&=\sigma
_{h}(W_{h}x_{t}+U_{h}y_{t-1}+b_{h})\\y_{t}&=\sigma
_{y}(W_{y}h_{t}+b_{y})\end{aligned}}}Variables
and
functions
xt{\displaystyle
x_{t}}:
input
vector
ht{\displaystyle
h_{t}}:
hidden
layer
vector
yt{\displaystyle
y_{t}}:
output
vector
W{\displaystyle
W},
U{\displaystyle
U}
and
b{\displaystyle
b}:
parameter
matrices
and
vector
σh{\displaystyle
\sigma
_{h}}
and
σy{\displaystyle
\sigma
_{y}}:
Activation
functions
===
Hopfield
===
The
Hopfield
network
is
an
RNN
in
which
all
connections
across
layers
are
equally
sized.
It
requires
stationary
inputs
and
is
thus
not
a
general
RNN,
as
it
does
not
process
sequences
of
patterns.
However,
it
guarantees
that
it
will
converge.
If
the
connections
are
trained
using
Hebbian
learning,
then
the
Hopfield
network
can
perform
as
robust
content-addressable
memory,
resistant
to
connection
alteration.
====
Bidirectional
associative
memory
====
Introduced
by
Bart
Kosko,
a
bidirectional
associative
memory
(BAM)
network
is
a
variant
of
a
Hopfield
network
that
stores
associative
data
as
a
vector.
The
bi-directionality
comes
from
passing
information
through
a
matrix
and
its
transpose.
Typically,
bipolar
encoding
is
preferred
to
binary
encoding
of
the
associative
pairs.
Recently,
stochastic
BAM
models
using
Markov
stepping
were
optimized
for
increased
network
stability
and
relevance
to
real-world
applications.A
BAM
network
has
two
layers,
either
of
which
can
be
driven
as
an
input
to
recall
an
association
and
produce
an
output
on
the
other
layer.
===
Echo
state
===
Echo
state
networks
(ESN)
have
a
sparsely
connected
random
hidden
layer.
The
weights
of
output
neurons
are
the
only
part
of
the
network
that
can
change
(be
trained).
ESNs
are
good
at
reproducing
certain
time
series.
A
variant
for
spiking
neurons
is
known
as
a
liquid
state
machine.
===
Independently
RNN
(IndRNN)
===
The
independently
recurrent
neural
network
(IndRNN)
addresses
the
gradient
vanishing
and
exploding
problems
in
the
traditional
fully
connected
RNN.
Each
neuron
in
one
layer
only
receives
its
own
past
state
as
context
information
(instead
of
full
connectivity
to
all
other
neurons
in
this
layer)
and
thus
neurons
are
independent
of
each
other's
history.
The
gradient
backpropagation
can
be
regulated
to
avoid
gradient
vanishing
and
exploding
in
order
to
keep
long
or
short-term
memory.
The
cross-neuron
information
is
explored
in
the
next
layers.
IndRNN
can
be
robustly
trained
with
non-saturated
nonlinear
functions
such
as
ReLU.
Deep
networks
can
be
trained
using
skip
connections.
===
Recursive
===
A
recursive
neural
network
is
created
by
applying
the
same
set
of
weights
recursively
over
a
differentiable
graph-like
structure
by
traversing
the
structure
in
topological
order.
Such
networks
are
typically
also
trained
by
the
reverse
mode
of
automatic
differentiation.
They
can
process
distributed
representations
of
structure,
such
as
logical
terms.
A
special
case
of
recursive
neural
networks
is
the
RNN
whose
structure
corresponds
to
a
linear
chain.
Recursive
neural
networks
have
been
applied
to
natural
language
processing.
The
Recursive
Neural
Tensor
Network
uses
a
tensor-based
composition
function
for
all
nodes
in
the
tree.
===
Neural
history
compressor
===
The
neural
history
compressor
is
an
unsupervised
stack
of
RNNs.
At
the
input
level,
it
learns
to
predict
its
next
input
from
the
previous
inputs.
Only
unpredictable
inputs
of
some
RNN
in
the
hierarchy
become
inputs
to
the
next
higher
level
RNN,
which
therefore
recomputes
its
internal
state
only
rarely.
Each
higher
level
RNN
thus
studies
a
compressed
representation
of
the
information
in
the
RNN
below.
This
is
done
such
that
the
input
sequence
can
be
precisely
reconstructed
from
the
representation
at
the
highest
level.
The
system
effectively
minimizes
the
description
length
or
the
negative
logarithm
of
the
probability
of
the
data.
Given
a
lot
of
learnable
predictability
in
the
incoming
data
sequence,
the
highest
level
RNN
can
use
supervised
learning
to
easily
classify
even
deep
sequences
with
long
intervals
between
important
events.
It
is
possible
to
distill
the
RNN
hierarchy
into
two
RNNs:
the
"conscious"
chunker
(higher
level)
and
the
"subconscious"
automatizer
(lower
level).
Once
the
chunker
has
learned
to
predict
and
compress
inputs
that
are
unpredictable
by
the
automatizer,
then
the
automatizer
can
be
forced
in
the
next
learning
phase
to
predict
or
imitate
through
additional
units
the
hidden
units
of
the
more
slowly
changing
chunker.
This
makes
it
easy
for
the
automatizer
to
learn
appropriate,
rarely
changing
memories
across
long
intervals.
In
turn,
this
helps
the
automatizer
to
make
many
of
its
once
unpredictable
inputs
predictable,
such
that
the
chunker
can
focus
on
the
remaining
unpredictable
events.A
generative
model
partially
overcame
the
vanishing
gradient
problem
of
automatic
differentiation
or
backpropagation
in
neural
networks
in
1992.
In
1993,
such
a
system
solved
a
"Very
Deep
Learning"
task
that
required
more
than
1000
subsequent
layers
in
an
RNN
unfolded
in
time.
===
Second
order
RNNs
===
Second-order
RNNs
use
higher
order
weights
wijk{\displaystyle
w{}_{ijk}}
instead
of
the
standard
wij{\displaystyle
w{}_{ij}}
weights,
and
states
can
be
a
product.
This
allows
a
direct
mapping
to
a
finite-state
machine
both
in
training,
stability,
and
representation.
Long
short-term
memory
is
an
example
of
this
but
has
no
such
formal
mappings
or
proof
of
stability.
===
Long
short-term
memory
===
Long
short-term
memory
(LSTM)
is
a
deep
learning
system
that
avoids
the
vanishing
gradient
problem.
LSTM
is
normally
augmented
by
recurrent
gates
called
"forget
gates".
LSTM
prevents
backpropagated
errors
from
vanishing
or
exploding.
Instead,
errors
can
flow
backward
through
unlimited
numbers
of
virtual
layers
unfolded
in
space.
That
is,
LSTM
can
learn
tasks
that
require
memories
of
events
that
happened
thousands
or
even
millions
of
discrete
time
steps
earlier.
Problem-specific
LSTM-like
topologies
can
be
evolved.
LSTM
works
even
given
long
delays
between
significant
events
and
can
handle
signals
that
mix
low
and
high-frequency
components.
Many
applications
use
stacks
of
LSTM
RNNs
and
train
them
by
connectionist
temporal
classification
(CTC)
to
find
an
RNN
weight
matrix
that
maximizes
the
probability
of
the
label
sequences
in
a
training
set,
given
the
corresponding
input
sequences.
CTC
achieves
both
alignment
and
recognition.
LSTM
can
learn
to
recognize
context-sensitive
languages
unlike
previous
models
based
on
hidden
Markov
models
(HMM)
and
similar
concepts.
===
Gated
recurrent
unit
===
Gated
recurrent
units
(GRUs)
are
a
gating
mechanism
in
recurrent
neural
networks
introduced
in
2014.
They
are
used
in
the
full
form
and
several
simplified
variants.
Their
performance
on
polyphonic
music
modeling
and
speech
signal
modeling
was
found
to
be
similar
to
that
of
long
short-term
memory.
They
have
fewer
parameters
than
LSTM,
as
they
lack
an
output
gate.
===
Bi-directional
===
Bi-directional
RNNs
use
a
finite
sequence
to
predict
or
label
each
element
of
the
sequence
based
on
the
element's
past
and
future
contexts.
This
is
done
by
concatenating
the
outputs
of
two
RNNs,
one
processing
the
sequence
from
left
to
right,
and
the
other
one
from
right
to
left.
The
combined
outputs
are
the
predictions
of
the
teacher-given
target
signals.
This
technique
has
been
proven
to
be
especially
useful
when
combined
with
LSTM
RNNs.
===
Continuous-time
===
A
continuous-time
recurrent
neural
network
(CTRNN)
uses
a
system
of
ordinary
differential
equations
to
model
the
effects
on
a
neuron
of
the
incoming
inputs.
For
a
neuron
i{\displaystyle
i}
in
the
network
with
activation
yi{\displaystyle
y_{i}},
the
rate
of
change
of
activation
is
given
by:
τiy˙i=−yi+∑j=1nwjiσ(yj−Θj)+Ii(t){\displaystyle
\tau
_{i}{\dot
{y}}_{i}=-y_{i}+\sum
_{j=1}^{n}w_{ji}\sigma
(y_{j}-\Theta
_{j})+I_{i}(t)}Where:
τi{\displaystyle
\tau
_{i}}
:
Time
constant
of
postsynaptic
node
yi{\displaystyle
y_{i}}
:
Activation
of
postsynaptic
node
y˙i{\displaystyle
{\dot
{y}}_{i}}
:
Rate
of
change
of
activation
of
postsynaptic
node
wji{\displaystyle
w{}_{ji}}
:
Weight
of
connection
from
pre
to
postsynaptic
node
σ(x){\displaystyle
\sigma
(x)}
:
Sigmoid
of
x
e.g.
σ(x)=1/(1+e−x){\displaystyle
\sigma
(x)=1/(1+e^{-x})}.
yj{\displaystyle
y_{j}}
:
Activation
of
presynaptic
node
Θj{\displaystyle
\Theta
_{j}}
:
Bias
of
presynaptic
node
Ii(t){\displaystyle
I_{i}(t)}
:
Input
(if
any)
to
nodeCTRNNs
have
been
applied
to
evolutionary
robotics
where
they
have
been
used
to
address
vision,
co-operation,
and
minimal
cognitive
behaviour.Note
that,
by
the
Shannon
sampling
theorem,
discrete-time
recurrent
neural
networks
can
be
viewed
as
continuous-time
recurrent
neural
networks
where
the
differential
equations
have
transformed
into
equivalent
difference
equations.
This
transformation
can
be
thought
of
as
occurring
after
the
post-synaptic
node
activation
functions
yi(t){\displaystyle
y_{i}(t)}
have
been
low-pass
filtered
but
prior
to
sampling.
===
Hierarchical
recurrent
neural
network
===
Hierarchical
recurrent
neural
networks
(HRNN)
connect
their
neurons
in
various
ways
to
decompose
hierarchical
behavior
into
useful
subprograms.
Such
hierarchical
structures
of
cognition
are
present
in
theories
of
memory
presented
by
philosopher
Henri
Bergson,
whose
philosophical
views
have
inspired
hierarchical
models.Hierarchical
recurrent
neural
networks
are
useful
in
forecasting,
helping
to
predict
disaggregated
inflation
components
of
the
consumer
price
index
(CPI).
The
HRNN
model
leverages
information
from
higher
levels
in
the
CPI
hierarchy
to
enhance
lower-level
predictions.
Evaluation
of
a
substantial
dataset
from
the
US
CPI-U
index
demonstrates
the
superior
performance
of
the
HRNN
model
compared
to
various
established
inflation
prediction
methods.
===
Recurrent
multilayer
perceptron
network
===
Generally,
a
recurrent
multilayer
perceptron
network
(RMLP
network)
consists
of
cascaded
subnetworks,
each
containing
multiple
layers
of
nodes.
Each
subnetwork
is
feed-forward
except
for
the
last
layer,
which
can
have
feedback
connections.
Each
of
these
subnets
is
connected
only
by
feed-forward
connections.
===
Multiple
timescales
model
===
A
multiple
timescales
recurrent
neural
network
(MTRNN)
is
a
neural-based
computational
model
that
can
simulate
the
functional
hierarchy
of
the
brain
through
self-organization
depending
on
the
spatial
connection
between
neurons
and
on
distinct
types
of
neuron
activities,
each
with
distinct
time
properties.
With
such
varied
neuronal
activities,
continuous
sequences
of
any
set
of
behaviors
are
segmented
into
reusable
primitives,
which
in
turn
are
flexibly
integrated
into
diverse
sequential
behaviors.
The
biological
approval
of
such
a
type
of
hierarchy
was
discussed
in
the
memory-prediction
theory
of
brain
function
by
Hawkins
in
his
book
On
Intelligence.
Such
a
hierarchy
also
agrees
with
theories
of
memory
posited
by
philosopher
Henri
Bergson,
which
have
been
incorporated
into
an
MTRNN
model.
===
Neural
Turing
machines
===
Neural
Turing
machines
(NTMs)
are
a
method
of
extending
recurrent
neural
networks
by
coupling
them
to
external
memory
resources
which
they
can
interact
with
by
attentional
processes.
The
combined
system
is
analogous
to
a
Turing
machine
or
Von
Neumann
architecture
but
is
differentiable
end-to-end,
allowing
it
to
be
efficiently
trained
with
gradient
descent.
===
Differentiable
neural
computer
===
Differentiable
neural
computers
(DNCs)
are
an
extension
of
Neural
Turing
machines,
allowing
for
the
usage
of
fuzzy
amounts
of
each
memory
address
and
a
record
of
chronology.
===
Neural
network
pushdown
automata
===
Neural
network
pushdown
automata
(NNPDA)
are
similar
to
NTMs,
but
tapes
are
replaced
by
analog
stacks
that
are
differentiable
and
trained.
In
this
way,
they
are
similar
in
complexity
to
recognizers
of
context
free
grammars
(CFGs).
===
Memristive
networks
===
Greg
Snider
of
HP
Labs
describes
a
system
of
cortical
computing
with
memristive
nanodevices.
The
memristors
(memory
resistors)
are
implemented
by
thin
film
materials
in
which
the
resistance
is
electrically
tuned
via
the
transport
of
ions
or
oxygen
vacancies
within
the
film.
DARPA's
SyNAPSE
project
has
funded
IBM
Research
and
HP
Labs,
in
collaboration
with
the
Boston
University
Department
of
Cognitive
and
Neural
Systems
(CNS),
to
develop
neuromorphic
architectures
that
may
be
based
on
memristive
systems.
Memristive
networks
are
a
particular
type
of
physical
neural
network
that
have
very
similar
properties
to
(Little-)Hopfield
networks,
as
they
have
continuous
dynamics,
a
limited
memory
capacity
and
natural
relaxation
via
the
minimization
of
a
function
which
is
asymptotic
to
the
Ising
model.
In
this
sense,
the
dynamics
of
a
memristive
circuit
have
the
advantage
compared
to
a
Resistor-Capacitor
network
to
have
a
more
interesting
non-linear
behavior.
From
this
point
of
view,
engineering
analog
memristive
networks
account
for
a
peculiar
type
of
neuromorphic
engineering
in
which
the
device
behavior
depends
on
the
circuit
wiring
or
topology.
The
evolution
of
these
networks
can
be
studied
analytically
using
variations
of
the
Caravelli–Traversa–Di
Ventra
equation.
==
Pseudocode
==
Given
a
time
series
x
of
length
sequence_length.
In
the
recurrent
neural
network,
there
is
a
loop
that
processes
all
entries
of
the
time
series
x
through
the
layers
neural_network
one
after
another.
These
have
as
return
value
in
each
time
step
i
both
the
prediction
y_pred[i]
and
an
updated
hidden
state
hidden,
which
has
the
length
hidden_size.
As
a
result,
after
the
loop,
the
collection
of
all
predictions
y_pred
is
returned.
The
following
pseudocode
(based
on
the
programming
language
Python)
illustrates
the
functionality
of
a
recurrent
neural
network.
Modern
libraries
provide
runtime-optimized
implementations
of
the
above
functionality
or
allow
to
speed
up
the
slow
loop
by
just-in-time
compilation.
==
Training
==
===
Gradient
descent
===
Gradient
descent
is
a
first-order
iterative
optimization
algorithm
for
finding
the
minimum
of
a
function.
In
neural
networks,
it
can
be
used
to
minimize
the
error
term
by
changing
each
weight
in
proportion
to
the
derivative
of
the
error
with
respect
to
that
weight,
provided
the
non-linear
activation
functions
are
differentiable.
Various
methods
for
doing
so
were
developed
in
the
1980s
and
early
1990s
by
Werbos,
Williams,
Robinson,
Schmidhuber,
Hochreiter,
Pearlmutter
and
others.
The
standard
method
is
called
"backpropagation
through
time"
or
BPTT,
and
is
a
generalization
of
back-propagation
for
feed-forward
networks.
Like
that
method,
it
is
an
instance
of
automatic
differentiation
in
the
reverse
accumulation
mode
of
Pontryagin's
minimum
principle.
A
more
computationally
expensive
online
variant
is
called
"Real-Time
Recurrent
Learning"
or
RTRL,
which
is
an
instance
of
automatic
differentiation
in
the
forward
accumulation
mode
with
stacked
tangent
vectors.
Unlike
BPTT,
this
algorithm
is
local
in
time
but
not
local
in
space.
In
this
context,
local
in
space
means
that
a
unit's
weight
vector
can
be
updated
using
only
information
stored
in
the
connected
units
and
the
unit
itself
such
that
update
complexity
of
a
single
unit
is
linear
in
the
dimensionality
of
the
weight
vector.
Local
in
time
means
that
the
updates
take
place
continually
(on-line)
and
depend
only
on
the
most
recent
time
step
rather
than
on
multiple
time
steps
within
a
given
time
horizon
as
in
BPTT.
Biological
neural
networks
appear
to
be
local
with
respect
to
both
time
and
space.For
recursively
computing
the
partial
derivatives,
RTRL
has
a
time-complexity
of
O(number
of
hidden
x
number
of
weights)
per
time
step
for
computing
the
Jacobian
matrices,
while
BPTT
only
takes
O(number
of
weights)
per
time
step,
at
the
cost
of
storing
all
forward
activations
within
the
given
time
horizon.
An
online
hybrid
between
BPTT
and
RTRL
with
intermediate
complexity
exists,
along
with
variants
for
continuous
time.A
major
problem
with
gradient
descent
for
standard
RNN
architectures
is
that
error
gradients
vanish
exponentially
quickly
with
the
size
of
the
time
lag
between
important
events.
LSTM
combined
with
a
BPTT/RTRL
hybrid
learning
method
attempts
to
overcome
these
problems.
This
problem
is
also
solved
in
the
independently
recurrent
neural
network
(IndRNN)
by
reducing
the
context
of
a
neuron
to
its
own
past
state
and
the
cross-neuron
information
can
then
be
explored
in
the
following
layers.
Memories
of
different
ranges
including
long-term
memory
can
be
learned
without
the
gradient
vanishing
and
exploding
problem.
The
on-line
algorithm
called
causal
recursive
backpropagation
(CRBP),
implements
and
combines
BPTT
and
RTRL
paradigms
for
locally
recurrent
networks.
It
works
with
the
most
general
locally
recurrent
networks.
The
CRBP
algorithm
can
minimize
the
global
error
term.
This
fact
improves
the
stability
of
the
algorithm,
providing
a
unifying
view
of
gradient
calculation
techniques
for
recurrent
networks
with
local
feedback.
One
approach
to
gradient
information
computation
in
RNNs
with
arbitrary
architectures
is
based
on
signal-flow
graphs
diagrammatic
derivation.
It
uses
the
BPTT
batch
algorithm,
based
on
Lee's
theorem
for
network
sensitivity
calculations.
It
was
proposed
by
Wan
and
Beaufays,
while
its
fast
online
version
was
proposed
by
Campolucci,
Uncini
and
Piazza.
===
Global
optimization
methods
===
Training
the
weights
in
a
neural
network
can
be
modeled
as
a
non-linear
global
optimization
problem.
A
target
function
can
be
formed
to
evaluate
the
fitness
or
error
of
a
particular
weight
vector
as
follows:
First,
the
weights
in
the
network
are
set
according
to
the
weight
vector.
Next,
the
network
is
evaluated
against
the
training
sequence.
Typically,
the
sum-squared
difference
between
the
predictions
and
the
target
values
specified
in
the
training
sequence
is
used
to
represent
the
error
of
the
current
weight
vector.
Arbitrary
global
optimization
techniques
may
then
be
used
to
minimize
this
target
function.
The
most
common
global
optimization
method
for
training
RNNs
is
genetic
algorithms,
especially
in
unstructured
networks.Initially,
the
genetic
algorithm
is
encoded
with
the
neural
network
weights
in
a
predefined
manner
where
one
gene
in
the
chromosome
represents
one
weight
link.
The
whole
network
is
represented
as
a
single
chromosome.
The
fitness
function
is
evaluated
as
follows:
Each
weight
encoded
in
the
chromosome
is
assigned
to
the
respective
weight
link
of
the
network.
The
training
set
is
presented
to
the
network
which
propagates
the
input
signals
forward.
The
mean-squared
error
is
returned
to
the
fitness
function.
This
function
drives
the
genetic
selection
process.Many
chromosomes
make
up
the
population;
therefore,
many
different
neural
networks
are
evolved
until
a
stopping
criterion
is
satisfied.
A
common
stopping
scheme
is:
When
the
neural
network
has
learned
a
certain
percentage
of
the
training
data
or
When
the
minimum
value
of
the
mean-squared-error
is
satisfied
or
When
the
maximum
number
of
training
generations
has
been
reached.The
fitness
function
evaluates
the
stopping
criterion
as
it
receives
the
mean-squared
error
reciprocal
from
each
network
during
training.
Therefore,
the
goal
of
the
genetic
algorithm
is
to
maximize
the
fitness
function,
reducing
the
mean-squared
error.
Other
global
(and/or
evolutionary)
optimization
techniques
may
be
used
to
seek
a
good
set
of
weights,
such
as
simulated
annealing
or
particle
swarm
optimization.
==
Related
fields
and
models
==
RNNs
may
behave
chaotically.
In
such
cases,
dynamical
systems
theory
may
be
used
for
analysis.
They
are
in
fact
recursive
neural
networks
with
a
particular
structure:
that
of
a
linear
chain.
Whereas
recursive
neural
networks
operate
on
any
hierarchical
structure,
combining
child
representations
into
parent
representations,
recurrent
neural
networks
operate
on
the
linear
progression
of
time,
combining
the
previous
time
step
and
a
hidden
representation
into
the
representation
for
the
current
time
step.
In
particular,
RNNs
can
appear
as
nonlinear
versions
of
finite
impulse
response
and
infinite
impulse
response
filters
and
also
as
a
nonlinear
autoregressive
exogenous
model
(NARX).The
effect
of
memory-based
learning
for
the
recognition
of
sequences
can
also
be
implemented
by
a
more
biological-based
model
which
uses
the
silencing
mechanism
exhibited
in
neurons
with
a
relatively
high
frequency
spiking
activity.
==
Libraries
==
Apache
Singa
Caffe:
Created
by
the
Berkeley
Vision
and
Learning
Center
(BVLC).
It
supports
both
CPU
and
GPU.
Developed
in
C++,
and
has
Python
and
MATLAB
wrappers.
Chainer:
Fully
in
Python,
production
support
for
CPU,
GPU,
distributed
training.
Deeplearning4j:
Deep
learning
in
Java
and
Scala
on
multi-GPU-enabled
Spark.
Flux:
includes
interfaces
for
RNNs,
including
GRUs
and
LSTMs,
written
in
Julia.
Keras:
High-level
API,
providing
a
wrapper
to
many
other
deep
learning
libraries.
Microsoft
Cognitive
Toolkit
MXNet:
an
open-source
deep
learning
framework
used
to
train
and
deploy
deep
neural
networks.
PyTorch:
Tensors
and
Dynamic
neural
networks
in
Python
with
GPU
acceleration.
TensorFlow:
Apache
2.0-licensed
Theano-like
library
with
support
for
CPU,
GPU
and
Google's
proprietary
TPU,
mobile
Theano:
A
deep-learning
library
for
Python
with
an
API
largely
compatible
with
the
NumPy
library.
Torch:
A
scientific
computing
framework
with
support
for
machine
learning
algorithms,
written
in
C
and
Lua.
==
Applications
==
Applications
of
recurrent
neural
networks
include:
Machine
translation
Robot
control
Time
series
prediction
Speech
recognition
Speech
synthesis
Brain–computer
interfaces
Time
series
anomaly
detection
Text-to-Video
model
Rhythm
learning
Music
composition
Grammar
learning
Handwriting
recognition
Human
action
recognition
Protein
homology
detection
Predicting
subcellular
localization
of
proteins
Several
prediction
tasks
in
the
area
of
business
process
management
Prediction
in
medical
care
pathways
Predictions
of
fusion
plasma
disruptions
in
reactors
(Fusion
Recurrent
Neural
Network
(FRNN)
code)
==
References
==
==
Further
reading
==
Mandic,
Danilo
P.;
Chambers,
Jonathon
A.
(2001).
Recurrent
Neural
Networks
for
Prediction:
Learning
Algorithms,
Architectures
and
Stability.
Wiley.
ISBN
978-0-471-49517-8.
==
External
links
==
Recurrent
Neural
Networks
with
over
60
RNN
papers
by
Jürgen
Schmidhuber's
group
at
Dalle
Molle
Institute
for
Artificial
Intelligence
Research
Elman
Neural
Network
implementation
for
WEKA
A
residual
neural
network
(also
referred
to
as
a
residual
network
or
ResNet)
is
a
deep
learning
model
in
which
the
weight
layers
learn
residual
functions
with
reference
to
the
layer
inputs.
It
behaves
like
a
highway
network
whose
gates
are
opened
through
strongly
positive
bias
weights.
This
enables
deep
learning
models
with
tens
or
hundreds
of
layers
to
train
easily
and
approach
better
accuracy
when
going
deeper.
The
identity
skip
connections,
often
referred
to
as
"residual
connections",
are
also
used
in
the
1997
LSTM
networks,
Transformer
models
(e.g.,
BERT,
GPT
models
such
as
ChatGPT),
the
AlphaGo
Zero
system,
the
AlphaStar
system,
and
the
AlphaFold
system.
Residual
networks
were
developed
by
Kaiming
He,
Xiangyu
Zhang,
Shaoqing
Ren,
and
Jian
Sun,
who
won
the
2015
ImageNet
competition.
==
Formulation
==
===
Background
===
The
AlexNet
model
developed
in
2012
for
ImageNet
was
an
eight-layer
convolutional
neural
network.
The
neural
networks
developed
in
2014
by
the
Visual
Geometry
Group
(VGG)
at
the
University
of
Oxford
approached
a
depth
of
19
layers
by
stacking
3-by-3
convolutional
layers.
However,
stacking
more
layers
led
to
a
steep
reduction
in
training
accuracy,
which
is
referred
to
as
the
"degradation"
problem.A
deeper
network
should
not
produce
a
higher
training
loss
than
its
shallower
counterpart,
if
this
deeper
network
can
be
constructed
by
its
shallower
counterpart
stacked
with
extra
layers.
If
the
extra
layers
can
be
set
as
identity
mappings,
the
deeper
network
would
represent
the
same
function
as
the
shallower
counterpart.
It
is
hypothesized
that
the
optimizer
is
not
able
to
approach
identity
mappings
for
the
parameterized
layers.
===
Residual
learning
===
In
a
multi-layer
neural
network
model,
consider
a
subnetwork
with
a
certain
number
(e.g.,
2
or
3)
of
stacked
layers.
Denote
the
underlying
function
performed
by
this
subnetwork
as
H(x){\textstyle
H(x)},
where
x{\textstyle
x}
is
the
input
to
this
subnetwork.
The
idea
of
"Residual
Learning"
re-parameterizes
this
subnetwork
and
lets
the
parameter
layers
represent
a
residual
function
F(x):=H(x)−x{\textstyle
F(x):=H(x)-x}.
The
output
y{\textstyle
y}
of
this
subnetwork
is
represented
as:
y=F(x)+x{\displaystyle
{\begin{aligned}y&=F(x)+x\end{aligned}}}This
is
also
the
principle
of
the
1997
LSTM
cell
computing
yt+1=F(xt)+xt{\textstyle
y_{t+1}=F(x_{t})+x_{t}},
which
becomes
y=F(x)+x{\textstyle
y=F(x)+x}
during
backpropagation
through
time.
The
function
F(x){\textstyle
F(x)}
is
often
represented
by
matrix
multiplication
interlaced
with
activation
functions
and
normalization
operations
(e.g.,
Batch
Normalization
or
Layer
Normalization).
This
subnetwork
is
referred
to
as
a
"Residual
Block".
A
deep
residual
network
is
constructed
by
stacking
a
series
of
residual
blocks.
The
operation
of
"+
x{\textstyle
+\
x}"
in
"y=F(x)+x{\textstyle
y=F(x)+x}"
is
approached
by
a
skip
connection
that
performs
identity
mapping
and
connects
the
input
of
a
residual
block
with
its
output.
This
connection
is
often
referred
to
as
a
"Residual
Connection"
in
later
work.
===
Signal
propagation
===
The
introduction
of
identity
mappings
facilitates
signal
propagation
in
both
forward
and
backward
paths.
====
Forward
propagation
====
If
the
output
of
the
ℓ{\textstyle
\ell
}-th
residual
block
is
the
input
to
the
(ℓ+1){\textstyle
(\ell
+1)}-th
residual
block
(i.e.,
assuming
no
activation
function
between
blocks),
we
have:
xℓ+1=F(xℓ)+xℓ{\displaystyle
{\begin{aligned}x_{\ell
+1}&=F(x_{\ell
})+x_{\ell
}\end{aligned}}}Applying
this
formulation
recursively,
e.g.,
xℓ+2=F(xℓ+1)+xℓ+1=F(xℓ+1)+F(xℓ)+xℓ{\displaystyle
{\begin{aligned}x_{\ell
+2}=F(x_{\ell
+1})+x_{\ell
+1}=F(x_{\ell
+1})+F(x_{\ell
})+x_{\ell
}\end{aligned}}},
we
have:
xL=xℓ+∑i=lL−1F(xi){\displaystyle
{\begin{aligned}x_{L}&=x_{\ell
}+\sum
_{i=l}^{L-1}F(x_{i})\\\end{aligned}}}where
L{\textstyle
L}
is
the
index
of
any
later
residual
block
(e.g.,
the
last
block)
and
ℓ{\textstyle
\ell
}
is
the
index
of
any
earlier
block.
This
formulation
suggests
that
there
is
always
a
signal
that
is
directly
sent
from
a
shallower
block
ℓ{\textstyle
\ell
}
to
a
deeper
block
L{\textstyle
L}.
====
Backward
propagation
====
The
Residual
Learning
formulation
provides
the
added
benefit
of
addressing
the
vanishing
gradient
problem
to
some
extent.
However,
it
is
crucial
to
acknowledge
that
the
vanishing
gradient
issue
is
not
the
root
cause
of
the
degradation
problem,
as
it
has
already
been
tackled
through
the
use
of
normalization
layers.
Taking
the
derivative
w.r.t.
xℓ{\textstyle
x_{\ell
}}
according
to
the
above
forward
propagation,
we
have:
∂E∂xℓ=∂E∂xL∂xL∂xℓ=∂E∂xL(1+∂∂xℓ∑i=lL−1F(xi))=∂E∂xL+∂E∂xL∂∂xℓ∑i=lL−1F(xi){\displaystyle
{\begin{aligned}{\frac
{\partial
{\mathcal
{E}}}{\partial
x_{\ell
}}}&={\frac
{\partial
{\mathcal
{E}}}{\partial
x_{L}}}{\frac
{\partial
x_{L}}{\partial
x_{\ell
}}}\\&={\frac
{\partial
{\mathcal
{E}}}{\partial
x_{L}}}\left(1+{\frac
{\partial
}{\partial
x_{\ell
}}}\sum
_{i=l}^{L-1}F(x_{i})\right)\\&={\frac
{\partial
{\mathcal
{E}}}{\partial
x_{L}}}+{\frac
{\partial
{\mathcal
{E}}}{\partial
x_{L}}}{\frac
{\partial
}{\partial
x_{\ell
}}}\sum
_{i=l}^{L-1}F(x_{i})\\\end{aligned}}}Here
E{\textstyle
{\mathcal
{E}}}
is
the
loss
function
to
be
minimized.
This
formulation
suggests
that
the
gradient
computation
of
a
shallower
layer
∂E∂xℓ{\textstyle
{\frac
{\partial
{\mathcal
{E}}}{\partial
x_{\ell
}}}}
always
has
a
term
∂E∂xL{\textstyle
{\frac
{\partial
{\mathcal
{E}}}{\partial
x_{L}}}}
that
is
directly
added.
Even
if
the
gradients
of
the
F(xi){\textstyle
F(x_{i})}
terms
are
small,
the
total
gradient
∂E∂xℓ{\textstyle
{\frac
{\partial
{\mathcal
{E}}}{\partial
x_{\ell
}}}}
is
not
vanishing
thanks
to
the
added
term
∂E∂xL{\textstyle
{\frac
{\partial
{\mathcal
{E}}}{\partial
x_{L}}}}.
==
Variants
of
residual
blocks
==
===
Basic
block
===
A
Basic
Block
is
the
simplest
building
block
studied
in
the
original
ResNet.
This
block
consists
of
two
sequential
3x3
convolutional
layers
and
a
residual
connection.
The
input
and
output
dimensions
of
both
layers
are
equal.
===
Bottleneck
block
===
A
Bottleneck
Block
consists
of
three
sequential
convolutional
layers
and
a
residual
connection.
The
first
layer
in
this
block
is
a
1x1
convolution
for
dimension
reduction,
e.g.,
to
1/4
of
the
input
dimension;
the
second
layer
performs
a
3x3
convolution;
the
last
layer
is
another
1x1
convolution
for
dimension
restoration.
The
models
of
ResNet-50,
ResNet-101,
and
ResNet-152
in
are
all
based
on
Bottleneck
Blocks.
===
Pre-activation
block
===
The
Pre-activation
Residual
Block
applies
the
activation
functions
(e.g.,
non-linearity
and
normalization)
before
applying
the
residual
function
F{\textstyle
F}.
Formally,
the
computation
of
a
Pre-activation
Residual
Block
can
be
written
as:
xℓ+1=F(ϕ(xℓ))+xℓ{\displaystyle
{\begin{aligned}x_{\ell
+1}&=F(\phi
(x_{\ell
}))+x_{\ell
}\end{aligned}}}where
ϕ{\textstyle
\phi
}
can
be
any
non-linearity
activation
(e.g.,
ReLU)
or
normalization
(e.g.,
LayerNorm)
operation.
This
design
reduces
the
number
of
non-identity
mappings
between
Residual
Blocks.
This
design
was
used
to
train
models
with
200
to
over
1000
layers.Since
GPT-2,
the
Transformer
Blocks
have
been
dominantly
implemented
as
Pre-activation
Blocks.
This
is
often
referred
to
as
"pre-normalization"
in
the
literature
of
Transformer
models.
===
Transformer
block
===
A
Transformer
Block
is
a
stack
of
two
Residual
Blocks.
Each
Residual
Block
has
a
Residual
Connection.
The
first
Residual
Block
is
a
Multi-Head
Attention
Block,
which
performs
(self-)attention
computation
followed
by
a
linear
projection.
The
second
Residual
Block
is
a
feed-forward
Multi-Layer
Perceptron
(MLP)
Block.
This
block
is
analogous
to
an
"inverse"
bottleneck
block:
it
has
a
linear
projection
layer
(which
is
equivalent
to
a
1x1
convolution
in
the
context
of
Convolutional
Neural
Networks)
that
increases
the
dimension,
and
another
linear
projection
that
reduces
the
dimension.
A
Transformer
Block
has
a
depth
of
4
layers
(linear
projections).
The
GPT-3
model
has
96
Transformer
Blocks
(in
the
literature
of
Transformers,
a
Transformer
Block
is
often
referred
to
as
a
"Transformer
Layer").
This
model
has
a
depth
of
about
400
projection
layers,
including
96x4
layers
in
Transformer
Blocks
and
a
few
extra
layers
for
input
embedding
and
output
prediction.
Very
deep
Transformer
models
cannot
be
successfully
trained
without
Residual
Connections.
==
Related
Work
==
In
1961,
Frank
Rosenblatt
described
a
three-layer
multilayer
perceptron
(MLP)
model
with
skip
connections.
The
model
was
referred
to
as
a
"cross-coupled
system",
and
the
skip
connections
were
forms
of
cross-coupled
connections.
In
two
books
published
in
1994
and
1996,
"skip-layer"
connections
were
presented
in
feed-forward
MLP
models:
"The
general
definition
[of
MLP]
allows
more
than
one
hidden
layer,
and
it
also
allows
'skip-layer'
connections
from
input
to
output"
(p261
in,
p144
in
),
"...
which
allows
the
non-linear
units
to
perturb
a
linear
functional
form"
(p262
in
).
This
description
suggests
that
the
non-linear
MLP
performs
like
a
residual
function
(perturbation)
added
to
a
linear
function.
Sepp
Hochreiter
analyzed
the
vanishing
gradient
problem
in
1991
and
attributed
to
it
the
reason
why
deep
learning
did
not
work
well.
To
overcome
this
problem,
long
short-term
memory
(LSTM)
recurrent
neural
networks
had
skip
connections
or
residual
connections
with
a
weight
of
1.0
in
every
LSTM
cell
(called
the
constant
error
carrousel)
to
compute
yt+1=F(xt)+xt{\textstyle
y_{t+1}=F(x_{t})+x_{t}}.
During
backpropagation
through
time,
this
becomes
the
above-mentioned
residual
formula
y=F(x)+x{\textstyle
y=F(x)+x}
for
feedforward
neural
networks.
This
enables
training
very
deep
recurrent
neural
networks
with
a
very
long
time
span
t.
A
later
LSTM
version
published
in
2000
modulates
the
identity
LSTM
connections
by
so-called
forget
gates
such
that
their
weights
are
not
fixed
to
1.0
but
can
be
learned.
In
experiments,
the
forget
gates
were
initialized
with
positive
bias
weights,
thus
being
opened,
addressing
the
vanishing
gradient
problem.
The
highway
network
of
May
2015
applies
these
principles
to
feedforward
neural
networks.
It
was
reported
to
be
"the
first
very
deep
feedforward
network
with
hundreds
of
layers".
It
is
like
an
LSTM
with
forget
gates
unfolded
in
time,
while
the
later
Residual
Nets
have
no
equivalent
of
forget
gates
and
are
like
the
unfolded
original
LSTM.
If
the
skip
connections
in
Highway
Networks
are
"without
gates",
or
if
their
gates
are
kept
open
(activation
1.0)
through
strong
positive
bias
weights,
they
become
the
identity
skip
connections
in
Residual
Networks.
The
original
Highway
Network
paper
not
only
introduced
the
basic
principle
for
very
deep
feedforward
networks,
but
also
included
experimental
results
with
20,
50,
and
100
layers
networks,
and
mentioned
ongoing
experiments
with
up
to
900
layers.
Networks
with
50
or
100
layers
had
lower
training
error
than
their
plain
network
counterparts,
but
no
lower
training
error
than
their
20
layers
counterpart
(on
the
MNIST
dataset,
Figure
1
in
).
No
improvement
on
test
accuracy
was
reported
with
networks
deeper
than
19
layers
(on
the
CIFAR-10
dataset;
Table
1
in
).
The
ResNet
paper,
however,
provided
strong
experimental
evidence
of
the
benefits
of
going
deeper
than
20
layers.
It
argued
that
the
identity
mapping
without
modulation
is
crucial
and
mentioned
that
modulation
in
the
skip
connection
can
still
lead
to
vanishing
signals
in
forward
and
backward
propagation
(Section
3
in
).
This
is
also
why
the
forget
gates
of
the
2000
LSTM
were
initially
opened
through
positive
bias
weights:
as
long
as
the
gates
are
open,
it
behaves
like
the
1997
LSTM.
Similarly,
a
Highway
Net
whose
gates
are
opened
through
strongly
positive
bias
weights
behaves
like
a
ResNet.
The
skip
connections
used
in
modern
neural
networks
(e.g.,
Transformers)
are
dominantly
identity
mappings.
DenseNets
in
2016
were
designed
as
deep
neural
networks
that
attempt
to
connect
each
layer
to
every
other
layer.
DenseNets
approached
this
goal
by
using
identity
mappings
as
skip
connections.
Unlike
ResNets,
DenseNets
merge
the
layer
output
with
skip
connections
by
concatenation,
not
addition.
Neural
networks
with
Stochastic
Depth
were
made
possible
given
the
Residual
Network
architectures.
This
training
procedure
randomly
drops
a
subset
of
layers
and
lets
the
signal
propagate
through
the
identity
skip
connection.
Also
known
as
"DropPath",
this
is
an
effective
regularization
method
for
training
large
and
deep
models,
such
as
the
Vision
Transformer
(ViT).
==
Biological
relation
==
The
original
Residual
Network
paper
made
no
claim
on
being
inspired
by
biological
systems.
But
research
later
on
has
related
Residual
Networks
to
biologically-plausible
algorithms.
A
study
published
in
Science
in
2023
disclosed
the
complete
connectome
of
an
insect
brain
(of
a
fruit
fly
larva).
This
study
discovered
"multilayer
shortcuts"
that
resemble
the
skip
connections
in
artificial
neural
networks,
including
ResNets.
==
References
==
A
Siamese
neural
network
(sometimes
called
a
twin
neural
network)
is
an
artificial
neural
network
that
uses
the
same
weights
while
working
in
tandem
on
two
different
input
vectors
to
compute
comparable
output
vectors.
Often
one
of
the
output
vectors
is
precomputed,
thus
forming
a
baseline
against
which
the
other
output
vector
is
compared.
This
is
similar
to
comparing
fingerprints
but
can
be
described
more
technically
as
a
distance
function
for
locality-sensitive
hashing.It
is
possible
to
build
an
architecture
that
is
functionally
similar
to
a
siamese
network
but
implements
a
slightly
different
function.
This
is
typically
used
for
comparing
similar
instances
in
different
type
sets.Uses
of
similarity
measures
where
a
twin
network
might
be
used
are
such
things
as
recognizing
handwritten
checks,
automatic
detection
of
faces
in
camera
images,
and
matching
queries
with
indexed
documents.
The
perhaps
most
well-known
application
of
twin
networks
are
face
recognition,
where
known
images
of
people
are
precomputed
and
compared
to
an
image
from
a
turnstile
or
similar.
It
is
not
obvious
at
first,
but
there
are
two
slightly
different
problems.
One
is
recognizing
a
person
among
a
large
number
of
other
persons,
that
is
the
facial
recognition
problem.
DeepFace
is
an
example
of
such
a
system.
In
its
most
extreme
form
this
is
recognizing
a
single
person
at
a
train
station
or
airport.
The
other
is
face
verification,
that
is
to
verify
whether
the
photo
in
a
pass
is
the
same
as
the
person
claiming
he
or
she
is
the
same
person.
The
twin
network
might
be
the
same,
but
the
implementation
can
be
quite
different.
==
Learning
==
Learning
in
twin
networks
can
be
done
with
triplet
loss
or
contrastive
loss.
For
learning
by
triplet
loss
a
baseline
vector
(anchor
image)
is
compared
against
a
positive
vector
(truthy
image)
and
a
negative
vector
(falsy
image).
The
negative
vector
will
force
learning
in
the
network,
while
the
positive
vector
will
act
like
a
regularizer.
For
learning
by
contrastive
loss
there
must
be
a
weight
decay
to
regularize
the
weights,
or
some
similar
operation
like
a
normalization.
A
distance
metric
for
a
loss
function
may
have
the
following
properties
Non-negativity:
δ(x,y)≥0{\displaystyle
\delta
(x,y)\geq
0}
Identity
of
Non-discernibles:
δ(x,y)=0⟺x=y{\displaystyle
\delta
(x,y)=0\iff
x=y}
Commutativity:
δ(x,y)=δ(y,x){\displaystyle
\delta
(x,y)=\delta
(y,x)}
Triangle
inequality:
δ(x,z)≤δ(x,y)+δ(y,z){\displaystyle
\delta
(x,z)\leq
\delta
(x,y)+\delta
(y,z)}In
particular,
the
triplet
loss
algorithm
is
often
defined
with
squared
Euclidean
(which
unlike
Euclidean,
does
not
have
triangle
inequality)
distance
at
its
core.
===
Predefined
metrics,
Euclidean
distance
metric
===
The
common
learning
goal
is
to
minimize
a
distance
metric
for
similar
objects
and
maximize
for
distinct
ones.
This
gives
a
loss
function
like
δ(x(i),x(j))={min
‖f⁡(x(i))−f⁡(x(j))‖,i=jmax
‖f⁡(x(i))−f⁡(x(j))‖,i≠j{\displaystyle
{\begin{aligned}\delta
(x^{(i)},x^{(j)})={\begin{cases}\min
\
\|\operatorname
{f}
\left(x^{(i)}\right)-\operatorname
{f}
\left(x^{(j)}\right)\|\,,i=j\\\max
\
\|\operatorname
{f}
\left(x^{(i)}\right)-\operatorname
{f}
\left(x^{(j)}\right)\|\,,i\neq
j\end{cases}}\end{aligned}}}
i,j{\displaystyle
i,j}
are
indexes
into
a
set
of
vectors
f⁡(⋅){\displaystyle
\operatorname
{f}
(\cdot
)}
function
implemented
by
the
twin
networkThe
most
common
distance
metric
used
is
Euclidean
distance,
in
case
of
which
the
loss
function
can
be
rewritten
in
matrix
form
as
δ⁡(x(i),x(j))≈(x(i)−x(j))T(x(i)−x(j)){\displaystyle
\operatorname
{\delta
}
(\mathbf
{x}
^{(i)},\mathbf
{x}
^{(j)})\approx
(\mathbf
{x}
^{(i)}-\mathbf
{x}
^{(j)})^{T}(\mathbf
{x}
^{(i)}-\mathbf
{x}
^{(j)})}
===
Learned
metrics,
nonlinear
distance
metric
===
A
more
general
case
is
where
the
output
vector
from
the
twin
network
is
passed
through
additional
network
layers
implementing
non-linear
distance
metrics.
ifi=jthenδ⁡[f⁡(x(i)),f⁡(x(j))]is
smallotherwiseδ⁡[f⁡(x(i)),f⁡(x(j))]is
large{\displaystyle
{\begin{aligned}{\text{if}}\,i=j\,{\text{then}}&\,\operatorname
{\delta
}
\left[\operatorname
{f}
\left(x^{(i)}\right),\,\operatorname
{f}
\left(x^{(j)}\right)\right]\,{\text{is
small}}\\{\text{otherwise}}&\,\operatorname
{\delta
}
\left[\operatorname
{f}
\left(x^{(i)}\right),\,\operatorname
{f}
\left(x^{(j)}\right)\right]\,{\text{is
large}}\end{aligned}}}
i,j{\displaystyle
i,j}
are
indexes
into
a
set
of
vectors
f⁡(⋅){\displaystyle
\operatorname
{f}
(\cdot
)}function
implemented
by
the
twin
network
δ⁡(⋅){\displaystyle
\operatorname
{\delta
}
(\cdot
)}function
implemented
by
the
network
joining
outputs
from
the
twin
networkOn
a
matrix
form
the
previous
is
often
approximated
as
a
Mahalanobis
distance
for
a
linear
space
as
δ⁡(x(i),x(j))≈(x(i)−x(j))TM(x(i)−x(j)){\displaystyle
\operatorname
{\delta
}
(\mathbf
{x}
^{(i)},\mathbf
{x}
^{(j)})\approx
(\mathbf
{x}
^{(i)}-\mathbf
{x}
^{(j)})^{T}\mathbf
{M}
(\mathbf
{x}
^{(i)}-\mathbf
{x}
^{(j)})}This
can
be
further
subdivided
in
at
least
Unsupervised
learning
and
Supervised
learning.
===
Learned
metrics,
half-twin
networks
===
This
form
also
allows
the
twin
network
to
be
more
of
a
half-twin,
implementing
a
slightly
different
functions
ifi=jthenδ⁡[f⁡(x(i)),g⁡(x(j))]is
smallotherwiseδ⁡[f⁡(x(i)),g⁡(x(j))]is
large{\displaystyle
{\begin{aligned}{\text{if}}\,i=j\,{\text{then}}&\,\operatorname
{\delta
}
\left[\operatorname
{f}
\left(x^{(i)}\right),\,\operatorname
{g}
\left(x^{(j)}\right)\right]\,{\text{is
small}}\\{\text{otherwise}}&\,\operatorname
{\delta
}
\left[\operatorname
{f}
\left(x^{(i)}\right),\,\operatorname
{g}
\left(x^{(j)}\right)\right]\,{\text{is
large}}\end{aligned}}}
i,j{\displaystyle
i,j}
are
indexes
into
a
set
of
vectors
f⁡(⋅),g⁡(⋅){\displaystyle
\operatorname
{f}
(\cdot
),\operatorname
{g}
(\cdot
)}function
implemented
by
the
half-twin
network
δ⁡(⋅){\displaystyle
\operatorname
{\delta
}
(\cdot
)}function
implemented
by
the
network
joining
outputs
from
the
twin
network
==
Twin
networks
for
object
tracking
==
Twin
networks
have
been
used
in
object
tracking
because
of
its
unique
two
tandem
inputs
and
similarity
measurement.
In
object
tracking,
one
input
of
the
twin
network
is
user
pre-selected
exemplar
image,
the
other
input
is
a
larger
search
image,
which
twin
network's
job
is
to
locate
exemplar
inside
of
search
image.
By
measuring
the
similarity
between
exemplar
and
each
part
of
the
search
image,
a
map
of
similarity
score
can
be
given
by
the
twin
network.
Furthermore,
using
a
Fully
Convolutional
Network,
the
process
of
computing
each
sector's
similarity
score
can
be
replaced
with
only
one
cross
correlation
layer.After
being
first
introduced
in
2016,
Twin
fully
convolutional
network
has
been
used
in
many
High-performance
Real-time
Object
Tracking
Neural
Networks.
Like
CFnet,
StructSiam,
SiamFC-tri,
DSiam,
SA-Siam,
SiamRPN,
DaSiamRPN,
Cascaded
SiamRPN,
SiamMask,
SiamRPN++,
Deeper
and
Wider
SiamRPN.
==
See
also
==
Artificial
neural
network
Triplet
loss
==
Further
reading
==
Chicco,
Davide
(2020),
"Siamese
neural
networks:
an
overview",
Artificial
Neural
Networks,
Methods
in
Molecular
Biology,
vol.
2190
(3rd
ed.),
New
York
City,
New
York,
USA:
Springer
Protocols,
Humana
Press,
pp.
73–94,
doi:10.1007/978-1-0716-0826-5_3,
ISBN
978-1-0716-0826-5,
PMID
32804361,
S2CID
221144012
==
References
==
Spiking
neural
networks
(SNNs)
are
artificial
neural
networks
(ANN)
that
more
closely
mimic
natural
neural
networks.
In
addition
to
neuronal
and
synaptic
state,
SNNs
incorporate
the
concept
of
time
into
their
operating
model.
The
idea
is
that
neurons
in
the
SNN
do
not
transmit
information
at
each
propagation
cycle
(as
it
happens
with
typical
multi-layer
perceptron
networks),
but
rather
transmit
information
only
when
a
membrane
potential—an
intrinsic
quality
of
the
neuron
related
to
its
membrane
electrical
charge—reaches
a
specific
value,
called
the
threshold.
When
the
membrane
potential
reaches
the
threshold,
the
neuron
fires,
and
generates
a
signal
that
travels
to
other
neurons
which,
in
turn,
increase
or
decrease
their
potentials
in
response
to
this
signal.
A
neuron
model
that
fires
at
the
moment
of
threshold
crossing
is
also
called
a
spiking
neuron
model.Although
it
was
previously
believed
that
the
brain
encoded
information
through
spike
rates,
which
can
be
considered
as
the
analogue
variable
output
of
a
traditional
ANN,
research
in
the
field
of
neurobiology
has
indicated
that
high
speed
processing
cannot
solely
be
performed
through
a
rate
based
scheme.
For
example
humans
can
perform
an
image
recognition
task
at
rate
requiring
no
more
than
10ms
of
processing
time
per
neuron
through
the
successive
layers
(going
from
the
retina
to
the
temporal
lobe).
This
time
window
is
too
short
for
a
rate
based
encoding.
The
precise
spike
timings
in
a
small
set
of
spiking
neurons
also
has
a
higher
information
coding
capacity
compared
with
a
rate
based
approach.The
most
prominent
spiking
neuron
model
is
the
leaky
integrate-and-fire
model.
In
the
integrate-and-fire
model,
the
momentary
activation
level
(modeled
as
a
differential
equation)
is
normally
considered
to
be
the
neuron's
state,
with
incoming
spikes
pushing
this
value
higher
or
lower,
until
the
state
eventually
either
decays
or—if
the
firing
threshold
is
reached—the
neuron
fires.
After
firing,
the
state
variable
is
reset
to
a
lower
value.
Various
decoding
methods
exist
for
interpreting
the
outgoing
spike
train
as
a
real-value
number,
relying
on
either
the
frequency
of
spikes
(rate-code),
the
time-to-first-spike
after
stimulation,
or
the
interval
between
spikes.
==
History
==
Many
multi-layer
artificial
neural
networks
are
fully
connected,
receiving
input
from
every
neuron
in
the
previous
layer
and
signalling
every
neuron
in
the
subsequent
layer.
Although
these
networks
have
achieved
breakthroughs
in
many
fields,
they
are
biologically
inaccurate
and
do
not
mimic
the
operation
mechanism
of
neurons
in
the
brain
of
a
living
thing.
The
biologically
inspired
Hodgkin–Huxley
model
of
a
spiking
neuron
was
proposed
in
1952.
This
model
describes
how
action
potentials
are
initiated
and
propagated.
Communication
between
neurons,
which
requires
the
exchange
of
chemical
neurotransmitters
in
the
synaptic
gap,
is
described
in
various
models,
such
as
the
integrate-and-fire
model,
FitzHugh–Nagumo
model
(1961–1962),
and
Hindmarsh–Rose
model
(1984).
The
leaky
integrate-and-fire
model
(or
a
derivative)
is
commonly
used
as
it
is
easier
to
compute
than
the
Hodgkin–Huxley
model.
==
Underpinnings
==
Information
in
the
brain
is
represented
as
action
potentials
(neuron
spikes),
which
may
be
grouped
into
spike
trains
or
even
coordinated
waves
of
brain
activity.
A
fundamental
question
of
neuroscience
is
to
determine
whether
neurons
communicate
by
a
rate
or
temporal
code.
Temporal
coding
suggests
that
a
single
spiking
neuron
can
replace
hundreds
of
hidden
units
on
a
sigmoidal
neural
net.An
SNN
computes
in
the
continuous
rather
than
the
discrete
domain.
The
idea
is
that
neurons
may
not
test
for
activation
in
every
iteration
of
propagation
(as
is
the
case
in
a
typical
multilayer
perceptron
network),
but
only
when
their
membrane
potentials
reach
a
certain
value.
When
a
neuron
is
activated,
it
produces
a
signal
that
is
passed
to
connected
neurons,
raising
or
lowering
their
membrane
potential.
In
a
spiking
neural
network,
a
neuron's
current
state
is
defined
as
its
membrane
potential
(possibly
modeled
as
a
differential
equation).
An
input
pulse
causes
the
membrane
potential
to
rise
for
a
period
of
time
and
then
gradually
decline.
Encoding
schemes
have
been
constructed
to
interpret
these
pulse
sequences
as
a
number,
taking
into
account
both
pulse
frequency
and
pulse
interval.
A
neural
network
model
based
on
pulse
generation
time
can
be
established.
Using
the
exact
time
of
pulse
occurrence,
a
neural
network
can
employ
more
information
and
offer
better
computing
properties.
The
SNN
approach
produces
a
continuous
output
instead
of
the
binary
output
of
traditional
artificial
neural
networks
(ANNs).
Pulse
trains
are
not
easily
interpretable,
hence
the
need
for
encoding
schemes
as
above.
However,
a
pulse
train
representation
may
be
more
suited
for
processing
spatiotemporal
data
(or
continual
real-world
sensory
data
classification).
SNNs
consider
space
by
connecting
neurons
only
to
nearby
neurons
so
that
they
process
input
blocks
separately
(similar
to
CNN
using
filters).
They
consider
time
by
encoding
information
as
pulse
trains
so
as
not
to
lose
information
in
a
binary
encoding.
This
avoids
the
additional
complexity
of
a
recurrent
neural
network
(RNN).
It
turns
out
that
impulse
neurons
are
more
powerful
computational
units
than
traditional
artificial
neurons.SNNs
are
theoretically
more
powerful
than
so
called
"second-generation
networks"
defined
in
as
"[ANNs]
based
on
computational
units
that
apply
activation
function
with
a
continuous
set
of
possible
output
values
to
a
weighted
sum
(or
polynomial)
of
the
inputs;
however,
SNN
training
issues
and
hardware
requirements
limit
their
use.
Although
unsupervised
biologically
inspired
learning
methods
are
available
such
as
Hebbian
learning
and
STDP,
no
effective
supervised
training
method
is
suitable
for
SNNs
that
can
provide
better
performance
than
second-generation
networks.
Spike-based
activation
of
SNNs
is
not
differentiable
thus
making
it
hard
to
develop
gradient
descent
based
training
methods
to
perform
error
backpropagation.
SNNs
have
much
larger
computational
costs
for
simulating
realistic
neural
models
than
traditional
ANNs.Pulse-coupled
neural
networks
(PCNN)
are
often
confused
with
SNNs.
A
PCNN
can
be
seen
as
a
kind
of
SNN.
Currently
there
are
a
few
challenges
when
using
SNNs
that
researchers
are
actively
working
on.
The
first
challenge
concerns
the
nondifferentiability
of
the
spiking
nonlinearity.
The
expressions
for
both
the
forward-
and
backward-learning
methods
contain
the
derivative
of
the
neural
activation
function
which
is
non-differentiable
because
neuron's
output
is
either
1
when
it
spikes,
and
0
otherwise.
This
all-or-nothing
behavior
of
the
binary
spiking
nonlinearity
stops
gradients
from
“flowing”
and
makes
LIF
neurons
unsuitable
for
gradient-based
optimization.
The
second
challenge
concerns
the
implementation
of
the
optimization
algorithm
itself.
Standard
BP
can
be
expensive
in
terms
of
computation,
memory,
and
communication
and
may
be
poorly
suited
to
the
constraints
dictated
by
the
hardware
that
implements
it
(e.g.,
a
computer,
brain,
or
neuromorphic
device).
Regarding
the
first
challenge
there
are
several
approached
in
order
to
overcome
it.
A
few
of
them
are:
resorting
to
entirely
biologically
inspired
local
learning
rules
for
the
hidden
units
translating
conventionally
trained
“rate-based”
NNs
to
SNNs
smoothing
the
network
model
to
be
continuously
differentiable
defining
an
SG
(Surogate
Gradient)
as
a
continuous
relaxation
of
the
real
gradientsIn
the
development
of
SNNs,
incorporating
additional
neuron
dynamics
like
Spike
Frequency
Adaptation
(SFA)
into
neuron
models
marks
a
notable
advance,
enhancing
both
efficiency
and
computational
power.
These
neurons
stand
in
between
biological
complexity
and
compuational
complexity.
Originating
from
biological
insights,
SFA
offers
significant
computational
benefits
by
reducing
power
usage
through
efficient
coding,
especially
in
cases
of
repetitive
or
intense
stimuli.
This
adaptation
improves
signal
clarity
against
background
noise
and
introduces
an
elementary
short-term
memory
at
the
neuron
level,
which
in
turn,
refines
the
accuracy
and
efficiency
of
information
processing.
Recently,
This
phenomenon
is
achieved
mostly
achieved
using
Compartmental
neuron
models.
The
simpler
versions
are
of
neuron
models
with
adaptive
thresholds,
indirect
way
of
achieving
SFA,
equips
SNNs
with
improved
learning
capabilities,
even
with
constrained
synaptic
plasticity,
and
elevates
computational
efficiency.
This
feature
lessens
the
demand
on
network
layers
by
decreasing
the
need
for
spike
processing,
thus
cutting
down
on
computational
load
and
memory
access
time—essential
aspects
of
neural
computation.
Moreover,
SNNs
utilizing
neurons
capable
of
SFA
achieve
levels
of
accuracy
that
rival
those
of
conventional
artificial
neural
networks,
including
those
based
on
long
short-term
memory
models,
while
also
requiring
fewer
neurons
for
comparable
computational
tasks.
This
efficiency
not
only
streamlines
the
computational
workflow
but
also
conserves
space
and
energy,
offering
a
pragmatic
step
forward
in
the
practical
application
of
SNNs
for
complex
computing
tasks,
all
while
maintaining
a
commitment
to
technical
integrity.
==
Applications
==
SNNs
can
in
principle
apply
to
the
same
applications
as
traditional
ANNs.
In
addition,
SNNs
can
model
the
central
nervous
system
of
biological
organisms,
such
as
an
insect
seeking
food
without
prior
knowledge
of
the
environment.
Due
to
their
relative
realism,
they
can
be
used
to
study
the
operation
of
biological
neural
circuits.
Starting
with
a
hypothesis
about
the
topology
of
a
biological
neuronal
circuit
and
its
function,
recordings
of
this
circuit
can
be
compared
to
the
output
of
the
corresponding
SNN,
evaluating
the
plausibility
of
the
hypothesis.
However,
there
is
a
lack
of
effective
training
mechanisms
for
SNNs,
which
can
be
inhibitory
for
some
applications,
including
computer
vision
tasks.
As
of
2019
SNNs
lag
behind
ANNs
in
terms
of
accuracy,
but
the
gap
is
decreasing,
and
has
vanished
on
some
tasks.When
using
SNNs
for
image
based
data
we
need
to
convert
static
images
into
binary
spike
trains
coding.
Types
of
encodings:
Temporal
coding
generates
one
spike
per
neuron
in
which
spike
latency
is
inversely
proportional
to
the
pixel
intensity.
Rate
coding
converts
pixel
intensity
into
a
spike
train
where
the
number
of
spikes
is
proportional
to
the
pixel
intensity.
Direct
coding
uses
a
trainable
layer
to
generate
float
value
for
each
time-step.
We
have
a
learnable
layer
which
converts
each
pixel
at
certain
time
step
in
float
number
and
then
threshold
is
used
on
the
generated
floating
numbers
to
see
if
they
will
be
1
or
0.
Phase
coding
encodes
temporal
information
into
spike
patterns
based
on
a
global
oscillator.
Burst
coding
transmits
the
burst
of
spikes
in
a
small-time
duration,
increasing
the
reliability
of
synaptic
communication
between
neurons.
==
Software
==
A
diverse
range
of
application
software
can
simulate
SNNs.
This
software
can
be
classified
according
to
its
uses:
===
SNN
simulation
===
These
simulate
complex
neural
models
with
a
high
level
of
detail
and
accuracy.
Large
networks
usually
require
lengthy
processing.
Candidates
include:Brian
–
developed
by
Romain
Brette
and
Dan
Goodman
at
the
École
Normale
Supérieure;
GENESIS
(the
GEneral
NEural
SImulation
System)
–
developed
in
James
Bower's
laboratory
at
Caltech;
NEST
–
developed
by
the
NEST
Initiative;
NEURON
–
mainly
developed
by
Michael
Hines,
John
W.
Moore
and
Ted
Carnevale
in
Yale
University
and
Duke
University;
RAVSim
(Runtime
Tool)
–
mainly
developed
by
Sanaullah
in
Bielefeld
University
of
Applied
Sciences
and
Arts;
==
Hardware
==
Future
neuromorphic
architectures
will
comprise
billions
of
such
nanosynapses,
which
require
a
clear
understanding
of
the
physical
mechanisms
responsible
for
plasticity.
Experimental
systems
based
on
ferroelectric
tunnel
junctions
have
been
used
to
show
that
STDP
can
be
harnessed
from
heterogeneous
polarization
switching.
Through
combined
scanning
probe
imaging,
electrical
transport
and
atomic-scale
molecular
dynamics,
conductance
variations
can
be
modelled
by
nucleation-dominated
reversal
of
domains.
Simulations
show
that
arrays
of
ferroelectric
nanosynapses
can
autonomously
learn
to
recognize
patterns
in
a
predictable
way,
opening
the
path
towards
unsupervised
learning.
Akida
is
a
completely
digital
event-based
neural
processing
device
with
1.2
million
artificial
neurons
and
10
billion
artificial
synapses
developed
by
BrainChip.
Utilizing
event-based
possessing,
it
analyzes
essential
inputs
at
specific
points.
Results
are
stored
in
the
on-chip
memory
units.
Neurogrid
is
a
board
that
can
simulate
spiking
neural
networks
directly
in
hardware.
(Stanford
University)
SpiNNaker
(Spiking
Neural
Network
Architecture)
uses
ARM
processors
as
the
building
blocks
of
a
massively
parallel
computing
platform
based
on
a
six-layer
thalamocortical
model.
(University
of
Manchester)
The
SpiNNaker
system
is
based
on
numerical
models
running
in
real
time
on
custom
digital
multicore
chips
using
the
ARM
architecture.
It
provides
custom
digital
chips,
each
with
eighteen
cores
and
a
shared
local
128
Mbyte
RAM,
with
a
total
of
over
1,000,000
cores.
A
single
chip
can
simulate
16,000
neurons
with
eight
million
plastic
synapses
running
in
real
time.
TrueNorth
is
a
processor
that
contains
5.4
billion
transistors
that
consumes
only
70
milliwatts;
most
processors
in
personal
computers
contain
about
1.4
billion
transistors
and
require
35
watts
or
more.
IBM
refers
to
the
design
principle
behind
TrueNorth
as
neuromorphic
computing.
Its
primary
purpose
is
pattern
recognition.
While
critics
say
the
chip
isn't
powerful
enough,
its
supporters
point
out
that
this
is
only
the
first
generation,
and
the
capabilities
of
improved
iterations
will
become
clear.
(IBM)
==
Benchmarks
==
Classification
capabilities
of
spiking
networks
trained
according
to
unsupervised
learning
methods
have
been
tested
on
the
common
benchmark
datasets,
such
as,
Iris,
Wisconsin
Breast
Cancer
or
Statlog
Landsat
dataset.
Various
approaches
to
information
encoding
and
network
design
have
been
used.
For
example,
a
2-layer
feedforward
network
for
data
clustering
and
classification.
Based
on
the
idea
proposed
in
Hopfield
(1995)
the
authors
implemented
models
of
local
receptive
fields
combining
the
properties
of
radial
basis
functions
(RBF)
and
spiking
neurons
to
convert
input
signals
(classified
data)
having
a
floating-point
representation
into
a
spiking
representation.
==
See
also
==
==
References
==
There
are
many
types
of
artificial
neural
networks
(ANN).
Artificial
neural
networks
are
computational
models
inspired
by
biological
neural
networks,
and
are
used
to
approximate
functions
that
are
generally
unknown.
Particularly,
they
are
inspired
by
the
behaviour
of
neurons
and
the
electrical
signals
they
convey
between
input
(such
as
from
the
eyes
or
nerve
endings
in
the
hand),
processing,
and
output
from
the
brain
(such
as
reacting
to
light,
touch,
or
heat).
The
way
neurons
semantically
communicate
is
an
area
of
ongoing
research.
Most
artificial
neural
networks
bear
only
some
resemblance
to
their
more
complex
biological
counterparts,
but
are
very
effective
at
their
intended
tasks
(e.g.
classification
or
segmentation).
Some
artificial
neural
networks
are
adaptive
systems
and
are
used
for
example
to
model
populations
and
environments,
which
constantly
change.
Neural
networks
can
be
hardware-
(neurons
are
represented
by
physical
components)
or
software-based
(computer
models),
and
can
use
a
variety
of
topologies
and
learning
algorithms.
==
Feedforward
==
The
feedforward
neural
network
was
the
first
and
simplest
type.
In
this
network
the
information
moves
only
from
the
input
layer
directly
through
any
hidden
layers
to
the
output
layer
without
cycles/loops.
Feedforward
networks
can
be
constructed
with
various
types
of
units,
such
as
binary
McCulloch–Pitts
neurons,
the
simplest
of
which
is
the
perceptron.
Continuous
neurons,
frequently
with
sigmoidal
activation,
are
used
in
the
context
of
backpropagation.
===
Group
method
of
data
handling
===
The
Group
Method
of
Data
Handling
(GMDH)
features
fully
automatic
structural
and
parametric
model
optimization.
The
node
activation
functions
are
Kolmogorov–Gabor
polynomials
that
permit
additions
and
multiplications.
It
uses
a
deep
multilayer
perceptron
with
eight
layers.
It
is
a
supervised
learning
network
that
grows
layer
by
layer,
where
each
layer
is
trained
by
regression
analysis.
Useless
items
are
detected
using
a
validation
set,
and
pruned
through
regularization.
The
size
and
depth
of
the
resulting
network
depends
on
the
task.
===
Autoencoder
===
An
autoencoder,
autoassociator
or
Diabolo
network:
19
is
similar
to
the
multilayer
perceptron
(MLP)
–
with
an
input
layer,
an
output
layer
and
one
or
more
hidden
layers
connecting
them.
However,
the
output
layer
has
the
same
number
of
units
as
the
input
layer.
Its
purpose
is
to
reconstruct
its
own
inputs
(instead
of
emitting
a
target
value).
Therefore,
autoencoders
are
unsupervised
learning
models.
An
autoencoder
is
used
for
unsupervised
learning
of
efficient
codings,
typically
for
the
purpose
of
dimensionality
reduction
and
for
learning
generative
models
of
data.
===
Probabilistic
===
A
probabilistic
neural
network
(PNN)
is
a
four-layer
feedforward
neural
network.
The
layers
are
Input,
hidden
pattern/summation,
and
output.
In
the
PNN
algorithm,
the
parent
probability
distribution
function
(PDF)
of
each
class
is
approximated
by
a
Parzen
window
and
a
non-parametric
function.
Then,
using
PDF
of
each
class,
the
class
probability
of
a
new
input
is
estimated
and
Bayes’
rule
is
employed
to
allocate
it
to
the
class
with
the
highest
posterior
probability.
It
was
derived
from
the
Bayesian
network
and
a
statistical
algorithm
called
Kernel
Fisher
discriminant
analysis.
It
is
used
for
classification
and
pattern
recognition.
===
Time
delay
===
A
time
delay
neural
network
(TDNN)
is
a
feedforward
architecture
for
sequential
data
that
recognizes
features
independent
of
sequence
position.
In
order
to
achieve
time-shift
invariance,
delays
are
added
to
the
input
so
that
multiple
data
points
(points
in
time)
are
analyzed
together.
It
usually
forms
part
of
a
larger
pattern
recognition
system.
It
has
been
implemented
using
a
perceptron
network
whose
connection
weights
were
trained
with
back
propagation
(supervised
learning).
===
Convolutional
===
A
convolutional
neural
network
(CNN,
or
ConvNet
or
shift
invariant
or
space
invariant)
is
a
class
of
deep
network,
composed
of
one
or
more
convolutional
layers
with
fully
connected
layers
(matching
those
in
typical
ANNs)
on
top.
It
uses
tied
weights
and
pooling
layers.
In
particular,
max-pooling.
It
is
often
structured
via
Fukushima's
convolutional
architecture.
They
are
variations
of
multilayer
perceptrons
that
use
minimal
preprocessing.
This
architecture
allows
CNNs
to
take
advantage
of
the
2D
structure
of
input
data.
Its
unit
connectivity
pattern
is
inspired
by
the
organization
of
the
visual
cortex.
Units
respond
to
stimuli
in
a
restricted
region
of
space
known
as
the
receptive
field.
Receptive
fields
partially
overlap,
over-covering
the
entire
visual
field.
Unit
response
can
be
approximated
mathematically
by
a
convolution
operation.CNNs
are
suitable
for
processing
visual
and
other
two-dimensional
data.
They
have
shown
superior
results
in
both
image
and
speech
applications.
They
can
be
trained
with
standard
backpropagation.
CNNs
are
easier
to
train
than
other
regular,
deep,
feed-forward
neural
networks
and
have
many
fewer
parameters
to
estimate.Capsule
Neural
Networks
(CapsNet)
add
structures
called
capsules
to
a
CNN
and
reuse
output
from
several
capsules
to
form
more
stable
(with
respect
to
various
perturbations)
representations.Examples
of
applications
in
computer
vision
include
DeepDream
and
robot
navigation.
They
have
wide
applications
in
image
and
video
recognition,
recommender
systems
and
natural
language
processing.
===
Deep
stacking
network
===
A
deep
stacking
network
(DSN)
(deep
convex
network)
is
based
on
a
hierarchy
of
blocks
of
simplified
neural
network
modules.
It
was
introduced
in
2011
by
Deng
and
Yu.
It
formulates
the
learning
as
a
convex
optimization
problem
with
a
closed-form
solution,
emphasizing
the
mechanism's
similarity
to
stacked
generalization.
Each
DSN
block
is
a
simple
module
that
is
easy
to
train
by
itself
in
a
supervised
fashion
without
backpropagation
for
the
entire
blocks.Each
block
consists
of
a
simplified
multi-layer
perceptron
(MLP)
with
a
single
hidden
layer.
The
hidden
layer
h
has
logistic
sigmoidal
units,
and
the
output
layer
has
linear
units.
Connections
between
these
layers
are
represented
by
weight
matrix
U;
input-to-hidden-layer
connections
have
weight
matrix
W.
Target
vectors
t
form
the
columns
of
matrix
T,
and
the
input
data
vectors
x
form
the
columns
of
matrix
X.
The
matrix
of
hidden
units
is
H=σ(WTX){\displaystyle
{\boldsymbol
{H}}=\sigma
({\boldsymbol
{W}}^{T}{\boldsymbol
{X}})}.
Modules
are
trained
in
order,
so
lower-layer
weights
W
are
known
at
each
stage.
The
function
performs
the
element-wise
logistic
sigmoid
operation.
Each
block
estimates
the
same
final
label
class
y,
and
its
estimate
is
concatenated
with
original
input
X
to
form
the
expanded
input
for
the
next
block.
Thus,
the
input
to
the
first
block
contains
the
original
data
only,
while
downstream
blocks'
input
adds
the
output
of
preceding
blocks.
Then
learning
the
upper-layer
weight
matrix
U
given
other
weights
in
the
network
can
be
formulated
as
a
convex
optimization
problem:
minUTf=‖UTH−T‖F2,{\displaystyle
\min
_{U^{T}}f=\|{\boldsymbol
{U}}^{T}{\boldsymbol
{H}}-{\boldsymbol
{T}}\|_{F}^{2},}which
has
a
closed-form
solution.Unlike
other
deep
architectures,
such
as
DBNs,
the
goal
is
not
to
discover
the
transformed
feature
representation.
The
structure
of
the
hierarchy
of
this
kind
of
architecture
makes
parallel
learning
straightforward,
as
a
batch-mode
optimization
problem.
In
purely
discriminative
tasks,
DSNs
outperform
conventional
DBNs.
====
Tensor
deep
stacking
networks
====
This
architecture
is
a
DSN
extension.
It
offers
two
important
improvements:
it
uses
higher-order
information
from
covariance
statistics,
and
it
transforms
the
non-convex
problem
of
a
lower-layer
to
a
convex
sub-problem
of
an
upper-layer.
TDSNs
use
covariance
statistics
in
a
bilinear
mapping
from
each
of
two
distinct
sets
of
hidden
units
in
the
same
layer
to
predictions,
via
a
third-order
tensor.
While
parallelization
and
scalability
are
not
considered
seriously
in
conventional
DNNs,
all
learning
for
DSNs
and
TDSNs
is
done
in
batch
mode,
to
allow
parallelization.
Parallelization
allows
scaling
the
design
to
larger
(deeper)
architectures
and
data
sets.
The
basic
architecture
is
suitable
for
diverse
tasks
such
as
classification
and
regression.
==
Regulatory
feedback
==
Regulatory
feedback
networks
started
as
a
model
to
explain
brain
phenomena
found
during
recognition
including
network-wide
bursting
and
difficulty
with
similarity
found
universally
in
sensory
recognition.
A
mechanism
to
perform
optimization
during
recognition
is
created
using
inhibitory
feedback
connections
back
to
the
same
inputs
that
activate
them.
This
reduces
requirements
during
learning
and
allows
learning
and
updating
to
be
easier
while
still
being
able
to
perform
complex
recognition.
A
regulatory
feedback
network
makes
inferences
using
negative
feedback.
The
feedback
is
used
to
find
the
optimal
activation
of
units.
It
is
most
similar
to
a
non-parametric
method
but
is
different
from
K-nearest
neighbor
in
that
it
mathematically
emulates
feedforward
networks.
==
Radial
basis
function
(RBF)
==
Radial
basis
functions
are
functions
that
have
a
distance
criterion
with
respect
to
a
center.
Radial
basis
functions
have
been
applied
as
a
replacement
for
the
sigmoidal
hidden
layer
transfer
characteristic
in
multi-layer
perceptrons.
RBF
networks
have
two
layers:
In
the
first,
input
is
mapped
onto
each
RBF
in
the
'hidden'
layer.
The
RBF
chosen
is
usually
a
Gaussian.
In
regression
problems
the
output
layer
is
a
linear
combination
of
hidden
layer
values
representing
mean
predicted
output.
The
interpretation
of
this
output
layer
value
is
the
same
as
a
regression
model
in
statistics.
In
classification
problems
the
output
layer
is
typically
a
sigmoid
function
of
a
linear
combination
of
hidden
layer
values,
representing
a
posterior
probability.
Performance
in
both
cases
is
often
improved
by
shrinkage
techniques,
known
as
ridge
regression
in
classical
statistics.
This
corresponds
to
a
prior
belief
in
small
parameter
values
(and
therefore
smooth
output
functions)
in
a
Bayesian
framework.
RBF
networks
have
the
advantage
of
avoiding
local
minima
in
the
same
way
as
multi-layer
perceptrons.
This
is
because
the
only
parameters
that
are
adjusted
in
the
learning
process
are
the
linear
mapping
from
hidden
layer
to
output
layer.
Linearity
ensures
that
the
error
surface
is
quadratic
and
therefore
has
a
single
easily
found
minimum.
In
regression
problems
this
can
be
found
in
one
matrix
operation.
In
classification
problems
the
fixed
non-linearity
introduced
by
the
sigmoid
output
function
is
most
efficiently
dealt
with
using
iteratively
re-weighted
least
squares.
RBF
networks
have
the
disadvantage
of
requiring
good
coverage
of
the
input
space
by
radial
basis
functions.
RBF
centres
are
determined
with
reference
to
the
distribution
of
the
input
data,
but
without
reference
to
the
prediction
task.
As
a
result,
representational
resources
may
be
wasted
on
areas
of
the
input
space
that
are
irrelevant
to
the
task.
A
common
solution
is
to
associate
each
data
point
with
its
own
centre,
although
this
can
expand
the
linear
system
to
be
solved
in
the
final
layer
and
requires
shrinkage
techniques
to
avoid
overfitting.
Associating
each
input
datum
with
an
RBF
leads
naturally
to
kernel
methods
such
as
support
vector
machines
(SVM)
and
Gaussian
processes
(the
RBF
is
the
kernel
function).
All
three
approaches
use
a
non-linear
kernel
function
to
project
the
input
data
into
a
space
where
the
learning
problem
can
be
solved
using
a
linear
model.
Like
Gaussian
processes,
and
unlike
SVMs,
RBF
networks
are
typically
trained
in
a
maximum
likelihood
framework
by
maximizing
the
probability
(minimizing
the
error).
SVMs
avoid
overfitting
by
maximizing
instead
a
margin.
SVMs
outperform
RBF
networks
in
most
classification
applications.
In
regression
applications
they
can
be
competitive
when
the
dimensionality
of
the
input
space
is
relatively
small.
===
How
RBF
networks
work
===
RBF
neural
networks
are
conceptually
similar
to
K-Nearest
Neighbor
(k-NN)
models.
The
basic
idea
is
that
similar
inputs
produce
similar
outputs.
Assume
that
each
case
in
a
training
set
has
two
predictor
variables,
x
and
y,
and
the
target
variable
has
two
categories,
positive
and
negative.
Given
a
new
case
with
predictor
values
x=6,
y=5.1,
how
is
the
target
variable
computed?
The
nearest
neighbor
classification
performed
for
this
example
depends
on
how
many
neighboring
points
are
considered.
If
1-NN
is
used
and
the
closest
point
is
negative,
then
the
new
point
should
be
classified
as
negative.
Alternatively,
if
9-NN
classification
is
used
and
the
closest
9
points
are
considered,
then
the
effect
of
the
surrounding
8
positive
points
may
outweigh
the
closest
9-th
(negative)
point.
An
RBF
network
positions
neurons
in
the
space
described
by
the
predictor
variables
(x,y
in
this
example).
This
space
has
as
many
dimensions
as
predictor
variables.
The
Euclidean
distance
is
computed
from
the
new
point
to
the
center
of
each
neuron,
and
a
radial
basis
function
(RBF,
also
called
a
kernel
function)
is
applied
to
the
distance
to
compute
the
weight
(influence)
for
each
neuron.
The
radial
basis
function
is
so
named
because
the
radius
distance
is
the
argument
to
the
function.
Weight
=
RBF(distance)
====
Radial
Basis
Function
====
The
value
for
the
new
point
is
found
by
summing
the
output
values
of
the
RBF
functions
multiplied
by
weights
computed
for
each
neuron.
The
radial
basis
function
for
a
neuron
has
a
center
and
a
radius
(also
called
a
spread).
The
radius
may
be
different
for
each
neuron,
and,
in
RBF
networks
generated
by
DTREG,
the
radius
may
be
different
in
each
dimension.
With
larger
spread,
neurons
at
a
distance
from
a
point
have
a
greater
influence.
====
Architecture
====
RBF
networks
have
three
layers:
Input
layer:
One
neuron
appears
in
the
input
layer
for
each
predictor
variable.
In
the
case
of
categorical
variables,
N-1
neurons
are
used
where
N
is
the
number
of
categories.
The
input
neurons
standardizes
the
value
ranges
by
subtracting
the
median
and
dividing
by
the
interquartile
range.
The
input
neurons
then
feed
the
values
to
each
of
the
neurons
in
the
hidden
layer.
Hidden
layer:
This
layer
has
a
variable
number
of
neurons
(determined
by
the
training
process).
Each
neuron
consists
of
a
radial
basis
function
centered
on
a
point
with
as
many
dimensions
as
predictor
variables.
The
spread
(radius)
of
the
RBF
function
may
be
different
for
each
dimension.
The
centers
and
spreads
are
determined
by
training.
When
presented
with
the
x
vector
of
input
values
from
the
input
layer,
a
hidden
neuron
computes
the
Euclidean
distance
of
the
test
case
from
the
neuron's
center
point
and
then
applies
the
RBF
kernel
function
to
this
distance
using
the
spread
values.
The
resulting
value
is
passed
to
the
summation
layer.
Summation
layer:
The
value
coming
out
of
a
neuron
in
the
hidden
layer
is
multiplied
by
a
weight
associated
with
the
neuron
and
adds
to
the
weighted
values
of
other
neurons.
This
sum
becomes
the
output.
For
classification
problems,
one
output
is
produced
(with
a
separate
set
of
weights
and
summation
unit)
for
each
target
category.
The
value
output
for
a
category
is
the
probability
that
the
case
being
evaluated
has
that
category.
====
Training
====
The
following
parameters
are
determined
by
the
training
process:
The
number
of
neurons
in
the
hidden
layer
The
coordinates
of
the
center
of
each
hidden-layer
RBF
function
The
radius
(spread)
of
each
RBF
function
in
each
dimension
The
weights
applied
to
the
RBF
function
outputs
as
they
pass
to
the
summation
layerVarious
methods
have
been
used
to
train
RBF
networks.
One
approach
first
uses
K-means
clustering
to
find
cluster
centers
which
are
then
used
as
the
centers
for
the
RBF
functions.
However,
K-means
clustering
is
computationally
intensive
and
it
often
does
not
generate
the
optimal
number
of
centers.
Another
approach
is
to
use
a
random
subset
of
the
training
points
as
the
centers.
DTREG
uses
a
training
algorithm
that
uses
an
evolutionary
approach
to
determine
the
optimal
center
points
and
spreads
for
each
neuron.
It
determines
when
to
stop
adding
neurons
to
the
network
by
monitoring
the
estimated
leave-one-out
(LOO)
error
and
terminating
when
the
LOO
error
begins
to
increase
because
of
overfitting.
The
computation
of
the
optimal
weights
between
the
neurons
in
the
hidden
layer
and
the
summation
layer
is
done
using
ridge
regression.
An
iterative
procedure
computes
the
optimal
regularization
Lambda
parameter
that
minimizes
the
generalized
cross-validation
(GCV)
error.
===
General
regression
neural
network
===
A
GRNN
is
an
associative
memory
neural
network
that
is
similar
to
the
probabilistic
neural
network
but
it
is
used
for
regression
and
approximation
rather
than
classification.
==
Deep
belief
network
==
A
deep
belief
network
(DBN)
is
a
probabilistic,
generative
model
made
up
of
multiple
hidden
layers.
It
can
be
considered
a
composition
of
simple
learning
modules.A
DBN
can
be
used
to
generatively
pre-train
a
deep
neural
network
(DNN)
by
using
the
learned
DBN
weights
as
the
initial
DNN
weights.
Various
discriminative
algorithms
can
then
tune
these
weights.
This
is
particularly
helpful
when
training
data
are
limited,
because
poorly
initialized
weights
can
significantly
hinder
learning.
These
pre-trained
weights
end
up
in
a
region
of
the
weight
space
that
is
closer
to
the
optimal
weights
than
random
choices.
This
allows
for
both
improved
modeling
and
faster
ultimate
convergence.
==
Recurrent
neural
network
==
Recurrent
neural
networks
(RNN)
propagate
data
forward,
but
also
backwards,
from
later
processing
stages
to
earlier
stages.
RNN
can
be
used
as
general
sequence
processors.
===
Fully
recurrent
===
This
architecture
was
developed
in
the
1980s.
Its
network
creates
a
directed
connection
between
every
pair
of
units.
Each
has
a
time-varying,
real-valued
(more
than
just
zero
or
one)
activation
(output).
Each
connection
has
a
modifiable
real-valued
weight.
Some
of
the
nodes
are
called
labeled
nodes,
some
output
nodes,
the
rest
hidden
nodes.
For
supervised
learning
in
discrete
time
settings,
training
sequences
of
real-valued
input
vectors
become
sequences
of
activations
of
the
input
nodes,
one
input
vector
at
a
time.
At
each
time
step,
each
non-input
unit
computes
its
current
activation
as
a
nonlinear
function
of
the
weighted
sum
of
the
activations
of
all
units
from
which
it
receives
connections.
The
system
can
explicitly
activate
(independent
of
incoming
signals)
some
output
units
at
certain
time
steps.
For
example,
if
the
input
sequence
is
a
speech
signal
corresponding
to
a
spoken
digit,
the
final
target
output
at
the
end
of
the
sequence
may
be
a
label
classifying
the
digit.
For
each
sequence,
its
error
is
the
sum
of
the
deviations
of
all
activations
computed
by
the
network
from
the
corresponding
target
signals.
For
a
training
set
of
numerous
sequences,
the
total
error
is
the
sum
of
the
errors
of
all
individual
sequences.
To
minimize
total
error,
gradient
descent
can
be
used
to
change
each
weight
in
proportion
to
its
derivative
with
respect
to
the
error,
provided
the
non-linear
activation
functions
are
differentiable.
The
standard
method
is
called
"backpropagation
through
time"
or
BPTT,
a
generalization
of
back-propagation
for
feedforward
networks.
A
more
computationally
expensive
online
variant
is
called
"Real-Time
Recurrent
Learning"
or
RTRL.
Unlike
BPTT
this
algorithm
is
local
in
time
but
not
local
in
space.
An
online
hybrid
between
BPTT
and
RTRL
with
intermediate
complexity
exists,
with
variants
for
continuous
time.
A
major
problem
with
gradient
descent
for
standard
RNN
architectures
is
that
error
gradients
vanish
exponentially
quickly
with
the
size
of
the
time
lag
between
important
events.
The
Long
short-term
memory
architecture
overcomes
these
problems.In
reinforcement
learning
settings,
no
teacher
provides
target
signals.
Instead
a
fitness
function
or
reward
function
or
utility
function
is
occasionally
used
to
evaluate
performance,
which
influences
its
input
stream
through
output
units
connected
to
actuators
that
affect
the
environment.
Variants
of
evolutionary
computation
are
often
used
to
optimize
the
weight
matrix.
====
Hopfield
====
The
Hopfield
network
(like
similar
attractor-based
networks)
is
of
historic
interest
although
it
is
not
a
general
RNN,
as
it
is
not
designed
to
process
sequences
of
patterns.
Instead
it
requires
stationary
inputs.
It
is
an
RNN
in
which
all
connections
are
symmetric.
It
guarantees
that
it
will
converge.
If
the
connections
are
trained
using
Hebbian
learning
the
Hopfield
network
can
perform
as
robust
content-addressable
memory,
resistant
to
connection
alteration.
====
Boltzmann
machine
====
The
Boltzmann
machine
can
be
thought
of
as
a
noisy
Hopfield
network.
It
is
one
of
the
first
neural
networks
to
demonstrate
learning
of
latent
variables
(hidden
units).
Boltzmann
machine
learning
was
at
first
slow
to
simulate,
but
the
contrastive
divergence
algorithm
speeds
up
training
for
Boltzmann
machines
and
Products
of
Experts.
====
Self-organizing
map
====
The
self-organizing
map
(SOM)
uses
unsupervised
learning.
A
set
of
neurons
learn
to
map
points
in
an
input
space
to
coordinates
in
an
output
space.
The
input
space
can
have
different
dimensions
and
topology
from
the
output
space,
and
SOM
attempts
to
preserve
these.
====
Learning
vector
quantization
====
Learning
vector
quantization
(LVQ)
can
be
interpreted
as
a
neural
network
architecture.
Prototypical
representatives
of
the
classes
parameterize,
together
with
an
appropriate
distance
measure,
in
a
distance-based
classification
scheme.
===
Simple
recurrent
===
Simple
recurrent
networks
have
three
layers,
with
the
addition
of
a
set
of
"context
units"
in
the
input
layer.
These
units
connect
from
the
hidden
layer
or
the
output
layer
with
a
fixed
weight
of
one.
At
each
time
step,
the
input
is
propagated
in
a
standard
feedforward
fashion,
and
then
a
backpropagation-like
learning
rule
is
applied
(not
performing
gradient
descent).
The
fixed
back
connections
leave
a
copy
of
the
previous
values
of
the
hidden
units
in
the
context
units
(since
they
propagate
over
the
connections
before
the
learning
rule
is
applied).
===
Reservoir
computing
===
Reservoir
computing
is
a
computation
framework
that
may
be
viewed
as
an
extension
of
neural
networks.
Typically
an
input
signal
is
fed
into
a
fixed
(random)
dynamical
system
called
a
reservoir
whose
dynamics
map
the
input
to
a
higher
dimension.
A
readout
mechanism
is
trained
to
map
the
reservoir
to
the
desired
output.
Training
is
performed
only
at
the
readout
stage.
Liquid-state
machines
are
a
type
of
reservoir
computing.
====
Echo
state
====
The
echo
state
network
(ESN)
employs
a
sparsely
connected
random
hidden
layer.
The
weights
of
output
neurons
are
the
only
part
of
the
network
that
are
trained.
ESN
are
good
at
reproducing
certain
time
series.
===
Long
short-term
memory
===
The
long
short-term
memory
(LSTM)
avoids
the
vanishing
gradient
problem.
It
works
even
when
with
long
delays
between
inputs
and
can
handle
signals
that
mix
low
and
high
frequency
components.
LSTM
RNN
outperformed
other
RNN
and
other
sequence
learning
methods
such
as
HMM
in
applications
such
as
language
learning
and
connected
handwriting
recognition.
===
Bi-directional
===
Bi-directional
RNN,
or
BRNN,
use
a
finite
sequence
to
predict
or
label
each
element
of
a
sequence
based
on
both
the
past
and
future
context
of
the
element.
This
is
done
by
adding
the
outputs
of
two
RNNs:
one
processing
the
sequence
from
left
to
right,
the
other
one
from
right
to
left.
The
combined
outputs
are
the
predictions
of
the
teacher-given
target
signals.
This
technique
proved
to
be
especially
useful
when
combined
with
LSTM.
===
Hierarchical
===
Hierarchical
RNN
connects
elements
in
various
ways
to
decompose
hierarchical
behavior
into
useful
subprograms.
===
Stochastic
===
A
district
from
conventional
neural
networks,
stochastic
artificial
neural
network
used
as
an
approximation
to
random
functions.
===
Genetic
Scale
===
A
RNN
(often
a
LSTM)
where
a
series
is
decomposed
into
a
number
of
scales
where
every
scale
informs
the
primary
length
between
two
consecutive
points.
A
first
order
scale
consists
of
a
normal
RNN,
a
second
order
consists
of
all
points
separated
by
two
indices
and
so
on.
The
Nth
order
RNN
connects
the
first
and
last
node.
The
outputs
from
all
the
various
scales
are
treated
as
a
Committee
of
Machines
and
the
associated
scores
are
used
genetically
for
the
next
iteration.
==
Modular
==
Biological
studies
have
shown
that
the
human
brain
operates
as
a
collection
of
small
networks.
This
realization
gave
birth
to
the
concept
of
modular
neural
networks,
in
which
several
small
networks
cooperate
or
compete
to
solve
problems.
===
Committee
of
machines
===
A
committee
of
machines
(CoM)
is
a
collection
of
different
neural
networks
that
together
"vote"
on
a
given
example.
This
generally
gives
a
much
better
result
than
individual
networks.
Because
neural
networks
suffer
from
local
minima,
starting
with
the
same
architecture
and
training
but
using
randomly
different
initial
weights
often
gives
vastly
different
results.
A
CoM
tends
to
stabilize
the
result.
The
CoM
is
similar
to
the
general
machine
learning
bagging
method,
except
that
the
necessary
variety
of
machines
in
the
committee
is
obtained
by
training
from
different
starting
weights
rather
than
training
on
different
randomly
selected
subsets
of
the
training
data.
===
Associative
===
The
associative
neural
network
(ASNN)
is
an
extension
of
committee
of
machines
that
combines
multiple
feedforward
neural
networks
and
the
k-nearest
neighbor
technique.
It
uses
the
correlation
between
ensemble
responses
as
a
measure
of
distance
amid
the
analyzed
cases
for
the
kNN.
This
corrects
the
Bias
of
the
neural
network
ensemble.
An
associative
neural
network
has
a
memory
that
can
coincide
with
the
training
set.
If
new
data
become
available,
the
network
instantly
improves
its
predictive
ability
and
provides
data
approximation
(self-learns)
without
retraining.
Another
important
feature
of
ASNN
is
the
possibility
to
interpret
neural
network
results
by
analysis
of
correlations
between
data
cases
in
the
space
of
models.
==
Physical
==
A
physical
neural
network
includes
electrically
adjustable
resistance
material
to
simulate
artificial
synapses.
Examples
include
the
ADALINE
memristor-based
neural
network.
An
optical
neural
network
is
a
physical
implementation
of
an
artificial
neural
network
with
optical
components.
==
Dynamic
==
Dynamic
neural
networks
address
nonlinear
multivariate
behaviour
and
include
(learning
of)
time-dependent
behaviour,
such
as
transient
phenomena
and
delay
effects.
Techniques
to
estimate
a
system
process
from
observed
data
fall
under
the
general
category
of
system
identification.
===
Cascading
===
Cascade
correlation
is
an
architecture
and
supervised
learning
algorithm.
Instead
of
just
adjusting
the
weights
in
a
network
of
fixed
topology,
Cascade-Correlation
begins
with
a
minimal
network,
then
automatically
trains
and
adds
new
hidden
units
one
by
one,
creating
a
multi-layer
structure.
Once
a
new
hidden
unit
has
been
added
to
the
network,
its
input-side
weights
are
frozen.
This
unit
then
becomes
a
permanent
feature-detector
in
the
network,
available
for
producing
outputs
or
for
creating
other,
more
complex
feature
detectors.
The
Cascade-Correlation
architecture
has
several
advantages:
It
learns
quickly,
determines
its
own
size
and
topology,
retains
the
structures
it
has
built
even
if
the
training
set
changes
and
requires
no
backpropagation.
===
Neuro-fuzzy
===
A
neuro-fuzzy
network
is
a
fuzzy
inference
system
in
the
body
of
an
artificial
neural
network.
Depending
on
the
FIS
type,
several
layers
simulate
the
processes
involved
in
a
fuzzy
inference-like
fuzzification,
inference,
aggregation
and
defuzzification.
Embedding
an
FIS
in
a
general
structure
of
an
ANN
has
the
benefit
of
using
available
ANN
training
methods
to
find
the
parameters
of
a
fuzzy
system.
===
Compositional
pattern-producing
===
Compositional
pattern-producing
networks
(CPPNs)
are
a
variation
of
artificial
neural
networks
which
differ
in
their
set
of
activation
functions
and
how
they
are
applied.
While
typical
artificial
neural
networks
often
contain
only
sigmoid
functions
(and
sometimes
Gaussian
functions),
CPPNs
can
include
both
types
of
functions
and
many
others.
Furthermore,
unlike
typical
artificial
neural
networks,
CPPNs
are
applied
across
the
entire
space
of
possible
inputs
so
that
they
can
represent
a
complete
image.
Since
they
are
compositions
of
functions,
CPPNs
in
effect
encode
images
at
infinite
resolution
and
can
be
sampled
for
a
particular
display
at
whatever
resolution
is
optimal.
==
Memory
networks
==
Memory
networks
incorporate
long-term
memory.
The
long-term
memory
can
be
read
and
written
to,
with
the
goal
of
using
it
for
prediction.
These
models
have
been
applied
in
the
context
of
question
answering
(QA)
where
the
long-term
memory
effectively
acts
as
a
(dynamic)
knowledge
base
and
the
output
is
a
textual
response.In
sparse
distributed
memory
or
hierarchical
temporal
memory,
the
patterns
encoded
by
neural
networks
are
used
as
addresses
for
content-addressable
memory,
with
"neurons"
essentially
serving
as
address
encoders
and
decoders.
However,
the
early
controllers
of
such
memories
were
not
differentiable.
===
One-shot
associative
memory
===
This
type
of
network
can
add
new
patterns
without
re-training.
It
is
done
by
creating
a
specific
memory
structure,
which
assigns
each
new
pattern
to
an
orthogonal
plane
using
adjacently
connected
hierarchical
arrays.
The
network
offers
real-time
pattern
recognition
and
high
scalability;
this
requires
parallel
processing
and
is
thus
best
suited
for
platforms
such
as
wireless
sensor
networks,
grid
computing,
and
GPGPUs.
===
Hierarchical
temporal
memory
===
Hierarchical
temporal
memory
(HTM)
models
some
of
the
structural
and
algorithmic
properties
of
the
neocortex.
HTM
is
a
biomimetic
model
based
on
memory-prediction
theory.
HTM
is
a
method
for
discovering
and
inferring
the
high-level
causes
of
observed
input
patterns
and
sequences,
thus
building
an
increasingly
complex
model
of
the
world.
HTM
combines
existing
ideas
to
mimic
the
neocortex
with
a
simple
design
that
provides
many
capabilities.
HTM
combines
and
extends
approaches
used
in
Bayesian
networks,
spatial
and
temporal
clustering
algorithms,
while
using
a
tree-shaped
hierarchy
of
nodes
that
is
common
in
neural
networks.
===
Holographic
associative
memory
===
Holographic
Associative
Memory
(HAM)
is
an
analog,
correlation-based,
associative,
stimulus-response
system.
Information
is
mapped
onto
the
phase
orientation
of
complex
numbers.
The
memory
is
effective
for
associative
memory
tasks,
generalization
and
pattern
recognition
with
changeable
attention.
Dynamic
search
localization
is
central
to
biological
memory.
In
visual
perception,
humans
focus
on
specific
objects
in
a
pattern.
Humans
can
change
focus
from
object
to
object
without
learning.
HAM
can
mimic
this
ability
by
creating
explicit
representations
for
focus.
It
uses
a
bi-modal
representation
of
pattern
and
a
hologram-like
complex
spherical
weight
state-space.
HAMs
are
useful
for
optical
realization
because
the
underlying
hyper-spherical
computations
can
be
implemented
with
optical
computation.
===
LSTM-related
differentiable
memory
structures
===
Apart
from
long
short-term
memory
(LSTM),
other
approaches
also
added
differentiable
memory
to
recurrent
functions.
For
example:
Differentiable
push
and
pop
actions
for
alternative
memory
networks
called
neural
stack
machines
Memory
networks
where
the
control
network's
external
differentiable
storage
is
in
the
fast
weights
of
another
network
LSTM
forget
gates
Self-referential
RNNs
with
special
output
units
for
addressing
and
rapidly
manipulating
the
RNN's
own
weights
in
differentiable
fashion
(internal
storage)
Learning
to
transduce
with
unbounded
memory
===
Neural
Turing
machines
===
Neural
Turing
machines
(NTM)
couple
LSTM
networks
to
external
memory
resources,
with
which
they
can
interact
by
attentional
processes.
The
combined
system
is
analogous
to
a
Turing
machine
but
is
differentiable
end-to-end,
allowing
it
to
be
efficiently
trained
by
gradient
descent.
Preliminary
results
demonstrate
that
neural
Turing
machines
can
infer
simple
algorithms
such
as
copying,
sorting
and
associative
recall
from
input
and
output
examples.
Differentiable
neural
computers
(DNC)
are
an
NTM
extension.
They
out-performed
Neural
turing
machines,
long
short-term
memory
systems
and
memory
networks
on
sequence-processing
tasks.
===
Semantic
hashing
===
Approaches
that
represent
previous
experiences
directly
and
use
a
similar
experience
to
form
a
local
model
are
often
called
nearest
neighbour
or
k-nearest
neighbors
methods.
Deep
learning
is
useful
in
semantic
hashing
where
a
deep
graphical
model
the
word-count
vectors
obtained
from
a
large
set
of
documents.
Documents
are
mapped
to
memory
addresses
in
such
a
way
that
semantically
similar
documents
are
located
at
nearby
addresses.
Documents
similar
to
a
query
document
can
then
be
found
by
accessing
all
the
addresses
that
differ
by
only
a
few
bits
from
the
address
of
the
query
document.
Unlike
sparse
distributed
memory
that
operates
on
1000-bit
addresses,
semantic
hashing
works
on
32
or
64-bit
addresses
found
in
a
conventional
computer
architecture.
===
Pointer
networks
===
Deep
neural
networks
can
be
potentially
improved
by
deepening
and
parameter
reduction,
while
maintaining
trainability.
While
training
extremely
deep
(e.g.,
1
million
layers)
neural
networks
might
not
be
practical,
CPU-like
architectures
such
as
pointer
networks
and
neural
random-access
machines
overcome
this
limitation
by
using
external
random-access
memory
and
other
components
that
typically
belong
to
a
computer
architecture
such
as
registers,
ALU
and
pointers.
Such
systems
operate
on
probability
distribution
vectors
stored
in
memory
cells
and
registers.
Thus,
the
model
is
fully
differentiable
and
trains
end-to-end.
The
key
characteristic
of
these
models
is
that
their
depth,
the
size
of
their
short-term
memory,
and
the
number
of
parameters
can
be
altered
independently.
==
Hybrids
==
===
Encoder–decoder
networks
===
Encoder–decoder
frameworks
are
based
on
neural
networks
that
map
highly
structured
input
to
highly
structured
output.
The
approach
arose
in
the
context
of
machine
translation,
where
the
input
and
output
are
written
sentences
in
two
natural
languages.
In
that
work,
an
LSTM
RNN
or
CNN
was
used
as
an
encoder
to
summarize
a
source
sentence,
and
the
summary
was
decoded
using
a
conditional
RNN
language
model
to
produce
the
translation.
These
systems
share
building
blocks:
gated
RNNs
and
CNNs
and
trained
attention
mechanisms.
==
Other
types
==
===
Instantaneously
trained
===
Instantaneously
trained
neural
networks
(ITNN)
were
inspired
by
the
phenomenon
of
short-term
learning
that
seems
to
occur
instantaneously.
In
these
networks
the
weights
of
the
hidden
and
the
output
layers
are
mapped
directly
from
the
training
vector
data.
Ordinarily,
they
work
on
binary
data,
but
versions
for
continuous
data
that
require
small
additional
processing
exist.
===
Spiking
===
Spiking
neural
networks
(SNN)
explicitly
consider
the
timing
of
inputs.
The
network
input
and
output
are
usually
represented
as
a
series
of
spikes
(delta
function
or
more
complex
shapes).
SNN
can
process
information
in
the
time
domain
(signals
that
vary
over
time).
They
are
often
implemented
as
recurrent
networks.
SNN
are
also
a
form
of
pulse
computer.Spiking
neural
networks
with
axonal
conduction
delays
exhibit
polychronization,
and
hence
could
have
a
very
large
memory
capacity.SNN
and
the
temporal
correlations
of
neural
assemblies
in
such
networks—have
been
used
to
model
figure/ground
separation
and
region
linking
in
the
visual
system.
===
Spatial
===
Spatial
neural
networks
(SNNs)
constitute
a
supercategory
of
tailored
neural
networks
(NNs)
for
representing
and
predicting
geographic
phenomena.
They
generally
improve
both
the
statistical
accuracy
and
reliability
of
the
a-spatial/classic
NNs
whenever
they
handle
geo-spatial
datasets,
and
also
of
the
other
spatial
(statistical)
models
(e.g.
spatial
regression
models)
whenever
the
geo-spatial
datasets'
variables
depict
non-linear
relations.
Examples
of
SNNs
are
the
OSFA
spatial
neural
networks,
SVANNs
and
GWNNs.
===
Neocognitron
===
The
neocognitron
is
a
hierarchical,
multilayered
network
that
was
modeled
after
the
visual
cortex.
It
uses
multiple
types
of
units,
(originally
two,
called
simple
and
complex
cells),
as
a
cascading
model
for
use
in
pattern
recognition
tasks.
Local
features
are
extracted
by
S-cells
whose
deformation
is
tolerated
by
C-cells.
Local
features
in
the
input
are
integrated
gradually
and
classified
at
higher
layers.
Among
the
various
kinds
of
neocognitron
are
systems
that
can
detect
multiple
patterns
in
the
same
input
by
using
back
propagation
to
achieve
selective
attention.
It
has
been
used
for
pattern
recognition
tasks
and
inspired
convolutional
neural
networks.
===
Compound
hierarchical-deep
models
===
Compound
hierarchical-deep
models
compose
deep
networks
with
non-parametric
Bayesian
models.
Features
can
be
learned
using
deep
architectures
such
as
DBNs,
deep
Boltzmann
machines
(DBM),
deep
auto
encoders,
convolutional
variants,
ssRBMs,
deep
coding
networks,
DBNs
with
sparse
feature
learning,
RNNs,
conditional
DBNs,
denoising
autoencoders.
This
provides
a
better
representation,
allowing
faster
learning
and
more
accurate
classification
with
high-dimensional
data.
However,
these
architectures
are
poor
at
learning
novel
classes
with
few
examples,
because
all
network
units
are
involved
in
representing
the
input
(a
distributed
representation)
and
must
be
adjusted
together
(high
degree
of
freedom).
Limiting
the
degree
of
freedom
reduces
the
number
of
parameters
to
learn,
facilitating
learning
of
new
classes
from
few
examples.
Hierarchical
Bayesian
(HB)
models
allow
learning
from
few
examples,
for
example
for
computer
vision,
statistics
and
cognitive
science.
Compound
HD
architectures
aim
to
integrate
characteristics
of
both
HB
and
deep
networks.
The
compound
HDP-DBM
architecture
is
a
hierarchical
Dirichlet
process
(HDP)
as
a
hierarchical
model,
incorporating
DBM
architecture.
It
is
a
full
generative
model,
generalized
from
abstract
concepts
flowing
through
the
model
layers,
which
is
able
to
synthesize
new
examples
in
novel
classes
that
look
"reasonably"
natural.
All
the
levels
are
learned
jointly
by
maximizing
a
joint
log-probability
score.In
a
DBM
with
three
hidden
layers,
the
probability
of
a
visible
input
''ν''
is:
p(ν,ψ)=1Z∑hexp⁡(∑ijWij(1)νihj1+∑jℓWjℓ(2)hj1hℓ2+∑ℓmWℓm(3)hℓ2hm3),{\displaystyle
p({\boldsymbol
{\nu
}},\psi
)={\frac
{1}{Z}}\sum
_{h}\exp
\left(\sum
_{ij}W_{ij}^{(1)}\nu
_{i}h_{j}^{1}+\sum
_{j\ell
}W_{j\ell
}^{(2)}h_{j}^{1}h_{\ell
}^{2}+\sum
_{\ell
m}W_{\ell
m}^{(3)}h_{\ell
}^{2}h_{m}^{3}\right),}where
h={h(1),h(2),h(3)}{\displaystyle
{\boldsymbol
{h}}=\{{\boldsymbol
{h}}^{(1)},{\boldsymbol
{h}}^{(2)},{\boldsymbol
{h}}^{(3)}\}}
is
the
set
of
hidden
units,
and
ψ={W(1),W(2),W(3)}{\displaystyle
\psi
=\{{\boldsymbol
{W}}^{(1)},{\boldsymbol
{W}}^{(2)},{\boldsymbol
{W}}^{(3)}\}}
are
the
model
parameters,
representing
visible-hidden
and
hidden-hidden
symmetric
interaction
terms.
A
learned
DBM
model
is
an
undirected
model
that
defines
the
joint
distribution
P(ν,h1,h2,h3){\displaystyle
P(\nu
,h^{1},h^{2},h^{3})}.
One
way
to
express
what
has
been
learned
is
the
conditional
model
P(ν,h1,h2∣h3){\displaystyle
P(\nu
,h^{1},h^{2}\mid
h^{3})}
and
a
prior
term
P(h3){\displaystyle
P(h^{3})}.
Here
P(ν,h1,h2∣h3){\displaystyle
P(\nu
,h^{1},h^{2}\mid
h^{3})}
represents
a
conditional
DBM
model,
which
can
be
viewed
as
a
two-layer
DBM
but
with
bias
terms
given
by
the
states
of
h3{\displaystyle
h^{3}}:
P(ν,h1,h2∣h3)=1Z(ψ,h3)exp⁡(∑ijWij(1)νihj1+∑jℓWjℓ(2)hj1hℓ2+∑ℓmWℓm(3)hℓ2hm3).{\displaystyle
P(\nu
,h^{1},h^{2}\mid
h^{3})={\frac
{1}{Z(\psi
,h^{3})}}\exp
\left(\sum
_{ij}W_{ij}^{(1)}\nu
_{i}h_{j}^{1}+\sum
_{j\ell
}W_{j\ell
}^{(2)}h_{j}^{1}h_{\ell
}^{2}+\sum
_{\ell
m}W_{\ell
m}^{(3)}h_{\ell
}^{2}h_{m}^{3}\right).}
===
Deep
predictive
coding
networks
===
A
deep
predictive
coding
network
(DPCN)
is
a
predictive
coding
scheme
that
uses
top-down
information
to
empirically
adjust
the
priors
needed
for
a
bottom-up
inference
procedure
by
means
of
a
deep,
locally
connected,
generative
model.
This
works
by
extracting
sparse
features
from
time-varying
observations
using
a
linear
dynamical
model.
Then,
a
pooling
strategy
is
used
to
learn
invariant
feature
representations.
These
units
compose
to
form
a
deep
architecture
and
are
trained
by
greedy
layer-wise
unsupervised
learning.
The
layers
constitute
a
kind
of
Markov
chain
such
that
the
states
at
any
layer
depend
only
on
the
preceding
and
succeeding
layers.
DPCNs
predict
the
representation
of
the
layer,
by
using
a
top-down
approach
using
the
information
in
upper
layer
and
temporal
dependencies
from
previous
states.DPCNs
can
be
extended
to
form
a
convolutional
network.
===
Multilayer
kernel
machine
===
Multilayer
kernel
machines
(MKM)
are
a
way
of
learning
highly
nonlinear
functions
by
iterative
application
of
weakly
nonlinear
kernels.
They
use
kernel
principal
component
analysis
(KPCA),
as
a
method
for
the
unsupervised
greedy
layer-wise
pre-training
step
of
deep
learning.Layer
ℓ+1{\displaystyle
\ell
+1}
learns
the
representation
of
the
previous
layer
ℓ{\displaystyle
\ell
},
extracting
the
nl{\displaystyle
n_{l}}
principal
component
(PC)
of
the
projection
layer
l{\displaystyle
l}
output
in
the
feature
domain
induced
by
the
kernel.
To
reduce
the
dimensionaliity
of
the
updated
representation
in
each
layer,
a
supervised
strategy
selects
the
best
informative
features
among
features
extracted
by
KPCA.
The
process
is:
rank
the
nℓ{\displaystyle
n_{\ell
}}
features
according
to
their
mutual
information
with
the
class
labels;
for
different
values
of
K
and
mℓ∈{1,…,nℓ}{\displaystyle
m_{\ell
}\in
\{1,\ldots
,n_{\ell
}\}},
compute
the
classification
error
rate
of
a
K-nearest
neighbor
(K-NN)
classifier
using
only
the
ml{\displaystyle
m_{l}}
most
informative
features
on
a
validation
set;
the
value
of
mℓ{\displaystyle
m_{\ell
}}
with
which
the
classifier
has
reached
the
lowest
error
rate
determines
the
number
of
features
to
retain.Some
drawbacks
accompany
the
KPCA
method
for
MKMs.
A
more
straightforward
way
to
use
kernel
machines
for
deep
learning
was
developed
for
spoken
language
understanding.
The
main
idea
is
to
use
a
kernel
machine
to
approximate
a
shallow
neural
net
with
an
infinite
number
of
hidden
units,
then
use
a
deep
stacking
network
to
splice
the
output
of
the
kernel
machine
and
the
raw
input
in
building
the
next,
higher
level
of
the
kernel
machine.
The
number
of
levels
in
the
deep
convex
network
is
a
hyper-parameter
of
the
overall
system,
to
be
determined
by
cross
validation.
==
See
also
==
==
References
==
==
Bibliography
==
Fukushima,
Kunihiko
(1987).
"A
hierarchical
neural
network
model
for
selective
attention".
In
Eckmiller,
R.;
Von
der
Malsburg,
C.
(eds.).
Neural
computers.
Springer-Verlag.
pp.
81–90.
Fukushima,
Kunihiko
(2007).
"Neocognitron".
Scholarpedia.
2
(1):
1717.
Bibcode:2007SchpJ...2.1717F.
doi:10.4249/scholarpedia.1717.
The
unified
modeling
language
(UML)
is
a
general-purpose
visual
modeling
language
that
is
intended
to
provide
a
standard
way
to
visualize
the
design
of
a
system.UML
provides
a
standard
notation
for
many
types
of
diagrams
which
can
be
roughly
divided
into
three
main
groups:
behavior
diagrams,
interaction
diagrams,
and
structure
diagrams.
The
creation
of
UML
was
originally
motivated
by
the
desire
to
standardize
the
disparate
notational
systems
and
approaches
to
software
design.
It
was
developed
at
Rational
Software
in
1994–1995,
with
further
development
led
by
them
through
1996.In
1997,
UML
was
adopted
as
a
standard
by
the
Object
Management
Group
(OMG),
and
has
been
managed
by
this
organization
ever
since.
In
2005,
UML
was
also
published
by
the
International
Organization
for
Standardization
(ISO)
and
the
International
Electrotechnical
Commission
(IEC)
as
the
ISO/IEC
19501
standard.
Since
then
the
standard
has
been
periodically
revised
to
cover
the
latest
revision
of
UML.In
software
engineering,
most
practitioners
do
not
use
UML,
but
instead
produce
informal
hand
drawn
diagrams;
these
diagrams,
however,
often
include
elements
from
UML.:
536
==
History
==
===
Before
UML
1.0
===
UML
has
been
evolved
since
the
second
half
of
the
1990s
and
has
its
roots
in
the
object-oriented
programming
methods
developed
in
the
late
1980s
and
early
1990s.
The
timeline
(see
image)
shows
the
highlights
of
the
history
of
object-oriented
modeling
methods
and
notation.
It
is
originally
based
on
the
notations
of
the
Booch
method,
the
object-modeling
technique
(OMT)
and
object-oriented
software
engineering
(OOSE),
which
it
has
integrated
into
a
single
language.Rational
Software
Corporation
hired
James
Rumbaugh
from
General
Electric
in
1994
and
after
that
the
company
became
the
source
for
two
of
the
most
popular
object-oriented
modeling
approaches
of
the
day:
Rumbaugh's
object-modeling
technique
(OMT)
and
Grady
Booch's
method.
They
were
soon
assisted
in
their
efforts
by
Ivar
Jacobson,
the
creator
of
the
object-oriented
software
engineering
(OOSE)
method,
who
joined
them
at
Rational
in
1995.
===
UML
1.x
===
Under
the
technical
leadership
of
those
three
(Rumbaugh,
Jacobson
and
Booch),
a
consortium
called
the
UML
Partners
was
organized
in
1996
to
complete
the
Unified
Modeling
Language
(UML)
specification,
and
propose
it
to
the
Object
Management
Group
(OMG)
for
standardization.
The
partnership
also
contained
additional
interested
parties
(for
example
HP,
DEC,
IBM
and
Microsoft).
The
UML
Partners'
UML
1.0
draft
was
proposed
to
the
OMG
in
January
1997
by
the
consortium.
During
the
same
month
the
UML
Partners
formed
a
group,
designed
to
define
the
exact
meaning
of
language
constructs,
chaired
by
Cris
Kobryn
and
administered
by
Ed
Eykholt,
to
finalize
the
specification
and
integrate
it
with
other
standardization
efforts.
The
result
of
this
work,
UML
1.1,
was
submitted
to
the
OMG
in
August
1997
and
adopted
by
the
OMG
in
November
1997.After
the
first
release
a
task
force
was
formed
to
improve
the
language,
which
released
several
minor
revisions,
1.3,
1.4,
and
1.5.The
standards
it
produced
(as
well
as
the
original
standard)
have
been
noted
as
being
ambiguous
and
inconsistent.
====
Cardinality
notation
====
As
with
database
Chen,
Bachman,
and
ISO
ER
diagrams,
class
models
are
specified
to
use
"look-across"
cardinalities,
even
though
several
authors
(Merise,
Elmasri
&
Navathe
amongst
others)
prefer
same-side
or
"look-here"
for
roles
and
both
minimum
and
maximum
cardinalities.
Recent
researchers
(Feinerer,
Dullea
et
al.)
have
shown
that
the
"look-across"
technique
used
by
UML
and
ER
diagrams
is
less
effective
and
less
coherent
when
applied
to
n-ary
relationships
of
order
strictly
greater
than
2.
Feinerer
says:
"Problems
arise
if
we
operate
under
the
look-across
semantics
as
used
for
UML
associations.
Hartmann
investigates
this
situation
and
shows
how
and
why
different
transformations
fail.",
and:
"As
we
will
see
on
the
next
few
pages,
the
look-across
interpretation
introduces
several
difficulties
which
prevent
the
extension
of
simple
mechanisms
from
binary
to
n-ary
associations."
===
UML
2
===
UML
2.0
major
revision
replaced
version
1.5
in
2005,
which
was
developed
with
an
enlarged
consortium
to
improve
the
language
further
to
reflect
new
experience
on
usage
of
its
features.Although
UML
2.1
was
never
released
as
a
formal
specification,
versions
2.1.1
and
2.1.2
appeared
in
2007,
followed
by
UML
2.2
in
February
2009.
UML
2.3
was
formally
released
in
May
2010.
UML
2.4.1
was
formally
released
in
August
2011.
UML
2.5
was
released
in
October
2012
as
an
"In
progress"
version
and
was
officially
released
in
June
2015.
Formal
version
2.5.1
was
adopted
in
December
2017.There
are
four
parts
to
the
UML
2.x
specification:
The
Superstructure
that
defines
the
notation
and
semantics
for
diagrams
and
their
model
elements
The
Infrastructure
that
defines
the
core
metamodel
on
which
the
Superstructure
is
based
The
Object
Constraint
Language
(OCL)
for
defining
rules
for
model
elements
The
UML
Diagram
Interchange
that
defines
how
UML
2
diagram
layouts
are
exchangedUntil
UML
2.4.1,
the
latest
versions
of
these
standards
were:
UML
Superstructure
version
2.4.1
UML
Infrastructure
version
2.4.1
OCL
version
2.3.1
UML
Diagram
Interchange
version
1.0.Since
version
2.5,
the
UML
Specification
has
been
simplified
(without
Superstructure
and
Infrastructure),
and
the
latest
versions
of
these
standards
are
now:
UML
Specification
2.5.1
OCL
version
2.4It
continues
to
be
updated
and
improved
by
the
revision
task
force,
who
resolve
any
issues
with
the
language.
==
Design
==
UML
offers
a
way
to
visualize
a
system's
architectural
blueprints
in
a
diagram,
including
elements
such
as:
any
activities
(jobs);
individual
components
of
the
system;
and
how
they
can
interact
with
other
software
components;
how
the
system
will
run;
how
entities
interact
with
others
(components
and
interfaces);
external
user
interface.Although
originally
intended
for
object-oriented
design
documentation,
UML
has
been
extended
to
a
larger
set
of
design
documentation
(as
listed
above),
and
been
found
useful
in
many
contexts.
===
Software
development
methods
===
UML
is
not
a
development
method
by
itself;
however,
it
was
designed
to
be
compatible
with
the
leading
object-oriented
software
development
methods
of
its
time,
for
example
OMT,
Booch
method,
Objectory
and
especially
RUP
that
it
was
originally
intended
to
be
used
with
when
work
began
at
Rational
Software.
===
Modeling
===
It
is
important
to
distinguish
between
the
UML
model
and
the
set
of
diagrams
of
a
system.
A
diagram
is
a
partial
graphic
representation
of
a
system's
model.
The
set
of
diagrams
need
not
completely
cover
the
model
and
deleting
a
diagram
does
not
change
the
model.
The
model
may
also
contain
documentation
that
drives
the
model
elements
and
diagrams
(such
as
written
use
cases).
UML
diagrams
represent
two
different
views
of
a
system
model:
Static
(or
structural)
view:
emphasizes
the
static
structure
of
the
system
using
objects,
attributes,
operations
and
relationships.
It
includes
class
diagrams
and
composite
structure
diagrams.
Dynamic
(or
behavioral)
view:
emphasizes
the
dynamic
behavior
of
the
system
by
showing
collaborations
among
objects
and
changes
to
the
internal
states
of
objects.
This
view
includes
sequence
diagrams,
activity
diagrams
and
state
machine
diagrams.UML
models
can
be
exchanged
among
UML
tools
by
using
the
XML
Metadata
Interchange
(XMI)
format.
In
UML,
one
of
the
key
tools
for
behavior
modeling
is
the
use-case
model,
caused
by
OOSE.
Use
cases
are
a
way
of
specifying
required
usages
of
a
system.
Typically,
they
are
used
to
capture
the
requirements
of
a
system,
that
is,
what
a
system
is
supposed
to
do.
==
Diagrams
==
UML
2
has
many
types
of
diagrams,
which
are
divided
into
two
categories.
Some
types
represent
structural
information,
and
the
rest
represent
general
types
of
behavior,
including
a
few
that
represent
different
aspects
of
interactions.
These
diagrams
can
be
categorized
hierarchically
as
shown
in
the
following
class
diagram:
These
diagrams
may
all
contain
comments
or
notes
explaining
usage,
constraint,
or
intent.
===
Structure
diagrams
===
Structure
diagrams
represent
the
static
aspects
of
the
system.
It
emphasizes
the
things
that
must
be
present
in
the
system
being
modeled.
Since
structure
diagrams
represent
the
structure,
they
are
used
extensively
in
documenting
the
software
architecture
of
software
systems.
For
example,
the
component
diagram
describes
how
a
software
system
is
split
up
into
components
and
shows
the
dependencies
among
these
components.
===
Behavior
diagrams
===
Behavior
diagrams
represent
the
dynamic
aspect
of
the
system.
It
emphasizes
what
must
happen
in
the
system
being
modeled.
Since
behavior
diagrams
illustrate
the
behavior
of
a
system,
they
are
used
extensively
to
describe
the
functionality
of
software
systems.
As
an
example,
the
activity
diagram
describes
the
business
and
operational
step-by-step
activities
of
the
components
in
a
system.
====
Interaction
diagrams
====
Interaction
diagrams,
a
subset
of
behavior
diagrams,
emphasize
the
flow
of
control
and
data
among
the
things
in
the
system
being
modeled.
For
example,
the
sequence
diagram
shows
how
objects
communicate
with
each
other
regarding
a
sequence
of
messages.
==
Metamodeling
==
The
Object
Management
Group
(OMG)
has
developed
a
metamodeling
architecture
to
define
the
UML,
called
the
Meta-Object
Facility.
MOF
is
designed
as
a
four-layered
architecture,
as
shown
in
the
image
at
right.
It
provides
a
meta-meta
model
at
the
top,
called
the
M3
layer.
This
M3-model
is
the
language
used
by
Meta-Object
Facility
to
build
metamodels,
called
M2-models.
The
most
prominent
example
of
a
Layer
2
Meta-Object
Facility
model
is
the
UML
metamodel,
which
describes
the
UML
itself.
These
M2-models
describe
elements
of
the
M1-layer,
and
thus
M1-models.
These
would
be,
for
example,
models
written
in
UML.
The
last
layer
is
the
M0-layer
or
data
layer.
It
is
used
to
describe
runtime
instances
of
the
system.The
meta-model
can
be
extended
using
a
mechanism
called
stereotyping.
This
has
been
criticized
as
being
insufficient/untenable
by
Brian
Henderson-Sellers
and
Cesar
Gonzalez-Perez
in
"Uses
and
Abuses
of
the
Stereotype
Mechanism
in
UML
1.x
and
2.0".
==
Adoption
==
Back
in
2013
UML
has
been
marketed
by
OMG
for
many
contexts,
but
aimed
primarily
at
software
development
with
limited
success.It
has
been
treated,
at
times,
as
a
design
silver
bullet,
which
leads
to
problems.
UML
misuse
includes
overuse
(designing
every
part
of
the
system
with
it,
which
is
unnecessary)
and
assuming
that
novices
can
design
with
it.It
is
considered
a
large
language,
with
many
constructs.
Some
people
(including
Jacobson)
feel
that
UML's
size
hinders
learning
(and
therefore
using)
it.MS
Visual
Studio
dropped
support
for
UML
in
2016
due
to
lack
of
usage.According
to
Google
Trends
UML
has
been
on
steady
decline
since
2004.
==
See
also
==
Applications
of
UML
Business
Process
Model
and
Notation
(BPMN)
C4
model
Department
of
Defense
Architecture
Framework
DOT
(graph
description
language)
List
of
Unified
Modeling
Language
tools
MODAF
Model-based
testing
Model-driven
engineering
Object-oriented
role
analysis
and
modeling
Process
Specification
Language
Systems
Modeling
Language
(SysML)
==
References
==
==
Further
reading
==
Ambler,
Scott
William
(2004).
The
Object
Primer:
Agile
Model
Driven
Development
with
UML
2.
Cambridge
University
Press.
ISBN
0-521-54018-6.
Archived
from
the
original
on
31
January
2010.
Retrieved
29
April
2006.
Chonoles,
Michael
Jesse;
James
A.
Schardt
(2003).
UML
2
for
Dummies.
Wiley
Publishing.
ISBN
0-7645-2614-6.
Fowler,
Martin
(2004).
UML
Distilled:
A
Brief
Guide
to
the
Standard
Object
Modeling
Language
(3rd
ed.).
Addison-Wesley.
ISBN
0-321-19368-7.
Jacobson,
Ivar;
Grady
Booch;
James
Rumbaugh
(1998).
The
Unified
Software
Development
Process.
Addison
Wesley
Longman.
ISBN
0-201-57169-2.
Martin,
Robert
Cecil
(2003).
UML
for
Java
Programmers.
Prentice
Hall.
ISBN
0-13-142848-9.
Noran,
Ovidiu
S.
"Business
Modelling:
UML
vs.
IDEF"
(PDF).
Retrieved
14
November
2022.
Horst
Kargl.
"Interactive
UML
Metamodel
with
additional
Examples".
Penker,
Magnus;
Hans-Erik
Eriksson
(2000).
Business
Modeling
with
UML.
John
Wiley
&
Sons.
ISBN
0-471-29551-5.
Douglass,
Bruce
Powel.
"Bruce
Douglass:
Real-Time
Agile
Systems
and
Software
Development"
(web).
Retrieved
1
January
2019.
Douglass,
Bruce
(2014).
Real-Time
UML
Workshop
2nd
Edition.
Newnes.
ISBN
978-0-471-29551-8.
Douglass,
Bruce
(2004).
Real-Time
UML
3rd
Edition.
Newnes.
ISBN
978-0321160768.
Douglass,
Bruce
(2002).
Real-Time
Design
Patterns.
Addison-Wesley
Professional.
ISBN
978-0201699562.
Douglass,
Bruce
(2009).
Real-Time
Agility.
Addison-Wesley
Professional.
ISBN
978-0321545497.
Douglass,
Bruce
(2010).
Design
Patterns
for
Embedded
Systems
in
C.
Newnes.
ISBN
978-1856177078.
==
External
links
==
Official
website
Current
Version
Specification